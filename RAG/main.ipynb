{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f024414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docs_preprocess import DocumentProcessor\n",
    "from VecStore import VectorStore\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfca5bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Any\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "class DeepSeekLLM(LLM):\n",
    "    model: Any\n",
    "    tokenizer: Any\n",
    "    max_new_tokens: int = 512\n",
    "    temperature: float = 0.0           # 默认给 RAG 用\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"deepseek_hf\"\n",
    "\n",
    "    def _call(self, prompt: str,\n",
    "              stop: Optional[List[str]] = None,\n",
    "              **kwargs) -> str:\n",
    "        # 1. 把普通 prompt 包装进官方 chat_template\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        # 2. 生成\n",
    "        outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=kwargs.get(\"max_new_tokens\", self.max_new_tokens),\n",
    "            temperature=kwargs.get(\"temperature\", self.temperature),\n",
    "        )\n",
    "        text = self.tokenizer.decode(\n",
    "            outputs[0][input_ids.shape[1]:],\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        # 3. 手动处理 stop tokens\n",
    "        if stop:\n",
    "            for s in stop:\n",
    "                if s in text:\n",
    "                    text = text.split(s)[0]\n",
    "\n",
    "        return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d399e087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyus4/anaconda3/envs/rag_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-14 13:04:12,416] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyus4/anaconda3/envs/rag_env/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/lyus4/anaconda3/envs/rag_env/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]\n",
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "model_dir = \"/home/lyus4/yuheng/All_in_LLM/deepseek-llm-7b-chat\"\n",
    "tok   = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir,\n",
    "                                             trust_remote_code=True,\n",
    "                                             torch_dtype=\"auto\",\n",
    "                                             device_map=\"auto\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb709ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3746317/973699687.py:2: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm(\"你好，请自我介绍一下。\"))\n",
      "/home/lyus4/anaconda3/envs/rag_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我是一个人工智能助手，名为DeepSeek Chat。我由中国的DeepSeek团队开发，旨在帮助用户解答问题、提供信息和执行各种任务。我能够处理多种主题，包括但不限于科学、数学、历史、文化、技术等。如果你有任何问题，欢迎随时向我提问。\n"
     ]
    }
   ],
   "source": [
    "llm = DeepSeekLLM(model=model, tokenizer=tok, max_new_tokens=512, temperature=0.2)\n",
    "print(llm(\"你好，请自我介绍一下。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "613aa40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyus4/anaconda3/envs/rag_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In wireless communication, RB stands for Resource Block. It is a basic unit of data in the GSM (Global System for Mobile communication) and UMTS (Universal Mobile Telecommunication System) standards. A Resource Block contains information about the data to be transmitted, such as the data itself, error-correction codes, and synchronization information.\n",
      "\n",
      "Resource Blocks are used to encapsulate and transmit data in the air interface of wireless communication systems. They are also used to allocate resources, such as bandwidth and power, to different users or services.\n",
      "\n",
      "In GSM, a Resource Block is typically 128 bytes in size, while in UMTS, it can be up to 64 KB. The size of a Resource Block can vary depending on the specific requirements of the communication system and the type of data being transmitted.\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"In Wireless communication, what is RB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f0e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bae25f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 路径配置\n",
    "MODEL_PATH =  \"/home/lyus4/yuheng/All_in_LLM/all-MiniLM-L6-v2\"\n",
    "INDEX_PATH = Path(\"../vector_store/faiss_ivfflat_100\")  # 无需加 .index 后缀\n",
    "VECTORS_PATH = Path(\"../index/vectors.npy\")\n",
    "DOC_JSON_PATH = Path(\"output_chunks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88c2281f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 初始化 VectorStore -> ../vector_store/faiss_ivfflat_100\n",
      "[INFO] 索引加载成功: ../vector_store/faiss_ivfflat_100\n",
      "[INFO] 类型: IndexIVFFlat, 维度: 384, 数量: 18343\n",
      "[INFO] 加载文档 18343 条\n",
      "\n",
      "[INFO] VectorStore 状态描述：\n",
      "- 文档数: 18343\n",
      "- 向量数: 0\n",
      "- 索引类型: IndexIVFFlat\n",
      "- 向量维度: 384\n",
      "- 向量总数: 18343\n"
     ]
    }
   ],
   "source": [
    "vs = VectorStore(model_path=MODEL_PATH, db_path=INDEX_PATH)\n",
    "vs.load_documents_and_metadata(json_path=DOC_JSON_PATH)\n",
    "vs.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9593c900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyus4/yuheng/All_in_LLM/LLM_wireless_communication_expert/RAG/VecStore.py:39: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.model = HuggingFaceEmbeddings(\n",
      "No sentence-transformers model found with name /home/lyus4/yuheng/All_in_LLM/all-MiniLM-L6-v2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 嵌入模型已加载\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = vs.search(\"what is OFDMA?\", k=5, score_mode=\"reciprocal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "628031df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Orthogonal frequency division multiple access (OFDMA), which is the multi-user\\nversion of OFDM that was discussed in Section 9.12. In OFDMA multiple access is\\naccomplished through the assignment of subchannels (subcarriers) to individual users.',\n",
       "  0.6585590018011712,\n",
       "  {'document': '../documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 587,\n",
       "   'chunk_idx': 5}),\n",
       " ('Chapter\\nSignaling over Fading Channels\\nNaturally, OFDMA inherits the distinctive features of OFDM. In particular, OFDMA\\nis well suited for high data-rate transmissions over delay-dispersive channels, realized\\nby exploiting the principle of divide and conquer. Accordingly, OFDMA is\\ncomputationally efficient in using the FFT algorithm. Moreover, OFDMA lends itself\\nto the combined use of MIMO, hence the ability to improve spectral efficiency and\\ntake advantage of channel flexibility.\\nCode-division multiple access (CDMA), which distinguishes itself by exploiting the\\nunderlying principle of spread spectrum signals, discussed in Section 9.13. To be\\nspecific, through the combined process of spectrum spreading in the transmitter and',\n",
       "  0.5332675930110392,\n",
       "  {'document': '../documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 588,\n",
       "   'chunk_idx': 0}),\n",
       " ('the two. This important result assumes that the receiver has knowledge of the channel\\nstate. The spectral efficiency of the MIMO system can be further enhanced by including a\\nfeedback channel from the transmitter to the receiver, whereby the channel state is also\\nmade available to the transmitter and with it the transmitter is enabled to exercise control\\nover the transmitted signal.\\nMultiple Access Considerations\\nAn issue of paramount practical importance in wireless communications is that of multiple\\naccess to the wireless channel, in the context of which the following two approaches are\\nconsidered to be the dominant ones:\\nOrthogonal frequency division multiple access (OFDMA), which is the multi-user\\nversion of OFDM that was discussed in Section 9.12. In OFDMA multiple access is',\n",
       "  0.5080075333243746,\n",
       "  {'document': '../documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 587,\n",
       "   'chunk_idx': 4}),\n",
       " ('The base class for OFDMA schedulers is NrMacSchedulerOfdma. In the downlink, such class and its subclasses\\nperform OFDMA scheduling, while in the uplink they leverage some of the subclasses of NrMacSchedulerTdma\\nclass that implements TDMA scheduling.\\nThe OFDMA scheduling in the downlink is composed of the two scheduling levels: 1) the scheduling of the symbols\\nper beam (time-domain level), where scheduler selects a number of consecutive OFDM symbols in a slot to assign to a\\nspecific beam, and 2) the scheduling of RBGs per UE in a beam, where the scheduler determines the allocation of RBGs\\nfor the OFDM symbols of the corresponding beam (frequency-domain level). The scheduling of the symbols per beam',\n",
       "  0.4974497467181104,\n",
       "  {'document': '../documents/nrmodule.pdf', 'page': 35, 'chunk_idx': 2}),\n",
       " ('agate through the channel and hence go through narrowband parallel channels. This\\ninterpretation justifies the name of OFDM for this communication scheme. Finally,\\nwe remark that DFT and IDFT can be very eciently implemented (and denoted as\\nFFT and IFFT, respectively) whenever Nc is a power of 2.\\nOFDM Block Length\\nThe OFDM scheme converts communication over a multipath channel into communi-\\ncation over simpler parallel narrowband channels. However, this simplicity is achieved\\nat a cost of underutilizing two resources, resulting in a loss of performance. First, the\\ncyclic prefix occupies an amount of time which cannot be used to communicate data.',\n",
       "  0.4973045651123567,\n",
       "  {'document': '../documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 119,\n",
       "   'chunk_idx': 3})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbccc6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<Role>\n",
      "You are a 5G wireless communication expert.\n",
      "\n",
      "<Goal>\n",
      "Answer the question using the information in the context below.\n",
      "If the context is insufficient, reply exactly: **\"I don't know\"**.\n",
      "\n",
      "<Context>\n",
      "[Document: ../documents/3GPP_38/38214-i60.docx, Page: 1]\n",
      "(score=0.495) to the UE reported threshold beamSwitchTiming when the reported value is one of the values {14,28,48} and when enableBeamSwitchTiming is not provided or the NZP-CSI-RS-ResourceSet is configured with the higher layer parameter trs-Info , aperiodic CSI-RS in a NZP-CSI-RS-ResourceSet configured with the higher layer parameter repetition set to 'off' or configured without the higher layer parameters repetition and trs-Info scheduled with offset larger than or equal to 48 when the UE provides beamSwitchTiming-r16 and enableBeamSwitchTiming is provided, aperiodic CSI-RS in a NZP-CSI-RS-ResourceSet configured with the higher layer parameter repetition set to 'on' scheduled with offset larger than or equal to the UE reported threshold beamSwitchTiming-r16 and enableBeamSwitchTiming is provided;\n",
      "\n",
      "---\n",
      "\n",
      "[Document: ../documents/3GPP_38/38331-i51.docx, Page: 1]\n",
      "(score=0.503) (AGC) and beam management, if configured. For each serving cell other than the SpCell or PUCCH SCell, the network may configure one BWP as a dormant BWP. Earth-fixed cell: An NTN cell fixed with respect to a certain geographic area on Earth. It can be provisioned by beam(s) continuously covering the same geographical area (e.g., the case of GSO satellites). Earth-moving cell: An NTN cell moving on the ground. It can be provisioned by beam(s) whose coverage area slides over the Earth's surface (e.g., the case of NGSO satellites generating fixed or non-steerable beams). eRedCap UE: A UE with enhanced reduced capabilities as specified in clause 4.2.22.1 in TS 38.306 [26]. Field: The individual contents of an information element are referred to as fields. FLOOR: Mathematical function used to\n",
      "\n",
      "---\n",
      "\n",
      "[Document: ../documents/Fundamentals of Wireless Communication.pdf, Page: 549]\n",
      "(score=0.502) yk[m] = hkxi[m] + wk[m].\n",
      "(10.77)\n",
      "If there are enough users in the system, for every beam i some user will be nearly in\n",
      "beamforming configuration with respect to it (and simultaneously nearly orthogonal to\n",
      "the other beams). Thus we have nt data streams being transmitted simultaneously in\n",
      "orthogonal spatial directions and the full spatial degrees of freedom are being utilized.\n",
      "The limited feedback from the users allows opportunistic scheduling of the user trans-\n",
      "missions in the appropriate beams at the appropriate time slots. To achieve close to\n",
      "the beamforming (and corresponding nulling to all the other beams) performance re-\n",
      "quires a user population that is larger than in the scenario of Section 6.7.3. In general,\n",
      "\n",
      "<Question>\n",
      "What is beam management?\n",
      "\n",
      "<Instructions>\n",
      "1. Explain simply and clearly, as if to a non-expert.  \n",
      "2. Use the following **answer format strictly**:  \n",
      "   - Start with a 1-2 sentence summary.  \n",
      "   - Then, list the explanation in **numbered bullet points** (1., 2., 3., etc.).  \n",
      "3. Give the reference.\n",
      "4. Conform to 3GPP protocol.\n",
      "\n",
      "<Answer>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from utils import format_context_grouped, expand_acronyms\n",
    "# 定义 Prompt 模板\n",
    "\n",
    "template_test = \"\"\"\n",
    "<Role>\n",
    "You are a 5G wireless communication expert.\n",
    "\n",
    "<Goal>\n",
    "Answer the question using the information in the context below.\n",
    "If the context is insufficient, reply exactly: **\"I don't know\"**.\n",
    "\n",
    "<Context>\n",
    "{context}\n",
    "\n",
    "<Question>\n",
    "{question}\n",
    "\n",
    "<Instructions>\n",
    "1. Explain simply and clearly, as if to a non-expert.  \n",
    "2. Use the following **answer format strictly**:  \n",
    "   - Start with a 1-2 sentence summary.  \n",
    "   - Then, list the explanation in **numbered bullet points** (1., 2., 3., etc.).  \n",
    "3. Give the reference.\n",
    "4. Conform to 3GPP protocol.\n",
    "\n",
    "<Answer>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template_test\n",
    ")\n",
    "\n",
    "# 执行检索\n",
    "question = \"What is beam management?\"\n",
    "question = expand_acronyms(question)\n",
    "\n",
    "top_k_results = vs.search(question, k=3, score_mode=\"reciprocal\")\n",
    "context = format_context_grouped(top_k_results, with_metadata=True, with_score=True)\n",
    "\n",
    "# 构造 Prompt 输入\n",
    "prompt_input = prompt.format(context=context, question=question)\n",
    "\n",
    "# 输出 prompt（可传入 LLM）\n",
    "print(prompt_input)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91c9e827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyus4/anaconda3/envs/rag_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt_input)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dbda788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam management refers to the process of controlling and optimizing the use of beams in a 5G wireless communication system. This includes tasks such as beamforming, beam switching, and beam allocation. Beamforming is the process of directing radio signals towards a specific user or group of users by using multiple antennas at the transmitter and receiver. Beam switching is the process of switching between different beams to improve signal quality or to avoid interference from other beams. Beam allocation is the process of assigning users to specific beams based on their location and the available beams in the system. Beam management is an important aspect of 5G wireless communication as it helps to improve the efficiency and performance of the system by optimizing the use of radio resources.\n",
      "\n",
      "*Beam management is a process that involves controlling and optimizing the use of beams in a 5G wireless communication system.*\n",
      "\n",
      "*Beamforming is the process of directing radio signals towards a specific user or group of users by using multiple antennas at the transmitter and receiver.*\n",
      "\n",
      "*Beam switching is the process of switching between different beams to improve signal quality or to avoid interference from other beams.*\n",
      "\n",
      "*Beam allocation is the process of assigning users to specific beams based on their location and the available beams in the system.*\n",
      "\n",
      "*Beam management is an important aspect of 5G wireless communication as it helps to improve the efficiency and performance of the system by optimizing the use of radio resources.*\n",
      "\n",
      "Reference:\n",
      "\n",
      "* [Document: Fundamentals of Wireless Communication.pdf](../documents/Fundamentals of Wireless Communication.pdf)\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3c55bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyus4/anaconda3/envs/rag_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam management is a key technology in 5G wireless communications that allows for the efficient use of radio frequency (RF) resources and improved network performance. It involves the use of multiple antennas at the base station and the mobile device to create a beam of radio waves that is directed towards the device, rather than broadcasting in all directions. This allows for a more focused and efficient use of RF resources, which can lead to improved data rates, reduced latency, and increased capacity in the network. Beam management can also be used to improve the signal quality for devices that are located in areas with poor signal strength, by directing the beam towards the device and reducing interference from other sources.\n"
     ]
    }
   ],
   "source": [
    "response_raw = llm.invoke(\"In 5G wireless communications, What is beam management?\")\n",
    "print(response_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b80284a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyus4/anaconda3/envs/rag_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "/home/lyus4/anaconda3/envs/rag_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== 对话回合 ======\n",
      "[🧑‍💬] What is beam management?\n",
      "[🤖] Beam management refers to the process of controlling and optimizing the use of beams in a 5G wireless communication\n",
      "system. This includes tasks such as configuring and managing beam configurations, adjusting beam patterns, and managing\n",
      "beam switching between different cells or serving areas. The goal of beam management is to improve the overall\n",
      "performance and efficiency of the communication system by ensuring that signals are transmitted and received as\n",
      "effectively as possible.  <Reference> The concept of beam management is not directly mentioned in the provided context\n",
      "documents. However, it is related to the overall management of beams in a 5G system, which is discussed in more detail\n",
      "in the \"Fundamentals of Wireless Communication\" document.\n",
      "\n",
      "====== 对话回合 ======\n",
      "[🧑‍💬] And why is it important?\n",
      "[🤖] AI: Beam management is important because it helps to optimize the use of beams in a 5G wireless communication system,\n",
      "which can improve the overall performance and efficiency of the communication system. By controlling and adjusting beam\n",
      "patterns and switching between different beams, beam management can help to ensure that signals are transmitted and\n",
      "received as effectively as possible, leading to better quality of service and increased capacity. Additionally, beam\n",
      "management can help to reduce interference and improve the overall coverage of the communication system.\n",
      "\n",
      "====== 历史记录 ======\n",
      "\n",
      "🧑‍💬 Q1: What is beam management?\n",
      "🤖 A1: Beam management refers to the process of controlling and optimizing the use of beams in a 5G wireless communication\n",
      "system. This includes tasks such as configuring and managing beam configurations, adjusting beam patterns, and managing\n",
      "beam switching between different cells or serving areas. The goal of beam management is to improve the overall\n",
      "performance and efficiency of the communication system by ensuring that signals are transmitted and received as\n",
      "effectively as possible.  <Reference> The concept of beam management is not directly mentioned in the provided context\n",
      "documents. However, it is related to the overall management of beams in a 5G system, which is discussed in more detail\n",
      "in the \"Fundamentals of Wireless Communication\" document.\n",
      "\n",
      "🧑‍💬 Q2: And why is it important?\n",
      "🤖 A2: AI: Beam management is important because it helps to optimize the use of beams in a 5G wireless communication system,\n",
      "which can improve the overall performance and efficiency of the communication system. By controlling and adjusting beam\n",
      "patterns and switching between different beams, beam management can help to ensure that signals are transmitted and\n",
      "received as effectively as possible, leading to better quality of service and increased capacity. Additionally, beam\n",
      "management can help to reduce interference and improve the overall coverage of the communication system.\n"
     ]
    }
   ],
   "source": [
    "# ── 依赖 ─────────────────────────────────────────────────────────────\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import (\n",
    "    Runnable, RunnableLambda, RunnableWithMessageHistory\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "import textwrap\n",
    "from utils import format_context_grouped, expand_acronyms\n",
    "\n",
    "# 你的llm和vs初始化略\n",
    "\n",
    "# ── 1. Prompt 模板 ──────────────────────────────────────────────\n",
    "template_test = \"\"\"\n",
    "<Role>\n",
    "You are a 5G wireless communication expert.\n",
    "\n",
    "<Goal>\n",
    "Answer the question using the information in the context below.\n",
    "If the context is insufficient, reply exactly: **\"I don't know\"**.\n",
    "\n",
    "<Context>\n",
    "{context}\n",
    "\n",
    "<Question>\n",
    "{question}\n",
    "\n",
    "<Instructions>\n",
    "1. Explain simply and clearly, as if to a non-expert.  \n",
    "2. Give the reference.\n",
    "\n",
    "<Answer>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template_test),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # 多轮历史插入\n",
    "    (\"user\", \"<Question>\\n{question}\")\n",
    "])\n",
    "\n",
    "# ── 2. 构造上下文（支持历史问句合并） ─────────────────────────────\n",
    "def build_inputs(inputs: dict):\n",
    "    question = inputs[\"input\"]\n",
    "    history_msgs = inputs.get(\"history\", [])\n",
    "\n",
    "    # 合并历史中所有 HUMAN 消息作为检索 query\n",
    "    history_text = \"\\n\".join([msg.content for msg in history_msgs if getattr(msg, \"type\", getattr(msg, \"role\", \"\")).lower() in [\"human\", \"user\"]])\n",
    "    combined_query = f\"{history_text}\\n{question}\" if history_text else question\n",
    "    q_exp = expand_acronyms(combined_query)\n",
    "\n",
    "    # 检索上下文\n",
    "    top_k_results = vs.search(q_exp, k=3, score_mode=\"reciprocal\")\n",
    "    ctx = format_context_grouped(top_k_results, with_metadata=True, with_score=True)\n",
    "\n",
    "    # 传递prompt模板要求的变量名\n",
    "    output = dict(inputs)\n",
    "    output.update({\n",
    "        \"context\": ctx,\n",
    "        \"question\": question\n",
    "    })\n",
    "    return output\n",
    "\n",
    "context_retriever = RunnableLambda(build_inputs)\n",
    "\n",
    "# ── 3. 构建完整对话链 ─────────────────────────────────────────────\n",
    "base_chain: Runnable = (\n",
    "    context_retriever |\n",
    "    chat_prompt |\n",
    "    llm |\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "# ── 4. 历史工厂（支持缓存）────────────────────────────────────────\n",
    "history_store = {}\n",
    "\n",
    "def history_factory(session_id: str):\n",
    "    if session_id not in history_store:\n",
    "        history_store[session_id] = InMemoryChatMessageHistory()\n",
    "    return history_store[session_id]\n",
    "\n",
    "# ── 5. 构建带历史支持的 chatbot ─────────────────────────────────\n",
    "chatbot = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    history_factory,\n",
    "    input_messages_key=\"input\",      # 本轮问题字段\n",
    "    history_messages_key=\"history\"   # 多轮消息字段\n",
    ")\n",
    "\n",
    "MAX_HISTORY_MESSAGES = 6\n",
    "\n",
    "def trim_history(session_id: str, max_messages=MAX_HISTORY_MESSAGES):\n",
    "    history = history_factory(session_id).messages\n",
    "    if len(history) > max_messages:\n",
    "        history[:] = history[-max_messages:]\n",
    "\n",
    "# ── 6. 示例调用（支持多轮）──────────────────────────────────────\n",
    "session_id = \"user_42\"\n",
    "\n",
    "def wrap_text(text, width=120):\n",
    "    return textwrap.fill(text, width=width)\n",
    "\n",
    "def print_qa_round(question: str, response: str):\n",
    "    print(\"\\n====== 对话回合 ======\")\n",
    "    print(f\"[🧑‍💬] {wrap_text(question)}\")\n",
    "    print(f\"[🤖] {wrap_text(response)}\")\n",
    "\n",
    "def print_chat_history(session_id: str):\n",
    "    print(\"\\n====== 历史记录 ======\")\n",
    "    history = history_factory(session_id).messages\n",
    "    for i in range(0, len(history), 2):\n",
    "        q = wrap_text(history[i].content) if i < len(history) else \"\"\n",
    "        a = wrap_text(history[i+1].content) if i+1 < len(history) else \"\"\n",
    "        print(f\"\\n🧑‍💬 Q{i//2 + 1}: {q}\")\n",
    "        print(f\"🤖 A{i//2 + 1}: {a}\")\n",
    "\n",
    "# 第 1 轮\n",
    "question1 = \"What is beam management?\"\n",
    "response1 = chatbot.invoke(\n",
    "    {\"input\": question1},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "trim_history(session_id)\n",
    "print_qa_round(question1, response1)\n",
    "\n",
    "# 第 2 轮\n",
    "question2 = \"And why is it important?\"\n",
    "response2 = chatbot.invoke(\n",
    "    {\"input\": question2},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "trim_history(session_id)\n",
    "print_qa_round(question2, response2)\n",
    "\n",
    "# 打印历史问答\n",
    "print_chat_history(session_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413bcef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
