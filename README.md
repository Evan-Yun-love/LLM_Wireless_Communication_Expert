# **LLM_Wireless_Communication_Expert**

ğŸ¯**å®ç°åŠŸèƒ½ï¼š**
1. å›ç­”é€šä¿¡é¢†åŸŸçŸ¥è¯†

ğŸš©**å®ç°æ­¥éª¤:**
1. è·å–é€šä¿¡çŸ¥è¯†æ–‡æ¡£ï¼š3GPPåè®®ã€é€šä¿¡ç”µå­ä¹¦
2. æ–‡æ¡£è¯»å–ï¼Œæ•°æ®æ¸…æ´—ï¼ˆç”¨æ­£åˆ™ï¼‰ï¼Œå‘é‡åŒ–
3. æ„å»ºé€šä¿¡RAGçŸ¥è¯†åº“
4. æ ¹æ®é€šä¿¡åœºæ™¯ï¼Œè®¾è®¡æé—®Propmtã€æ¨¡ç‰ˆ
5. è¿›è¡Œå¾®è°ƒï¼Œä½¿å¾—æ¨¡å‹å›ç­”æ›´åƒé€šä¿¡ä¸“å®¶
6. å¢åŠ é€šä¿¡å…¬å¼è®¡ç®—ï¼Œå°†ä¸€äº›å…¬å¼å°è£…ä¸ºå‡½æ•°ï¼Œæ ¹æ®æç¤ºä»¥åŠç»™å®šå‚æ•°è¿›è¡Œè®¡ç®—

## ç³»ç»Ÿæ¡†å›¾
![ç³»ç»Ÿæ¡†å›¾](./LWCEç³»ç»Ÿæ¡†å›¾.png)

## ä¸€ã€æ„å»ºé€šä¿¡çŸ¥è¯†åº“
1. æ”¶é›†æ–‡æ¡£ï¼š  
æ”¶é›†å¸¸ç”¨5G NR 3GPPåè®®ä»¥åŠé€šä¿¡ç”µå­ä¹¦

![image](https://github.com/user-attachments/assets/c488cfbc-df9a-43ed-8a4b-36b121cb7cac)

2. æ–‡æ¡£è¯»å–ï¼š  
å¯¹äºdocsï¼Œç”¨`Docx2txtLoader`è¯»å–; å¯¹äºpdfï¼Œç”¨`PyMuPDFLoader`è¯»å–ã€‚ç›®å‰ä»…æ”¯æŒè¿™ä¸¤ç§æ ¼å¼
```python
def read_docs(self, file_path: Path):
        """è¯»å–æŒ‡å®šæ ¼å¼çš„æ–‡æ¡£å†…å®¹
        Args:
            file_path: ç›®æ ‡æ–‡ä»¶è·¯å¾„
        Returns:
            æ–‡æ¡£å†…å®¹å¯¹è±¡åˆ—è¡¨
        Raises:
            ValueError: é‡åˆ°ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼æ—¶æŠ›å‡º
        """
        # æ ¹æ®æ–‡ä»¶åç¼€é€‰æ‹©å¯¹åº”çš„æ–‡æ¡£åŠ è½½å™¨
        if file_path.suffix.lower() == '.pdf':
            return PyMuPDFLoader(str(file_path)).load()
        elif file_path.suffix.lower() == '.docx':
            return Docx2txtLoader(str(file_path)).load()
        else:
            raise ValueError(f"Unsupported file format: {file_path.suffix}")
```

3. æ•°æ®æ¸…æ´—ï¼š  
ç”¨æ­£åˆ™è¡¨è¾¾å¼å¯¹æ–‡æ¡£è¿›è¡Œæ¸…æ´—ã€‚å¸¸è§å™ªå£°ä¸»è¦æ˜¯ï¼šæŠ€æœ¯è§„èŒƒç¼–å·ã€å•†æ ‡ä¿¡æ¯ã€è”ç³»æ–¹å¼ã€é“¾æ¥ã€è”ç³»æ–¹å¼ã€ç‰ˆæƒå£°æ˜ã€æ ‡é¢˜ã€ç« èŠ‚å·ã€é¡µç ã€é¡µçœ‰é¡µè„šç­‰ç­‰ã€‚å› ä¸ºæ–‡æ¡£æ¯”è¾ƒå°‘ï¼Œæ ¼å¼æ¯”è¾ƒå›ºå®šï¼Œæ‰€ä»¥æ•°æ®æ¸…æ´—è®©GPTä»£åŠ³å³å¯ã€‚
```python
def docs_clean(self, text: str) -> str:
        """æ‰§è¡Œå¤šé˜¶æ®µæ–‡æœ¬æ¸…æ´—å¤„ç†
        Args:
            text: åŸå§‹æ–‡æœ¬å†…å®¹
        Returns:
            æ¸…æ´—åçš„æ ‡å‡†åŒ–æ–‡æœ¬
        """
        # ç»Ÿä¸€æ¢è¡Œç¬¦å’Œç‰¹æ®Šå­—ç¬¦
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        text = text.replace('ï¬', 'fi').replace('ï¬‚', 'fl').replace('â€“', '-').replace('â€”', '-').replace(' ', ' ')
        
        # æ­£åˆ™è¡¨è¾¾å¼å¤„ç†åºåˆ—ï¼ˆå¤„ç†æ ¼å¼å™ªå£°ã€é¡µçœ‰é¡µè„šã€ç‰ˆæƒä¿¡æ¯ç­‰ï¼‰
        text = re.sub(r'(?<=\b)(?:[a-zA-Z]\s){2,}[a-zA-Z](?=\b)', lambda m: m.group(0).replace(' ', ''), text)  # ä¿®å¤è¢«ç©ºæ ¼åˆ†å‰²çš„å•è¯
        text = re.sub(r'(?:^.{1,20}\n){3,}', lambda m: m.group(0).replace('\n', ' '), text, flags=re.MULTILINE)  # åˆå¹¶çŸ­è¡Œ
        text = re.sub(r'3GPP TS \d+\.\d+ V\d+\.\d+\.\d+ \(\d{4}-\d{2}\)', '', text)  # åˆ é™¤æŠ€æœ¯è§„èŒƒç¼–å·
        text = re.sub(r'Release \d+', '', text, flags=re.IGNORECASE)  # åˆ é™¤ç‰ˆæœ¬ä¿¡æ¯
        text = re.sub(r'Page \d+', '', text, flags=re.IGNORECASE)  # åˆ é™¤é¡µç 
        text = re.sub(r'^\s*\d+\s*$', '', text, flags=re.MULTILINE)  # åˆ é™¤çº¯æ•°å­—è¡Œ
        text = text.replace('\f', '')  # åˆ é™¤æ¢é¡µç¬¦
        text = re.sub(r'Copyright Notification.*?All rights reserved\.', '', text, flags=re.DOTALL | re.IGNORECASE)  # åˆ é™¤ç‰ˆæƒå£°æ˜
        text = re.sub(r'Postal address.*?http://www\.3gpp\.org', '', text, flags=re.DOTALL | re.IGNORECASE)  # åˆ é™¤è”ç³»ä¿¡æ¯
        text = re.sub(r'UMTSâ„¢.*?GSMÂ® and the GSM logo.*?GSM Association', '', text, flags=re.DOTALL | re.IGNORECASE)  # åˆ é™¤å•†æ ‡ä¿¡æ¯
        text = re.sub(r'^\s*(\d+\.){1,4}[^\n]*\d{1,4}\s*$', '', text, flags=re.MULTILINE)  # åˆ é™¤ç« èŠ‚ç¼–å·
        text = re.sub(r'\s+\d{1,4}$', '', text, flags=re.MULTILINE)  # åˆ é™¤è¡Œå°¾é¡µç 
        text = re.sub(r'^\s*(\d+\.){1,5}\s*', '', text, flags=re.MULTILINE)  # åˆ é™¤å¤šçº§ç¼–å·
        text = re.sub(r'^\s*(Foreword|Contents|General|Scope|References|Introduction|Abbreviations|Definitions)\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE)  # åˆ é™¤ç« èŠ‚æ ‡é¢˜
        text = re.sub(r'^[-=_]{3,}$', '', text, flags=re.MULTILINE)  # åˆ é™¤åˆ†éš”çº¿
        text = re.sub(r'\n{3,}', '\n\n', text)  # å‹ç¼©å¤šä½™ç©ºè¡Œ
        text = re.sub(r'[ \t]+$', '', text, flags=re.MULTILINE)  # åˆ é™¤è¡Œå°¾ç©ºæ ¼
        text = re.sub(r'\s{2,}', ' ', text)  # å‹ç¼©è¿ç»­ç©ºæ ¼
        return text.strip()
```
4. æ–‡æœ¬åˆ‡å‰²ï¼š  
ç”±äºæŠ€æœ¯æ–‡æ¡£ä¸Šä¸‹æ–‡å…³ç³»è¾ƒå¼ºï¼Œéœ€è¦å°½é‡ä¿ç•™æ®µè½ã€å¥å­å®Œæ•´ï¼Œå‡å°‘æˆªæ–­ï¼Œæ‰€ä»¥ä½¿ç”¨`RecursiveCharacterTextSplitter`ã€‚  
å…³äº```chunk_size```å’Œ```chunk_overlap```çš„è®¾ç½®ï¼Œchunk_sizeçš„è®¾ç½®è¦è€ƒè™‘ä½¿ç”¨çš„embed_modelçš„å¤§å°,å› ä¸ºç®—åŠ›æœ‰é™ï¼Œç›®å‰é‡‡ç”¨çš„æ¨¡å‹æ˜¯```all-MiniLM-L6-v2```ï¼Œæ¨¡å‹æ¯”è¾ƒå°ï¼Œç¼–ç ç»´åº¦```dim=384```ï¼Œæ‰€ä»¥ä¸å®œè®¾ç½®å¤ªå¤§çš„chunk_sizeï¼Œå¦åˆ™å‘é‡éš¾ä»¥è¡¨å¾chunkè¯­ä¹‰ã€‚æ‰€ä»¥è®¾ç½®æ˜¯```chunk_size=800, chunk_overlap=160```ï¼Œchunk_overlapæŒ‰ç…§chunk_sizeçš„``20%``æ¥è®¡ã€‚  
```python
def chunks_split(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²ä¸ºæŒ‡å®šå¤§å°çš„å—
        Args:
            text: æ¸…æ´—åçš„å®Œæ•´æ–‡æœ¬
        Returns:
            æ–‡æœ¬å—åˆ—è¡¨ï¼Œæ¯ä¸ªå—çº¦800å­—ç¬¦ï¼Œå—é—´é‡å 100å­—ç¬¦
        """
        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=160)
        return splitter.split_text(text)
```
5. chunkå‘é‡åŒ–:  
é‡‡ç”¨çš„æ¨¡å‹æ˜¯```all-MiniLM-L6-v2```ï¼Œæ¯”è¾ƒå°å‹åŒ–ï¼Œç›´æ¥éƒ¨ç½²æœ¬åœ°ã€‚æ”¯æŒä¸­è‹±æ–‡ã€‚
```python
embed_model = HuggingFaceEmbeddings(
        model_name='../all-MiniLM-L6-v2',
        model_kwargs={'device': 'cuda'},
        encode_kwargs={'normalize_embeddings': True}
    )
```
```python
def chunks_tokenize(self, chunks: List[str], batch_size=256) -> np.ndarray:
        """å°†æ–‡æœ¬å—æ‰¹é‡è½¬æ¢ä¸ºå‘é‡
        Args:
            chunks: æ–‡æœ¬å—åˆ—è¡¨
            batch_size: æ‰¹é‡å¤„ç†å¤§å°ï¼ˆé˜²æ­¢å†…å­˜æº¢å‡ºï¼‰
        Returns:
            å½¢çŠ¶ä¸º(n_chunks, embedding_dim)çš„å‘é‡çŸ©é˜µ
        """
        all_vecs = []
        # åˆ†æ‰¹æ¬¡å¤„ç†æ–‡æœ¬å—
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i+batch_size]
            vecs = self.model.embed_documents(batch)
            all_vecs.append(vecs)
        return np.vstack(all_vecs)
```
6. æ„å»ºFAISSå‘é‡åº“ï¼š  
é€’å½’æŸ¥è¯¢ç›®å½•ä¸‹çš„æ–‡æ¡£ï¼Œè¯»å–pdf,docxï¼Œé€æ–‡æ¡£å¤„ç†ï¼ŒåŒæ—¶åŠ å…¥é‡å¤æ£€æµ‹åŠŸèƒ½ï¼Œé¿å…å¤šæ¬¡æ·»åŠ åŒä¸€æ–‡æ¡£åˆ°å‘é‡åº“ä¸­ï¼Œå¯ä»¥å¢é‡ã€æŒä¹…åŒ–æ·»åŠ ç´¢å¼•ã€‚
```python
def process_documents(self) -> None:
        """æ–‡æ¡£å¤„ç†ä¸»æµç¨‹ï¼šéå†ç›®å½•ã€å¤„ç†æ–‡æ¡£ã€æ›´æ–°ç´¢å¼•"""
        processed = 0
        # é€’å½’éå†æ–‡æ¡£ç›®å½•
        for file_path in self.docs_path.rglob('*'):
            # è·³è¿‡éæ”¯æŒæ ¼å¼æ–‡ä»¶
            if not file_path.is_file() or file_path.suffix.lower() not in self.SUPPORTED_FORMATS:
                continue
            try:
                print(f"Processing {file_path} ...")
                docs = self.read_docs(file_path)
                document_chunks = []
                chunk_hashes = []

                # å¤„ç†å•ä¸ªæ–‡æ¡£çš„å¤šä¸ªé¡µé¢å†…å®¹
                for doc in docs:
                    cleaned_text = self.docs_clean(doc.page_content)
                    chunks = self.chunks_split(cleaned_text)
                    # è®¡ç®—å“ˆå¸Œå€¼è¿›è¡Œé‡å¤æ£€æµ‹
                    for chunk in chunks:
                        h = hashlib.md5(chunk.encode('utf-8')).hexdigest()
                        if h in self.hash_set:
                            print(f"Duplicate chunk detected in {file_path}, skipping entire document.")
                            raise ValueError("Duplicate document")
                        document_chunks.append(chunk)
                        chunk_hashes.append(h)

                # å°†æœ‰æ•ˆå—æ·»åŠ åˆ°ç´¢å¼•ç³»ç»Ÿ
                if document_chunks:
                    vectors = self.chunks_tokenize(document_chunks)
                    # åˆå§‹åŒ–ç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                    if self.index is None:
                        dim = vectors.shape[1]
                        self.index = faiss.IndexFlatL2(dim)
                    self.index.add(vectors.astype('float32'))

                    # ä¿å­˜å…ƒæ•°æ®å’Œå“ˆå¸Œå€¼
                    for i, chunk in enumerate(document_chunks):
                        self.metadata_list.append({
                            "file": str(file_path.relative_to(self.docs_path)),
                            "chunk_idx": i,
                            "content_preview": chunk[:40]
                        })
                        self.hash_set.add(chunk_hashes[i])
                    processed += 1

            except Exception as e:
                print(f"Error processing {file_path}: {str(e)}")
                continue

        # å¤„ç†å®ŒæˆåæŒä¹…åŒ–å­˜å‚¨
        if self.index is not None:
            faiss.write_index(self.index, str(self.db_path))
            pickle.dump(self.metadata_list, open(self.meta_path, "wb"))
            pickle.dump(self.hash_set, open(self.hash_path, "wb"))
            print(f"All done! Processed {processed} documents. Index size: {self.index.ntotal}.")
        else:
            print("No documents processed!")
```

## äºŒã€å‘é‡æ£€ç´¢

## ä¸‰ã€Promptæ„å»º

## å››ã€LLMå¾®è°ƒ

## äº”ã€æ€§èƒ½è¯„ä¼°


