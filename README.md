# **LLM_Wireless_Communication_Expert**

ğŸ¯**å®ç°åŠŸèƒ½ï¼š**
1. å›ç­”é€šä¿¡é¢†åŸŸçŸ¥è¯†

ğŸš©**å®ç°æ­¥éª¤:**
1. è·å–é€šä¿¡çŸ¥è¯†æ–‡æ¡£ï¼š3GPPåè®®ã€é€šä¿¡ç”µå­ä¹¦
2. æ–‡æ¡£è¯»å–ï¼Œæ•°æ®æ¸…æ´—ï¼ˆç”¨æ­£åˆ™ï¼‰ï¼Œå‘é‡åŒ–
3. æ„å»ºé€šä¿¡RAGçŸ¥è¯†åº“
4. æ ¹æ®é€šä¿¡åœºæ™¯ï¼Œè®¾è®¡æé—®Propmtã€æ¨¡ç‰ˆ
5. è¿›è¡Œå¾®è°ƒï¼Œä½¿å¾—æ¨¡å‹å›ç­”æ›´åƒé€šä¿¡ä¸“å®¶
6. å¢åŠ é€šä¿¡å…¬å¼è®¡ç®—ï¼Œå°†ä¸€äº›å…¬å¼å°è£…ä¸ºå‡½æ•°ï¼Œæ ¹æ®æç¤ºä»¥åŠç»™å®šå‚æ•°è¿›è¡Œè®¡ç®—

## ç³»ç»Ÿæ¡†å›¾
![ç³»ç»Ÿæ¡†å›¾](./LWCEç³»ç»Ÿæ¡†å›¾.png)

## ä¸€ã€æ–‡æ¡£å¤„ç†
1. æ”¶é›†æ–‡æ¡£ï¼š  
æ”¶é›†å¸¸ç”¨5G NR 3GPPåè®®ä»¥åŠé€šä¿¡ç”µå­ä¹¦

![image](https://github.com/user-attachments/assets/c488cfbc-df9a-43ed-8a4b-36b121cb7cac)

2. æ–‡æ¡£è¯»å–ï¼š  
å¯¹äºdocsï¼Œç”¨`Docx2txtLoader`è¯»å–; å¯¹äºpdfï¼Œç”¨`PyMuPDFLoader`è¯»å–ã€‚ç›®å‰ä»…æ”¯æŒè¿™ä¸¤ç§æ ¼å¼

3. æ•°æ®æ¸…æ´—ï¼š  
ç”¨æ­£åˆ™è¡¨è¾¾å¼å¯¹æ–‡æ¡£è¿›è¡Œæ¸…æ´—ã€‚å¸¸è§å™ªå£°ä¸»è¦æ˜¯ï¼šæŠ€æœ¯è§„èŒƒç¼–å·ã€å•†æ ‡ä¿¡æ¯ã€è”ç³»æ–¹å¼ã€é“¾æ¥ã€è”ç³»æ–¹å¼ã€ç‰ˆæƒå£°æ˜ã€æ ‡é¢˜ã€ç« èŠ‚å·ã€é¡µç ã€é¡µçœ‰é¡µè„šç­‰ç­‰ã€‚å› ä¸ºæ–‡æ¡£æ¯”è¾ƒå°‘ï¼Œæ ¼å¼æ¯”è¾ƒå›ºå®šï¼Œæ‰€ä»¥æ•°æ®æ¸…æ´—è®©GPTä»£åŠ³å³å¯ã€‚

4. æ–‡æœ¬åˆ‡å‰²ï¼š  
ç”±äºæŠ€æœ¯æ–‡æ¡£ä¸Šä¸‹æ–‡å…³ç³»è¾ƒå¼ºï¼Œéœ€è¦å°½é‡ä¿ç•™æ®µè½ã€å¥å­å®Œæ•´ï¼Œå‡å°‘æˆªæ–­ï¼Œæ‰€ä»¥ä½¿ç”¨`RecursiveCharacterTextSplitter`ã€‚  
å…³äº```chunk_size```å’Œ```chunk_overlap```çš„è®¾ç½®ï¼Œchunk_sizeçš„è®¾ç½®è¦è€ƒè™‘ä½¿ç”¨çš„embed_modelçš„å¤§å°,å› ä¸ºç®—åŠ›æœ‰é™ï¼Œç›®å‰é‡‡ç”¨çš„æ¨¡å‹æ˜¯```all-MiniLM-L6-v2```ï¼Œæ¨¡å‹æ¯”è¾ƒå°ï¼Œç¼–ç ç»´åº¦```dim=384```ï¼Œæ‰€ä»¥ä¸å®œè®¾ç½®å¤ªå¤§çš„chunk_sizeï¼Œå¦åˆ™å‘é‡éš¾ä»¥è¡¨å¾chunkè¯­ä¹‰ã€‚æ‰€ä»¥è®¾ç½®æ˜¯```chunk_size=800, chunk_overlap=160```ï¼Œchunk_overlapæŒ‰ç…§chunk_sizeçš„``20%``æ¥è®¡ã€‚  

```python
import os
import re
import json
from dataclasses import dataclass
from pathlib import Path
from typing import List, Union

from langchain_community.document_loaders import PyMuPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

@dataclass
class PageDocument:
    """
    è¡¨ç¤ºå•é¡µæ–‡æ¡£çš„æ•°æ®ç±»
    
    å±æ€§:
        content: æ–‡æ¡£é¡µé¢çš„æ–‡æœ¬å†…å®¹
        metadata: åŒ…å«æ–‡æ¡£å…ƒæ•°æ®çš„å­—å…¸ï¼ˆå¦‚æ–‡ä»¶è·¯å¾„ã€é¡µç ç­‰ï¼‰
    """
    content: str
    metadata: dict

class DocumentProcessor:
    """
    æ–‡æ¡£å¤„ç†ç®¡é“ï¼Œæ”¯æŒPDF/DOCXæ ¼å¼çš„è¯»å–ã€æ–‡æœ¬æ¸…æ´—å’Œåˆ†å—å¤„ç†
    
    ä¸»è¦åŠŸèƒ½:
        - é€’å½’æ‰«æç›®å½•è·å–æ”¯æŒæ ¼å¼çš„æ–‡ä»¶åˆ—è¡¨
        - åŠ è½½PDF/DOCXæ–‡æ¡£å¹¶æå–æ¯é¡µå†…å®¹
        - å¯¹æ–‡æœ¬è¿›è¡Œå¤šçº§æ¸…æ´—å¤„ç†
        - å°†æ–‡æ¡£åˆ†å‰²æˆæŒ‡å®šå¤§å°çš„æ–‡æœ¬å—
        - æ”¯æŒä¿å­˜/åŠ è½½å¤„ç†ç»“æœ
    
    å±æ€§:
        SUPPORTED_FORMATS: æ”¯æŒå¤„ç†çš„æ–‡ä»¶æ‰©å±•åé›†åˆ
        docs_path: æ–‡æ¡£å­˜å‚¨çš„æ ¹ç›®å½•è·¯å¾„
        files: æ‰«æåˆ°çš„æœ‰æ•ˆæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        documents: åŸå§‹é¡µé¢æ–‡æ¡£å¯¹è±¡åˆ—è¡¨
        cleaned_documents: æ¸…æ´—åçš„é¡µé¢æ–‡æ¡£å¯¹è±¡åˆ—è¡¨
        chunks: åˆ†å‰²åçš„æ–‡æœ¬å—å†…å®¹åˆ—è¡¨
        chunk_metadata: å¯¹åº”æ–‡æœ¬å—çš„å…ƒæ•°æ®åˆ—è¡¨
    """
    SUPPORTED_FORMATS = {'.pdf', '.docx'}

    def __init__(self, docs_path: Union[str, Path]):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        
        å‚æ•°:
            docs_path: åŒ…å«æ–‡æ¡£çš„ç›®å½•è·¯å¾„
        """
        self.docs_path = Path(docs_path)
        self.files: List[str] = self._get_files()

        # ä¸­é—´ç»“æœ
        self.documents: List[PageDocument] = []
        self.cleaned_documents: List[PageDocument] = []
        self.chunks: List[str] = []
        self.chunk_metadata: List[dict] = []

    def _get_files(self) -> List[str]:
        """
        é€’å½’æ‰«ææ–‡æ¡£ç›®å½•ï¼Œè·å–æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        
        è¿”å›:
            ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶ç»å¯¹è·¯å¾„åˆ—è¡¨
        """
        file_list = []
        for filepath, _, filenames in os.walk(self.docs_path):
            for filename in filenames:
                ext = os.path.splitext(filename)[1].lower()
                if ext in self.SUPPORTED_FORMATS:
                    file_list.append(os.path.join(filepath, filename))
        return file_list

    def load_documents(self) -> None:
        """
        åŠ è½½æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶å†…å®¹
        
        ä½¿ç”¨PyMuPDFLoaderå¤„ç†PDFæ–‡ä»¶ï¼ŒDocx2txtLoaderå¤„ç†DOCXæ–‡ä»¶
        å°†æ¯é¡µå†…å®¹å°è£…ä¸ºPageDocumentå¯¹è±¡å¹¶å­˜å‚¨åŸå§‹æ–‡æ¡£åˆ—è¡¨
        """
        for file_path in self.files:
            try:
                # æ ¹æ®æ‰©å±•åé€‰æ‹©åŠ è½½å™¨
                if file_path.endswith('.pdf'):
                    pages = [p.page_content for p in PyMuPDFLoader(file_path).load()]
                elif file_path.endswith('.docx'):
                    pages = [p.page_content for p in Docx2txtLoader(file_path).load()]
                else:
                    continue
                # å°è£…æ¯é¡µå†…å®¹å¹¶è®°å½•å…ƒæ•°æ®
                for idx, content in enumerate(pages, 1):
                    self.documents.append(PageDocument(content=content, metadata={"document": file_path, "page": idx}))
            except Exception as e:
                print(f"[WARN] è¯»å–å¤±è´¥: {file_path}ï¼Œé”™è¯¯: {e}")

    def clean_documents(self) -> None:
        """
        å¯¹åŸå§‹æ–‡æ¡£è¿›è¡Œæ–‡æœ¬æ¸…æ´—å¤„ç†
        
        éå†æ‰€æœ‰PageDocumentå¯¹è±¡ï¼Œåº”ç”¨clean_textæ–¹æ³•æ¸…æ´—æ–‡æœ¬å†…å®¹
        å°†æ¸…æ´—ç»“æœå­˜å‚¨åˆ°cleaned_documentsåˆ—è¡¨
        """
        for doc in self.documents:
            cleaned = self.clean_text(doc.content)
            self.cleaned_documents.append(PageDocument(content=cleaned, metadata=doc.metadata))

    def split_documents(self, chunk_size: int = 800, chunk_overlap: int = 160) -> None:
        """
        å°†æ¸…æ´—åçš„æ–‡æ¡£åˆ†å‰²æˆæ–‡æœ¬å—
        
        å‚æ•°:
            chunk_size: æ¯ä¸ªæ–‡æœ¬å—çš„ç›®æ ‡å­—ç¬¦æ•°
            chunk_overlap: å—é—´é‡å çš„å­—ç¬¦æ•°
            
        ä½¿ç”¨é€’å½’å­—ç¬¦åˆ†å‰²å™¨å¤„ç†æ¯ä¸ªæ–‡æ¡£ï¼Œè®°å½•åˆ†å—å†…å®¹åŠå…¶å…ƒæ•°æ®
        """
        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        for doc in self.cleaned_documents:
            chunks = splitter.split_text(doc.content)
            # ä¸ºæ¯ä¸ªå—æ·»åŠ ç´¢å¼•ä¿¡æ¯åˆ°å…ƒæ•°æ®
            for i, chunk in enumerate(chunks):
                self.chunks.append(chunk)
                meta = dict(doc.metadata)
                meta["chunk_idx"] = i
                self.chunk_metadata.append(meta)

    def run_all(self):
        """
        æ‰§è¡Œå®Œæ•´å¤„ç†æµç¨‹
        
        ä¾æ¬¡æ‰§è¡Œ: åŠ è½½æ–‡æ¡£ -> æ¸…æ´—æ–‡æœ¬ -> åˆ†å‰²æ–‡æœ¬
        æ‰“å°å„é˜¶æ®µå¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        print(f"[INFO] å¼€å§‹å¤„ç† {len(self.files)} ä¸ªæ–‡ä»¶...")
        self.load_documents()
        print(f"[INFO] å·²åŠ è½½ {len(self.documents)} é¡µ")
        self.clean_documents()
        print(f"[INFO] æ¸…æ´—å®Œæˆ")
        self.split_documents()
        print(f"[INFO] åˆ†å‰²å®Œæˆï¼Œç”Ÿæˆ chunk æ•°: {len(self.chunks)}")

    def save_chunks_and_metadata(self, save_path: Union[str, Path]) -> None:
        """
        ä¿å­˜åˆ†å—ç»“æœå’Œå…ƒæ•°æ®åˆ°JSONæ–‡ä»¶
        
        å‚æ•°:
            save_path: ç»“æœä¿å­˜è·¯å¾„
        """
        save_path = Path(save_path)
        # ç»„åˆåˆ†å—å†…å®¹å’Œå…ƒæ•°æ®
        data = [{"chunk": c, "metadata": m} for c, m in zip(self.chunks, self.chunk_metadata)]
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"[INFO] å·²ä¿å­˜è‡³: {save_path}")

    def load_chunks_and_metadata(self, load_path: Union[str, Path]) -> None:
        """
        ä»JSONæ–‡ä»¶åŠ è½½åˆ†å—ç»“æœå’Œå…ƒæ•°æ®
        
        å‚æ•°:
            load_path: ç»“æœæ–‡ä»¶è·¯å¾„
        """
        load_path = Path(load_path)
        if not load_path.exists():
            raise FileNotFoundError(f"{load_path} ä¸å­˜åœ¨")
        with open(load_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        # ä»åŠ è½½çš„æ•°æ®ä¸­åˆ†ç¦»å†…å®¹å’Œå…ƒæ•°æ®
        self.chunks = [item["chunk"] for item in data]
        self.chunk_metadata = [item["metadata"] for item in data]
        print(f"[INFO] æˆåŠŸåŠ è½½ {len(self.chunks)} ä¸ª chunk")

    @staticmethod
    def clean_text(text: str) -> str:
        """
        æ‰§è¡Œå¤šçº§æ–‡æœ¬æ¸…æ´—å’Œæ ‡å‡†åŒ–å¤„ç†
        
        åŒ…æ‹¬:
          - æ¢è¡Œç¬¦æ ‡å‡†åŒ–
          - ç‰¹æ®Šå­—ç¬¦æ›¿æ¢
          - ç§»é™¤ç‰¹å®šæ ¼å¼æ–‡æœ¬ï¼ˆå¦‚3GPPæ–‡æ¡£å¤´ã€é¡µç ç­‰ï¼‰
          - æ¸…é™¤URLå’Œç”µå­é‚®ä»¶
          - åˆ é™¤éASCIIå­—ç¬¦
          - å¤šä½™ç©ºæ ¼å’Œç©ºè¡Œå‹ç¼©
        
        å‚æ•°:
            text: åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²
        è¿”å›:
            æ¸…æ´—åçš„æ–‡æœ¬å­—ç¬¦ä¸²
        """
        # ç»Ÿä¸€æ¢è¡Œç¬¦å’Œç©ºæ ¼
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        text = text.replace('ï¬', 'fi').replace('ï¬‚', 'fl').replace('â€“', '-').replace('â€”', '-').replace(' ', ' ')
        
        # ä¿®å¤è¢«ç©ºæ ¼åˆ†éš”çš„å•è¯
        text = re.sub(r'(?<=\b)(?:[a-zA-Z]\s){2,}[a-zA-Z](?=\b)', lambda m: m.group(0).replace(' ', ''), text)
        # åˆå¹¶çŸ­è¡Œ
        text = re.sub(r'(?:^.{1,20}\n){3,}', lambda m: m.group(0).replace('\n', ' '), text, flags=re.MULTILINE)
        
        # ç§»é™¤ç‰¹å®šæ ¼å¼æ–‡æœ¬
        text = re.sub(r'3GPP TS \d+\.\d+ V\d+\.\d+\.\d+ \(\d{4}-\d{2}\)', '', text)
        text = re.sub(r'Release \d+', '', text, flags=re.IGNORECASE)
        text = re.sub(r'Page \d+', '', text, flags=re.IGNORECASE)
        text = re.sub(r'^\s*\d+\s*$', '', text, flags=re.MULTILINE)
        text = text.replace('\f', '')
        
        # ç§»é™¤ç‰ˆæƒå£°æ˜å’Œè”ç³»ä¿¡æ¯
        text = re.sub(r'Copyright Notification.*?All rights reserved\.', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'Postal address.*?http://www\.3gpp\.org', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'UMTSâ„¢.*?GSMÂ® and the GSM logo.*?GSM Association', '', text, flags=re.DOTALL | re.IGNORECASE)
        
        # æ¸…é™¤æ–‡æ¡£ç¼–å·å’Œç« èŠ‚æ ‡é¢˜
        text = re.sub(r'^\s*(\d+\.){1,4}[^\n]*\d{1,4}\s*$', '', text, flags=re.MULTILINE)
        text = re.sub(r'\s+\d{1,4}$', '', text, flags=re.MULTILINE)
        text = re.sub(r'^\s*(\d+\.){1,5}\s*', '', text, flags=re.MULTILINE)
        text = re.sub(r'^\s*(Foreword|Contents|General|Scope|References|Introduction|Abbreviations|Definitions)\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE)
        
        # æ¸…ç†æ ¼å¼æ ‡è®°å’Œå¤šä½™ç©ºç™½
        text = re.sub(r'^[-=_]{3,}$', '', text, flags=re.MULTILINE)
        text = re.sub(r'\n{3,}', '\n\n', text)
        text = re.sub(r'[ \t]+$', '', text, flags=re.MULTILINE)
        text = re.sub(r'\s{2,}', ' ', text)
        
        # ç§»é™¤URLå’Œç½‘ç»œåœ°å€
        text = re.sub(r'(https?://[^\s]+)', '', text)
        text = re.sub(r'(www\.[^\s]+)', '', text)
        text = re.sub(r'\b\S+\.(com|org|net|edu|gov|cn)\b', '', text)
        text = re.sub(r'[Ff]ax[:ï¼š]?\s*[\(\)\d\-â€“ â€”]+', '', text)
        
        # è¿‡æ»¤éASCIIå­—ç¬¦
        text = re.sub(r'[^\x00-\x7F]+', '', text)

        return text.strip()

```

5. chunkå‘é‡åŒ–:  
é‡‡ç”¨çš„æ¨¡å‹æ˜¯```all-MiniLM-L6-v2```ï¼Œæ¯”è¾ƒå°å‹åŒ–ï¼Œç›´æ¥éƒ¨ç½²æœ¬åœ°ã€‚æ”¯æŒä¸­è‹±æ–‡ã€‚
```python
embed_model = HuggingFaceEmbeddings(
        model_name='../all-MiniLM-L6-v2',
        model_kwargs={'device': 'cuda'},
        encode_kwargs={'normalize_embeddings': True}
    )
```
```python
def chunks_tokenize(self, chunks: List[str], batch_size=256) -> np.ndarray:
        """å°†æ–‡æœ¬å—æ‰¹é‡è½¬æ¢ä¸ºå‘é‡
        Args:
            chunks: æ–‡æœ¬å—åˆ—è¡¨
            batch_size: æ‰¹é‡å¤„ç†å¤§å°ï¼ˆé˜²æ­¢å†…å­˜æº¢å‡ºï¼‰
        Returns:
            å½¢çŠ¶ä¸º(n_chunks, embedding_dim)çš„å‘é‡çŸ©é˜µ
        """
        all_vecs = []
        # åˆ†æ‰¹æ¬¡å¤„ç†æ–‡æœ¬å—
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i+batch_size]
            vecs = self.model.embed_documents(batch)
            all_vecs.append(vecs)
        return np.vstack(all_vecs)
```
6. æ„å»ºFAISSå‘é‡åº“ï¼š  
é€’å½’æŸ¥è¯¢ç›®å½•ä¸‹çš„æ–‡æ¡£ï¼Œè¯»å–pdf,docxï¼Œé€æ–‡æ¡£å¤„ç†ï¼ŒåŒæ—¶åŠ å…¥é‡å¤æ£€æµ‹åŠŸèƒ½ï¼Œé¿å…å¤šæ¬¡æ·»åŠ åŒä¸€æ–‡æ¡£åˆ°å‘é‡åº“ä¸­ï¼Œå¯ä»¥å¢é‡ã€æŒä¹…åŒ–æ·»åŠ ç´¢å¼•ã€‚
```python
def process_documents(self) -> None:
        """æ–‡æ¡£å¤„ç†ä¸»æµç¨‹ï¼šéå†ç›®å½•ã€å¤„ç†æ–‡æ¡£ã€æ›´æ–°ç´¢å¼•"""
        processed = 0
        # é€’å½’éå†æ–‡æ¡£ç›®å½•
        for file_path in self.docs_path.rglob('*'):
            # è·³è¿‡éæ”¯æŒæ ¼å¼æ–‡ä»¶
            if not file_path.is_file() or file_path.suffix.lower() not in self.SUPPORTED_FORMATS:
                continue
            try:
                print(f"Processing {file_path} ...")
                docs = self.read_docs(file_path)
                document_chunks = []
                chunk_hashes = []

                # å¤„ç†å•ä¸ªæ–‡æ¡£çš„å¤šä¸ªé¡µé¢å†…å®¹
                for doc in docs:
                    cleaned_text = self.docs_clean(doc.page_content)
                    chunks = self.chunks_split(cleaned_text)
                    # è®¡ç®—å“ˆå¸Œå€¼è¿›è¡Œé‡å¤æ£€æµ‹
                    for chunk in chunks:
                        h = hashlib.md5(chunk.encode('utf-8')).hexdigest()
                        if h in self.hash_set:
                            print(f"Duplicate chunk detected in {file_path}, skipping entire document.")
                            raise ValueError("Duplicate document")
                        document_chunks.append(chunk)
                        chunk_hashes.append(h)

                # å°†æœ‰æ•ˆå—æ·»åŠ åˆ°ç´¢å¼•ç³»ç»Ÿ
                if document_chunks:
                    vectors = self.chunks_tokenize(document_chunks)
                    # åˆå§‹åŒ–ç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                    if self.index is None:
                        dim = vectors.shape[1]
                        self.index = faiss.IndexFlatL2(dim)
                    self.index.add(vectors.astype('float32'))

                    # ä¿å­˜å…ƒæ•°æ®å’Œå“ˆå¸Œå€¼
                    for i, chunk in enumerate(document_chunks):
                        self.metadata_list.append({
                            "file": str(file_path.relative_to(self.docs_path)),
                            "chunk_idx": i,
                            "content_preview": chunk[:40]
                        })
                        self.hash_set.add(chunk_hashes[i])
                    processed += 1

            except Exception as e:
                print(f"Error processing {file_path}: {str(e)}")
                continue

        # å¤„ç†å®ŒæˆåæŒä¹…åŒ–å­˜å‚¨
        if self.index is not None:
            faiss.write_index(self.index, str(self.db_path))
            pickle.dump(self.metadata_list, open(self.meta_path, "wb"))
            pickle.dump(self.hash_set, open(self.hash_path, "wb"))
            print(f"All done! Processed {processed} documents. Index size: {self.index.ntotal}.")
        else:
            print("No documents processed!")
```

## äºŒã€å‘é‡æ£€ç´¢
ä¸“ä¸šæœ¯è¯­ç¼©å†™é—®é¢˜å¤„ç†

## ä¸‰ã€Promptæ„å»º

## å››ã€LLMå¾®è°ƒ

## äº”ã€æ€§èƒ½è¯„ä¼°

## é‡åˆ°çš„ä¸€äº›é—®é¢˜ï¼Œä»¥åŠè§£å†³æ–¹æ¡ˆ
1. æµ‹è¯•é›†çš„æ„å»ºï¼Œäººå·¥è¿˜æ˜¯è‡ªåŠ¨ï¼Œè‡ªåŠ¨çš„è¯ï¼Œè°ƒAPIï¼ŒæŠŠchunkå–‚ç»™llmï¼Œè®©å®ƒç”Ÿæˆå¯¹åº”çš„æé—®å’Œå›ç­”
2. ä¸“ä¸šæœ¯è¯­å¤„ç†ï¼Œä¸€äº›ä¹¦é‡Œé¢æ²¡æœ‰ä¸“ä¸šæœ¯è¯­çš„å…¨ç§°è¯´æ˜ï¼Œllmæ²¡æœ‰è¿™æ–¹é¢çŸ¥è¯†ï¼Œä¼šä¹±ç¼–ã€‚è§£å†³æ–¹æ¡ˆå°±æ˜¯æŠŠä¸“ä¸šæœ¯è¯­è¿›è¡Œæ‰©å±•

