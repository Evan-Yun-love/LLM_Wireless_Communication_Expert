{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ecaaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "\n",
    "client = ZhipuAI(api_key=\"")\n",
    "\n",
    "    \n",
    "def qa_generator_llm(context: str, client: ZhipuAI, model: str = \"GLM-4-Flash-250414\"):\n",
    "    \n",
    "    genration_prompt = \"\"\"\n",
    "    Your task is to write a factoid question and an answer given a context.\n",
    "    Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "    Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "    This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "    The answer should be vivid and no less than 20 words. \n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Output:::\n",
    "    Factoid question: (your factoid question)\n",
    "    Answer: (your answer to the factoid question)\n",
    "\n",
    "    Now here is the context.\n",
    "\n",
    "    Context: {context}\\n\n",
    "    Output:::\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a question-answer pair generator.\"},\n",
    "            {\"role\": \"user\", \"content\": genration_prompt.format(context = context)},\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        top_p = 0.9,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f75cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunk': 'Communication systems Input message sequence m Encoded output c c(0) c(1) t(1) c(2) t(2)  RSC encoder 1 RSC encoder 2 n Noisy channel output r Estimate of message vector m Decoder 1 Extrinsic information 1  -1  La,1 Lp(r(0)) r(0) Lp(z(1)) z(1) Lp(r(1)) r(1) Decoder 2 Extrinsic information 2 La,2 Lp(z(2)) z(2) Lp(c(2)) r(2) (a) (b) Simon Haykin digital',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'ASSOCIATE PUBLISHER Daniel Sayre EDITORIAL ASSISTANT Jessica Knecht MARKETING MANAGER Christopher Ruel PRODUCTION MANAGEMENT SERVICES\\nPublishing Services CREATIVE DIRECTOR Harry Nolan COVER DESIGNER Kristine Carney\\nCover Image: The figure on the cover, depicting the UMTS-turbo code, is adapted from the doctoral thesis of Dr.\\nLiang Li, Department of Electronics and Computer Science, University of Southampton, United Kingdom, with\\nthe permission of Dr. Li, his Supervisor Dr. Robert Maunder, and Professor Lajos Hanzo; the figure also appears\\non of the book.\\nThis book was set in Times by Publishing Services and printed and bound by RRD Von Hoffmann. The cover\\nwas printed by RRD Von Hoffmann.\\nThis book is printed on acid free paper. \\nFounded in 1807, John Wiley & Sons, Inc. has been a valued source of knowledge and understanding for more\\nthan 200 years, helping people around the world meet their needs and fulfill their aspirations. Our company is\\nbuilt on a foundation of principles that include responsibility to the communities we serve and where we live and\\nwork. In 2008, we launched a Corporate Citizenship Initiative, a global effort to address the environmental,\\nsocial, economic, and ethical challenges we face in our business. Among the issues we are addressing are carbon\\nimpact, paper specifications and procurement, ethical conduct within our business and among our vendors, and\\ncommunity and charitable support. For more information, please visit our website: \\ncitizenship.\\nCopyright  2014 John Wiley & Sons, Inc. All rights reserved.\\nNo part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any\\nmeans, electronic, mechanical, photocopying, recording, scanning or otherwise, except as permitted under\\nSection 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission of the\\nPublisher or authorization through payment of the appropriate per-copy fee to the Copyright Clearance Center,',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 4,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Publisher or authorization through payment of the appropriate per-copy fee to the Copyright Clearance Center,\\nInc., 222 Rosewood Drive, Danvers, MA 01923, website  Requests to the Publisher for\\npermission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street,\\nHoboken, NJ 07030-5774, (201) 748-6011, , website \\nEvaluation copies are provided to qualified academics and professionals for review purposes only, for use in their\\ncourses during the next academic year. These copies are licensed and may not be sold or transferred to a third\\nparty. Upon completion of the review period, please return the evaluation copy to Wiley. Return instructions and\\na free of charge return mailing label are available at  If you have chosen to adopt\\nthis textbook for use in your course, please accept this book as your complimentary desk copy. Outside of the\\nUnited States, please contact your local sales representative.\\nISBN: 978-0-471-64735-5\\nPrinted in the United States of America\\n10 9 8 7 6 5 4 3 2',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 4,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'v\\nPreface\\nThe study of digital communications is an essential element of the undergraduate and\\npostgraduate levels of present-day electrical and computer engineering programs. This\\nbook is appropriate for both levels.\\nA Tour of the Book\\nThe introductory chapter is motivational, beginning with a brief history of digital\\ncommunications, and continuing with sections on the communication process, digital\\ncommunications, multiple-access and multiplexing techniques, and the Internet. Four\\nthemes organize the remaining nine chapters of the book.\\nTheme\\nMathematics of Digital Communications\\nThe first theme of the book provides a detailed expos of the mathematical underpinnings\\nof digital communications, with continuous mathematics aimed at the communication\\nchannel and interfering signals, and discrete mathematics aimed at the transmitter and\\nreceiver:\\n\\nChapter 2, Fourier Analysis of Signals and Systems, lays down the fundamentals for\\nthe representation of signals and linear time-invariant systems, as well as analog\\nmodulation theory.\\n\\nChapter 3, Probability Theory and Bayesian Inference, presents the underlying\\nmathematics for dealing with uncertainty and the Bayesian paradigm for\\nprobabilistic reasoning.\\n\\nChapter 4, Stochastic Processes, focuses on weakly or wide-sense stationary\\nprocesses, their statistical properties, and their roles in formulating models for\\nPoisson, Gaussian, Rayleigh, and Rician distributions.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 7,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter 5, Information Theory, presents the notions of entropy and mutual\\ninformation for discrete as well continuous random variables, leading to Shannons\\ncelebrated theorems on source coding, channel coding, and information capacity, as\\nwell as rate-distortion theory.\\nTheme\\nFrom Analog to Digital Communications\\nThe second theme of the book, covered in Chapter 6, describes how analog waveforms are\\ntransformed into coded pulses. It addresses the challenge of performing the transformation\\nwith robustness, bandwidth preservation, or minimal computational complexity.\\nTheme\\nSignaling Techniques\\nThree chapters address the third theme, each focusing on a specific form of channel\\nimpairment:\\n\\nIn Chapter 7, Signaling over Additive White Gaussian Noise (AWGN) Channels, the\\nimpairment is the unavoidable presence of channel noise, which is modeled as',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 7,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'vi\\nPreface\\nadditive white Gaussian noise (AWGN). This model is well-suited for the signal-\\nspace diagram, which brings insight into the study of phase-shift keying (PSK),\\nquadrature-amplitude modulation (QAM), and frequency-shift keying (FSK) as\\ndifferent ways of accommodating the transmission and reception of binary data.\\n\\nIn Chapter 8, Signaling over Band-Limited Channels, bandwidth limitation assumes\\ncenter stage, with intersymbol interference (ISI) as the source of channel impairment.\\n\\nChapter 9, Signaling over Fading Channels, focuses on fading channels in wireless\\ncommunications and the practical challenges they present. The channel impairment\\nhere is attributed to the multipath phenomenon, so called because the transmitted\\nsignal reaches the receiver via a multiplicity of paths.\\nTheme\\nError-control Coding\\nChapter 10 addresses the practical issue of reliable communications. To this end, various\\ntechniques of the feedforward variety are derived therein, so as to satisfy Shannons\\ncelebrated coding theorem.\\nTwo families of error-correcting codes are studied in the chapter:\\n\\nLegacy (classic) codes, which embody linear block codes, cyclic codes, and\\nconvolutional codes. Although different in their structural compositions, they look\\nto algebraic mathematics as the procedure for approaching the Shannon limit.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 8,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Probabilistic compound codes, which embody turbo codes and low-density parity-\\ncheck (LDPC) codes. What is remarkable about these two codes is that they both\\napproach the Shannon limit with doable computational complexity in a way that was\\nnot feasible until 1993. The trick behind this powerful information-processing\\ncapability is the adoption of random codes, the origin of which could be traced to\\nShannons 1948 classic paper.\\nFeatures of the Book\\nFeature\\nAnalog in Digital Communication\\nWhen we think of digital communications, we must not overlook the fact that such a\\nsystem is of a hybrid nature. The channel across which data are transmitted is analog,\\nexemplified by traditional telephone and wireless channels, and many of the sources\\nresponsible for the generation of data (e.g., speech and video) are of an analog kind.\\nMoreover, certain principles of analog modulation theory, namely double sideband-\\nsuppressed carrier (DSB-SC) and vestigial sideband (VSB) modulation schemes, include\\nbinary phase-shift keying (PSK) and offset QPSK as special cases, respectively.\\nIt is with these points in mind that Chapter 2 includes\\n\\ndetailed discussion of communication channels as examples of linear systems,\\n\\nanalog modulation theory, and\\n\\nphase and group delays.\\nFeature\\nHilbert Transform\\nThe Hilbert transform, discussed in Chapter 2, plays a key role in the complex\\nrepresentation of signals and systems, whereby\\n\\na band-pass signal, formulated around a sinusoidal carrier, is transformed into an\\nequivalent complex low-pass signal;',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 8,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Preface vii  a band-pass system, be it a linear channel or filter with a midband frequency, is\\ntransformed into an equivalent complex low-pass system.\\nBoth transformations are performed without loss of information, and their use changes a\\ndifficult task into a much simpler one in mathematical terms, suitable for simulation on a\\ncomputer. However, one must accommodate the use of complex variables.\\nThe Hilbert transform also plays a key role in Chapter 7. In formulating the method of\\northogonal modulation, we show that one can derive the well-known formulas for the\\nnoncoherent detection of binary frequency-shift keying (FSK) and differential phase-shift\\nkeying (DPSK) signals, given unknown phase, in a much simpler manner than following\\ntraditional approaches that involve the use of Rician distribution.\\nFeature\\nDiscrete-time Signal Processing\\nIn Chapter 2, we briefly review finite-direction impulse response (FIR) or tapped-delay\\nline (TDL) filters, followed by the discrete Fourier transform (DFT) and a well-known fast\\nFourier transform (FFT) algorithm for its computational implementations. FIR filters and\\nFFT algorithms feature prominently in:\\n\\nModeling of the raised-cosine spectrum (RCS) and its square-root version\\n(SQRCS), which are used in Chapter 8 to mitigate the ISI in band-limited channels;\\n\\nImplementing the Jakes model for fast fading channels, demonstrated in Chapter 9;\\n\\nUsing FIR filtering to simplify the mathematical exposition of the most difficult\\nform of channel fading, namely, the doubly spread channel (in Chapter 9).\\nAnother topic of importance in discrete-time signal processing is linear adaptive filtering,\\nwhich appears:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 9,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'In Chapter 6, dealing with differential pulse-code modulation (DPCM), where an\\nadaptive predictor constitutes a key functional block in both the transmitter and\\nreceiver. The motivation here is to preserve channel bandwidth at the expense of\\nincreased computational complexity. The algorithm described therein is the widely\\nused least mean-square (LMS) algorithm.\\n\\nIn Chapter 7, dealing with the need for synchronizing the receiver to the transmitter,\\nwhere two algorithms are described, one for recursive estimation of the group delay\\n(essential for timing recovery) and the other for recursive estimation of the unknown\\ncarrier phase (essential for carrier recovery). Both algorithms build on the LMS\\nprinciple so as to maintain linear computational complexity.\\nFeature\\nDigital Subscriber Lines\\nDigital subscriber lines (DSLs), covered in Chapter 8, have established themselves as an\\nessential tool for transforming a linear wideband channel, exemplified by the twisted-wire\\npair, into a discrete multitone (DMT) channel that is capable of accommodating data\\ntransmission at multiple megabits per second. Moreover, the transformation is afforded\\npractical reality by exploiting the FFT algorithm, with the inverse FFT used in the\\ntransmitter and the FFT used in the receiver.\\nFeature\\nDiversity Techniques\\nAs already mentioned, the wireless channel is one of the most challenging media for\\ndigital communications. The difficulty of reliable data transmission over a wireless',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 9,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'viii\\nPreface\\nchannel is attributed to the multipath phenomenon. Three diversity techniques developed\\nto get around this practical difficulty are covered in Chapter 9:\\n\\nDiversity on receive, the traditional approach, whereby an array of multiple antennas\\noperating independently is deployed at the receiving end of a wireless channel.\\n\\nDiversity on transmit, which operates by deploying two or more independent\\nantennas at the transmit end of the wireless channel.\\n\\nMultiple-input multiple-output (MIMO) channels, where multiple antennas (again\\noperating independently) are deployed at both ends of the wireless channel.\\nAmong these three forms of diversity, the MIMO channel is naturally the most powerful in\\ninformation-theoretic terms: an advantage gained at the expense of increased\\ncomputational complexity.\\nFeature\\nTurbo Codes\\nError-control coding has established itself as the most commonly used technique for\\nreliable data transmission over a noisy channel. Among the challenging legacies bestowed\\nby Claude Shannon was how to design a code that would closely approach the so-called\\nShannon limit. For over four decades, increasingly more powerful coding algorithms were\\ndescribed in the literature; however it was the turbo code that had the honor of closely\\napproaching the Shannon limit, and doing so in a computationally feasible manner.\\nTurbo codes, together with the associated maximum a posteriori (MAP) decoding\\nalgorithm, occupy a large portion of Chapter 10, which also includes:\\n\\nDetailed derivation of the MAP algorithm and an illustrative example of how it\\noperates;\\n\\nThe extrinsic information transfer (EXIT) chart, which provides an experimental\\ntool for the design of turbo codes;',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 10,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The extrinsic information transfer (EXIT) chart, which provides an experimental\\ntool for the design of turbo codes;\\n\\nTurbo equalization, for demonstrating applicability of the turbo principle beyond\\nerror-control coding.\\nFeature\\nPlacement of Information Theory\\nTypically, information theory is placed just before the chapter on error-control coding. In\\nthis book, it is introduced early because:\\nInformation theory is not only of basic importance to error-control coding but\\nalso other topics in digital communications.\\nTo elaborate:\\n\\nChapter 6 presents the relevance of source coding to pulse-code modulation (PCM),\\ndifferential pulse-code modulation (DPCM), and delta modulation.\\n\\nComparative evaluation of M-ary PSK versus M-ary FSK, done in Chapter 7,\\nrequires knowledge of Shannons information capacity law.\\n\\nAnalysis and design of DSL, presented in Chapter 8, also builds on Shannons\\ninformation capacity law.\\n\\nChannel capacity in Shannons coding theorem is important to diversity techniques,\\nparticularly of the MIMO kind, discussed in Chapter 9.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 10,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Preface\\nix\\nExamples, Computer Experiments, and Problems\\nExcept for Chapter 1, each of the remaining nine chapters offers the following:\\n\\nIllustrative examples are included to strengthen the understanding of a theorem or\\ntopic in as much detail as possible. Some of the examples are in the form of\\ncomputer experiments.\\n\\nAn extensive list of end-of-chapter problems are grouped by section to fit the\\nmaterial covered in each chapter. The problems range from relatively easy ones all\\nthe way to more challenging ones.\\n\\nIn addition to the computer-oriented examples, nine computer-oriented experiments\\nare included in the end-of-chapter problems.\\nThe Matlab codes for all of the computer-oriented examples in the text, as well as other\\ncalculations performed on the computer, are available at \\nAppendices\\nEleven appendices broaden the scope of the theoretical as well as practical material\\ncovered in the book:\\n\\nAppendix A, Advanced Probabilistic Models, covers the chi-square distribution,\\nlog-normal distribution, and Nakagami distribution that includes the Rayleigh\\ndistribution as a special case and is somewhat similar to the Rician distribution.\\nMoreover, an experiment is included therein that demonstrates, in a step-by-step\\nmanner, how the Nakagami distribution evolves into the log-normal distribution in\\nan approximate manner, demonstrating its adaptive capability.\\n\\nAppendix B develops tight bounds on the Q-function.\\n\\nAppendix C discussed the ordinary Bessel function and its modified form.\\n\\nAppendix D describes the method of Lagrange multipliers for solving constrained\\noptimization problems.\\n\\nAppendix E derives the formula for the channel capacity of the MIMO channel\\nunder two scenarios: one that assumes no knowledge of the channel by the\\ntransmitter, and the other that assumes this knowledge is available to the transmitter\\nvia a narrowband feedback link.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 11,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Appendix F discusses the idea of interleaving, which is needed for dealing with\\nbursts of interfering signals experienced in wireless communications.\\n\\nAppendix G addresses the peak-to-average power reduction (PAPR) problem,\\nwhich arises in the use of orthogonal frequency-division multiplexing (OFDM) for\\nboth wireless and DSL applications.\\n\\nAppendix H discusses solid-state nonlinear power amplifiers, which play a critical\\nrole in the limited life of batteries in wireless communications.\\n\\nAppendix I presents a short expos of Monte Carlo integration: a theorem that deals\\nwith mathematically intractable problems.\\n\\nAppendix J studies maximal-length sequences, also called m-sequences, which are\\nused for implementing linear feedback shift registers (LFSRs). An important\\napplication of maximal-length sequences (viewed as pseudo-random noise) is in',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 11,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'x\\nPreface\\ndesigning direct-sequence spread-spectrum communications for code-division\\nmultiple access (CDMA).\\n\\nFinally, Appendix K provides a useful list of mathematical formulas and functions.\\nTwo Noteworthy Symbols\\nTypically, the square-root of minus one is denoted by the italic symbol j, and the\\ndifferential operator (used in differentiation as well as integration) is denoted by the italic\\nsymbol d. In reality, however, both of these terms are operators, each one in its own way:\\nit is therefore incorrect to use italic symbols for their notations. Furthermore, italic j and\\nitalic d are also frequently used as indices or to represent other matters, thereby raising the\\npotential for confusion. According, throughout the book, roman j and roman d are used to\\ndenote the square root of minus one and the differential operator, respectively.\\nConcluding Remarks\\nIn writing this book every effort has been made to present the material in the manner\\neasiest to read so as to enhance understanding of the topics covered. Moreover, cross-\\nreferences within a chapter as well as from chapter to chapter have been included\\nwherever the need calls for it.\\nFinally, every effort has been made by the author as well as compositor of the book to\\nmake it as error-free as humanly possible. In this context, the author would welcome\\nreceiving notice of any errors discovered after publication of the book.\\nAcknowledgements\\nIn writing this book I have benefited enormously from technical input, persistent support,\\nand permissions provided by many.\\nI am grateful to colleagues around the world for technical inputs that have made a\\nsignificant difference in the book; in alphabetical order, they are:\\n\\nDr. Daniel Costello, Jr., University of Notre Dame, for reading and providing useful\\ncomments on the maximum likelihood decoding and maximum a posteriori\\ndecoding materials in Chapter 10.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 12,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Dr. Daniel Costello, Jr., University of Notre Dame, for reading and providing useful\\ncomments on the maximum likelihood decoding and maximum a posteriori\\ndecoding materials in Chapter 10.\\n\\nDr. Dimitri Bertsekas, MIT, for permission to use Table 3.1 on the Q-function in\\nChapter 3, taken from his co-authored book on the theory of probability.\\n\\nDr. Lajos Hanzo, University of Southampton, UK, for many useful comments on\\nturbo codes as well as low-density parity-check codes in Chapter 10. I am also\\nindebted to him for putting me in touch with his colleagues at the University of\\nSouthampton, Dr. R. G. Maunder and Dr. L. Li, who were extremely helpfully in\\nperforming the insightful computer experiments on UMTS-turbo codes and EXIT\\ncharts in Chapter 10.\\n\\nDr. Phillip Regalia, Catholic University, Washington DC, for contributing a section\\non serial-concatenated turbo codes in Chapter 10. This section has been edited by\\nmyself to follow the books writing style, and for its inclusion I take full\\nresponsibility.\\n\\nDr. Sam Shanmugan, University of Kansas, for his insightful inputs on the use of\\nFIR filters and FFT algorithms for modeling the raised-cosine spectrum (RCS) and',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 12,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Preface\\nxi\\nits square-root version (SQRCS) in Chapter 8, implementing the Jakes model in\\nChapter 9, as well as other simulation-oriented issues.\\n\\nDr. Yanbo Xue, University of Alberta, Canada, for performing computer-oriented\\nexperiments and many other graphical computations throughout the book, using\\nwell-developed Matlab codes.\\n\\nDr. Q. T. Zhang, The City University of Hong Kong, for reading through an early\\nversion of the manuscript and offering many valuable suggestions for improving it. I\\nam also grateful to his student, Jiayi Chen, for performing the graphical\\ncomputations on the Nakagami distribution in Appendix A.\\nId also like to thank the reviewers who read drafts of the manuscript and provided\\nvaluable commentary:\\n\\nEnder Ayanoglu, University of California, Irvine\\n\\nTolga M. Duman, Arizona State University\\n\\nBruce A. Harvey, Florida State University\\n\\nBing W. Kwan, FAMU-FSU College of Engineering\\n\\nChung-Chieh Lee, Northwestern University\\n\\nHeung-No Lee, University of Pittsburgh\\n\\nMichael Rice, Brigham Young University\\n\\nJames Ritcey, University of Washington\\n\\nLei Wei, University of Central Florida\\nProduction of the book would not have been possible without the following:\\n\\nDaniel Sayre, Associate Publisher at John Wiley & Sons, who maintained not only\\nhis faith in this book but also provided sustained support for it over the past few\\nyears. In am deeply indebted to Dan for what he has done to make this book a\\nreality.\\n\\nCindy Johnson, Publishing Services, Newburyport, MA, for her dedicated\\ncommitment to the beautiful layout and composition of the book. I am grateful for\\nher tireless efforts to print the book in as errorless manner as humanly possible.\\nI salute everyone, and others too many to list, for their individual and collective\\ncontributions, without which this book would not have been a reality.\\nSimon Haykin Ancaster, Ontario Canada December,',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 13,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'xiii Contents 1 Introduction 1 1.1 Historical Background The Communication Process Multiple-Access Techniques\\n4 1.4 Networks 6 1.5 Digital Communications Organization of the Book Fourier Analysis of Signals and Systems\\n13 2.1 Introduction 13 2.2 The Fourier Series 13 2.3 The Fourier Transform The Inverse Relationship between Time-Domain and Frequency-Domain\\nRepresentations 25 2.5 The Dirac Delta Function Fourier Transforms of Periodic Signals Transmission of Signals through Linear Time-Invariant Systems\\n37 2.8 Hilbert Transform 42 2.9 Pre-envelopes 45 2.10 Complex Envelopes of Band-Pass Signals\\n11 Canonical Representation of Band-Pass Signals\\n12 Complex Low-Pass Representations of Band-Pass Systems\\n13 Putting the Complex Representations of Band-Pass Signals and Systems\\nAll Together\\n14 Linear Modulation Theory\\n15 Phase and Group Delays\\n16 Numerical Computation of the Fourier Transform\\n17 Summary and Discussion Probability Theory and Bayesian Inference\\n87 3.1 Introduction 87 3.2 Set Theory 88 3.3 Probability Theory 90 3.4 Random Variables 97 3.5 Distribution Functions The Concept of Expectation',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 15,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'xiv Contents 3.7 Second-Order Statistical Averages Characteristic Function The Gaussian Distribution\\n10 The Central Limit Theorem\\n11 Bayesian Inference\\n12 Parameter Estimation\\n13 Hypothesis Testing\\n14 Composite Hypothesis Testing\\n15 Summary and Discussion\\n133 4 Stochastic Processes 145 4.1 Introduction 145 4.2 Mathematical Definition of a Stochastic Process Two Classes of Stochastic Processes: Strictly Stationary and\\nWeakly Stationary 147 4.4 Mean, Correlation, and Covariance Functions of Weakly Stationary Processes\\n149 4.5 Ergodic Processes 157 4.6 Transmission of a Weakly Stationary Process through a\\nLinear Time-invariant Filter Power Spectral Density of a Weakly Stationary Process Another Definition of the Power Spectral Density Cross-spectral Densities\\n10 The Poisson Process\\n11 The Gaussian Process\\n176 4.12 Noise 179 4.13 Narrowband Noise\\n14 Sine Wave Plus Narrowband Noise\\n15 Summary and Discussion\\n195 5 Information Theory 207 5.1 Introduction 207 5.2 Entropy 207 5.3 Source-coding Theorem Lossless Data Compression Algorithms Discrete Memoryless Channels\\n223 5.6 Mutual Information 226 5.7 Channel Capacity 230 5.8 Channel-coding Theorem Differential Entropy and Mutual Information for Continuous\\nRandom Ensembles',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 16,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'xv\\n10 Information Capacity Law\\n11 Implications of the Information Capacity Law\\n12 Information Capacity of Colored Noisy Channel\\n13 Rate Distortion Theory\\n14 Summary and Discussion Conversion of Analog Waveforms into Coded Pulses\\n267 6.1 Introduction 267 6.2 Sampling Theory 268 6.3 Pulse-Amplitude Modulation Quantization and its Statistical Characterization Pulse-Code Modulation Noise Considerations in PCM Systems Prediction-Error Filtering for Redundancy Reduction Differential Pulse-Code Modulation\\n301 6.9 Delta Modulation 305 6.10 Line Codes 309 6.11 Summary and Discussion Signaling over AWGN Channels\\n323 7.1 Introduction 323 7.2 Geometric Representation of Signals Conversion of the Continuous AWGN Channel into a\\nVector Channel 332 7.4 Optimum Receivers Using Coherent Detection\\n337 7.5 Probability of Error 344 7.6 Phase-Shift Keying Techniques Using Coherent Detection M-ary Quadrature Amplitude Modulation Frequency-Shift Keying Techniques Using Coherent Detection Comparison of M-ary PSK and M-ary FSK from an\\nInformation-Theoretic Viewpoint\\n10 Detection of Signals with Unknown Phase\\n11 Noncoherent Orthogonal Modulation Techniques\\n12 Binary Frequency-Shift Keying Using Noncoherent Detection\\n13 Differential Phase-Shift Keying\\n14 BER Comparison of Signaling Schemes over AWGN Channels\\n415 7.15 Synchronization 418 7.16 Recursive Maximum Likelihood Estimation for Synchronization\\n17 Summary and Discussion',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 17,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'xvi Contents 8 Signaling over Band-Limited Channels\\n445 8.1 Introduction 445 8.2 Error Rate Due to Channel Noise in a Matched-Filter Receiver Intersymbol Interference Signal Design for Zero ISI Ideal Nyquist Pulse for Distortionless Baseband\\nData Transmission 450 8.6 Raised-Cosine Spectrum Square-Root Raised-Cosine Spectrum Post-Processing Techniques: The Eye Pattern Adaptive Equalization\\n10 Broadband Backbone Data Network: Signaling over Multiple\\nBaseband Channels\\n11 Digital Subscriber Lines\\n12 Capacity of AWGN Channel Revisited\\n13 Partitioning Continuous-Time Channel into a Set\\nof Subchannels\\n14 Water-Filling Interpretation of the Constrained\\nOptimization Problem\\n15 DMT System Using Discrete Fourier Transform\\n16 Summary and Discussion Signaling over Fading Channels\\n501 9.1 Introduction 501 9.2 Propagation Effects 502 9.3 Jakes Model 506 9.4 Statistical Characterization of Wideband Wireless Channels FIR Modeling of Doubly Spread Channels Comparison of Modulation Schemes: Effects of Flat Fading\\n525 9.7 Diversity Techniques 527 9.8 Space Diversity-on-Receive Systems Space Diversity-on-Transmit Systems\\n10 Multiple-Input, Multiple-Output Systems: Basic Considerations\\n11 MIMO Capacity for Channel Known at the Receiver\\n12 Orthogonal Frequency Division Multiplexing\\n13 Spread Spectrum Signals\\n14 Code-Division Multiple Access\\n15 The RAKE Receiver and Multipath Diversity\\n16 Summary and Discussion',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 18,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Contents xvii 10 Error-Control Coding 577 10.1 Introduction 577 10.2 Error Control Using Forward Error Correction Discrete Memoryless Channels\\n579 10.4 Linear Block Codes 582 10.5 Cyclic Codes 593 10.6 Convolutional Codes 605 10.7 Optimum Decoding of Convolutional Codes Maximum Likelihood Decoding of Convolutional Codes Maximum a Posteriori Probability Decoding of\\nConvolutional Codes\\n10 Illustrative Procedure for Map Decoding in the Log-Domain\\n11 New Generation of Probabilistic Compound Codes\\n644 10.12 Turbo Codes 645 10.13 EXIT Charts 657 10.14 Low-Density Parity-Check Codes\\n15 Trellis-Coded Modulation\\n16 Turbo Decoding of Serial Concatenated Codes\\n17 Summary and Discussion\\n688 Appendices A Advanced Probabilistic Models\\nA1\\nA.1 The Chi-Square Distribution\\nA1\\nA.2 The Log-Normal Distribution\\nA3\\nA.3 The Nakagami Distribution\\nA6\\nB\\nBounds on the Q-Function\\nA11 C Bessel Functions A13 C.1 Series Solution of Bessels Equation\\nA13\\nC.2 Properties of the Bessel Function\\nA14\\nC.3 Modified Bessel Function\\nA16\\nD\\nMethod of Lagrange Multipliers\\nA19\\nD.1 Optimization Involving a Single Equality Constraint\\nA19\\nE\\nInformation Capacity of MIMO Channels\\nA21\\nE.1 Log-Det Capacity Formula of MIMO Channels\\nA21\\nE.2 MIMO Capacity for Channel Known at the Transmitter\\nA24 F Interleaving A29 F.1 Block Interleaving A30 F.2 Convolutional Interleaving\\nA32 F.3 Random Interleaving A33',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 19,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'xviii Contents G The Peak-Power Reduction Problem in OFDM\\nA35\\nG.1 PAPR Properties of OFDM Signals\\nA35\\nG.2 Maximum PAPR in OFDM Using M-ary PSK\\nA36\\nG.3 Clipping-Filtering: A Technique for PAPR Reduction\\nA37\\nH\\nNonlinear Solid-State Power Amplifiers\\nA39\\nH.1 Power Amplifier Nonlinearities\\nA39\\nH.2 Nonlinear Modeling of Band-Pass Power Amplifiers\\nA42\\nI\\nMonte Carlo Integration\\nA45\\nJ\\nMaximal-Length Sequences\\nA47\\nJ.1 Properties of Maximal-Length Sequences\\nA47\\nJ.2 Choosing a Maximal-Length Sequence\\nA50 K Mathematical Tables A55 Glossary G1 Bibliography B1 Index I1 Credits C1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 20,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '1 CHAPTER 1 Introduction 1.1 Historical Background\\nIn order to provide a sense of motivation, this introductory treatment of digital\\ncommunications begins with a historical background of the subject, brief but succinct as it\\nmay be. In this first section of the introductory chapter we present some historical notes\\nthat identify the pioneering contributors to digital communications specifically, focusing\\non three important topics: information theory and coding, the Internet, and wireless\\ncommunications. In their individual ways, these three topics have impacted digital\\ncommunications in revolutionary ways.\\nInformation Theory and Coding\\nIn 1948, the theoretical foundations of digital communications were laid down by Claude\\nShannon in a paper entitled A mathematical theory of communication. Shannons paper\\nwas received with immediate and enthusiastic acclaim. It was perhaps this response that\\nemboldened Shannon to amend the title of his classic paper to The mathematical theory\\nof communication when it was reprinted later in a book co-authored with Warren Weaver.\\nIt is noteworthy that, prior to the publication of Shannons 1948 classic paper, it was\\nbelieved that increasing the rate of transmission over a channel would increase the\\nprobability of error; the communication theory community was taken by surprise when\\nShannon proved that this was not true, provided the transmission rate was below the\\nchannel capacity.\\nShannons 1948 paper was followed by three ground-breaking advances in coding\\ntheory, which include the following:\\nDevelopment of the first nontrivial error-correcting code by Golay in 1949 and\\nHamming in 1950.\\nDevelopment of turbo codes by Berrou, Glavieux and Thitimjshima in 1993; turbo\\ncodes provide near-optimum error-correcting coding and decoding performance in\\nadditive white Gaussian noise.\\nRediscovery of low-density parity-check (LDPC) codes, which were first described\\nby Gallager in 1962; the rediscovery occurred in 1981 when Tanner provided a new',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 21,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'additive white Gaussian noise.\\nRediscovery of low-density parity-check (LDPC) codes, which were first described\\nby Gallager in 1962; the rediscovery occurred in 1981 when Tanner provided a new\\ninterpretation of LDPC codes from a graphical perspective. Most importantly, it was\\nthe discovery of turbo codes in 1993 that reignited interest in LDPC codes.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 21,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '2 Chapter 1 Introduction The Internet From 1950 to 1970, various studies were made on computer networks. However, the most\\nsignificant of them all in terms of impact on computer communications was the Advanced\\nResearch Project Agency Network (ARPANET), which was put into service in 1971. The\\ndevelopment of ARPANET was sponsored by the Advanced Research Projects Agency\\n(ARPA) of the United States Department of Defense. The pioneering work in packet\\nswitching was done on the ARPANET. In 1985, ARPANET was renamed the Internet.\\nHowever, the turning point in the evolution of the Internet occurred in 1990 when Berners-\\nLee proposed a hypermedia software interface to the Internet, which he named the World\\nWide Web. Thereupon, in the space of only about 2 years, the Web went from nonexistence\\nto worldwide popularity, culminating in its commercialization in 1994. The Internet has\\ndramatically changed the way in which we communicate on a daily basis, using a\\nwirelined network.\\nWireless Communications\\nIn 1864, James Clerk Maxwell formulated the electromagnetic theory of light and\\npredicted the existence of radio waves; the set of four equations that connect electric and\\nmagnetic quantities bears his name. Later on in 1984, Henrich Herz demonstrated the\\nexistence of radio waves experimentally.\\nHowever, it was on December 12, 1901, that Guglielmo Marconi received a radio\\nsignal at Signal Hill in Newfoundland; the radio signal had originated in Cornwall,\\nEngland, 2100 miles away across the Atlantic. Last but by no means least, in the early\\ndays of wireless communications, it was Fessenden, a self-educated academic, who in\\n1906 made history by conducting the first radio broadcast, transmitting music and voice\\nusing a technique that came to be known as amplitude modulation (AM) radio.\\nIn 1988, the first digital cellular system was introduced in Europe; it was known as the\\nGlobal System for Mobile (GSM) Communications. Originally, GSM was intended to',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 22,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'In 1988, the first digital cellular system was introduced in Europe; it was known as the\\nGlobal System for Mobile (GSM) Communications. Originally, GSM was intended to\\nprovide a pan-European standard to replace the myriad of incompatible analog wireless\\ncommunication systems. The introduction of GSM was soon followed by the North\\nAmerican IS-54 digital standard. As with the Internet, wireless communication has also\\ndramatically changed the way we communicate on a daily basis.\\nWhat we have just described under the three headings, namely, information theory and\\ncoding, the Internet, and wireless communications, have collectively not only made\\ncommunications essentially digital, but have also changed the world of communications\\nand made it global. The Communication Process\\nToday, communication enters our daily lives in so many different ways that it is very easy\\nto overlook the multitude of its facets. The telephones as well as mobile smart phones and\\ndevices at our hands, the radios and televisions in our living rooms, the computer terminals\\nwith access to the Internet in our offices and homes, and our newspapers are all capable of\\nproviding rapid communications from every corner of the globe. Communication provides\\nthe senses for ships on the high seas, aircraft in flight, and rockets and satellites in space.\\nCommunication through a wireless telephone keeps a car driver in touch with the office or',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 22,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '2 The Communication Process home miles away, no matter where. Communication provides the means for social\\nnetworks to engage in different ways (texting, speaking, visualizing), whereby people are\\nbrought together around the world. Communication keeps a weather forecaster informed\\nof conditions measured by a multitude of sensors and satellites. Indeed, the list of\\napplications involving the use of communication in one way or another is almost endless.\\nIn the most fundamental sense, communication involves implicitly the transmission of\\ninformation from one point to another through a succession of processes:\\nThe generation of a message signal - voice, music, picture, or computer data.\\nThe description of that message signal with a certain measure of precision, using a\\nset of symbols - electrical, aural, or visual.\\nThe encoding of those symbols in a suitable form for transmission over a physical\\nmedium of interest.\\nThe transmission of the encoded symbols to the desired destination.\\nThe decoding and reproduction of the original symbols.\\nThe re-creation of the original message signal with some definable degradation in\\nquality, the degradation being caused by unavoidable imperfections in the system.\\nThere are, of course, many other forms of communication that do not directly involve the\\nhuman mind in real time. For example, in computer communications involving\\ncommunication between two or more computers, human decisions may enter only in\\nsetting up the programs or commands for the computer, or in monitoring the results.\\nIrrespective of the form of communication process being considered, there are three\\nbasic elements to every communication system, namely, transmitter, channel, and\\nreceiver, as depicted in Figure 1.1. The transmitter is located at one point in space, the\\nreceiver is located at some other point separate from the transmitter, and the channel is the\\nphysical medium that connects them together as an integrated communication system. The',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 23,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'receiver is located at some other point separate from the transmitter, and the channel is the\\nphysical medium that connects them together as an integrated communication system. The\\npurpose of the transmitter is to convert the message signal produced by the source of\\ninformation into a form suitable for transmission over the channel. However, as the\\ntransmitted signal propagates along the channel, it is distorted due to channel\\nimperfections. Moreover, noise and interfering signals (originating from other sources) are\\nadded to the channel output, with the result that the received signal is a corrupted version\\nof the transmitted signal. The receiver has the task of operating on the received signal so\\nas to reconstruct a recognizable form of the original message signal for an end user or\\ninformation sink.\\nFigure 1.1 Elements of a communication system.\\nCommunication System Estimate of message signal Transmitted signal Received signal Message signal Source of information Transmitter Channel Receiver User of information',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 23,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Chapter 1 Introduction There are two basic modes of communication:\\nBroadcasting, which involves the use of a single powerful transmitter and numerous\\nreceivers that are relatively inexpensive to build. Here, information-bearing signals\\nflow only in one direction.\\nPoint-to-point communication, in which the communication process takes place over\\na link between a single transmitter and a receiver. In this case, there is usually a\\nbidirectional flow of information-bearing signals, which requires the combined use\\nof a transmitter and receiver (i.e., a transceiver) at each end of the link.\\nThe underlying communication process in every communication system, irrespective of its\\nkind, is statistical in nature. Indeed, it is for this important reason that much of this book is\\ndevoted to the statistical underpinnings of digital communication systems. In so doing, we\\ndevelop a wealth of knowledge on the fundamental issues involved in the study of digital\\ncommunications. Multiple-Access Techniques\\nContinuing with the communication process, multiple-access is a technique whereby\\nmany subscribers or local stations can share the use of a communication channel at the\\nsame time or nearly so, despite the fact that their individual transmissions may originate\\nfrom widely different locations. Stated in another way, a multiple-access technique\\npermits the communication resources of the channel to be shared by a large number of\\nusers seeking to communicate with each other.\\nThere are subtle differences between multiple access and multiplexing that should be\\nnoted:\\n\\nMultiple access refers to the remote sharing of a communication channel such as a\\nsatellite or radio channel by users in highly dispersed locations. On the other hand,\\nmultiplexing refers to the sharing of a channel such as a telephone channel by users\\nconfined to a local site.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 24,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'In a multiplexed system, user requirements are ordinarily fixed. In contrast, in a\\nmultiple-access system user requirements can change dynamically with time, in\\nwhich case provisions are necessary for dynamic channel allocation.\\nFor obvious reasons it is desirable that in a multiple-access system the sharing of resources\\nof the channel be accomplished without causing serious interference between users of the\\nsystem. In this context, we may identify four basic types of multiple access:\\nFrequency-division multiple access (FDMA).\\nIn this technique, disjoint subbands of frequencies are allocated to the different users\\non a continuous-time basis. In order to reduce interference between users allocated\\nadjacent channel bands, guard bands are used to act as buffer zones, as illustrated in\\nFigure 1.2a. These guard bands are necessary because of the impossibility of\\nachieving ideal filtering or separating the different users.\\nTime-division multiple access (TDMA).\\nIn this second technique, each user is allocated the full spectral occupancy of the\\nchannel, but only for a short duration of time called a time slot. As shown in Figure\\n2b, buffer zones in the form of guard times are inserted between the assigned time',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 24,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '3 Multiple-Access Techniques slots. This is done to reduce interference between users by allowing for time\\nuncertainty that arises due to system imperfections, especially in synchronization\\nschemes.\\nCode-division multiple access (CDMA).\\nIn FDMA, the resources of the channel are shared by dividing them along the\\nfrequency coordinate into disjoint frequency bands, as illustrated in Figure 1.2a. In\\nTDMA, the resources are shared by dividing them along the time coordinate into\\ndisjoint time slots, as illustrated in Figure 1.2b. In Figure 1.2c, we illustrate another\\ntechnique for sharing the channel resources by using a hybrid combination of\\nFDMA and TDMA, which represents a specific form of code-division multiple\\naccess (CDMA). For example, frequency hopping may be employed to ensure that\\nduring each successive time slot, the frequency bands assigned to the users are\\nreordered in an essentially random manner. To be specific, during time slot 1, user\\noccupies frequency band 1, user 2 occupies frequency band 2, user 3 occupies\\nfrequency band 3, and so on. During time slot 2, user 1 hops to frequency band 3,\\nuser 2 hops to frequency band 1, user 3 hops to frequency band 2, and so on. Such an\\narrangement has the appearance of the users playing a game of musical chairs. An\\nimportant advantage of CDMA over both FDMA and TDMA is that it can provide\\nfor secure communications. In the type of CDMA illustrated in Figure 1.2c, the\\nfrequency hopping mechanism can be implemented through the use of a pseudo-\\nnoise (PN) sequence.\\nSpace-division multiple access (SDMA).\\nIn this multiple-access technique, resource allocation is achieved by exploiting the\\nspatial separation of the individual users. In particular, multibeam antennas are used\\nto separate radio signals by pointing them along different directions. Thus, different\\nusers are enabled to access the channel simultaneously on the same frequency or in\\nthe same time slot.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 25,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'to separate radio signals by pointing them along different directions. Thus, different\\nusers are enabled to access the channel simultaneously on the same frequency or in\\nthe same time slot.\\nThese multiple-access techniques share a common feature: allocating the communication\\nresources of the channel through the use of disjointedness (or orthogonality in a loose\\nsense) in time, frequency, or space.\\nFigure 1.2 Illustrating the ideas behind multiple-access techniques. (a) Frequency-division\\nmultiple access. (b) Time-division multiple access. (c) Frequency-hop multiple access.\\nFrequency Frequency Time Time Frequency Time Frequency band 2 Frequency band 1 Guard band Guard band Guard time Guard time User 1 User 2 User 2 User 3 User 1 User 1 User 2 User 3 (c) (b) (a) Frequency band 3 Time slot 1 Time slot 2 Time slot 3 User',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 25,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 Chapter 1 Introduction 1.4 Networks A communication network or simply network1, illustrated in Figure 1.3, consists of an\\ninterconnection of a number of nodes made up of intelligent processors (e.g.,\\nmicrocomputers). The primary purpose of these nodes is to route data through the\\nnetwork. Each node has one or more stations attached to it; stations refer to devices\\nwishing to communicate. The network is designed to serve as a shared resource for\\nmoving data exchanged between stations in an efficient manner and also to provide a\\nframework to support new applications and services. The traditional telephone network is\\nan example of a communication network in which circuit switching is used to provide a\\ndedicated communication path or circuit between two stations. The circuit consists of a\\nconnected sequence of links from source to destination. The links may consist of time\\nslots in a time-division multiplexed (TDM) system or frequency slots in a frequency-\\ndivision multiplexed (FDM) system. The circuit, once in place, remains uninterrupted for\\nthe entire duration of transmission. Circuit switching is usually controlled by a centralized\\nhierarchical control mechanism with knowledge of the networks organization. To\\nestablish a circuit-switched connection, an available path through the network is seized\\nand then dedicated to the exclusive use of the two stations wishing to communicate. In\\nparticular, a call-request signal must propagate all the way to the destination, and be\\nacknowledged, before transmission can begin. Then, the network is effectively transparent\\nto the users. This means that, during the connection time, the bandwidth and resources\\nallocated to the circuit are essentially owned by the two stations, until the circuit is\\ndisconnected. The circuit thus represents an efficient use of resources only to the extent\\nthat the allocated bandwidth is properly utilized. Although the telephone network is used',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 26,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'disconnected. The circuit thus represents an efficient use of resources only to the extent\\nthat the allocated bandwidth is properly utilized. Although the telephone network is used\\nto transmit data, voice constitutes the bulk of the networks traffic. Indeed, circuit\\nswitching is well suited to the transmission of voice signals, since voice conversations\\ntend to be of long duration (about 2 min on average) compared with the time required for\\nsetting up the circuit (about 0.1-0.5 s). Moreover, in most voice conversations, there is\\ninformation flow for a relatively large percentage of the connection time, which makes\\ncircuit switching all the more suitable for voice conversations.\\nFigure 1.3 Communication network.\\nBoundary of subnet Nodes Stations',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 26,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Networks In circuit switching, a communication link is shared between the different sessions\\nusing that link on a fixed allocation basis. In packet switching, on the other hand, the\\nsharing is done on a demand basis and, therefore, it has an advantage over circuit\\nswitching in that when a link has traffic to send, the link may be more fully utilized.\\nThe basic network principle of packet switching is store and forward. Specifically, in\\na packet-switched network, any message larger than a specified size is subdivided prior to\\ntransmission into segments not exceeding the specified size. The segments are commonly\\nreferred to as packets. The original message is reassembled at the destination on a packet-\\nby-packet basis. The network may be viewed as a distributed pool of network resources\\n(i.e., channel bandwidth, buffers, and switching processors) whose capacity is shared\\ndynamically by a community of competing users (stations) wishing to communicate. In\\ncontrast, in a circuit-switched network, resources are dedicated to a pair of stations for the\\nentire period they are in session. Accordingly, packet switching is far better suited to a\\ncomputer-communication environment in which bursts of data are exchanged between\\nstations on an occasional basis. The use of packet switching, however, requires that careful\\ncontrol be exercised on user demands; otherwise, the network may be seriously abused.\\nThe design of a data network (i.e., a network in which the stations are all made up of\\ncomputers and terminals) may proceed in an orderly way by looking at the network in\\nterms of a layered architecture, regarded as a hierarchy of nested layers. A layer refers to a\\nprocess or device inside a computer system, designed to perform a specific function.\\nNaturally, the designers of a layer will be intimately familiar with its internal details and\\noperation. At the system level, however, a user views the layer merely as a black box',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 27,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Naturally, the designers of a layer will be intimately familiar with its internal details and\\noperation. At the system level, however, a user views the layer merely as a black box\\nthat is described in terms of the inputs, the outputs, and the functional relationship\\nbetween outputs and inputs. In a layered architecture, each layer regards the next lower\\nlayer as one or more black boxes with some given functional specification to be used by\\nthe given higher layer. Thus, the highly complex communication problem in data networks\\nis resolved as a manageable set of well-defined interlocking functions. It is this line of\\nreasoning that has led to the development of the open systems interconnection (OSI)2\\nreference model by a subcommittee of the International Organization for Standardization.\\nThe term open refers to the ability of any two systems conforming to the reference\\nmodel and its associated standards to interconnect.\\nIn the OSI reference model, the communications and related-connection functions are\\norganized as a series of layers or levels with well-defined interfaces, and with each layer\\nbuilt on its predecessor. In particular, each layer performs a related subset of primitive\\nfunctions, and it relies on the next lower layer to perform additional primitive functions.\\nMoreover, each layer offers certain services to the next higher layer and shields the latter\\nfrom the implementation details of those services. Between each pair of layers, there is an\\ninterface. It is the interface that defines the services offered by the lower layer to the upper\\nlayer.\\nThe OSI model is composed of seven layers, as illustrated in Figure 1.4; this figure also\\nincludes a description of the functions of the individual layers of the model. Layer k on\\nsystem A, say, communicates with layer k on some other system B in accordance with a set\\nof rules and conventions, collectively constituting the layer k protocol, where k = 1, 2, ...,',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 27,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'system A, say, communicates with layer k on some other system B in accordance with a set\\nof rules and conventions, collectively constituting the layer k protocol, where k = 1, 2, ...,\\n(The term protocol has been borrowed from common usage, describing conventional\\nsocial behavior between human beings.) The entities that comprise the corresponding\\nlayers on different systems are referred to as peer processes. In other words,',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 27,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': '8 Chapter 1 Introduction Figure 1.4 OSI model; DLC stands for data link control.\\nLayer End-user X End-user Y Transmission of raw bits of data over a physical channel;\\nthis layer deals with the mechanical, electrical, functional,\\nand procedural requirements to access the channel.\\nError control for the reliable transfer of information\\nacross the channel.\\nRouting of packets through the network and flow control\\ndesigned to guarantee good performance over a\\ncommunication link found by the routing procedure.\\nEnd-to-end (i.e., source-to-destination) control of the\\nmessages exchanged between users.\\nProvision of the control structure for communication\\nbetween two cooperating users, and the orderly\\nmanagement of the dialogue between them.\\nTransformation of the input data to provide services\\nselected by the application layer; an example of data\\ntransformation is encryption to provide security.\\nProvision of access to the OSI environment for end-users.\\nLayer 2 protocol Layer 2 protocol Layer 3 protocol Layer 3 protocol Physical link Physical link System A System B Subnet node Layer 4 protocol Layer 5 protocol Layer 6 protocol Layer 7 protocol 7 6 5 4 3 2 1 Function Application Data link control Presentation Session Transport Network Physical Application Data link control Presentation Session Transport Network Physical Physical Physical DLC Network DLC',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 28,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '5 Digital Communications communication is achieved by having the peer processes in two different systems\\ncommunicate via a protocol, with the protocol itself being defined by a set of rules of\\nprocedure. Physical communication between peer processes exits only at layer 1. On the\\nother hand, layers 2 through 7 are in virtual communication with their distant peers.\\nHowever, each of these six layers can exchange data and control information with its\\nneighboring layers (below and above) through layer-to-layer interfaces. In Figure 1.4,\\nphysical communication is shown by solid lines and virtual communication by dashed\\nlines. The major principles involved in arriving at seven layers of the OSI reference model\\nare as follows:\\nEach layer performs well-defined functions.\\nA boundary is created at a point where the description of services offered is small\\nand the number of interactions across the boundary is the minimum possible.\\nA layer is created from easily localized functions, so that the architecture of the\\nmodel may permit modifications to the layer protocol to reflect changes in\\ntechnology without affecting the other layers.\\nA boundary is created at some point with an eye toward standardization of the\\nassociated interface.\\nA layer is created only when a different level of abstraction is needed to handle the data.\\nThe number of layers employed should be large enough to assign distinct functions to\\ndifferent layers, yet small enough to maintain a manageable architecture for the model.\\nNote that the OSI reference model is not a network architecture; rather, it is an\\ninternational standard for computer communications, which just tells what each layer\\nshould do. Digital Communications\\nTodays public communication networks are highly complicated systems. Specifically,\\npublic switched telephone networks (collectively referred to as PSTNs), the Internet, and\\nwireless communications (including satellite communications) provide seamless',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 29,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'public switched telephone networks (collectively referred to as PSTNs), the Internet, and\\nwireless communications (including satellite communications) provide seamless\\nconnections between cities, across oceans, and between different countries, languages, and\\ncultures; hence the reference to the world as a global village.\\nThere are three layers of the OSI model where it can affect the design of digital\\ncommunication systems, which is the subject of interest of this book:\\nPhysical layer. This lowest layer of the OSI model embodies the physical\\nmechanism involved in transmitting bits (i.e., binary digits) between any pair of\\nnodes in the communication network. Communication between the two nodes is\\naccomplished by means of modulation in the transmitter, transmission across the\\nchannel, and demodulation in the receiver. The module for performing modulation\\nand demodulation is often called a modem.\\nData-link layer. Communication links are nearly always corrupted by the\\nunavoidable presence of noise and interference. One purpose of the data-link layer,\\ntherefore, is to perform error correction or detection, although this function is also\\nshared with the physical layer. Often, the data-link layer will retransmit packets that\\nare received in error but, for some applications, it discards them. This layer is also',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 29,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '10 Chapter 1 Introduction responsible for the way in which different users share the transmission medium. A\\nportion of the data-link layer, called the medium access control (MAC) sublayer, is\\nresponsible for allowing frames to be sent over the shared transmission media\\nwithout undue interference with other nodes. This aspect is referred to as multiple-\\naccess communications.\\nNetwork layer. This layer has several functions, one of which is to determine the\\nrouting of information, to get it from the source to its ultimate destination. A second\\nfunction is to determine the quality of service. A third function is flow control, to\\nensure that the network does not become congested.\\nThese are three layers of a seven-layer model for the functions that occur in the\\ncommunications process. Although the three layers occupy a subspace within the OSI\\nmodel, the functions that they perform are of critical importance to the model.\\nBlock Diagram of Digital Communication System\\nTypically, in the design of a digital communication system the information source,\\ncommunication channel, and information sink (end user) are all specified. The challenge is\\nto design the transmitter and the receiver with the following guidelines in mind:\\n\\nEncode/modulate the message signal generated by the source of information,\\ntransmit it over the channel, and produce an estimate of it at the receiver output\\nthat satisfies the requirements of the end user.\\n\\nDo all of this at an affordable cost.\\nIn a digital communication system represented by the block diagram of Figure 1.6, the\\nrationale for which is rooted in information theory, the functional blocks of the transmitter\\nand the receiver starting from the far end of the channel are paired as follows:\\n\\nsource encoder-decoder;\\n\\nchannel encoder-decoder;',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 30,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'source encoder-decoder;\\n\\nchannel encoder-decoder;\\n\\nmodulator-demodulator.\\nThe source encoder removes redundant information from the message signal and is\\nresponsible for efficient use of the channel. The resulting sequence of symbols is called\\nthe source codeword. The data stream is processed next by the channel encoder, which\\nproduces a new sequence of symbols called the channel codeword. The channel codeword\\nis longer than the source code word by virtue of the controlled redundancy built into its\\nconstruction. Finally, the modulator represents each symbol of the channel codeword by a\\ncorresponding analog symbol, appropriately selected from a finite set of possible analog\\nsymbols. The sequence of analog symbols produced by the modulator is called a\\nwaveform, which is suitable for transmission over the channel. At the receiver, the channel\\noutput (received signal) is processed in reverse order to that in the transmitter, thereby\\nreconstructing a recognizable version of the original message signal. The reconstructed\\nmessage signal is finally delivered to the user of information at the destination. From this\\ndescription it is apparent that the design of a digital communication system is rather\\ncomplex in conceptual terms but easy to build. Moreover, the system is robust, offering\\ngreater tolerance of physical effects (e.g., temperature variations, aging, mechanical\\nvibrations) than its analog counterpart; hence the ever-increasing use of digital\\ncommunications.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 30,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 Organization of the Book Organization of the Book\\nThe main part of the book is organized in ten chapters, which, after this introductory\\nchapter, are organized into five parts of varying sizes as summarized herein.\\nMathematical Background\\nChapter 2 presents a detailed treatment of the Fourier transform, its properties and\\nalgorithmic implementations. This chapter also includes two important related topics:\\n\\nThe Hilbert transform, which provides the mathematical basis for transforming\\nreal-valued band-pass signals and systems into their low-pass equivalent\\nrepresentations without loss of information.\\n\\nOverview of analog modulation theory, thereby facilitating an insightful link\\nbetween analog and digital communications.\\nChapter 3 presents a mathematical review of probability theory and Bayesian\\ninference, the understanding of which is essential to the study of digital\\ncommunications.\\nChapter 4 is devoted to the study of stochastic processes, the theory of which is\\nbasic to the characterization of sources of information and communication channels.\\nChapter 5 discusses the fundamental limits of information theory, postulated in\\nterms of source coding, channel capacity, and rate-distortion theory.\\nFigure 1.6 Block diagram of a digital communication system.\\nSource of information Source encoder Channel encoder Modulator Source decoder Channel decoder Demodulator Channel Source codeword Channel codeword Waveform Received signal Estimate of source codeword Estimate of channel codeword User of information Estimate of message signal Message signal Receiver Transmitter',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 31,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '12 Chapter 1 Introduction 2. Transition from Analog to Digital Communications\\nThis material is covered in Chapter 6. Simply put, the study therein discusses the\\ndifferent ways in which analog waveforms are converted into digitally encoded\\nsequences.\\nSignaling Techniques\\nThis third part of the book includes three chapters:\\n\\nChapter 7 discusses the different techniques for signaling over additive white\\nGaussian noise (AWGN) channels.\\n\\nChapter 8 discusses signaling over band-limited channels, as in data transmission\\nover telephonic channels and the Internet.\\n\\nChapter 9 is devoted to signaling over fading channels, as in wireless\\ncommunications.\\nError-Control Coding\\nThe reliability of data transmission over a communication channel is of profound\\npractical importance. Chapter 10 studies the different methods for the encoding of\\nmessage sequences in the transmitter and decoding them in the receiver. Here, we\\ncover two classes of error-control coding techniques:\\n\\nclassic codes rooted in algebraic mathematics, and\\n\\nnew generation of probabilistic compound codes, exemplified by turbo codes and\\nLDPC codes.\\nAppendices\\nLast but by no means least, the book includes appendices to provide back-up\\nmaterial for different chapters in the book, as they are needed.\\nNotes\\nFor a detailed discussion on communication networks, see the classic book by Tanenbaum,\\nentitled Computer Networks (2003).\\nThe OSI reference model was developed by a subcommittee of the International Organization for\\nStandardization (ISO) in 1977. For a discussion of the principles involved in arriving at the seven\\nlayers of the OSI model and a description of the layers themselves, see Tanenbaum (2003).',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 32,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '13 CHAPTER 2 Fourier Analysis of Signals and Systems 2.1 Introduction The study of communication systems involves:\\n\\nthe processing of a modulated message signal generated at the transmitter output so\\nas to facilitate its transportation across a physical channel and',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 33,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the processing of a modulated message signal generated at the transmitter output so\\nas to facilitate its transportation across a physical channel and\\n\\nsubsequent processing of the received signal in the receiver so as to deliver an\\nestimate of the original message signal to a user at the receiver output.\\nIn this study, the representation of signals and systems features prominently. More\\nspecifically, the Fourier transform plays a key role in this representation.\\nThe Fourier transform provides the mathematical link between the time-domain\\nrepresentation (i.e., waveform) of a signal and its frequency-domain description (i.e.,\\nspectrum). Most importantly, we can go back and forth between these two descriptions of\\nthe signal with no loss of information. Indeed, we may invoke a similar transformation in\\nthe representation of linear systems. In this latter case, the time-domain and frequency-\\ndomain descriptions of a linear time-invariant system are defined in terms of its impulse\\nresponse and frequency response, respectively.\\nIn light of this background, it is in order that we begin a mathematical study of\\ncommunication systems by presenting a review of Fourier analysis. This review, in turn,\\npaves the way for the formulation of simplified representations of band-pass signals and\\nsystems to which we resort in subsequent chapters. We begin the study by developing the\\ntransition from the Fourier series representation of a periodic signal to the Fourier\\ntransform representation of a nonperiodic signal; this we do in the next two sections.\\n2 The Fourier Series Let denote a periodic signal, where the subscript T0 denotes the duration of\\nperiodicity. By using a Fourier series expansion of this signal, we are able to resolve it into\\nan infinite sum of sine and cosine terms, as shown by\\n(2.1) gT0 t( ) gT0 t( ) a0 2 an 2nf0t ( ) bn 2nf0t ( ) sin + cos [ ] n 1 =   + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 33,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nwhere\\n(2.2)\\nis the fundamental frequency. The coefficients an and bn represent the amplitudes of the\\ncosine and sine terms, respectively. The quantity nf0 represents the nth harmonic of the\\nfundamental frequency f0. Each of the terms cos(2nf0t) and sin(2nf0t) is called a basis\\nfunction. These basis functions form an orthogonal set over the interval T0, in that they\\nsatisfy three conditions:\\n(2.3) (2.4) (2.5) To determine the coefficient a0, we integrate both sides of (2.1) over a complete period.\\nWe thus find that a0 is the mean value of the periodic signal over one period, as\\nshown by the time average\\n(2.6)\\nTo determine the coefficient an, we multiply both sides of (2.1) by cos(2nf0t) and\\nintegrate over the interval -T0/2 to T0/2. Then, using (2.3) and (2.4), we find that\\n(2.7)\\nSimilarly, we find that (2.8)\\nA basic question that arises at this point is the following:\\nGiven a periodic signal of period T0, how do we know that the Fourier\\nseries expansion of (2.1) is convergent in that the infinite sum of terms in this\\nexpansion is exactly equal to\\n?\\nTo resolve this fundamental issue, we have to show that, for the coefficients a0, an, and bn\\ncalculated in accordance with (2.6) to (2.8), this series will indeed converge to\\n. In\\ngeneral, for a periodic signal of arbitrary waveform, there is no guarantee that the\\nseries of (2.1) will converge to or that the coefficients a0, an, and bn will even exist.\\nIn a rigorous sense, we may say that a periodic signal can be expanded in a Fourier',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 34,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'series of (2.1) will converge to or that the coefficients a0, an, and bn will even exist.\\nIn a rigorous sense, we may say that a periodic signal can be expanded in a Fourier\\nf0 1 T0 ----- = 2mf0t ( ) 2nf0t ( ) cos cos dt T0 2  - T0 2   T0 2,  m n = 0, m n     = 2mf0t ( ) 2nf0t ( ) sin cos dt T0 2  - T0 2   0 for all m and n , = 2mf0t ( ) sin 2nf0t ( ) sin dt T0 2  - T0 2   T0 2,  m n = 0, m n     = gT0 t( ) a0 1 T0 ----- gT0 t( ) dt T0 2  - T0 2   = an 1 T0 ----- gT0 t( ) 2nf0t ( ) cos dt, n T0 2  - T0 2   1 2  , , = = bn 1 T0 ----- gT0 t( ) 2nf0t ( ) sin dt, n T0 2  - T0 2   1 2  , , = = gT0 t( ) gT0 t( ) gT0 t( ) gT0 t( ) gT0 t( ) gT0 t( )',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 34,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '2 The Fourier Series series if the signal satisfies the Dirichlet conditions:1\\nThe function is single valued within the interval T0.\\nThe function has at most a finite number of discontinuities in the interval T0.\\nThe function has a finite number of maxima and minima in the interval T0.\\nThe function is absolutely integrable; that is,\\nFrom an engineering perspective, however, it suffices to say that the Dirichlet conditions\\nare satisfied by the periodic signals encountered in communication systems.\\nComplex Exponential Fourier Series\\nThe Fourier series of (2.1) can be put into a much simpler and more elegant form with the\\nuse of complex exponentials. We do this by substituting into (2.1) the exponential forms\\nfor the cosine and sine, namely: where . We thus obtain (2.9) Let cn denote a complex coefficient related to an and bn by\\n(2.10)\\nThen, we may simplify (2.9) into\\n(2.11) where (2.12) The series expansion of (2.11) is referred to as the complex exponential Fourier series.\\nThe cn themselves are called the complex Fourier coefficients.\\ngT0 t( ) gT0 t( ) gT0 t( ) gT0 t( ) gT0 t( ) gT0 t( ) dt  < T0 2  - T0 2   2nf0t ( ) cos 1 2--- j2nf0t ( ) j2nf0t - ( ) exp + exp [ ] = 2nf0t ( ) sin 1 2j ----- j2nf0t ( ) j2nf0t - ( ) exp - exp [ ] = j 1 - = gT0 t( ) a0 an j - bn ( ) j2nf0t ( ) an jbn + ( ) j2nf0t - ( ) exp + exp [ ] n=1   + = cn an jbn, - n 0 > a0, n 0 = an jbn, + n 0 <        = gT0 t( ) cn j2nf0t ( ) exp n  - =   = cn 1 T0 ----- gT0 t( ) j2nf0t - ( ) exp dt, n 0 1 2  ,  ,  , = T0 2  - T0 2   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 35,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nGiven a periodic signal\\n, (2.12) states that we may determine the\\ncomplete set of complex Fourier coefficients. On the other hand, (2.11) states\\nthat, given this set of coefficients, we may reconstruct the original periodic\\nsignal exactly.\\nThe integral on the right-hand side of (2.12) is said to be an inner product of the signal with the basis functions exp(-j2nf0t), by whose linear combination all square\\nintegrable functions can be expressed as in (2.11).\\nAccording to this representation, a periodic signal contains all frequencies (both\\npositive and negative) that are harmonically related to the fundamental frequency f0. The\\npresence of negative frequencies is simply a result of the fact that the mathematical model\\nof the signal as described by (2.11) requires the use of negative frequencies. Indeed, this\\nrepresentation also requires the use of complex-valued basis functions, namely\\nexp(j2nf0t), which have no physical meaning either. The reason for using complex-\\nvalued basis functions and negative frequency components is merely to provide a compact\\nmathematical description of a periodic signal, which is well-suited for both theoretical and\\npractical work. The Fourier Transform\\nIn the previous section, we used the Fourier series to represent a periodic signal. We now\\nwish to develop a similar representation for a signal g(t) that is nonperiodic. In order to do\\nthis, we first construct a periodic function of period T0 in such a way that g(t)\\ndefines exactly one cycle of this periodic function, as illustrated in Figure 2.1. In the limit,\\nwe let the period T0 become infinitely large, so that we may express g(t) as\\n(2.13) gT0 t( ) gT0 t( ) gT0 t( ) Figure 2.1 Illustrating the use of an arbitrarily defined function of time to\\nconstruct a periodic waveform. (a) Arbitrarily defined function of time g(t).\\n(b) Periodic waveform gT0(t) based on g(t).\\ngT0 t( ) g t( ) gT0 t( ) T0   lim = -T0 g(t) gT0 (t) T0 0 0 (a) (b)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 36,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '3 The Fourier Transform Representing the periodic function in terms of the complex exponential form of the\\nFourier series, we write where\\nHere, we have purposely replaced f0 with 1/T0 in the exponents. Define\\nand\\nWe may then go on to modify the original Fourier series representation of given in\\n(2.11) into a new form described by\\n(2.14) where (2.15) Equations (2.14) and (2.15) apply to a periodic signal\\n. What we would like to do\\nnext is to go one step further and develop a corresponding pair of formulas that apply to a\\nnonperiodic signal g(t). To do this transition, we use the defining equation (2.13).\\nSpecifically, two things happen:\\nThe discrete frequency fn in (2.14) and (2.15) approaches the continuous frequency\\nvariable f.\\nThe discrete sum of (2.14) becomes an integral defining the area under the function\\nG(f)exp(j2ft), integrated with respect to time t.\\nAccordingly, piecing these points together, we may respectively rewrite the limiting forms\\nof (2.15) and (2.14) as\\n(2.16) and (2.17) gT0 t( ) gT0 t( ) cn j2nt T0 --------------     exp n  - =   = cn 1 T0 ----- gT0 t( ) j2nt T0 -------------- -     exp dt T0 2  - T0 2   = f 1 T0 ----- = fn n T0 ----- = G fn ( ) cnT0 = gT0 t( ) gT0 t( ) G fn ( ) j2fnt ( )f exp n  - =   = G fn ( ) gT0 t( ) j2fnt - ( ) exp dt T0 2  - T0 2   = gT0 t( ) G f( ) g t( ) j2ft - ( ) exp dt  -   = g t( ) G f( ) j2ft ( ) exp df  -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 37,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nIn words, we may say:\\n\\nthe Fourier transform of the nonperiodic signal g(t) is defined by (2.16);',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 38,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the Fourier transform of the nonperiodic signal g(t) is defined by (2.16);\\n\\ngiven the Fourier transform G(f), the original signal g(t) is recovered exactly from\\nthe inverse Fourier transform of (2.17).\\nFigure 2.2 illustrates the interplay between these two formulas, where we see that the\\nfrequency-domain description based on (2.16) plays the role of analysis and the time-\\ndomain description based on (2.17) plays the role of synthesis.\\nFrom a notational point of view, note that in (2.16) and (2.17) we have used a lowercase\\nletter to denote the time function and an uppercase letter to denote the corresponding\\nfrequency function. Note also that these two equations are of identical mathematical form,\\nexcept for changes in the algebraic signs of the exponents.\\nFor the Fourier transform of a signal g(t) to exist, it is sufficient but not necessary that\\nthe nonperiodic signal g(t) satisfies three Dirichlets conditions of its own:\\nThe function g(t) is single valued, with a finite number of maxima and minima in\\nany finite time interval.\\nThe function g(t) has a finite number of discontinuities in any finite time interval.\\nThe function g(t) is absolutely integrable; that is,\\nIn practice, we may safely ignore the question of the existence of the Fourier transform of\\na time function g(t) when it is an accurately specified description of a physically realizable\\nsignal. In other words, physical realizability is a sufficient condition for the existence of a\\nFourier transform. Indeed, we may go one step further and state:\\nAll energy signals are Fourier transformable.\\nA signal g(t) is said to be an energy signal if the condition\\n(2.18)\\nholds.2\\nFigure 2.2 Sketch of the interplay between the synthesis\\nand analysis equations embodied in Fourier transformation.\\ng t( ) dt  <  -   g t( ) 2dt  <  -   Analysis equation: Synthesis equation: g(t) = Time-domain description: g(t) Frequency-domain description: G(f ) G (f ) exp(j 2 ft )df  - G (f) = g(t) exp(- j 2 ft )dt  -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 38,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '3 The Fourier Transform The Fourier transform provides the mathematical tool for measuring the frequency\\ncontent, or spectrum, of a signal. For this reason, the terms Fourier transform and\\nspectrum are used interchangeably. Thus, given a signal g(t) with Fourier transform G(f),\\nwe may refer to G(f) as the spectrum of the signal g(t). By the same token, we refer to\\n|G(f)| as the magnitude spectrum of the signal g(t), and refer to arg[G(f)] as its phase\\nspectrum.\\nIf the signal g(t) is real valued, then the magnitude spectrum of the signal is an even\\nfunction of frequency f, while the phase spectrum is an odd function of f. In such a case,\\nknowledge of the spectrum of the signal for positive frequencies uniquely defines the\\nspectrum for negative frequencies.\\nNotations\\nFor convenience of presentation, it is customary to express (2.17) in the short-hand form\\nwhere F plays the role of an operator. In a corresponding way, (2.18) is expressed in the\\nshort-hand form\\nwhere F-1 plays the role of an inverse operator.\\nThe time function g(t) and the corresponding frequency function G(f) are said to\\nconstitute a Fourier-transform pair. To emphasize this point, we write\\nwhere the top arrow indicates the forward transformation from g(t) to G(f) and the bottom\\narrow indicates the inverse transformation. One other notation: the asterisk is used to\\ndenote complex conjugation.\\nTables of Fourier Tranformations\\nTo assist the user of this book, two tables of Fourier transformations are included:\\nTable 2.1 on summarizes the properties of Fourier transforms; proofs of\\nthem are presented as end-of-chapter problems.\\nTable 2.2 on presents a list of Fourier-transform pairs, where the items\\nlisted on the left-hand side of the table are time functions and those in the center\\ncolumn are their Fourier transforms.\\nEXAMPLE\\nBinary Sequence for Energy Calculations\\nConsider the five-digit binary sequence 10010. This sequence is represented by two',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 39,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'column are their Fourier transforms.\\nEXAMPLE\\nBinary Sequence for Energy Calculations\\nConsider the five-digit binary sequence 10010. This sequence is represented by two\\ndifferent waveforms, one based on the rectangular function rect(t), and the other based on\\nthe sinc function sinc(t). Despite this difference, both waveforms are denoted by g(t),\\nwhich implies they both have exactly the same total energy, to be demonstrated next.\\nCase 1:\\nrect(t) as the basis function.\\nLet binary symbol 1 be represented by +rect(t) and binary symbol 0 be represented by\\nrect(t). Accordingly, the binary sequence 10010 is represented by the waveform\\nG f( ) Fgt( ) [ ] = g t( ) F 1 - G f( ) [ ] = g t( ) G f( )',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 39,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nshown in Figure 2.3. From this figure, we readily see that, regardless of the\\nrepresentation rect(t), each symbol contributes a single unit of energy; hence the total\\nenergy for Case 1 is five units.\\nCase 2:\\nsinc(t) as the basis function.\\nConsider next the representation of symbol 1 by +sinc(t) and the representation of symbol\\n0 by sinc(t), which do not interfere with each other in constructing the waveform for the\\nbinary sequence 10010. Unfortunately, this time around, it is difficult to calculate the total\\nwaveform energy in the time domain. To overcome this difficulty, we do the calculation in\\nthe frequency domain.\\nTo this end, in parts a and b of Figure 2.4, we display the waveform of the sinc function\\nin the time domain and its Fourier transform, respectively. On this basis, Figure 2.5\\ndisplays the frequency-domain representation of the binary sequence 10010, with part a of\\nthe figure displaying the magnitude response\\n, and part b displaying the\\ncorresponding phrase response expressed in radians. Then, applying\\nRayleighs energy theorem, described in Property 14 in Table 2.2, to part a of Figure 2.5,\\nwe readily find that the energy of the pulse, sinc(t), is equal to one unit, regardless of its\\namplitude. The total energy of the sinc-based waveform representing the given binary\\nsequence is also exactly five units, confirming what was said at the beginning of this\\nexample.\\nFigure 2.3 Waveform of binary sequence 10010, using rect(t) for symbol 1\\nand -rect(t) for symbol 0. See Table 2.2 for the definition of rect(t).\\nFigure 2.4 (a) Sinc pulse g(t). (b) Fourier transform G(f).\\nTime t Binary sequence g(t) 1 -1.0 1.0 0 0 1 0 - 1 2 1 2 5 2 7 2 9 2 3 2 ... G f( ) arg G f( ) [ ] g 2W 0 0 (t t ) G(f f ) (a) (b) 3 2W 3 2W 1 2W - - - 1 W -W W 1 W 1 2W 1 2W',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 40,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '3 The Fourier Transform Observations\\nThe dual basis functions, rect(t) and sinc(t), are dilated to their simplest forms, each\\nof which has an energy of one unit, hence the equality of the results presented under\\nCases 1 and 2.\\nExamining the waveform g(t) in Figure 2.3, we clearly see the discrimination\\nbetween binary symbols 1 and 0. On the other hand, it is the phase response in part b of Figure 2.5 that shows the discrimination between binary\\nsymbols 1 and 0. EXAMPLE 2 Unit Gaussian Pulse Typically, a pulse signal g(t) and its Fourier transform G(f) have different mathematical\\nforms. This observation is illustrated by the Fourier-transform pair studied in Example 1.\\nIn this second example, we consider an exception to this observation. In particular, we use\\nthe differentiation property of the Fourier transform to derive the particular form of a pulse\\nsignal that has the same mathematical form as its own Fourier transform.\\nLet g(t) denote the pulse signal expressed as a function of time t and G(f) denote its\\nFourier transform. Differentiating the Fourier transform formula of (2.6) with respect to\\nfrequency f yields or, equivalently, (2.19) Figure 2.5 (a) Magnitude spectrum of the sequence 10010. (b) Phase spectrum\\nof the sequence. Binary sequence Frequency, Hz Magnitude |G(f)| Phase argG(f) (radians)  1 1.0 0 0 0 1 0 - - - 1 2 1 2 1 2 3 2 5 2 7 2 9 2 1 2 3 2 5 2 7 2 9 2 (a) (b) 0 arg G f( ) [ ] j2tg t( ) -  d df -----G f( ) 2tg t( ) j d df -----G f( )',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 41,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nUse of the Fourier-transform property on differentiation in the time domain listed in Table\\n1 yields\\n(2.20)\\nSuppose we now impose the equality condition on the left-hand sides of (2.19) and (2.20):\\n(2.21)\\nThen, in a corresponding way, it follows that the right-hand sides of these two equations\\nmust (after canceling the common multiplying factor j) satisfy the condition\\n(2.22)\\nEquations (2.21) and (2.22) show that the pulse signal g(t) and its Fourier transform G(f)\\nhave exactly the same mathematical form. In other words, provided that the pulse signal\\ng(t) satisfies the differential equation (2.21), then G(f) = g(f), where g(f) is obtained from\\ng(t) simply by substituting f for t. Solving (2.21) for g(t), we obtain\\n(2.23)\\nwhich has a bell-shaped waveform, as illustrated in Figure 2.6. Such a pulse is called a\\nGaussian pulse, the name of which follows from the similarity of the function g(t) to the\\nGaussian probability density function of probability theory, to be discussed in Chapter 3.\\nBy applying the Fourier-transform property on the area under g(t) listed in Table 2.1, we\\nhave\\n(2.24)\\nWhen the central ordinate and the area under the curve of a pulse are both unity, as in\\n(2.23) and (2.24), we say that the Gaussian pulse is a unit pulse. Therefore, we may state\\nthat the unit Gaussian pulse is its own Fourier transform, as shown by\\n(2.25)\\nFigure 2.6 Gaussian pulse.\\nd\\ndt\\n-----g t( ) j2fG f( )\\nd dt -----g t( ) 2tg t( ) = d df -----G f( ) 2fG f( ) = g t( ) t2 - ( ) exp = t2 - ( ) exp dt  -   1 = t2 - ( ) exp  f 2 - ( ) exp 1.0 0 -0.47 0.47 0.5 t g(t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 42,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '3 The Fourier Transform Table 2.1 Fourier-transform theorems\\nProperty\\nMathematical description\\nLinearity\\nwhere a and b are constants\\nDilation where a is a constant\\nDuality If , then 4. Time shifting 5. Frequency shifting\\nArea under g(t)\\nArea under G(f)\\nDifferentiation in the time domain\\nIntegration in the time domain\\nConjugate functions\\nIf , then 11. Multiplication in the time domain\\nConvolution in the time domain\\nCorrelation theorem\\nRayleighs energy theorem\\nParsevals power theorem for\\nperiodic signal of period T0\\nag1 t( ) bg2 t( ) + aG1 f( ) bG2 f( ) + g at ( )  1 a-----Gf a---     g t( ) G f( ) G t( ) g f - ( ) g t t0 - ( ) G f( ) j2ft0 - ( ) exp g t( ) j2f0t - ( ) exp G f f0 - ( ) g t( )dt  -   G 0 ( ) = g 0 ( ) G f( )df  -   = d dt -----g t( ) j2fG f( )\\ng  ( )d  - t   1 j2f ----------G f( ) G 0 ( ) 2 ------------ f( ) + g t( ) G f( ) g* t( ) G* f - ( ) g1 t( )g2 t( )  G1  ( )G2 f  - ( )d  -   g1  ( )g2 t  - ( )d  - t  G1 f( )G2 f( ) g1 t( )g2 * t  - ( )d  -   G1 f( )G2 * f( ) g t( ) 2dt  -   = G f( ) 2df  -   1 T0 ----- g t( ) 2dt T0 2  - T0 2   G fn ( ) 2 n  - =   fn n T0  = , =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 43,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nTable 2.2 Fourier-transform pairs and commonly used time functions\\nTime function Fourier transform Definitions 1. Unit step function: 2. 3. 4. Dirac delta function: for and 5. 6. Rectangular function:\\n8. Signum function: 9. 10. 11. Sinc function: 12. 13. Gaussian function: 14. 15. 16. rect t T---     T sinc fT   u t 1, t 0  1 2---, t 0 = 0, t 0         = sinc 2Wt   1 2W --------rect f 2Wf ----------     at -  u t a 0   exp 1 a j2f + -------------------- a t -   a 0   exp 2a a2 2f  2 + ---------------------------\\nt 0 = t 0  tdt  -   1 = t2 -   exp f2 -   exp 1 t T---- t T   - 0 t T        T sinc2 fT   rect t 1, 1 2--- t 1 2---   - 0, otherwise      = t 1 1 f t sgn +1, t 0  0, t 0 = 1, - t 0       = t t0 -   j2ft0 -   exp exp j2fct   f fc - cos 2fct   1 2---   f fc - f fc + + sinc t sin t   t ----------------- = sin 2fct   1 2---   f fc - f fc +  - sgn t 1 jf ------- g t t2 -   exp = 1 t ----- j sgn - f u t 1 2---f 1 j2f ---------- + t iT0 -   i  - =   f0 f nf0 -  , f0 1 T0 ----- = n  - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 44,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '4 The Inverse Relationship between Time-Domain and Frequency-Domain Representations The Inverse Relationship between Time-Domain and\\nFrequency-Domain Representations\\nThe time-domain and frequency-domain descriptions of a signal are inversely related. In\\nthis context, we may make four important statements:\\nIf the time-domain description of a signal is changed, the frequency-domain\\ndescription of the signal is changed in an inverse manner, and vice versa. This\\ninverse relationship prevents arbitrary specifications of a signal in both domains. In\\nother words:\\nWe may specify an arbitrary function of time or an arbitrary spectrum, but we\\ncannot specify them both together.\\nIf a signal is strictly limited in frequency, then the time-domain description of the\\nsignal will trail on indefinitely, even though its amplitude may assume a\\nprogressively smaller value. To be specific, we say:\\nA signal is strictly limited in frequency (i.e., strictly band limited) if its Fourier\\ntransform is exactly zero outside a finite band of frequencies.\\nConsider, for example, the band-limited sinc pulse defined by\\nwhose waveform and spectrum are respectively shown in Figure 2.4: part a shows\\nthat the sinc pulse is asymptotically limited in time and part b of the figure shows\\nthat the sinc pulse is indeed strictly band limited, thereby confirming statement 2.\\nIn a dual manner to statement 2, we say:\\nIf a signal is strictly limited in time (i.e., the signal is exactly zero outside a\\nfinite time interval), then the spectrum of the signal is infinite in extent, even\\nthough the magnitude spectrum may assume a progressively smaller value.\\nThis third statement is exemplified by a rectangular pulse, the waveform and\\nspectrum of which are defined in accordance with item 1 in Table 2.2.\\nIn light of the duality described under statements 2 and 3, we now make the final\\nstatement:\\nA signal cannot be strictly limited in both time and frequency.\\nThe Bandwidth Dilemma',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 45,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'In light of the duality described under statements 2 and 3, we now make the final\\nstatement:\\nA signal cannot be strictly limited in both time and frequency.\\nThe Bandwidth Dilemma\\nThe statements we have just made have an important bearing on the bandwidth of a signal,\\nwhich provides a measure of the extent of significant spectral content of the signal for\\npositive frequencies. When the signal is strictly band limited, the bandwidth is well\\ndefined. For example, the sinc pulse sinc(2Wt) has a bandwidth equal to W. However,\\nwhen the signal is not strictly band limited, as is often the case, we encounter difficulty in\\ndefining the bandwidth of the signal. The difficulty arises because the meaning of\\nsignificant attached to the spectral content of the signal is mathematically imprecise.\\nConsequently, there is no universally accepted definition of bandwidth. It is in this sense\\nthat we speak of the bandwidth dilemma.\\nsinc t( ) t ( ) sin t ------------------ =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 45,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nNevertheless, there are some commonly used definitions for bandwidth, as discussed\\nnext. When the spectrum of a signal is symmetric with a main lobe bounded by well-\\ndefined nulls (i.e., frequencies at which the spectrum is zero), we may use the main lobe as\\nthe basis for defining the bandwidth of the signal. Specifically:\\nIf a signal is low-pass (i.e., its spectral content is centered around the origin\\nf = 0), the bandwidth is defined as one-half the total width of the main spectral\\nlobe, since only one-half of this lobe lies inside the positive frequency region.\\nFor example, a rectangular pulse of duration T seconds has a main spectral lobe of total\\nwidth (2/T) hertz centered at the origin. Accordingly, we may define the bandwidth of this\\nrectangular pulse as (1/T) hertz.\\nIf, on the other hand, the signal is band-pass with main spectral lobes centered around\\nfc, where fc is large enough, the bandwidth is defined as the width of the main lobe for\\npositive frequencies. This definition of bandwidth is called the null-to-null bandwidth.\\nConsider, for example, a radio-frequency (RF) pulse of duration T seconds and frequency\\nfc, shown in Figure 2.7. The spectrum of this pulse has main spectral lobes of width (2/T)\\nhertz centered around fc, where it is assumed that fc is large compared with (1/T). Hence,\\nwe define the null-to-null bandwidth of the RF pulse of Figure 2.7 as (2/T) hertz.\\nOn the basis of the definitions presented here, we may state that shifting the spectral\\ncontent of a low-pass signal by a sufficiently large frequency has the effect of doubling the\\nbandwidth of the signal; this frequency translation is attained by using the process of\\nmodulation. Basically, the modulation moves the spectral content of the signal for negative\\nfrequencies into the positive frequency region, whereupon the negative frequencies\\nbecome physically measurable.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 46,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'frequencies into the positive frequency region, whereupon the negative frequencies\\nbecome physically measurable.\\nAnother popular definition of bandwidth is the 3 dB bandwidth. Specifically, if the\\nsignal is low-pass, we say:\\nThe 3 dB bandwidth of a low-pass signal is defined as the separation between\\nzero frequency, where the magnitude spectrum attains its peak value, and the\\npositive frequency at which the amplitude spectrum drops to of its\\npeak value.\\nFigure 2.7 Magnitude spectrum of the RF pulse, showing the null-to-null bandwidth to be 2/T,\\ncentered on the mid-band frequency fc.\\n1 2  0 f |G(f)| -fc fc T 2 2 T 2 T',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 46,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 The Inverse Relationship between Time-Domain and Frequency-Domain Representations For example, the decaying exponential function exp(-at) has a 3 dB bandwidth of (a/2)\\nhertz.\\nIf, on the other hand, the signal is of a band-pass kind, centered at fc, the 3 dB\\nbandwidth is defined as the separation (along the positive frequency axis) between the two\\nfrequencies at which the magnitude spectrum of the signal drops to\\nof its peak value\\nat fc.\\nRegardless of whether we have a low-pass or band-pass signal, the 3 dB bandwidth has\\nthe advantage that it can be read directly from a plot of the magnitude spectrum. However,\\nit has the disadvantage that it may be misleading if the magnitude spectrum has slowly\\ndecreasing tails.\\nTime-Bandwidth Product\\nFor any family of pulse signals that differ by a time-scaling factor, the product of the\\nsignals duration and its bandwidth is always a constant, as shown by\\nduration  bandwidth = constant\\nThis product is called the time-bandwidth product. The constancy of the time-bandwidth\\nproduct is another manifestation of the inverse relationship that exists between the time-\\ndomain and frequency-domain descriptions of a signal. In particular, if the duration of a\\npulse signal is decreased by reducing the time scale by a factor a, the frequency scale of\\nthe signals spectrum, and therefore the bandwidth of the signal is increased by the same\\nfactor a. This statement follows from the dilation property of the Fourier transform\\n(defined in Property 2 of Table 2.1). The time-bandwidth product of the signal is therefore\\nmaintained constant. For example, a rectangular pulse of duration T seconds has a\\nbandwidth (defined on the basis of the positive-frequency part of the main lobe) equal to\\n(1/T) hertz; in this example, the time-bandwidth product of the pulse equals unity.\\nThe important point to take from this discussion is that whatever definitions we use for\\nthe bandwidth and duration of a signal, the time-bandwidth product remains constant over',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 47,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The important point to take from this discussion is that whatever definitions we use for\\nthe bandwidth and duration of a signal, the time-bandwidth product remains constant over\\ncertain classes of pulse signals; the choice of particular definitions for bandwidth and\\nduration merely change the value of the constant.\\nRoot-Mean-Square Definitions of Bandwidth and Duration\\nTo put matters pertaining to the bandwidth and duration of a signal on a firm mathematical\\nbasis, we first introduce the following definition for bandwidth:\\nThe root-mean-square (rms) bandwidth is defined as the square root of the\\nsecond moment of a normalized form of the squared magnitude spectrum of the\\nsignal about a suitably chosen frequency.\\nTo be specific, we assume that the signal g(t) is of a low-pass kind, in which case the\\nsecond moment is taken about the origin f = 0. The squared magnitude spectrum of the\\nsignal is denoted by |G(f)|2. To formulate a nonnegative function, the total area under\\nwhose curve is unity, we use the normalizing function\\n1 2  G f( ) 2df  -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 47,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nWe thus mathematically define the rms bandwidth of a low-pass signal g(t) with Fourier\\ntransform G(f) as\\n(2.26)\\nwhich describes the dispersion of the spectrum G(f) around f = 0. An attractive feature of\\nthe rms bandwidth Wrms is that it lends itself readily to mathematical evaluation. But, it is\\nnot as easily measurable in the laboratory.\\nIn a manner corresponding to the rms bandwidth, the rms duration of the signal g(t) is\\nmathematically defined by\\n(2.27)\\nwhere it is assumed that the signal g(t) is centered around the origin t = 0. In Problem 2.7,\\nit is shown that, using the rms definitions of (2.26) and (2.27), the time-bandwidth product\\ntakes the form\\n(2.28)\\nIn Problem 2.7, it is also shown that the Gaussian pulse exp(-t2) satisfies this condition\\nexactly with the equality sign. The Dirac Delta Function\\nStrictly speaking, the theory of the Fourier transform, presented in Section 2.3, is\\napplicable only to time functions that satisfy the Dirichlet conditions. As mentioned\\npreviously, such functions naturally include energy signals. However, it would be highly\\ndesirable to extend this theory in two ways:\\nTo combine the Fourier series and Fourier transform into a unified theory, so that the\\nFourier series may be treated as a special case of the Fourier transform.\\nTo include power signals in the list of signals to which we may apply the Fourier\\ntransform. A signal g(t) is said to be a power signal if the condition holds, where T is the observation interval.\\nIt turns out that both of these objectives can be met through the proper use of the Dirac\\ndelta function, or unit impulse.\\nWrms f2 G f( ) 2df  -   G f( ) 2df  -   ------------------------------------\\n           1 2  = Trms t2 g t( ) 2dt  -   g t( ) 2dt  -   -----------------------------------\\n           1 2  = TrmsWrms 1 4 ------  1 T--- g t( ) 2dt  < T 2  - T 2',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 48,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '5 The Dirac Delta Function The Dirac delta function3 or just delta function, denoted by (t), is defined as having\\nzero amplitude everywhere except at t = 0, where it is infinitely large in such a way that it\\ncontains unit area under its curve; that is,\\n(2.29) and (2.30) An implication of this pair of relations is that the delta function (t) is an even function of\\ntime t, centered at the origin t = 0. Perhaps, the simplest way of describing the Dirac delta\\nfunction is to view it as the rectangular pulse\\nwhose duration is T and amplitude is 1/T, as illustrated in Figure 2.8. As T approaches\\nzero, the rectangular pulse g(t) approaches the Dirac delta function (t) in the limit.\\nFor the delta function to have meaning, however, it has to appear as a factor in the\\nintegrand of an integral with respect to time, and then, strictly speaking, only when the\\nother factor in the integrand is a continuous function of time. Let g(t) be such a function,\\nand consider the product of g(t) and the time-shifted delta function (t - t0). In light of the\\ntwo defining equations (2.29) and (2.30), we may express the integral of this product as\\n(2.31)\\nThe operation indicated on the left-hand side of this equation sifts out the value g(t0) of the\\nfunction g(t) at time t = t0, where\\n. Accordingly, (2.31) is referred to as the\\nsifting property of the delta function. This property is sometimes used as the defining\\nequation of a delta function; in effect, it incorporates (2.29) and (2.30) into a single\\nrelation.\\nNoting that the delta function (t) is an even function of t, we may rewrite (2.31) so as\\nto emphasize its resemblance to the convolution integral, as shown by\\n(2.32)\\nFigure 2.8 Illustrative example of the Dirac delta function as the\\nlimiting form of rectangular pulse rect as T approaches zero.\\n t( ) 0, = t 0   t( ) dt  -   1 = g t( ) 1 T--- rect t T---     = g t( ) t t0 - ( )dt  -   g t0 ( ) =  t  < < - g  ( ) t  - ( ) d  -   g t( ) = g(t t ) 1 T Area = 1 T 1 T--- t T---',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 49,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nIn words, the convolution of any function with the delta function leaves that function\\nunchanged. We refer to this statement as the replication property of the delta function.\\nIt is important to realize that no function in the ordinary sense has the two properties of\\n(2.29) and (2.30) or the equivalent sifting property of (2.31). However, we can imagine a\\nsequence of functions that have progressively taller and thinner peaks at t = 0, with the\\narea under the curve consistently remaining equal to unity; as this progression is being\\nperformed, the value of the function tends to zero at every point except t = 0, where it\\ntends to infinity, as illustrated in Figure 2.8, for example. We may therefore say:\\nThe delta function may be viewed as the limiting form of a pulse of unit area as\\nthe duration of the pulse approaches zero.\\nIt is immaterial what sort of pulse shape is used, so long as it is symmetric with respect to\\nthe origin; this symmetry is needed to maintain the even function property of the delta\\nfunction.\\nTwo other points are noteworthy:\\nApplicability of the delta function is not confined to the time domain. Rather, it can\\nequally well be applied in the frequency domain; all that we have to do is to replace\\ntime t by frequency f in the defining equations (2.29) and (2.30).\\nThe area covered by the delta function defines its strength. As such, the units, in\\nterms of which the strength is measured, are determined by the specifications of the\\ntwo coordinates that define the delta function.\\nEXAMPLE\\nThe Sinc Function as a Limiting Form of the Delta Function\\nin the Time Domain\\nAs another illustrative example, consider the scaled sinc function 2Wsinc(2Wt), whose\\nwaveform covers an area equal to unity for all W.\\nFigure 2.9 displays the evolution of this time function toward the delta function as the\\nparameter W is varied in three stages: W = 1, W = 2, and W = 5. Referring back to Figure',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 50,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Figure 2.9 displays the evolution of this time function toward the delta function as the\\nparameter W is varied in three stages: W = 1, W = 2, and W = 5. Referring back to Figure\\n4, we may infer that as the parameter W characterizing the sinc pulse is increased, the\\namplitude of the pulse at time t = 0 increases linearly, while at the same time the duration\\nof the main lobe of the pulse decreases inversely. With this objective in mind, as the\\nparameter W is progressively increased, Figure 2.9 teaches us two important things:\\nThe scaled sinc function becomes more like a delta function.\\nThe constancy of the functions spectrum is maintained at unity across an\\nincreasingly wider frequency band, in accordance with the constraint that the area\\nunder the function is to remain constant at unity; see Property 6 of Table 2.1 for a\\nvalidation of this point.\\nBased on the trend exhibited in Figure 2.9, we may write\\n(2.33)\\nwhich, in addition to the rectangular pulse considered in Figure 2.8, is another way of\\nrealizing a delta function in the time domain.\\n t( ) 2W sinc W   lim 2Wt ( ) =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 50,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '5 The Dirac Delta Function Figure 2.9 Evolution of the sinc function 2W sinc(2Wt) toward the delta function as the\\nparameter W progressively increases.\\n10 8 6 4 2 0 - 2 - 3 - 2 - 1 0 1 2 3 Amplitude W = 5 t 10 8 6 4 2 0 - 2 - 3 - 2 - 1 0 1 2 3 Amplitude t 10 8 6 4 2 0 - 2 - 3 - 2 - 1 0 1 2 3 Amplitude t W = 2 W =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 51,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nEXAMPLE\\nEvolution of the Sum of Complex Exponentials toward the Delta Function in\\nthe Frequency Domain\\nFor yet another entirely different example, consider the infinite summation term over the interval\\n. Using Eulers formula\\nwe may express the given summation as\\nThe imaginary part of the summation is zero for two reasons. First, sin(2mf) is zero for\\nm = 0. Second, since sin(-2mf) = -sin(2mf), the remaining imaginary terms cancel\\neach other. Therefore, Figure 2.10 plots this real-valued summation versus frequency f over the interval\\nfor three ranges of m:\\n-5  m  5 2. -10  m  10 3. -20  m  20 Building on the results exhibited in Figure 2.10, we may go on to say\\n(2.34)\\nwhich is one way of realizing a delta function in the frequency domain. Note that the area\\nunder the summation term on the right-hand side of (2.34) is equal to unity; we say so\\nbecause\\nThis result, formulated in the frequency domain, confirms (2.34) as one way of defining\\nthe delta function ( f ).\\nj2mf ( ) exp m  - =   1 2 f   1 2  < - j2mf ( ) exp 2mf ( ) j 2mf ( ) sin + cos = j2mf ( ) exp m  - =   2mf ( ) j 2mf ( ) sin m  - =   + cos m  - =   = j2mf ( ) 2mf ( ) cos m  - =   = exp m  - =   1 2 f   1 2  < -  f( ) 2mf ( ), 1 2--- f 1 2--- <  - cos m  - =   = 2mf ( ) df cos m  - =   1 2  - 1 2   2mf ( ) cos df 1 2  - 1 2   m  - =   = 2mf ( ) sin 2m --------------------------\\nf 1 2  - = 1 2  m  - =   = m ( ) sin m -------------------- m  - =   = 1 for m 0 = 0 otherwise    =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 52,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '5 The Dirac Delta Function Figure 2.10 Evolution of the sum of m complex exponentials toward a delta function in the\\nfrequency domain as m becomes increasingly larger.\\n500 400 300 200 100 0 -100 -0.5 0 0.5 Amplitude f 500 400 300 200 100 0 -100 -0.5 0 0.5 Amplitude f 500 400 300 200 100 0 -100 -0.5 0 0.5 Amplitude f m = 5 m = 10 m =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 53,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems Fourier Transforms of Periodic Signals\\nWe began the study of Fourier analysis by reviewing the Fourier series expansion of\\nperiodic signals, which, in turn, paved the way for the formulation of the Fourier\\ntransform. Now that we have equipped ourselves with the Dirac delta function, we would\\nlike to revisit the Fourier series and show that it can indeed be treated as a special case of\\nthe Fourier transform.\\nTo this end, let g(t) be a pulse-like function, which equals a periodic signal over\\none period T0 of the signal and is zero elsewhere, as shown by (2.35) The periodic signal itself may be expressed in terms of the function g(t) as an\\ninfinite summation, as shown by\\n(2.36)\\nIn light of the definition of the pulselike function g(t) in (2.35), we may view this function\\nas a generating function, so called as it generates the periodic signal in accordance\\nwith (2.36). Clearly, the generating function g(t) is Fourier transformable; let G(f) denote its\\nFourier transform. Correspondingly, let\\ndenote the Fourier transform of the\\nperiodic signal\\n. Hence, taking the Fourier transforms of both sides of (2.36) and\\napplying the time-shifting property of the Fourier transform (Property 4 of Table 2.1), we\\nmay write\\n(2.37)\\nwhere we have taken G(f) outside the summation because it is independent of m.\\nIn Example 4, we showed that\\nLet this result be expanded to cover the entire frequency range, as shown by\\n(2.38)\\nEquation (2.38) (see Problem 2.8c) represents a Dirac comb, consisting of an infinite\\nsequence of uniformly spaced delta functions, as depicted in Figure 2.11.\\ngT0 t( ) g t( ) gT0 t( ), T0 2----- t T0 2-----  < - 0, elsewhere      = gT0 t( ) gT0 t( ) g t mT0 - ( ) m  - =   = gT0 t( ) GT0 f( ) gT0 t( ) GT0 f( ) G f( ) j2mfT0 - ( ),  f  < < - exp m  - =   = j2mf ( ) exp m  - =   j2mf ( ) cos m  - =    f( ), 1 2--- f 1 2--- <  - = = j2mf ( ) exp m  - =    f n - ( ),  f  < < - n  - =   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 54,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '6 Fourier Transforms of Periodic Signals Next, introducing the frequency-scaling factor f0 = 1/T0 into (2.38), we\\ncorrespondingly write\\n(2.39)\\nHence, substituting (2.39) into the right-hand side of (2.37), we get\\n(2.40)\\n.\\nWhat we have to show next is that the inverse Fourier transform of defined in (2.40)\\nis exactly the same as in the Fourier series formula of (2.14). Specifically, substituting (2.40)\\ninto the inverse Fourier transform formula of (2.17), we get Figure 2.11 (a) Dirac comb. (b) Spectrum of the Dirac comb.\\n0 (a) T0(t) t 0 (b) F| T0(t)| f -3T0 5 T0 -2T0 T0 2T0 3T0 -T0 - - - -- - 4 T0 3 T0 2 T0 1 T0 1 T0 2 T0 3 T0 4 T0 5 T0   j2mfT0 ( ) exp m  - =   f0  f nf0 - ( ),  f  < < - n  - =   = GT0 f( ) f0G f( )  f nf0 - ( ) n  - =   = f0 G fn ( ) f fn - ( )  f  < < - , n  - =   = where fn nf0 = GT0 f( ) gT0 t( ) f0 G fn ( ) f fn - ( ) n  - =   j2ft ( ) exp df  -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 55,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nInterchanging the order of summation and integration, and then invoking the sifting\\nproperty of the Dirac delta function (this time in the frequency domain), we may go on to\\nwrite\\nwhich is an exact rewrite of (2.14) with\\n. Equivalently, in light of (2.36), we may\\nformulate the Fourier transform pair (2.41) The result derived in (2.41) is one form of Poissons sum formula.\\nWe have thus demonstrated that the Fourier series representation of a periodic signal is\\nembodied in the Fourier transformation of (2.16) and (2.17), provided, of course, we\\npermit the use of the Dirac delta function. In so doing, we have closed the circle by\\ngoing from the Fourier series to the Fourier transform, and then back to the Fourier series.\\nConsequences of Ideal Sampling\\nConsider a Fourier transformable pulselike signal g(t) with its Fourier transform denoted\\nby G(f). Setting fn = nf0 in (2.41) and using (2.38), we may express Poissons sum formula\\n(2.42)\\nwhere f0 = 1/T0. The summation on the left-hand side of this Fourier-transform pair is a\\nperiodic signal with period T0. The summation on the right-hand side of the pair is a\\nuniformly sampled version of the spectrum G(f). We may therefore make the following\\nstatement:\\nUniform sampling of the spectrum G(f) in the frequency domain introduces\\nperiodicity of the function g(t) in the time domain.\\nApplying the duality property of the Fourier transform (Property 3 of Table 2.1) to (2.42),\\nwe may also write\\n(2.43)\\nin light of which we may make the following dual statement:\\nUniform sampling of the Fourier transformable function g(t) in the time domain\\nintroduces periodicity of the spectrum G(f) in the frequency domain.\\ngT0 t( ) f0 n  - =   G fn ( ) j2ft ( ) f fn - ( ) exp df  -   = f0 G fn ( ) j2fnt ( ) exp n  - =   = f0 f  = g t mT0) - ( m  - =   f0 G fn ( )exp j2fnt ( ) n  - =   = g t mT0 - ( ) m  - =   f0 G nf0 ( ) f nf0 - ( ) n  - =   T0 g mT0 ( ) t mT0 - ( ) m  - =    G f nf0 - ( ) n  - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 56,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '7 Transmission of Signals through Linear Time-Invariant Systems Transmission of Signals through Linear Time-Invariant Systems\\nA system refers to any physical entity that produces an output signal in response to an\\ninput signal. It is customary to refer to the input signal as the excitation and to the output\\nsignal as the response. In a linear system, the principle of superposition holds; that is, the\\nresponse of a linear system to a number of excitations applied simultaneously is equal to\\nthe sum of the responses of the system when each excitation is applied individually.\\nIn the time domain, a linear system is usually described in terms of its impulse\\nresponse, which is formally defined as follows:\\nThe impulse response of a linear system is the response of the system (with zero\\ninitial conditions) to a unit impulse or delta function (t) applied to the input of\\nthe system at time t = 0.\\nIf the system is also time invariant, then the shape of the impulse response is the same no\\nmatter when the unit impulse is applied to the system. Thus, with the unit impulse or delta\\nfunction applied to the system at time t = 0, the impulse response of a linear time-invariant\\nsystem is denoted by h(t).\\nSuppose that a system described by the impulse response h(t) is subjected to an\\narbitrary excitation x(t), as depicted in Figure 2.12. The resulting response of the system\\ny(t), is defined in terms of the impulse response h(t) by\\n(2.44)\\nwhich is called the convolution integral. Equivalently, we may write\\n(2.45)\\nEquations (2.44) and (2.45) state that convolution is commutative.\\nExamining the convolution integral of (2.44), we see that three different time scales are\\ninvolved: excitation time , response time t, and system-memory time t - . This relation is\\nthe basis of time-domain analysis of linear time-invariant systems. According to (2.44),\\nthe present value of the response of a linear time-invariant system is an integral over the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 57,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the basis of time-domain analysis of linear time-invariant systems. According to (2.44),\\nthe present value of the response of a linear time-invariant system is an integral over the\\npast history of the input signal, weighted according to the impulse response of the system.\\nThus, the impulse response acts as a memory function of the system.\\nCausality and Stability\\nA linear system with impulse response h(t) is said to be causal if its impulse response h(t)\\nsatisfies the condition for t <\\nFigure 2.12 Illustrating the roles of excitation x(t), impulse response h(t),\\nand response y(t) in the context of a linear time-invariant system.\\ny t( ) x  ( )h t  - ( ) d  -   = y t( ) h  ( )x t  - ( ) d  -   = h t( ) 0 = Linear system: impulse response h(t) Excitation x(t) Response y(t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 57,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nThe essence of causality is that no response can appear at the output of the system before\\nan excitation is applied to its input. Causality is a necessary requirement for on-line\\noperation of the system. In other words, for a system operating in real time to be\\nphysically realizable, it has to be causal.\\nAnother important property of a linear system is stability. A necessary and sufficient\\ncondition for the system to be stable is that its impulse response h(t) must satisfy the\\ninequality\\nThis requirement follows from the commonly used criterion of bounded input-bounded\\noutput. Basically, for the system to be stable, its impulse response must be absolutely\\nintegrable.\\nFrequency Response\\nLet X(f), H(f), and Y(f) denote the Fourier transforms of the excitation x(t), impulse\\nresponse h(t), and response y(t), respectively. Then, applying Property 12 of the Fourier\\ntransform in Table 2.1 to the convolution integral, be it written in the form of (2.44) or\\n(2.45), we get\\n(2.46)\\nEquivalently, we may write\\n(2.47)\\nThe new frequency function H(f) is called the transfer function or frequency response of\\nthe system; these two terms are used interchangeably. Based on (2.47), we may now\\nformally say:\\nThe frequency response of a linear time-invariant system is defined as the ratio\\nof the Fourier transform of the response of the system to the Fourier transform\\nof the excitation applied to the system.\\nIn general, the frequency response H(f) is a complex quantity, so we may express it in the form\\n(2.48)\\nwhere |H(f)| is called the magnitude response, and (f) is the phase response, or simply\\nphase. When the impulse response of the system is real valued, the frequency response\\nexhibits conjugate symmetry, which means that\\nand\\nThat is, the magnitude response |H(f)| of a linear system with real-valued impulse\\nresponse is an even function of frequency, whereas the phase (f) is an odd function of\\nfrequency.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 58,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'and\\nThat is, the magnitude response |H(f)| of a linear system with real-valued impulse\\nresponse is an even function of frequency, whereas the phase (f) is an odd function of\\nfrequency.\\nIn some applications it is preferable to work with the logarithm of H(f) expressed in\\npolar form, rather than with H(f) itself. Using ln to denote the natural logarithm, let\\n(2.49) h t( ) dt  <  -   Y f( ) H f( )X f( ) = H f( ) Y f( ) X f( ) ---------- = H f( ) H f( ) j f( ) [ ] exp = H f( ) H f - ( ) =  f( )  f - ( ) - = H f( ) ln  f( ) j f( ) + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 58,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '7 Transmission of Signals through Linear Time-Invariant Systems\\n39 where (2.50) The function (f) is called the gain of the system; it is measured in nepers. The phase (f)\\nis measured in radians. Equation (2.49) indicates that the gain (f) and phase (f) are,\\nrespectively, the real and imaginary parts of the (natural) logarithm of the transfer function\\nH(f). The gain may also be expressed in decibels (dB) by using the definition\\nThe two gain functions (f) and are related by\\nThat is, 1 neper is equal to 8.69 dB.\\nAs a means of specifying the constancy of the magnitude response |H(f)| or gain (f)\\nof a system, we use the notion of bandwidth. In the case of a low-pass system, the\\nbandwidth is customarily defined as the frequency at which the magnitude response |H(f)|\\nis times its value at zero frequency or, equivalently, the frequency at which the gain drops by 3 dB below its value at zero frequency, as illustrated in Figure 2.13a. In the\\ncase of a band-pass system, the bandwidth is defined as the range of frequencies over\\nwhich the magnitude response |H(f)| remains within times its value at the mid-band\\nfrequency, as illustrated in Figure 2.13b.\\nFigure 2.13 Illustrating the definition of system bandwidth. (a) Low-pass system.\\n(b) Band-pass system.\\n f( ) H f( ) ln =  f( ) 20 H f( ) 10 log =  f( )  f( ) 8.69 f( ) = 1 2   f( ) 1 2  (a) B -B f |H(f )| |H(0)| |H(0)| --- ------ ------ 0 2 (b) -fc -ffc - B - B -f + B c f |H(f )| |H( fc)| |H( fc)| 0 c fc + B fc  2',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 59,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nPaley-Wiener Criterion: Another Way of Assessing Causality\\nA necessary and sufficient condition for a function (f) to be the gain of a causal filter is\\nthe convergence of the integral\\n(2.51)\\nThis condition is known as the Paley-Wiener criterion.4 The criterion states that provided\\nthe gain (f) satisfies the condition of (2.51), then we may associate with this gain a\\nsuitable phase (f), such that the resulting filter has a causal impulse response that is zero\\nfor negative time. In other words, the Paley-Wiener criterion is the frequency-domain\\nequivalent of the causality requirement. A system with a realizable gain characteristic may\\nhave infinite attenuation for a discrete set of frequencies, but it cannot have infinite\\nattenuation over a band of frequencies; otherwise, the Paley-Wiener criterion is violated.\\nFinite-Duration Impulse Response (FIR) Filters\\nConsider next a linear time-invariant filter with impulse response h(t). We make two\\nassumptions:\\nCausality, which means that the impulse response h(t) is zero for t < 0.\\nFinite support, which means that the impulse response of the filter is of some finite\\nduration Tf, so that we may write h(t) = 0 for t  Tf.\\nUnder these two assumptions, we may express the filter output y(t) produced in response\\nto the input x(t) as\\n(2.52)\\nLet the input x(t), impulse response h(t), and output y(t) be uniformly sampled at the rate\\n(1/) samples per second, so that we may put\\nand\\nwhere k and n are integers and  is the sampling period. Assuming that  is small\\nenough for the product h()x(t - ) to remain essentially constant for k   (k + 1)\\nfor all values of k and , we may approximate (2.52) by the convolution sum\\nwhere N  = Tf. To simplify the notations used in this summation formula, we introduce\\nthree definitions:  f( ) 1 f2 + ------------- df  <  -   y t( ) h  ( )x t  - ( ) d 0 Tf = t n =  k = y n ( ) h(k)x n k - ( ) k 0 = N 1 -  = wk h k ( ) = x n ( ) xn = y n ( ) yn =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 60,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '7 Transmission of Signals through Linear Time-Invariant Systems We may then rewrite the formula for y(n) in the compact form\\n(2.53)\\nEquation (2.53) may be realized using the structure shown in Figure 2.14, which consists\\nof a set of delay elements (each producing a delay of  seconds), a set of multipliers\\nconnected to the delay-line taps, a corresponding set of weights supplied to the\\nmultipliers, and a summer for adding the multiplier outputs. The sequences xn and yn, for\\ninteger values of n as described in (2.53), are referred to as the input and output sequences,\\nrespectively.\\nIn the digital signal-processing literature, the structure of Figure 2.14 is known as a\\nfinite-duration impulse response (FIR) filter. This filter offers some highly desirable\\npractical features:\\nThe filter is inherently stable, in the sense that a bounded input sequence produces a\\nbounded output sequence.\\nDepending on how the weights are designated, the filter can perform the\\nfunction of a low-pass filter or band-pass filter. Moreover, the phase response of the\\nfilter can be configured to be a linear function of frequency, which means that there\\nwill be no delay distortion.\\nIn a digital realization of the filter, the filter assumes a programmable form whereby\\nthe application of the filter can be changed merely by making appropriate changes to\\nthe weights, leaving the structure of the filter completely unchanged; this kind of\\nflexibility is not available with analog filters.\\nWe will have more to say on the FIR filter in subsequent chapters of the book.\\nFigure 2.14 Tapped-delay-line (TDL) filter; also referred to as FIR filter.\\nw N -2 w N -1 Delay Sampled input x (n ) Sampled output y(n ) Weights  w 0 w 1 w 2 Delay Delay w N -3 Delay ... ...       yn wkxn k - n 0 1 2  ,  ,  , = , k 0 = N 1 -  = wk { }k = 0 N -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 61,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems Hilbert Transform\\nThe Fourier transform is particularly useful for evaluating the frequency content of an\\nenergy signal or, in a limiting sense, that of a power signal. As such, it provides the\\nmathematical basis for analyzing and designing frequency-selective filters for the\\nseparation of signals on the basis of their frequency content. Another method of separating\\nsignals is based on phase selectivity, which uses phase shifts between the pertinent signals\\nto achieve the desired separation. A phase shift of special interest in this context is that of\\n90. In particular, when the phase angles of all components of a given signal are shifted\\nby 90, the resulting function of time is known as the Hilbert transform of the signal. The\\nHilbert transform is called a quadrature filter; it is so called to emphasize its distinct\\nproperty of providing a 90 phase shift.\\nTo be specific, consider a Fourier transformable signal g(t) with its Fourier transform\\ndenoted by G(f). The Hilbert transform of g(t), which we denote by\\n, is defined by5\\n(2.54)\\nTable 2.3 Hilbert-transform pairs*\\nTime function Hilbert transform 1. m(t)cos(2fct) m(t)sin(2fct) 2. m(t)sin(2fct) -m(t)cos(2fct) 3. cos(2fct) sin(2fct) 4. sin(2fct) -cos(2fct) 5. 6. rect(t) 7. (t) 8. 9. -(t) Notes: (t) denotes Dirac delta function; rect(t) denotes rectangular function; ln denotes natural logarithm.\\n* In the first two pairs, it is assumed that m(t) is band limited to the interval -W  f  W, where W < fc.\\ng t( ) g t( ) 1 --- g ( ) t  - ---------- d  -   = t sin t --------- 1 t cos - t ------------------- 1 --- t 1 2  - t 1 2  + ------------------ ln - 1 t ----- 1 1 t2 + ------------- t 1 t2 + ------------- 1 t---',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 62,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '8 Hilbert Transform Clearly, Hilbert transformation is a linear operation. The inverse Hilbert transform, by\\nmeans of which the original signal g(t) is linearly recovered from\\n, is defined by\\n(2.55)\\nThe functions g(t) and are said to constitute a Hilbert-transform pair. A short table of\\nHilbert-transform pairs is given in Table 2.3 on .\\nThe definition of the Hilbert transform given in (2.54) may be interpreted as the\\nconvolution of g(t) with the time function 1/(t). We know from the convolution theorem\\nlisted in Table 2.1 that the convolution of two functions in the time domain is transformed\\ninto the multiplication of their Fourier transforms in the frequency domain.\\nFor the time function 1/(t), we have the Fourier-transform pair (see Property 14 in\\nTable 2.2)\\nwhere sgn(f) is the signum function, defined in the frequency domain as\\n(2.56)\\nIt follows, therefore, that the Fourier transform of is given by (2.57) Equation (2.57) states that given a Fourier transformable signal g(t), we may obtain the\\nFourier transform of its Hilbert transform by passing g(t) through a linear time-\\ninvariant system whose frequency response is equal to -jsgn(f). This system may be\\nconsidered as one that produces a phase shift of -90 for all positive frequencies of the input\\nsignal and +90 degrees for all negative frequencies, as in Figure 2.15. The amplitudes of all\\nfrequency components in the signal, however, are unaffected by transmission through the\\ndevice. Such an ideal system is referred to as a Hilbert transformer, or quadrature filter.\\nProperties of the Hilbert Transform\\nThe Hilbert transform differs from the Fourier transform in that it operates exclusively in\\nthe time domain. It has a number of useful properties of its own, some of which are listed\\nnext. The signal g(t) is assumed to be real valued, which is the usual domain of application\\nof the Hilbert transform. For this class of signals, the Hilbert transform has the following\\nproperties.\\nPROPERTY',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 63,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'of the Hilbert transform. For this class of signals, the Hilbert transform has the following\\nproperties.\\nPROPERTY\\nA signal g(t) and its Hilbert transform have the same magnitude spectrum.\\nThat is to say, g t( ) g t( ) 1 --- - g ( ) t  - ---------- d  -   = g t( ) g t( ) 1 t -----  j f( ) sgn - f( ) sgn 1, f 0 > 0, f 0 = 1, - f 0 <      = G f( ) g t( ) G f( ) j f( )G f( ) sgn - = g t( ) g t( ) G f( ) G f( ) =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 63,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nPROPERTY\\nIf is the Hilbert transform of g(t), then the Hilbert transform of is -g(t).\\nAnother way of stating this property is to write\\nPROPERTY\\nA signal g(t) and its Hilbert transform are orthogonal over the entire time interval\\n.\\nIn mathematical terms, the orthogonality of g(t) and is described by\\nProofs of these properties follow from (2.54), (2.55), and (2.57).\\nEXAMPLE\\nHilbert Transform of Low-Pass Signal\\nConsider Figure 2.16a that depicts the Fourier transform of a low-pass signal g(t), whose\\nfrequency content extends from -W to W. Applying the Hilbert transform to this signal\\nyields a new signal whose Fourier transform,\\n, is depicted in Figure 2.16b. This\\nfigure illustrates that the frequency content of a Fourier transformable signal can be\\nradically changed as a result of Hilbert transformation.\\nFigure 2.15\\n(a) Magnitude response and\\n(b) phase response of Hilbert\\ntransform. (a) (b) |H(f)| ( arg[Hf f f )] 0 0 1.0 +90 -90 g t( ) g t( ) G f( ) [ ] arg G f( ) { } arg - = g t( )  , - ( ) g t( ) g t( )g t( )dt  -   0 = g t( ) G f( )',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 64,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '9 Pre-envelopes 45 2.9 Pre-envelopes The Hilbert transform of a signal is defined for both positive and negative frequencies. In\\nlight of the spectrum shaping illustrated in Example 5, a question that begs itself is:\\nHow can we modify the frequency content of a real-valued signal g(t) such that\\nall negative frequency components are completely eliminated?\\nThe answer to this fundamental question lies in the idea of a complex-valued signal called\\nthe pre-envelope6 of g(t), formally defined as\\n(2.58)\\nwhere is the Hilbert transform of g(t). According to this definition, the given signal\\ng(t) is the real part of the pre-envelope g+(t), and the Hilbert transform is the\\nimaginary part of the pre-envelope. An important feature of the pre-envelope g+(t) is the\\nbehavior of its Fourier transform. Let G+(f) denote the Fourier transform of g+(t). Then,\\nusing (2.57) and (2.58) we may write\\n(2.59)\\nNext, invoking the definition of the signum function given in (2.56), we may rewrite (2.59)\\nin the equivalent form\\n(2.60)\\nFigure 2.16 Illustrating application of the Hilbert transform to a low-pass signal:\\n(a) Spectrum of the signal g(t); (b) Spectrum of the Hilbert transform\\n. (a) (b) G(f) G(0) G(0) -G(0) G(f) f 0 -W W -W W f 0 g t( ) g+ t( ) g t( ) jg t( ) + = g t( ) g t( ) G+ f( ) G f( ) f( )G f( ) sgn + = G+ f( ) 2G f( ), f 0 > G 0 ( ), f 0 = 0, f 0 <      =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 65,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nwhere G(0) is the value of G(f) at the origin f = 0. Equation (2.60) clearly shows that the\\npre-envelope of the signal g(t) has no frequency content (i.e., its Fourier transform\\nvanishes) for all negative frequencies, and the question that was posed earlier has indeed\\nbeen answered. Note, however, in order to do this, we had to introduce the complex-valued\\nversion of a real-valued signal as described in (2.58).\\nFrom the foregoing analysis it is apparent that for a given signal g(t) we may determine\\nits pre-envelope g+(t) in one of two equivalent procedures.\\nTime-domain procedure. Given the signal g(t), we use (2.58) to compute the pre-\\nenvelope g+(t).\\nFrequency-domain procedure. We first determine the Fourier transform G(f) of the\\nsignal g(t), then use (2.60) to determine G+(f), and finally evaluate the inverse\\nFourier transform of G+(f) to obtain\\n(2.61)\\nDepending on the description of the signal, procedure 1 may be easier than procedure 2, or\\nvice versa.\\nEquation (2.58) defines the pre-envelope g+(t) for positive frequencies. Symmetrically,\\nwe may define the pre-envelope for negative frequencies as\\n(2.62)\\nThe two pre-envelopes g+(t) and g-(t) are simply the complex conjugate of each other, as\\nshown by\\n(2.63)\\nwhere the asterisk denotes complex conjugation. The spectrum of the pre-envelope g+(t) is\\nnonzero only for positive frequencies; hence the use of a plus sign as the subscript. On the\\nother hand, the use of a minus sign as the subscript is intended to indicate that the\\nspectrum of the other pre-envelope g-(t) is nonzero only for negative frequencies, as\\nshown by the Fourier transform\\n(2.64)\\nThus, the pre-envelope g+(t) and g-(t) constitute a complementary pair of complex-valued\\nsignals. Note also that the sum of g+(t) and g-(t) is exactly twice the original signal g(t).\\nGiven a real-valued signal, (2.60) teaches us that the pre-envelope g+(t) is uniquely',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 66,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'signals. Note also that the sum of g+(t) and g-(t) is exactly twice the original signal g(t).\\nGiven a real-valued signal, (2.60) teaches us that the pre-envelope g+(t) is uniquely\\ndefined by the spectral content of the signal for positive frequencies. By the same token,\\n(2.64) teaches us that the other pre-envelope g-(t) is uniquely defined by the spectral\\ncontent of the signal for negative frequencies. Since g-(t) is simply the complex conjugate\\nof g+(t) as indicated in (2.63), we may now make the following statement:\\nThe spectral content of a Fourier transformable real-valued signal for positive\\nfrequencies uniquely defines that signal.\\ng+ t( ) 2 G f( ) j2ft ( ) exp df 0   = g- t( ) g t( ) jg t( ) - = g- t( ) g+* t( ) = G- f( ) 0, f 0 > G 0 ( ), f 0 = 2G f( ), f 0 <      =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 66,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '10 Complex Envelopes of Band-Pass Signals In other words, given the spectral content of such a signal for positive frequencies, we may\\nuniquely define the spectral content of the signal for negative frequencies. Here then is the\\nmathematical justification for basing the bandwidth of a Fourier transformable signal on\\nits spectral content exclusively for positive frequencies, which is exactly what we did in\\nSection 2.4, dealing with bandwidth.\\nEXAMPLE\\nPre-envelopes of Low-Pass Signal\\nContinuing with the low-pass signal g(t) considered in Example 5, Figure 2.17a and b depict\\nthe corresponding spectra of the pre-envelope g+(t) and the second pre-envelope g-(t), both\\nof which belong to g(t). Whereas the spectrum of g(t) is defined for -W f W as in Figure\\n16a, we clearly see from Figure 2.17 that the spectral content of g+(t) is confined entirely\\nto 0  f  W, and the spectral content of g-(t) is confined entirely to -W f 0.\\nPractical Importance of the Hilbert Transformation\\nAn astute reader may see an analogy between the use of phasors and that of pre-envelopes.\\nIn particular, just as the use of phasors simplifies the manipulations of alternating currents\\nand voltages in the study of circuit theory, so we find the pre-envelope simplifies the\\nanalysis of band-pass signals and band-pass systems in signal theory.\\nMore specifically, by applying the concept of pre-envelope to a band-pass signal, the\\nsignal is transformed into an equivalent low-pass representation. In a corresponding way, a\\nband-pass filter is transformed into its own equivalent low-pass representation. Both\\ntransformations, rooted in the Hilbert transform, play a key role in the formulation of\\nmodulated signals and their demodulation, as demonstrated in what follows in this and\\nsubsequent chapters. Complex Envelopes of Band-Pass Signals\\nThe idea of pre-envelopes introduced in Section 2.9 applies to any real-valued signal, be it\\nof a low-pass or band-pass kind; the only requirement is that the signal be Fourier',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 67,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The idea of pre-envelopes introduced in Section 2.9 applies to any real-valued signal, be it\\nof a low-pass or band-pass kind; the only requirement is that the signal be Fourier\\ntransformable. From this point on and for the rest of the chapter, we will restrict attention\\nto band-pass signals. Such signals are exemplified by signals modulated onto a sinusoidal\\nFigure 2.17 Another illustrative application of the Hilbert transform to a low-pass signal:\\n(a) Spectrum of the pre-envelope g+(t); (b) Spectrum of the other pre-envelope g-(t).\\n(a) (b) 2G(0) 2G(0) G+(f) G-(f) f f -W 0 W',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 67,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\ncarrier. In a corresponding way, when it comes to systems we restrict attention to band-\\npass systems. The primary reason for these restrictions is that the material so presented is\\ndirectly applicable to analog modulation theory, to be covered in Section 2.14, as well as\\nother digital modulation schemes covered in subsequent chapters of the book. With this\\nobjective in mind and the desire to make a consistent use of notation with respect to\\nmaterial to be presented in subsequent chapters, henceforth we will use s(t) to denote a\\nmodulated signal. When such a signal is applied to the input of a band-pass system, such\\nas a communication channel, we will use x(t) to denote the resulting system (e.g., channel)\\noutput. However, as before, we will use h(t) as the impulse response of the system.\\nTo proceed then, let the band-pass signal of interest be denoted by s(t) and its Fourier\\ntransform be denoted by S(f). We assume that the Fourier transform S(f) is essentially\\nconfined to a band of frequencies of total extent 2W, centered about some frequency fc, as\\nillustrated in Figure 2.18a. We refer to fc as the carrier frequency; this terminology is\\nFigure 2.18 (a) Magnitude spectrum of band-pass signal s(t); (b) Magnitude spectrum of\\npre-envelope s+(t); (c) Magnitude spectrum of complex envelope\\n. (a) (b) (c) 2W fff -fc 0 0 0 W -W 2W fc fc fc - W fc + W |S( f )| | S ( f )| | S+( f)| |S( fc)| |S ( fc)| ~ 2 |S ( fc)| 2 s t( )',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 68,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '11 Canonical Representation of Band-Pass Signals borrowed from modulation theory. In the majority of communication signals encountered\\nin practice, we find that the bandwidth 2W is small compared with fc, so we may refer to\\nthe signal s(t) as a narrowband signal. However, a precise statement about how small the\\nbandwidth must be for the signal to be considered narrowband is not necessary for our\\npresent discussion. Hereafter, the terms band-pass and narrowband are used\\ninterchangeably.\\nLet the pre-envelope of the narrowband signal s(t) be expressed in the form\\n(2.65)\\nWe refer to as the complex envelope of the band-pass signal s(t). Equation (2.65)\\nmay be viewed as the basis of a definition for the complex envelope in terms of the\\npre-envelope s+(t). In light of the narrowband assumption imposed on the spectrum of\\nthe band-pass signal s(t), we find that the spectrum of the pre-envelope s+(t) is limited\\nto the positive frequency band fc - W  f  fc + W, as illustrated in Figure 2.18b.\\nTherefore, applying the frequency-shifting property of the Fourier transform to (2.65),\\nwe find that the spectrum of the complex envelope is correspondingly limited to\\nthe band -W f W and centered at the origin f = 0, as illustrated in Figure 2.18c. In\\nother words, the complex envelope of the band-pass signal s(t) is a complex low-\\npass signal. The essence of the mapping from the band-pass signal s(t) to the complex\\nlow-pass signal is summarized in the following threefold statement:\\n\\nThe information content of a modulated signal s(t) is fully preserved in the complex\\nenvelope .  Analysis of the band-pass signal s(t) is complicated by the presence of the carrier\\nfrequency fc; in contrast, the complex envelope dispenses with fc, making its\\nanalysis simpler to deal with.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 69,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The use of requires having to handle complex notations. Canonical Representation of Band-Pass Signals\\nBy definition, the real part of the pre-envelope s+(t) is equal to the original band-pass\\nsignal s(t). We may therefore express the band-pass signal s(t) in terms of its\\ncorresponding complex envelope as\\n(2.66)\\nwhere the operator Re[.] denotes the real part of the quantity enclosed inside the square\\nbrackets. Since, in general, is a complex-valued quantity, we emphasize this property\\nby expressing it in the Cartesian form\\n(2.67)\\nwhere sI(t) and sQ(t) are both real-valued low-pass functions; their low-pass property is\\ninherited from the complex envelope\\n. We may therefore use (2.67) in (2.66) to\\nexpress the original band-pass signal s(t) in the canonical or standard form\\n(2.68)\\nWe refer to sI(t) as the in-phase component of the band-pass signal s(t) and refer to sQ(t) as\\nthe quadrature-phase component or simply the quadrature component of the signal s(t).\\ns+ t( ) s t( ) j2fct ( ) exp = s t( ) s t( ) s t( ) s t( ) s t( ) s t( ) s t( ) s t( ) s t( ) s t( ) Re s t( ) j2fct ( ) exp [ ] = s t( ) s t( ) sI t( ) jsQ t( ) + = s t( ) s t( ) sI t( ) 2fct ( ) sQ t( ) 2fct ( ) sin - cos =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 69,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nThis nomenclature follows from the following observation: if cos(2fct), the multiplying\\nfactor of sI(t), is viewed as the reference sinusoidal carrier, then sin(2fct), the multiplying\\nfactor of sQ(t), is in phase quadrature with respect to cos(2fct).\\nAccording to (2.66), the complex envelope may be pictured as a time-varying\\nphasor positioned at the origin of the (sI, sQ)-plane, as indicated in Figure 2.19a. With\\ntime t varying continuously, the end of the phasor moves about in the plane. Figure 2.19b\\ndepicts the phasor representation of the complex exponential exp(2fct). In the definition\\ngiven in (2.66), the complex envelope is multiplied by the complex exponential\\nexp(j2fct). The angles of these two phasors, therefore, add and their lengths multiply, as\\nshown in Figure 2.19c. Moreover, in this latter figure, we show the (sI, sQ)-phase rotating\\nwith an angular velocity equal to 2fc radians per second. Thus, in the picture portrayed in\\nthe figure, the phasor representing the complex envelope moves in the (sI, sQ)-plane,\\nwhile at the very same time the plane itself rotates about the origin. The original band-pass\\nsignal s(t) is the projection of this time-varying phasor on a fixed line representing the real\\naxis, as indicated in Figure 2.19c.\\nSince both sI(t) and sQ(t) are low-pass signals limited to the band -W  f  W, they may\\nbe extracted from the band-pass signal s(t) using the scheme shown in Figure 2.20a. Both\\nlow-pass filters in this figure are designed identically, each with a bandwidth equal to W.\\nFigure 2.19 Illustrating an interpretation of the complex envelope and its multiplication by\\nexp(j2fct). s t( ) s t( ) s t( ) sI + sQ 2 2 sI sQ (a) 0 tan-1 sQ sI ( ) (b) 0 sI + sQ 2 2 sI sQ (c) tan-1 ( ) Real axis Imaginary axis 1 2 fc 2 fct s(t) 2 fc Rotate at the rate 2 fc Rotate at the rate Real axis 0 t sQ sI     s t( )',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 70,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '11 Canonical Representation of Band-Pass Signals To reconstruct s(t) from its in-phase and quadrature components, we may use the scheme\\nshown in Figure 2.20b. In light of these statements, we may refer to the scheme in Figure\\n20a as an analyzer, in the sense that it extracts the in-phase and quadrature components,\\nsI(t) and sQ(t), from the band-pass signal s(t). By the same token, we may refer to the\\nsecond scheme in Figure 2.20b as a synthesizer, in the sense it reconstructs the band-pass\\nsignal s(t) from its in-phase and quadrature components, sI(t) and sQ(t).\\nThe two schemes shown in Figure 2.20 are basic to the study of linear modulation\\nschemes, be they of an analog or digital kind. Multiplication of the low-pass in-phase\\ncomponent sI(t) by cos(2fct) and multiplication of the quadrature component sQ(t) by\\nsin(2fct) represent linear forms of modulation. Provided that the carrier frequency fc is\\nlarger than the low-pass bandwidth W, the resulting band-pass function s(t) defined in\\n(2.68) is referred to as a passband signal waveform. Correspondingly, the mapping from\\nsI(t) and sQ(t) combined into s(t) is known as passband modulation.\\nPolar Representation of Band-Pass Signals\\nEquation (2.67) is the Cartesian form of defining the complex envelope of the band-\\npass signal s(t). Alternatively, we may define in the polar form as\\n(2.69)\\nwhere a(t) and (t) are both real-valued low-pass functions. Based on the polar\\nrepresentation of (2.69), the original band-pass signal s(t) is itself defined by\\n(2.70)\\nWe refer to a(t) as the natural envelope or simply the envelope of the band-pass signal s(t)\\nand refer to (t) as the phase of the signal. We now see why the term pre-envelope was\\nused in referring to (2.58), the formulation of which preceded that of (2.70).\\nFigure 2.20 (a) Scheme for deriving the in-phase and quadrature components of a band-pass\\nsignal g(t). (b) Scheme for reconstructing the band-pass signal from its in-phase and quadrature',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 71,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Figure 2.20 (a) Scheme for deriving the in-phase and quadrature components of a band-pass\\nsignal g(t). (b) Scheme for reconstructing the band-pass signal from its in-phase and quadrature\\ncomponents. s(t) (a) (b) Oscillator 90-phase shifter Low-pass filter Low-pass filter Oscillator -90-phase shifter sI(t) sQ(t) 2 cos (2fct) -2 sin (2fct) sin (2fct) cos (2fct) sI(t) s(t) + - sQ(t) s t( ) s t( ) s t( ) a t( ) j t( ) [ ] exp = s t( ) a t( ) 2fct  t( ) + [ ] cos =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 71,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nRelationship Between Cartesian and Polar Representations of\\nBand-Pass Signal\\nThe envelope a(t) and phase (t) of a band-pass signal s(t) are respectively related to the\\nin-phase and quadrature components sI(t) and sQ(t) as follows (see the time-varying\\nphasor representation of Figure 2.19a):\\n(2.71) and (2.72) Conversely, we may write\\n(2.73) and (2.74) Thus, both the in-phase and quadrature components of a band-pass signal contain\\namplitude and phase information, both of which are uniquely defined for a prescribed\\nphase (t), modulo 2. Complex Low-Pass Representations of Band-Pass Systems\\nNow that we know how to handle the complex low-pass representation of band-pass\\nsignals, it is logical that we develop a corresponding procedure for handling the\\nrepresentation of linear time-invariant band-pass systems. Specifically, we wish to show\\nthat the analysis of band-pass systems is greatly simplified by establishing an analogy,\\nmore precisely an isomorphism, between band-pass and low-pass systems. For example,\\nthis analogy would help us to facilitate the computer simulation of a wireless\\ncommunication channel driven by a sinusoidally modulated signal, which otherwise could\\nbe a difficult proposition.\\nConsider a narrowband signal s(t), with its Fourier transform denoted by S(f). We\\nassume that the spectrum of the signal s(t) is limited to frequencies within W hertz of the\\ncarrier frequency fc. We also assume that W < fc. Let the signal s(t) be applied to a linear\\ntime-invariant band-pass system with impulse response h(t) and frequency response H(f).\\nWe assume that the frequency response of the system is limited to frequencies within B\\nof the carrier frequency fc. The system bandwidth 2B is usually narrower than or equal to\\nthe input signal bandwidth 2W. We wish to represent the band-pass impulse response h(t)\\nin terms of two quadrature components, denoted by hI(t) and hQ(t). In particular, by',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 72,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the input signal bandwidth 2W. We wish to represent the band-pass impulse response h(t)\\nin terms of two quadrature components, denoted by hI(t) and hQ(t). In particular, by\\nanalogy to the representation of band-pass signals, we express h(t) in the form\\n(2.75)\\nCorrespondingly, we define the complex impulse response of the band-pass system as\\n(2.76) a t( ) sI 2 t( ) sQ 2 t( ) + =  t( ) sQ t( ) sI t( ) ------------     1 - tan = sI t( ) a t( )  t( ) [ ] cos = sQ t( ) a t( )  t( ) [ ] sin = h t( ) hI t( ) 2fct ( ) hQ t( ) 2fct ( ) sin - cos = h t( ) hI t( ) jhQ t( ) + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 72,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '12 Complex Low-Pass Representations of Band-Pass Systems Hence, following (2.66), we may express h(t) in terms of as\\n(2.77)\\nNote that hI(t), hQ(t), and are all low-pass functions, limited to the frequency band\\n-B  f  B.\\nWe may determine the complex impulse response in terms of the in-phase and\\nquadrature components hI(t) and hQ(t) of the band-pass impulse response h(t) by building\\non (2.76). Alternatively, we may determine it from the band-pass frequency response H(f)\\nin the following way. We first use (2.77) to write (2.78) where is the complex conjugate of\\n; the rationale for introducing the factor of\\non the left-hand side of (2.78) follows from the fact that if we add a complex signal and its\\ncomplex conjugate, the sum adds up to twice the real part and the imaginary parts cancel.\\nApplying the Fourier transform to both sides of (2.78) and using the complex-conjugation\\nproperty of the Fourier transform, we get (2.79) where and . Equation (2.79) satisfies the requirement that\\nH*(f) = H(-f) for a real-valued impulse response h(t). Since represents a low-pass\\nfrequency response limited to | f |  B with B < fc, we infer from (2.79) that\\n(2.80)\\nEquation (2.80) states:\\nFor a specified band-pass frequency response H(f), we may determine the\\ncorresponding complex low-pass frequency response by taking the part of\\nH(f) defined for positive frequencies, shifting it to the origin, and scaling it by\\nthe factor 2.\\nHaving determined the complex frequency response\\n, we decompose it into its in-\\nphase and quadrature components, as shown by\\n(2.81)\\nwhere the in-phase component is defined by\\n(2.82)\\nand the quadrature component is defined by\\n(2.83)\\nFinally, to determine the complex impulse response of the band-pass system, we take\\nthe inverse Fourier transform of\\n, obtaining\\n(2.84)\\nwhich is the formula we have been seeking.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 73,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '(2.83)\\nFinally, to determine the complex impulse response of the band-pass system, we take\\nthe inverse Fourier transform of\\n, obtaining\\n(2.84)\\nwhich is the formula we have been seeking.\\nh t( ) h t( ) Re h t( ) j2fct ( ) exp [ ] = h t( ) h t( ) 2h t( ) h t( ) j2fct ( ) h* t( ) j2fct - ( ) exp + exp = h* t( ) h t( ) 2H f( ) H f fc - ( ) H * f - fc - ( ) + = H f( ) h t( ) H f( ) h t( ) H f( ) H f fc - ( ) 2H f( ), f 0 > = H f( ) H f( ) H f( ) H I f( ) jH Q f( ) + = H I f( ) 1 2--- H f( ) H * f - ( ) + [ ] = H Q f( ) 1 2j ----- H f( ) jH * f - ( ) - [ ] = h t( ) H f( ) h t( ) H f( ) j2ft ( ) exp df  -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 73,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems Putting the Complex Representations of Band-Pass Signals\\nand Systems All Together\\nExamining (2.66) and (2.77), we immediately see that these two equations share a\\ncommon multiplying factor: the exponential exp(j2fct). In practical terms, the inclusion\\nof this factor accounts for a sinusoidal carrier of frequency fc, which facilitates\\ntransmission of the modulated (band-pass) signal s(t) across a band-pass channel of\\nmidband frequency fc. In analytic terms, however, the presence of this exponential factor\\nin both (2.66) and (2.77) complicates the analysis of the band-pass system driven by the\\nmodulated signal s(t). This analysis can be simplified through the combined use of\\ncomplex low-pass equivalent representations of both the modulated signal s(t) and the\\nband-pass system characterized by the impulse response h(t). The simplification can be\\ncarried out in the time domain or frequency domain, as discussed next.\\nThe Time-Domain Procedure\\nEquipped with the complex representations of band-pass signals and systems, we are\\nready to derive an analytically efficient method for determining the output of a band-pass\\nsystem driven by a corresponding band-pass signal. To proceed with the derivation,\\nassume that S(f), denoting the spectrum of the input signal s(t), and H(f), denoting the\\nfrequency response of the system, are both centered around the same frequency fc. In\\npractice, there is no need to consider a situation in which the carrier frequency of the input\\nsignal is not aligned with the midband frequency of the band-pass system, since we have\\nconsiderable freedom in choosing the carrier or midband frequency. Thus, changing the\\ncarrier frequency of the input signal by an amount fc, for example, simply corresponds to\\nabsorbing (or removing) the factor exp(j2fct) in the complex envelope of the input\\nsignal or the complex impulse response of the band-pass system. We are therefore justified',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 74,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'absorbing (or removing) the factor exp(j2fct) in the complex envelope of the input\\nsignal or the complex impulse response of the band-pass system. We are therefore justified\\nin proceeding on the assumption that S(f) and H(f) are both centered around the same\\ncarrier frequency fc.\\nLet x(t) denote the output signal of the band-pass system produced in response to the\\nincoming band-pass signal s(t). Clearly, x(t) is also a band-pass signal, so we may\\nrepresent it in terms of its own low-pass complex envelope as\\n(2.85)\\nThe output signal x(t) is related to the input signal s(t) and impulse response h(t) of the\\nsystem in the usual way by the convolution integral\\n(2.86)\\nIn terms of pre-envelopes, we have h(t) = Re[h+(t)] and s(t) = Re[s+(t)]. We may therefore\\nrewrite (2.86) in terms of the pre-envelopes s+(t) and h+(t) as\\n(2.87) x t( ) x t( ) Re x t( ) j2fct ( ) exp [ ] = x t( ) h  ( )s t  - ( ) d  -   = x t( ) Re h+  ( ) [ ]Re s+ t  - ( ) [ ] d  -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 74,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '13 Putting the Complex Representations of Band-Pass Signals and Systems All Together To proceed further, we make use of a basic property of pre-envelopes that is described by\\nthe following relation:\\n(2.88)\\nwhere we have used  as the integration variable to be consistent with that in (2.87); details\\nof (2.88) are presented in Problem 2.20. Next, from Fourier-transform theory we note that\\nusing s(-) in place of s() has the effect of removing the complex conjugation on the\\nright-hand side of (2.88). Hence, bearing in mind the algebraic difference between the\\nargument of s+() in (2.88) and that of s+(t - ) in (2.87), and using the relationship\\nbetween the pre-envelope and complex envelope of a band-pass signal, we may express\\n(2.87) in the equivalent form\\n(2.89)\\nThus, comparing the right-hand sides of (2.85) and (2.89), we readily find that for a large\\nenough carrier frequency fc, the complex envelope of the output signal is simply\\ndefined in terms of the complex envelope of the input signal and the complex impulse\\nresponse of the band-pass system as follows:\\n(2.90)\\nThis important relationship is the result of the isomorphism between a band-pass function\\nand the corresponding complex low-pass function, in light of which we may now make the\\nfollowing summarizing statement:\\nExcept for the scaling factor 1/2, the complex envelope of the output\\nsignal of a band-pass system is obtained by convolving the complex impulse\\nresponse of the system with the complex envelope of the input\\nband-pass signal.\\nIn computational terms, the significance of this statement is profound. Specifically, in\\ndealing with band-pass signals and systems, we need only concern ourselves with the\\nfunctions , , and , representing the complex low-pass equivalents of the\\nexcitation applied to the input of the system, the response produced at the output of the\\nsystem, and the impulse response of the system respectively, as illustrated in Figure 2.21.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 75,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'excitation applied to the input of the system, the response produced at the output of the\\nsystem, and the impulse response of the system respectively, as illustrated in Figure 2.21.\\nThe essence of the filtering process performed in the original system of Figure 2.21a is\\ncompletely retained in the complex low-pass equivalent representation depicted in Figure\\n21b.\\nThe complex envelope of the input band-pass signal and the complex impulse\\nresponse of the band-pass system are defined in terms of their respective in-phase\\nRe h+  ( ) [ ]Re s+  ( ) [ ]d  -   1 2---Re h+  ( )s+ *  ( ) d  -   = x t( ) 1 2---Re h+  ( )  -   s+ t  - ( ) d = 1 2---Re h  ( ) j2fc ( )s t  - ( ) exp  -   j2fc t  - ( ) [ ] d exp       = 1 2---Re j2fct ( ) h  ( )s t  - ( ) d  -   exp = x t( ) s t( ) h t( ) x t( ) 1 2--- h t( )s t  - ( ) d  -   = x t( ) h t( ) s t( ) s t( ) x t( ) h t( ) s t( ) h t( )',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 75,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nand quadrature components by (2.67) and (2.76), respectively. Substituting these relations\\ninto (2.90), we get (2.91) where the symbol denotes convolution. Because convolution is distributive, we may\\nrewrite (2.91) in the equivalent form\\n(2.92)\\nLet the complex envelope of the response be defined in terms of its in-phase and\\nquadrature components as\\n(2.93)\\nThen, comparing the real and imaginary parts in (2.92) and (2.93), we find that the in-\\nphase component xI(t) is defined by the relation\\n(2.94)\\nand its quadrature component xQ(t) is defined by the relation\\n(2.95)\\nThus, for the purpose of evaluating the in-phase and quadrature components of the\\ncomplex envelope of the system output, we may use the low-pass equivalent model\\nshown in Figure 2.22. All the signals and impulse responses shown in this model are real-\\nvalued low-pass functions; hence a time-domain procedure for simplifying the analysis of\\nband-pass systems driven by band-pass signals.\\nThe Frequency-Domain Procedure\\nAlternatively, Fourier-transforming the convolution integral of (2.90) and recognizing that\\nconvolution in the time domain is changed into multiplication in the frequency domain, we\\nget (2.96) Figure 2.21 (a) Input-output description of a band-pass\\nsystem; (b) Complex low-pass equivalent\\nmodel of the band-pass system.\\n(a)\\n(b)\\nInput band-pass signal\\nBand-pass impulse response Complex impulse response Output signal Scaled complex envelope of the output Complex envelope of the input s(t) h(t) h(t) x(t) 2x(t) s(t) Mapper 2x t( ) h t( )s t( ) = hI t( ) jhQ t( ) + [ ]sI t( ) jsQ t( ) + [ ] =  2x t( ) hI t( )sI t( ) hQ t( )sQ t( ) - [ ] j hQ t( )sI t( ) hIsQ t( ) + [ ] + = x t( ) x t( ) xI t( ) jxQ t( ) + = 2xI t( ) hI t( )sI t( ) hQ t( )sQ t( ) - = 2xQ t( ) hQ t( )sI t( ) hI t( )sQ t( ) + = x t( ) X f( ) 1 2---H f( )S f( ) =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 76,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '13 Putting the Complex Representations of Band-Pass Signals and Systems All Together\\n57 where , and . The is itself related to the\\nfrequency response H(f) of the band-pass system by (2.80). Thus, assuming that H(f) is\\nknown, we may use the frequency-domain procedure summarized in Table 2.4 for\\ncomputing the system output x(t) in response to the system input s(t).\\nIn actual fact, the procedure of Table 2.4 is the frequency-domain representation of the\\nlow-pass equivalent to the band-pass system, depicted in Figure 2.21b. In computational\\nterms, this procedure is of profound practical significance. We say so because its use\\nalleviates the analytic and computational difficulty encountered in having to include the\\ncarrier frequency fc in the pertinent calculations.\\nAs discussed earlier in the chapter, the theoretical formulation of the low-pass\\nequivalent in Figure 2.21b is rooted in the Hilbert transformation, the evaluation of which\\nposes a practical problem of its own, because of the wideband 90o-phase shifter involved\\nin its theory. Fortunately, however, we do not need to invoke the Hilbert transform in\\nconstructing the low-pass equivalent. This is indeed so, when a message signal modulated\\nonto a sinusoidal carrier is processed by a band-pass filter, as explained here:\\nTypically, the message signal is band limited for all practical purposes. Moreover,\\nthe carrier frequency is larger than the highest frequency component of the signal;\\nthe modulated signal is therefore a band-pass signal with a well-defined passband.\\nHence, the in-phase and quadrature components of the modulated signal s(t),\\nrepresented respectively by sI(t) and sQ(t), are readily obtained from the canonical\\nrepresentation of s(t), described in (2.68).\\nGiven the well-defined frequency response H(f) of the band-pass system, we may\\nreadily evaluate the corresponding complex low-pass frequency response\\n; see\\n(2.80). Hence, we may compute the system output x(t) produced in response to the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 77,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'readily evaluate the corresponding complex low-pass frequency response\\n; see\\n(2.80). Hence, we may compute the system output x(t) produced in response to the\\ncarrier-modulated input s(t) without invoking the Hilbert transform.\\nFigure 2.22 Block diagram illustrating the relationship between the\\nin-phase and quadrature components of the response of a band-pass\\nfilter and those of the input signal.\\n2xI (t) hI (t) sI (t) sQ(t) hQ(t) + - 2xQ(t) hQ(t) hI (t) + + s t( ) S f( ), h t( ) H f( ) x t( ) X f( ) H f( ) H f( )',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 77,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nProcedure for Efficient Simulation of Communication Systems\\nTo summarize, the frequency-domain procedure described in Table 2.4 is well suited for\\nthe efficient simulation of communication systems on a computer for two reasons:\\nThe low-pass equivalents of the incoming band-pass signal and the band-pass system\\nwork by eliminating the exponential factor exp(j2fct) from the computation without\\nloss of information.\\nThe fast Fourier transform (FFT) algorithm, discussed later in the chapter, is used\\nfor numerical computation of the Fourier transform. This algorithm is used twice in\\nTable 2.4, once in step 2 to perform Fourier transformation, and then again in step\\nto perform inverse Fourier transformation.\\nThe procedure of this table, rooted largely in the frequency domain, assumes availability\\nof the band-pass systems frequency response H(f). If, however, it is the systems impulse\\nresponse h(t) that is known, then all we need is an additional step to Fourier transform h(t)\\ninto H(f) before initiating the procedure of Table 2.4. Linear Modulation Theory\\nThe material presented in Sections 2.8-2.13 on the complex low-pass representation of\\nband-pass signals and systems is of profound importance in the study of communication\\ntheory. In particular, we may use the canonical formula of (2.68) as the mathematical basis\\nfor a unified treatment of linear modulation theory, which is the subject matter of this\\nsection.\\nTable 2.4 Procedure for the computational analysis of a band-pass system\\ndriven by a band-pass signal\\nGiven the frequency response H(f) of a band-pass system, computation of the output\\nsignal x(t) of the system in response to an input band-pass signal s(t) is summarized as\\nfollows:\\nUse (2.80), namely\\n, for f > 0 to determine\\n.\\nExpressing the input band-pass signal s(t) in the canonical form of (2.68), evaluate\\nthe complex envelope where sI(t) is the in-phase component',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 78,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'follows:\\nUse (2.80), namely\\n, for f > 0 to determine\\n.\\nExpressing the input band-pass signal s(t) in the canonical form of (2.68), evaluate\\nthe complex envelope where sI(t) is the in-phase component\\nof s(t) and sQ(t) is its quadrature component. Hence, compute the Fourier\\ntransform\\nUsing (2.96), compute\\n, which defines the Fourier transform of\\nthe complex envelope of the output signal x(t).\\nCompute the inverse Fourier transform of\\n, yielding\\nUse (2.85) to compute the desired output signal\\nH f fc - ( ) 2H f( ) = H f( ) s t( ) sI t( ) jsQ t( ) + = S f( ) F s t( ) [ ] = X f( ) 1 2---H f( )S f( ) = x t( ) X f( ) x t( ) F 1 - X f( ) [ ] = x t( ) Re x t( ) j2fct ( ) exp [ ] =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 78,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '14 Linear Modulation Theory We start this treatment with a formal definition:\\nModulation is a process by means of which one or more parameters of a\\nsinusoidal carrier are varied in accordance with a message signal so as to\\nfacilitate transmission of that signal over a communication channel.\\nThe message signal (e.g., voice, video, data sequence) is referred to as the modulating\\nsignal, and the result of the modulation process is referred to as the modulated signal.\\nNaturally, in a communication system, modulation is performed in the transmitter. The\\nreverse of modulation, aimed at recovery of the original message signal in the receiver, is\\ncalled demodulation.\\nConsider the block diagram of Figure 2.23, depicting a modulator, where m(t) is the\\nmessage signal, cos(2fct) is the carrier, and s(t) is the modulated signal. To apply (2.68)\\nto this modulator, the in-phase component sI(t) in that equation is treated simply as a\\nscaled version of the message signal denoted by m(t). As for the quadrature component\\nsQ(t), it is defined by a spectrally shaped version of m(t) that is performed linearly. In such\\na scenario, it follows that a modulated signal s(t) defined by (2.68) is a linear function of\\nthe message signal m(t); hence the reference to this equation as the mathematical basis of\\nlinear modulation theory.\\nTo recover the original message signal m(t) from the modulated signal s(t), we may use\\na demodulator, the block diagram of which is depicted in Figure 2.24. An elegant feature\\nof linear modulation theory is that demodulation of s(t) is also achieved using linear\\noperations. However, for linear demodulation of s(t) to be feasible, the locally generated\\ncarrier in the demodulator of Figure 2.24 has to be synchronous with the original\\nsinusoidal carrier used in the modulator of Figure 2.23. Accordingly, we speak of\\nsynchronous demodulation or coherent detection.\\nFigure 2.23 Block diagram of a modulator.\\nFigure 2.24 Block diagram of a demodulator.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 79,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'synchronous demodulation or coherent detection.\\nFigure 2.23 Block diagram of a modulator.\\nFigure 2.24 Block diagram of a demodulator.\\nModulator Message signal m(t) Carrier cos(2fct) Modulated signal s(t) Demodulator Locally generated carrier cos(2fct) Demodulated signal m(t) Modulated signal s(t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 79,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nDepending on the spectral composition of the modulated signal, we have three kinds of\\nlinear modulation in analog communications:\\n\\ndouble sideband-suppressed carrier (DSB-SC) modulation;\\n\\nvestigial sideband (VSB) modulation;\\n\\nsingle sideband (SSB) modulation.\\nThese three methods of modulation are discussed in what follows and in this order.\\nDSB-SC Modulation\\nDSB-SC modulation is the simplest form of linear modulation, which is obtained by\\nsetting\\nand\\nAccordingly, (2.68) is reduced to\\n(2.97)\\nthe implementation of which simply requires a product modulator that multiplies the\\nmessage signal m(t) by the carrier\\n, assumed to be of unit amplitude.\\nFor a frequency-domain description of the DSB-SC-modulated signal defined in (2.97),\\nsuppose that the message signal m(t) occupies the frequency band -W  f  W, as depicted\\nin Figure 2.25a; hereafter, W is referred to as the message bandwidth. Then, provided that\\nthe carrier frequency satisfies the condition fc > W, we find that the spectrum of the DSB-\\nSC-modulated signal consists of an upper sideband and lower sideband, as depicted in\\nFigure 2.25b. Comparing the two parts of this figure, we immediately see that the channel\\nbandwidth, B, required to support the transmission of the DSB-SC-modulated signal from\\nthe transmitter to the receiver is twice the message bandwidth.\\nFigure 2.25 (a) Message spectrum. (b) Spectrum of DSB-SC\\nmodulated wave s(t), assuming fc > W.\\nsI t( ) m t( ) = sQ t( ) 0 = s t( ) m t( ) 2fct ( ) cos = 2nfct ( ) cos Lower sideband Upper sideband M(f ) S(f ) -fc 0 (a) (b) f -fc - W -fc + W fc fc - W fc + W -W W 0 f 1 2 M(0) M(0)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 80,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '14 Linear Modulation Theory One other interesting point apparent from Figure 2.25b is that the spectrum of the DSB-SC\\nmodulated signal is entirely void of delta functions. This statement is further testimony to the\\nfact that the carrier is suppressed from the generation of the modulated signal s(t) of (2.97).\\nSummarizing the useful features of DSB-SC modulation:\\n\\nsuppression of the carrier, which results in saving of transmitted power;\\n\\ndesirable spectral characteristics, which make it applicable to the modulation of\\nband-limited message signals;',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 81,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'ease of synchronizing the receiver to the transmitter for coherent detection.\\nOn the downside, DSB-SC modulation is wasteful of channel bandwidth. We say so for\\nthe following reason. The two sidebands, constituting the spectral composition of the\\nmodulated signal s(t), are actually the image of each other with respect to the carrier\\nfrequency fc; hence, the transmission of either sideband is sufficient for transporting s(t)\\nacross the channel.\\nVSB Modulation\\nIn VSB modulation, one sideband is partially suppressed and a vestige of the other\\nsideband is configured in such a way to compensate for the partial sideband suppression\\nby exploiting the fact that the two sidebands in DSB-SC modulation are the image of each\\nother. A popular method of achieving this design objective is to use the frequency\\ndiscrimination method. Specifically, a DSB-SC-modulated signal is first generated using a\\nproduct modulator, followed by a band-pass filter, as shown in Figure 2.26. The desired\\nspectral shaping is thereby realized through the appropriate design of the band-pass filter.\\nSuppose that a vestige of the lower sideband is to be transmitted. Then, the frequency\\nresponse of the band-pass filter, H(f), takes the form shown in Figure 2.27; to simplify\\nmatters, only the frequency response for positive frequencies is shown in the figure.\\nExamination of this figure reveals two characteristics of the band-pass filter:\\nNormalization of the frequency response, which means that\\n(2.98)\\nwhere f is the vestigial bandwidth and the other parameters are as previously\\ndefined.\\nOdd symmetry of the cutoff portion inside the transition interval fc - f  | f |  fc + f,\\nwhich means that values of the frequency response H(f) at any two frequencies\\nequally spaced above and below the carrier frequency add up to unity.\\nH f( ) 1 for fc f f fc W + <  + 1 2--- for f fc =      = Figure 2.26 Frequency-discrimination method\\nfor producing VSB modulation\\nwhere the intermediate signal sI(t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 81,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'H f( ) 1 for fc f f fc W + <  + 1 2--- for f fc =      = Figure 2.26 Frequency-discrimination method\\nfor producing VSB modulation\\nwhere the intermediate signal sI(t)\\nis DSB-SC modulated. Product modulator Band-pass filter H(f ) Carrier cos(2fct) Modulated signal s(t) Message signal m(t) sI(t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 81,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nConsequently, we find that shifted versions of the frequency response H(f) satisfy the\\ncondition (2.99) Outside the frequency band of interest defined by | f |  fc + W, the frequency response\\nH(f) can assume arbitrary values. We may thus express the channel bandwidth required\\nfor the transmission of VSB-modulated signals as\\n(2.100)\\nWith this background, we now address the issue of how to specify H(f). We first use the\\ncanonical formula of (2.68) to express the VSB-modulated signal s1(t), containing a\\nvestige of the lower sideband, as\\n(2.101)\\nwhere m(t) is the message signal, as before, and mQ(t) is the spectrally shaped version of\\nm(t); the reason for the factor 1/2 will become apparent later. Note that if mQ(t) is set equal\\nto zero, (2.101) reduces to DSB-SC modulation. It is therefore in the quadrature signal\\nmQ(t) that VSB modulation distinguishes itself from DSB-SC modulation. In particular,\\nthe role of mQ(t) is to interfere with the message signal m(t) in such a way that power in\\none of the sidebands of the VSB-modulated signal s(t) (e.g., the lower sideband in Figure\\n27) is appropriately reduced.\\nTo determine mQ(t), we examine two different procedures:\\nPhase-discrimination, which is rooted in the time-domain description of (2.101);\\ntransforming this equation into the frequency domain, we obtain\\n(2.102)\\nwhere\\nFrequency-discrimination, which is structured in the manner described in Figure\\n26; passing the DSB-SC-modulated signal (i.e., the intermediate signal sI(t) in\\nFigure 2.26) through the band-pass filter, we write\\n(2.103)\\nFigure 2.27 Magnitude response of VSB filter; only the\\npositive-frequency portion is shown',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 82,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Figure 2.26) through the band-pass filter, we write\\n(2.103)\\nFigure 2.27 Magnitude response of VSB filter; only the\\npositive-frequency portion is shown\\n5 0 1.0 |H( f )| fc - fv fc + fv fc + W fc f Hf fc - ( ) H f fc + ( ) + 1 for WfW   - = B W f + = s1 t( ) 1 2---m t( ) 2fct ( ) 1 2---mQ t( ) 2fct ( ) sin - cos = Sl f( ) 1 4--- M f fc - ( ) M f fc + ( ) + [ ] 1 4j ----- MQ f fc - ( ) MQ f fc + ( ) - [ ] - = M f( ) Fmt( ) [ ] and MQ f( ) F mQ t( ) [ ] = = Sl f( ) 1 2--- M f fc - ( ) M f fc + ( ) + [ ]H f( ) =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 82,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '14 Linear Modulation Theory In both (2.102) and (2.103), the spectrum S1(f) is defined in the frequency interval\\nfc - W  | f |  fc + W\\nEquating the right-hand sides of these two equations, we get (after canceling common\\nterms)\\n(2.104)\\nShifting both sides of (2.104) to the left by the amount fc, we get (after canceling common\\nterms) (2.105) where the terms and are ignored as they both lie outside the\\ninterval - W  | f |  W. Next, shifting both sides of (2.104) by the amount fc, but this time\\nto the right, we get (after canceling common terms)\\n(2.106)\\nwhere, this time, the terms and are ignored as they both lie\\noutside the interval - W  | f |  W.\\nGiven (2.105) and (2.106), all that remains to be done now is to follow two simple\\nsteps:\\nAdding these two equations and then factoring out the common term M(f), we get\\nthe condition of (2.99) previously imposed on H(f); indeed, it is with this condition\\nin mind that we introduced the scaling factor 1/2 in (2.101).\\nSubtracting (2.105) from (2.106) and rearranging terms, we get the desired\\nrelationship between MQ(f) and M(f):\\n(2.107)\\nLet HQ(f) denote the frequency response of a quadrature filter that operates on the\\nmessage spectrum M(f) to produce MQ(f). In light of (2.107), we may readily define\\nHQ(f) in terms of H(f) as\\n(2.108)\\nEquation (2.108) provides the frequency-domain basis for the phase-discrimination\\nmethod for generating the VSB-modulated signal s1(t), where only a vestige of the lower\\nsideband is retained. With this equation at hand, it is instructive to plot the frequency\\nresponse HQ(f). For the frequency interval -W  f  W, the term H(f - fc) is defined by the\\nresponse H(f) for negative frequencies shifted to the right by fc, whereas the term H(f + fc)\\nis defined by the response H(f) for positive frequencies shifted to the left by fc.\\nAccordingly, building on the positive frequency response plotted in Figure 2.27, we find\\nthat the corresponding plot of HQ(f) is shaped as shown in Figure 2.28.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 83,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Accordingly, building on the positive frequency response plotted in Figure 2.27, we find\\nthat the corresponding plot of HQ(f) is shaped as shown in Figure 2.28.\\n1 2--- M f fc - ( ) M f fc + ( ) + [ ] 1 2j ----- MQ f fc - ( ) MQ f fc + ( ) - [ ] - M f fc - ( ) M f fc + ( ) + [ ]H f( ) = 1 2---M f( ) 1 2j----MQ f( ) - M f( )H f fc + ( ), W - f W   = M f 2fc + ( ) MQ f 2fc + ( ) 1 2---M f( ) 1 2j----MQ f( ) + M f( )H f fc - ( ), W - f W   = M f 2fc - ( ) MQ f 2fc - ( ) MQ f( ) j H [ f fc - ( ) H f fc + ( )]M f( ) - , W - f W   = HQ f( ) MQ f( ) M f( ) --------------- = jHf fc - ( ) H f fc + ( ) - [ ] W - , f W   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 83,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nThe discussion on VSB modulation has thus far focused on the case where a vestige of the\\nlower sideband is transmitted. For the alternative case when a vestige of the upper sideband\\nis transmitted, we find that the corresponding VSB-modulated wave is described by\\n(2.109)\\nwhere the quadrature signal is constructed from the message signal m(t) in exactly\\nthe same way as before.\\nEquations (2.101) and (2.109) are of the same mathematical form, except for an\\nalgebraic difference; they may, therefore, be combined into the single formula\\n(2.110)\\nwhere the minus sign applies to a VSB-modulated signal containing a vestige of the lower\\nsideband and the plus sign applies to the alternative case when the modulated signal\\ncontains a vestige of the upper sideband.\\nThe formula of (2.110) for VSB modulation includes DSB-SC modulation as a special\\ncase. Specifically, setting mQ(t) = 0, this formula reduces to that of (2.97) for DSB-SC\\nmodulation, except for the trivial scaling factor of 1/2.\\nSSB Modulation\\nNext, considering SSB modulation, we may identify two choices:\\nThe carrier and the lower sideband are both suppressed, leaving the upper sideband\\nfor transmission in its full spectral content; this first SSB-modulated signal is\\ndenoted by sUSB(t).\\nThe carrier and the upper sideband are both suppressed, leaving the lower sideband\\nfor transmission in its full spectral content; this second SSB-modulated signal is\\ndenoted by sLSB(t).\\nThe Fourier transforms of these two modulated signals are the image of each other with\\nrespect to the carrier frequency fc, which, as mentioned previously, emphasizes that the\\ntransmission of either sideband is actually sufficient for transporting the message signal\\nm(t) over the communication channel. In practical terms, both sUSB(t) and sLSB(t) require\\nFigure 2.28 Frequency response of the quadrature filter for\\nproducing the quadrature component of the VSB wave.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 84,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Figure 2.28 Frequency response of the quadrature filter for\\nproducing the quadrature component of the VSB wave.\\nHQ( f ) 1 j 1.0 -fv fv -1.0 f s2 t( ) 1 2---m t( ) 2fct ( ) 1 2---mQ t( ) 2fct ( ) sin + cos = mQ t( ) s t( ) 1 2---m t( ) 2fct ( ) 1 2---mQ t( ) 2fct ( ) sin + cos =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 84,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '14 Linear Modulation Theory the smallest feasible channel bandwidth, B=W, without compromising the perfect\\nrecovery of the message signal under noiseless conditions. It is for these reasons that we\\nsay SSB modulation is the optimum form of linear modulation for analog\\ncommunications, preserving both the transmitted power and channel bandwidth in the best\\nmanner possible.\\nSSB modulation may be viewed as a special case of VSB modulation. Specifically,\\nsetting the vestigial bandwidth f = 0, we find that the frequency response of the\\nquadrature filter plotted in Figure 2.28 takes the limiting form of the signum function\\nshown in Figure 2.29. In light of the material presented in (2.60) on Hilbert\\ntransformation, we therefore find that for f = 0 the quadrature component mQ(t) becomes\\nthe Hilbert transform of the message signal m(t), denoted by\\n. Accordingly, using in place of mQ(t) in (2.110) yields the SSB formula\\n(2.111)\\nwhere the minus sign applies to the SSB-modulated signal sUSB(t) and the plus sign\\napplies to the alternative SSB-modulated signal sLSB(t).\\nUnlike DSB-SC and VSB methods of modulation, SSB modulation is of limited\\napplicability. Specifically, we say:\\nFor SSB modulation to be feasible in practical terms, the spectral content of the\\nmessage signal m(t) must have an energy gap centered on the origin.\\nThis requirement, illustrated in Figure 2.30, is imposed on the message signal m(t) so that\\nthe band-pass filter in the frequency-discrimination method of Figure 2.26 has a finite\\ntransition band for the filter to be physically realizable. With the transition band\\nseparating the pass-band from the stop-band, it is only when the transition band is finite\\nthat the undesired sideband can be suppressed. An example of message signals for which\\nthe energy-gap requirement is satisfied is voice signals; for such signals, the energy gap is\\nabout 600 Hz, extending from -300 to +300 Hz.\\nIn contrast, the spectral contents of television signals and wideband data extend',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 85,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'about 600 Hz, extending from -300 to +300 Hz.\\nIn contrast, the spectral contents of television signals and wideband data extend\\npractically to a few hertz, thereby ruling out the applicability of SSB modulation to this\\nsecond class of message signals. It is for this reason that VSB modulation is preferred over\\nSSB modulation for the transmission of wideband signals.\\nFigure 2.29\\nFrequency response of the quadrature\\nfilter in SSB modulation.\\nHQ( 1 j f) f 0 1.0 -1.0 m t( ) m t( ) s t( ) 1 2---m t( ) 2fct ( ) 1 2---m t( ) 2fct ( ) sin + cos =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 85,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nSummary of Linear Modulation Methods\\nEquation (2.97) for DSB-SC modulation, (2.110) for VSB modulation, and (2.111) for\\nSSB modulation are summarized in Table 2.5 as special cases of the canonical formula of\\n(2.68). Correspondingly, we may treat the time-domain generations of these three linearly\\nmodulated signals as special cases of the synthesizer depicted in Figure 2.20b. Phase and Group Delays\\nA discussion of signal transmission through linear time-invariant systems is incomplete\\nwithout considering the phase and group delays involved in the signal transmission\\nprocess.\\nWhenever a signal is transmitted through a dispersive system, exemplified by a\\ncommunication channel (or band-pass filter), some delay is introduced into the output\\nsignal, the delay being measured with respect to the input signal. In an ideal channel, the\\nphase response varies linearly with frequency inside the passband of the channel, in which\\ncase the filter introduces a constant delay equal to t0, where the parameter t0 controls the\\nslope of the linear phase response of the channel. Now, what if the phase response of the\\nchannel is a nonlinear function of frequency, which is frequently the case in practice? The\\npurpose of this section is to address this practical issue.\\nFigure 2.30\\nSpectrum of a message signal m(t) with an\\nenergy gap centered around the origin.\\n|M( f )| -fb fb f -fa fa 0 Energy gap Table 2.5 Summary of linear modulation methods viewed as special cases of the\\ncanonical formula s(t) = sI(t)cos(2fct) - sQ(t)sin(2fct)\\nType of modulation In-phase component, sI(t) Quadrature component, sQ(t) Comments DSB-SC m(t) zero m(t) = message signal\\nVSB\\nPlus sign applies to using vestige of\\nlower sideband and minus sign applies\\nto using vestige of upper sideband\\nSSB\\nPlus sign applies to transmission of\\nupper sideband and minus sign applies\\nto transmission of lower sideband\\n1 2---m t 1 2---mQ t  1 2---m t 1 2---m t',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 86,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '15 Phase and Group Delays To begin the discussion, suppose that a steady sinusoidal signal at frequency fc is\\ntransmitted through a dispersive channel that has a phase-shift of (fc) radians at that\\nfrequency. By using two phasors to represent the input signal and the received signal, we\\nsee that the received signal phasor lags the input signal phasor by (fc) radians. The time\\ntaken by the received signal phasor to sweep out this phase lag is simply equal to the ratio\\n(fc)/(2fc) seconds. This time is called the phase delay of the channel.\\nIt is important to realize, however, that the phase delay is not necessarily the true signal\\ndelay. This follows from the fact that a steady sinusoidal signal does not carry information,\\nso it would be incorrect to deduce from the above reasoning that the phase delay is the true\\nsignal delay. To substantiate this statement, suppose that a slowly varying signal, over the\\ninterval -(T/2)  t  (T/2), is multiplied by the carrier, so that the resulting modulated\\nsignal consists of a narrow group of frequencies centered around the carrier frequency; the\\nDSB-SC waveform of Figure 2.31 illustrates such a modulated signal. When this\\nmodulated signal is transmitted through a communication channel, we find that there is\\nindeed a delay between the envelope of the input signal and that of the received signal.\\nThis delay, called the envelope or group delay of the channel, represents the true signal\\ndelay insofar as the information-bearing signal is concerned.\\nAssume that the dispersive channel is described by the transfer function\\n(2.112)\\nwhere the amplitude K is a constant scaling factor and the phase (f) is a nonlinear\\nfunction of frequency f; it is the nonlinearity of (f) that is responsible for the dispersive\\nH f( ) K j f( ) [ ] exp = Figure 2.31 (a) Block diagram of product modulator; (b) Baseband signal;\\n(c) DSB-SC modulated wave.\\n(a) DSB-SC modulated wave s(t) = Acm(t) cos (2fct)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 87,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'H f( ) K j f( ) [ ] exp = Figure 2.31 (a) Block diagram of product modulator; (b) Baseband signal;\\n(c) DSB-SC modulated wave.\\n(a) DSB-SC modulated wave s(t) = Acm(t) cos (2fct)\\nCarrier Ac cos (2fct) Product modulator Baseband signal m(t) (b) 0 m(t) t (c) 0 s(t) t Phase reversals',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 87,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nnature of the channel. The input signal s(t) is assumed to be of the kind displayed in Figure\\n31; that is, the DSB-SC-modulated signal\\n(2.113)\\nwhere m(t) is the message signal, assumed to be of a low-pass kind and limited to the\\nfrequency interval | f |  W. Moreover, we assume that the carrier frequency fc > W. By\\nexpanding the phase (f) in a Taylor series about the point f = fc and retaining only the\\nfirst two terms, we may approximate (f) as\\n(2.114)\\nDefine two new terms:\\n(2.115) and (2.116) Then, we may rewrite (2.114) in the equivalent form\\n(2.117)\\nCorrespondingly, the transfer function of the channel takes the approximate form\\n(2.118)\\nFollowing the band-pass-to-low-pass transformation described in Section 2.12, in\\nparticular using (2.80), we may replace the band-pass channel described by H(f) by an\\nequivalent low-pass filter whose transfer function is approximately given by\\n(2.119)\\nCorrespondingly, using (2.67) we may replace the modulated signal s(t) of (2.113) by its\\nlow-pass complex envelope, which, for the DSB-SC example at hand, is simply defined by\\n(2.120)\\nTransforming into the frequency domain, we may write\\n(2.121)\\nTherefore, in light of (2.96), the Fourier transform of the complex envelope of the signal\\nreceived at the channel output is given by\\n(2.122)\\nWe note that the multiplying factor is a constant for fixed values of fc\\nand p. We also note from the time-shifting property of the Fourier transform that the term represents the Fourier transform of the delayed signal m(t - g).\\nAccordingly, the complex envelope of the channel output is',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 88,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Accordingly, the complex envelope of the channel output is\\n(2.123) s t( ) m t( ) 2fct ( ) cos =  f( )  fc ( ) f fc - ( ) f( ) f ------------- f fc = +  p  fc ( ) 2fc ------------ - = g 1 2 ------  f( ) f ------------- f fc = - =  f( ) 2fcp - 2 f fc - ( )g -  H f( ) K j2fcp - j2 f fc - ( )g - [ ] exp  H f( ) 2K j2fcp - j2fg - ( ) f fc > , exp  s t( ) m t( ) = s t( )  S f( ) M f( ) = X f( ) 1 2---H f( )S f( ) = K j2fcp - ( ) j2fcg - ( )M f( ) exp exp  K j2fcp - ( ) exp j2fcg - ( )M f( ) exp x t( ) K j2fcp - ( )m t g - ( ) exp =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 88,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '16 Numerical Computation of the Fourier Transform Finally, using (2.66) we find that the actual channel output is itself given by\\n(2.124)\\nEquation (2.124) reveals that, as a result of transmitting the modulated signal s(t) through\\nthe dispersive channel, two different delay effects occur at the channel output:\\nThe sinusoidal carrier wave cos(2fct) is delayed by p seconds; hence, p\\nrepresents the phase delay; sometimes p is referred to as the carrier delay.\\nThe envelope m(t) is delayed by g seconds; hence, g represents the envelope or\\ngroup delay.\\nNote that g is related to the slope of the phase (f), measured at f = fc. Note also that\\nwhen the phase response (f) varies linearly with frequency f and (fc) is zero, the phase\\ndelay and group delay assume a common value. It is only then that we can think of these\\ntwo delays being equal. Numerical Computation of the Fourier Transform\\nThe material presented in this chapter clearly testifies to the importance of the Fourier\\ntransform as a theoretical tool for the representation of deterministic signals and linear\\ntime-invariant systems, be they of the low-pass or band-pass kind. The importance of the\\nFourier transform is further enhanced by the fact that there exists a class of algorithms\\ncalled FFT algorithms6 for numerical computation of the Fourier transform in an efficient\\nmanner.\\nThe FFT algorithm is derived from the discrete Fourier transform (DFT) in which, as\\nthe name implies, both time and frequency are represented in discrete form. The DFT\\nprovides an approximation to the Fourier transform. In order to properly represent the\\ninformation content of the original signal, we have to take special care in performing the\\nsampling operations involved in defining the DFT. A detailed treatment of the sampling\\nprocess is presented in Chapter 6. For the present, it suffices to say that, given a band-\\nlimited signal, the sampling rate should be greater than twice the highest frequency',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 89,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'process is presented in Chapter 6. For the present, it suffices to say that, given a band-\\nlimited signal, the sampling rate should be greater than twice the highest frequency\\ncomponent of the input signal. Moreover, if the samples are uniformly spaced by Ts\\nseconds, the spectrum of the signal becomes periodic, repeating every fs = (1/Ts) hz in\\naccordance with (2.43). Let N denote the number of frequency samples contained in the\\ninterval fs. Hence, the frequency resolution involved in numerical computation of the\\nFourier transform is defined by\\n(2.125)\\nwhere T is the total duration of the signal.\\nConsider then a finite data sequence {g0, g1, ..., gN - 1}. For brevity, we refer to this\\nsequence as gn, in which the subscript is the time index n = 0, 1, ..., N - 1. Such a sequence\\nmay represent the result of sampling an analog signal g(t) at times t = 0, Ts, ..., (N - 1)Ts,\\nwhere Ts is the sampling interval. The ordering of the data sequence defines the sample\\nx t( ) Re x t( ) j2fct ( ) exp [ ] = Km t g - ( ) 2fc t p - ( ) [ ] cos = f fs N---- 1 NTs --------- 1 T--- = = =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 89,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\ntime in that g0, g1, ..., gN - 1 denote samples of g(t) taken at times 0, Ts, ..., (N - 1)Ts,\\nrespectively. Thus we have\\n(2.126)\\nWe formally define the DFT of gn as\\n(2.127)\\nThe sequence {G0, G1, ..., GN - 1} is called the transform sequence. For brevity, we refer\\nto this second sequence simply as Gk, in which the subscript is the frequency index k = 0,\\n1, ..., N - 1.\\nCorrespondingly, we define the inverse discrete Fourier transform (IDFT) of Gk as\\n(2.128)\\nThe DFT and the IDFT form a discrete transform pair. Specifically, given a data sequence\\ngn, we may use the DFT to compute the transform sequence Gk; and given the transform\\nsequence Gk, we may use the IDFT to recover the original data sequence gn. A distinctive\\nfeature of the DFT is that, for the finite summations defined in (2.127) and (2.128), there is\\nno question of convergence.\\nWhen discussing the DFT (and algorithms for its computation), the words sample\\nand point are used interchangeably to refer to a sequence value. Also, it is common\\npractice to refer to a sequence of length N as an N-point sequence and to refer to the DFT\\nof a data sequence of length N as an N-point DFT.\\nInterpretation of the DFT and the IDFT\\nWe may visualize the DFT process described in (2.127) as a collection of N complex\\nheterodyning and averaging operations, as shown in Figure 2.32a. We say that the\\nheterodyning is complex in that samples of the data sequence are multiplied by complex\\nexponential sequences. There is a total of N complex exponential sequences to be\\nconsidered, corresponding to the frequency index k = 0, 1, ..., N - 1. Their periods have\\nbeen selected in such a way that each complex exponential sequence has precisely an\\ninteger number of cycles in the total interval 0 to N - 1. The zero-frequency response,\\ncorresponding to k = 0, is the only exception.\\nFor the interpretation of the IDFT process, described in (2.128), we may use the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 90,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'corresponding to k = 0, is the only exception.\\nFor the interpretation of the IDFT process, described in (2.128), we may use the\\nscheme shown in Figure 2.32b. Here we have a collection of N complex signal generators,\\neach of which produces the complex exponential sequence\\n(2.129)\\nThus, in reality, each complex signal generator consists of a pair of generators that output\\na cosinusoidal and a sinusoidal sequence of k cycles per observation interval. The output\\ngn g nTs ( ) = Gk gn j2 N --------kn -     k 0 1  N 1 - , , , = exp n 0 = N 1 -  = gn 1 N---- Gk j2 N --------kn     n 0 1  N 1 - , , , = exp k 0 = N 1 -  = j2 N --------kn     exp 2 N ------kn     j 2 N ------kn     sin + cos = 2 N ------kn     2 N ------kn     sin , cos      N 1 - k 0 = =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 90,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '16 Numerical Computation of the Fourier Transform of each complex signal generator is weighted by the complex Fourier coefficient Gk. At\\neach time index n, an output is formed by summing the weighted complex generator\\noutputs.\\nIt is noteworthy that although the DFT and the IDFT are similar in their mathematical\\nformulations, as described in (2.127) and (2.128), their interpretations as depicted in\\nFigure 2.32a and b are so completely different.\\nFigure 2.32 Interpretations of (a) the DFT and (b) the IDFT.\\n( ) ( ) (N - 1)n ( ) ( ) ( ) ( ) ( ) (a) (b) Sum over n G0 Sum over n G1 Sum over n G2 Sum over n GN - 1 G0 G1 G2 GN - 1 gn N ... ... ... 1/ N gn ... ... 0n j2 ___ N exp - 0n j2 ___ N exp n j2 ___ N exp 2n j2 ___ N exp n j2 ___ N exp - 2n j2 ___ N exp - j2 ___ N exp - (N - 1)n ( ) j2 ___ N exp',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 91,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nAlso, the addition of harmonically related periodic signals, involved in these two parts\\nof the figure, suggests that their outputs Gk and gn must be both periodic. Moreover, the\\nprocessors shown in Figure 2.32 are linear, suggesting that the DFT and IDFT are both\\nlinear operations. This important property is also obvious from the defining equations\\n(2.127) and (2.128).\\nFFT Algorithms\\nIn the DFT both the input and the output consist of sequences of numbers defined at\\nuniformly spaced points in time and frequency, respectively. This feature makes the DFT\\nideally suited for direct numerical evaluation on a computer. Moreover, the computation\\ncan be implemented most efficiently using a class of algorithms, collectively called FFT\\nalgorithms. An algorithm refers to a recipe that can be written in the form of a computer\\nprogram.\\nFFT algorithms are efficient because they use a greatly reduced number of arithmetic\\noperations as compared with the brute force (i.e., direct) computation of the DFT.\\nBasically, an FFT algorithm attains its computational efficiency by following the\\nengineering strategy of divide and conquer, whereby the original DFT computation is\\ndecomposed successively into smaller DFT computations. In this section, we describe one\\nversion of a popular FFT algorithm, the development of which is based on such a strategy.\\nTo proceed with the development, we first rewrite (2.127), defining the DFT of gn, in\\nthe convenient mathematical form (2.130)\\nwhere we have introduced the complex parameter\\n(2.131)\\nFrom this definition, we readily see that That is, Wkn is periodic with period N. The periodicity of Wkn is a key feature in the\\ndevelopment of FFT algorithms.\\nLet N, the number of points in the data sequence, be an integer power of two, as shown\\nby\\nwhere L is an integer; the rationale for this choice is explained later. Since N is an even',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 92,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Let N, the number of points in the data sequence, be an integer power of two, as shown\\nby\\nwhere L is an integer; the rationale for this choice is explained later. Since N is an even\\ninteger, N/2 is an integer, and so we may divide the data sequence into the first half and\\nlast half of the points.\\nGk gnWkn k , 0 1  N 1 - , , , = n 0 = N 1 -  = W j2 N -------- -     exp = WN 1 = WN 2  1 - = W l lN + ( ) n mN + ( ) Wkn = , m l, ( ) = 0 1 2  ,  ,  , N 2L =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 92,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '16 Numerical Computation of the Fourier Transform Thus, we may rewrite (2.130) as (2.132) Since , we have Accordingly, the factor WkN/2 in (2.132) takes on only one of two possible values, namely\\n+1 or -1, depending on whether the frequency index k is even or odd, respectively. These\\ntwo cases are considered in what follows.\\nFirst, let k be even, so that WkN/2 = 1. Also let and define (2.133) Then, we may put (2.132) into the new form (2.134)\\nFrom the definition of W given in (2.131), we readily see that\\nHence, we recognize the sum on the right-hand side of (2.134) as the (N/2)-point DFT of\\nthe sequence xn.\\nNext, let k be odd so that WkN/2 = -1. Also, let\\nGk gnWkn gnWkn n N 2  = N 1 -  + n 0 = N 2  ( ) 1 -  = gnWkn gn N 2  + Wk n N 2  + ( ) n 0 = N 2  ( ) 1 -  + n 0 = N 2  ( ) 1 -  = gn gn N 2  + WkN 2  + ( )Wkn k 0 1  N 1 - , , , = n 0 = N 2  ( ) 1 -  = WN 2 / 1 - = WkN 2  1 - ( )k = k 2l l 0 1  N 2---- , , , = 1 - , = xn gn gn N 2  + + = G2l xnW2 ln n 0 = N 2  ( ) 1 -  = xn W2 ( ) ln l 0 1  N 2---- , , , = 1 - n 0 = N 2  ( ) 1 -  = W2 j4 N -------- -     exp = j2 N 2  ----------- -     exp = k 2l 1 l 0 1  N 2---- , , , = 1 - , + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 93,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nand define\\n(2.135)\\nThen, we may put (2.132) into the corresponding form (2.136)\\nWe recognize the sum on the right-hand side of (2.136) as the (N/2)-point DFT of the\\nsequence ynWn. The parameter Wn associated with yn is called the twiddle factor.\\nEquations (2.134) and (2.136) show that the even- and odd-valued samples of the\\ntransform sequence Gk can be obtained from the (N/2)-point DFTs of the sequences xn and\\nynWn, respectively. The sequences xn and yn are themselves related to the original data\\nsequence gn by (2.133) and (2.135), respectively. Thus, the problem of computing an\\nN-point DFT is reduced to that of computing two (N/2)-point DFTs. The procedure just\\ndescribed is repeated a second time, whereby an (N/2)-point DFT is decomposed into two\\n(N/4)-point DFTs. The decomposition procedure is continued in this fashion until (after\\nL = log2N stages) we reach the trivial case of N single-point DFTs.\\nFigure 2.33 illustrates the computations involved in applying the formulas of (2.134)\\nand (2.136) to an eight-point data sequence; that is, N = 8. In constructing left-hand\\nportions of the figure, we have used signal-flow graph notation. A signal-flow graph\\nconsists of an interconnection of nodes and branches. The direction of signal transmission\\nalong a branch is indicated by an arrow. A branch multiplies the variable at a node (to\\nwhich it is connected) by the branch transmittance. A node sums the outputs of all\\nincoming branches. The convention used for branch transmittances in Figure 2.33 is as\\nfollows. When no coefficient is indicated on a branch, the transmittance of that branch is\\nassumed to be unity. For other branches, the transmittance of a branch is indicated by -1 or\\nan integer power of W, placed alongside the arrow on the branch.\\nThus, in Figure 2.33a the computation of an eight-point DFT is reduced to that of two\\nfour-point DFTs. The procedure for the eight-point DFT may be mimicked to simplify the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 94,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Thus, in Figure 2.33a the computation of an eight-point DFT is reduced to that of two\\nfour-point DFTs. The procedure for the eight-point DFT may be mimicked to simplify the\\ncomputation of the four-point DFT. This is illustrated in Figure 2.33b, where the\\ncomputation of a four-point DFT is reduced to that of two two-point DFTs. Finally, the\\ncomputation of a two-point DFT is shown in Figure 2.33c.\\nCombining the ideas described in Figure 2.33, we obtain the complete signal-flow\\ngraph of Figure 2.34 for the computation of the eight-point DFT. A repetitive structure,\\ncalled the butterfly with two inputs and two outputs, can be discerned in the FFT algorithm\\nof Figure 2.34. Examples of butterflies (for the three stages of the algorithm) are shown by\\nthe bold-faced lines in Figure 2.34.\\nFor the general case of N = 2L, the algorithm requires L = log2N stages of computation.\\nEach stage requires (N/2) butterflies. Each butterfly involves one complex multiplication\\nand two complex additions (to be precise, one addition and one subtraction). Accordingly,\\nthe FFT structure described here requires (N/2)log2N complex multiplications and Nlog2N\\nyn gn gn N 2  + - = G2l 1 + ynW 2l 1 + ( )n n 0 = N 2  ( ) 1 -  = ynWn [ ] W2 ( ) ln l 0 1  N 2---- 1 - , , , = n 0 = N 2  ( ) 1 -  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 94,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '16 Numerical Computation of the Fourier Transform Figure 2.33 (a) Reduction of eight-point DFT into two four-point DFTs. (b) Reduction of four-point\\nDFT into two two-point DFTs. (c) Trivial case of two-point DFT.\\nTransform sequence Coefficients for even frequencies Data sequence 4-point DFT Transform sequence Coefficients for even frequencies Data sequence 2-point DFT Transform sequence Data sequence Coefficients for odd frequencies 2-point DFT W 0 -1 -1 -1 -1 -1 -1 -1 W 1 W 2 W 0 W 1 W 3 Coefficients for odd frequencies 4-point DFT (a) (b) (c) g0 G0 G2 G4 G6 G0 G1 G2 G3 G1 G3 G5 G7 g1 g0 G0 G1 g1 g2 g3 g4 g5 g6 g7 g0 g1 g2 g3',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 95,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\ncomplex additions; actually, the number of multiplications quoted is pessimistic, because\\nwe may omit all twiddle factors W0 = 1 and WN/2 = -1, WN/4 = j, W3N/4 = -j. This\\ncomputational complexity is significantly smaller than that of the N2 complex\\nmultiplications and N(N - 1) complex additions required for direct computation of the\\nDFT. The computational savings made possible by the FFT algorithm become more\\nsubstantial as we increase the data length N. For example, for N = 8192 = 211, the direct\\napproach requires approximately 630 times as many arithmetic operations as the FFT\\nalgorithm, hence the popular use of the FFT algorithm in computing the DFT.\\nWe may establish two other important features of the FFT algorithm by carefully\\nexamining the signal-flow graph shown in Figure 2.34:\\nAt each stage of the computation, the new set of N complex numbers resulting from\\nthe computation can be stored in the same memory locations used to store the\\nprevious set. This kind of computation is referred to as in-place computation.\\nFigure 2.34\\nDecimation-in-frequency\\nFFT algorithm. Stage I Stage II Stage III Transform sequence Data sequence W 0 W 0 W 0 W 1 W 5 W 2 W 2 W 4 W 4 W 4 W 6 W 3 W 2 W 7 W 6 W 6 -1 -1 -1 -1 g0 G0 G4 G2 G6 G1 G5 G3 G7 g1 g2 g4 g5 g6 g7 g3',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 96,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '16 Numerical Computation of the Fourier Transform\\nThe samples of the transform sequence Gk are stored in a bit-reversed order. To\\nillustrate the meaning of this terminology, consider Table 2.6 constructed for the\\ncase of N = 8. At the left of the table, we show the eight possible values of the\\nfrequency index k (in their natural order) and their 3-bit binary representations. At\\nthe right of the table, we show the corresponding bit-reversed binary representations\\nand indices. We observe that the bit-reversed indices in the rightmost column of\\nTable 2.6 appear in the same order as the indices at the output of the FFT algorithm\\nin Figure 2.34.\\nThe FFT algorithm depicted in Figure 2.34 is referred to as a decimation-in-frequency\\nalgorithm, because the transform (frequency) sequence Gk is divided successively into\\nsmaller subsequences. In another popular FFT algorithm, called a decimation-in-time\\nalgorithm, the data (time) sequence gn is divided successively into smaller subsequences.\\nBoth algorithms have the same computational complexity. They differ from each other in\\ntwo respects. First, for decimation-in-frequency, the input is in natural order, whereas the\\noutput is in bit-reversed order; the reverse is true for decimation-in-time. Second, the\\nbutterfly for decimation-in-time is slightly different from that for decimation-in-\\nfrequency. The reader is invited to derive the details of the decimation-in-time algorithm\\nusing the divide-and-conquer strategy that led to the development of the algorithm\\ndescribed in Figure 2.34.\\nIn devising the FFT algorithm presented herein, we placed the factor 1N in the formula\\nfor the forward DFT, as shown in (2.128). In some other FFT algorithms, location of the\\nfactor 1N is reversed. In yet other formulations, the factor is placed in the\\nformulas for both the forward and inverse DFTs for the sake of symmetry.\\nComputation of the IDFT\\nThe IDFT of the transform Gk is defined by (2.128). We may rewrite this equation in terms',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 97,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'formulas for both the forward and inverse DFTs for the sake of symmetry.\\nComputation of the IDFT\\nThe IDFT of the transform Gk is defined by (2.128). We may rewrite this equation in terms\\nof the complex parameter W as (2.137)\\nTable 2.6 Illustrating bit reversal\\nFrequency index, k Binary representation Bit-reversed binary representation\\nBit-reversed index 0 000 000 0 1 001 100 4 2 010 010 2 3 011 110 6 4 100 001 1 5 101 101 5 6 110 011 3 7 111 111 7 1 N  gn 1 N---- GkW kn - n 0 1 N 1 -   =  k 0 = N 1 -  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 97,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nTaking the complex conjugate of (2.137) and multiplying by N, we get\\n(2.138)\\nThe right-hand side of (2.138) is recognized as the N-point DFT of the complex-\\nconjugated sequence\\n. Accordingly, (2.138) suggests that we may compute the desired\\nsequence gn using the scheme shown in Figure 2.35, based on an N-point FFT algorithm.\\nThus, the same FFT algorithm can be used to handle the computation of both the IDFT\\nand the DFT. Summary and Discussion\\nIn this chapter we have described the Fourier transform as a fundamental tool for relating\\nthe time-domain and frequency-domain descriptions of a deterministic signal. The signal\\nof interest may be an energy signal or a power signal. The Fourier transform includes the\\nexponential Fourier series as a special case, provided that we permit the use of the Dirac\\ndelta function.\\nAn inverse relationship exists between the time-domain and frequency-domain\\ndescriptions of a signal. Whenever an operation is performed on the waveform of a signal\\nin the time domain, a corresponding modification is applied to the spectrum of the signal\\nin the frequency domain. An important consequence of this inverse relationship is the fact\\nthat the time-bandwidth product of an energy signal is a constant; the definitions of signal\\nduration and bandwidth merely affect the value of the constant.\\nAn important signal-processing operation frequently encountered in communication\\nsystems is that of linear filtering. This operation involves the convolution of the input\\nsignal with the impulse response of the filter or, equivalently, the multiplication of the\\nFourier transform of the input signal by the transfer function (i.e., Fourier transform of the\\nimpulse response) of the filter. Low-pass and band-pass filters represent two commonly\\nused types of filters. Band-pass filtering is usually more complicated than low-pass\\nfiltering. However, through the combined use of a complex envelope for the representation',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 98,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'used types of filters. Band-pass filtering is usually more complicated than low-pass\\nfiltering. However, through the combined use of a complex envelope for the representation\\nof an input band-pass signal and the complex impulse response for the representation of a\\nband-pass filter, we may formulate a complex low-pass equivalent for the band-pass\\nfiltering problem and thereby replace a difficult problem with a much simpler one. It is\\nalso important to note that there is no loss of information in establishing this equivalence.\\nA rigorous treatment of the concepts of complex envelope and complex impulse response\\nas presented in this chapter is rooted in Hilbert transformation.\\nThe material on Fourier analysis, as presented in this chapter, deals with signals whose\\nwaveforms can be nonperiodic or periodic, and whose spectra can be continuous or\\ndiscrete functions of frequency. In this sense, the material has general appeal.\\nFigure 2.35 Use of the FFT algorithm for computing the IDFT.\\nComplex conjugate FFT Complex conjugate Divide by N Gk G *k Ng*n Ngn gn Ngn* Gk*W kn - n 0 1  N 1 - , , , = , k 0 = N 1 -  = Gk*',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 98,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems Building on the canonical representation of a band-pass signal involving the in-phase\\nand quadrature components of the signal, we showed that this representation provides an\\nelegant way of describing the three basic forms of linear modulation, namely DSB-SC,\\nVSB, and SSB.\\nWith the Fourier transform playing such a pervasive role in the study of signals and\\nlinear systems, we finally described the FFT algorithm as an efficient tool for numerical\\ncomputation of the DFT that represents the uniformly sampled versions of the forward and\\ninverse forms of the ordinary Fourier transform.\\nProblems\\nThe Fourier Transform Prove the dilation property of the Fourier transform, listed as Property 2 in Table 2.1. a. Prove the duality property of the Fourier transform, listed as Property 3 in Table 2.1.\\nb. Prove the time-shifting property, listed as Property 4; and then use the duality property to prove\\nthe frequency-shifting property, listed as Property 5 in the table.\\nc. Using the frequency-shifting property, determine the Fourier transform of the radio frequency RF\\npulse\\nassuming that fc is larger than (1/T). a. Prove the multiplication-in-the-time-domain property of the Fourier transform, listed as Property\\n11 in Table 2.1.\\nb. Prove the convolution in the time-domain property, listed as Property 12.\\nc. Using the result obtained in part b, prove the correlation theorem, listed as Property 13. Prove Rayleighs energy theorem listed as Property 14 in Table 2.1. The following expression may be viewed as an approximate representation of a pulse with finite rise\\ntime:\\nwhere it is assumed that T >> . Determine the Fourier transform of g(t). What happens to this\\ntransform when we allow  to become zero? Hint: Express g(t) as the superposition of two signals,\\none corresponding to integration from t - T to 0, and the other from 0 to t + T. The Fourier transform of a signal g(t) is denoted by G(f). Prove the following properties of the\\nFourier transform:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 99,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'one corresponding to integration from t - T to 0, and the other from 0 to t + T. The Fourier transform of a signal g(t) is denoted by G(f). Prove the following properties of the\\nFourier transform:\\na. If a real signal g(t) is an even function of time t, the Fourier transform G(f) is purely real. If a\\nreal signal g(t) is an odd function of time t, the Fourier transform G(f) is purely imaginary.\\nb.\\nwhere G(n)(f) is the nth derivative of G(f) with respect to f.\\nc. g t( ) Arect t T---   2fct ( ) cos = g t( ) 1 --- u2 2 --------- -       exp udtT - t T +  = tng t( )  j 2 ------    n Gn ( ) f( ) tng t( ) dt  -   j 2 ------    n Gn ( ) 0 ( ) =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 99,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nd. Assuming that both g1(t) and g2(t) are complex signals, show that:\\nand 2.7 a. The root mean-square (rms) bandwidth of a low-pass signal g(t) of finite energy is defined by\\nwhere |G(f)|2 is the energy spectral density of the signal. Correspondingly, the root mean-square\\n(rms) duration of the signal is defined by\\nUsing these definitions, show that\\nAssume that faster than as . b. Consider a Gaussian pulse defined by\\nShow that for this signal the equality\\nis satisfied.\\nHint: Use Schwarzs inequality\\nin which we set and 2.8 The Dirac comb, formulated in the time domain, is defined by\\nwhere T0 is the period.\\ng1 t( )g2 * t( )  G1  ( )  -   G2 *  f - ( ) d g1 t( )g2 * t( )dt  -   G1 f( )  -   G2 * f( ) df = Wrms f2 G f( ) 2df  -   G f( ) 2df  -   -----------------------------------\\n1 2  = Trms t2 g t( ) 2dt  -   g t( ) 2dt  -   ----------------------------------\\n1 2  = TrmsWrms 1 4 ------  g t( ) 0  1 t  t   g t( ) t2 - ( ) exp = TrmsWrms 1 4 ------ = g1 * t( )g2 t( ) g1 t( )g2* t( ) + [ ]dt  -      2 4 g1 t( ) 2dt g2 t( ) 2dt  -    -    g1 t( ) tg t( ) = g2 t( ) dg t( ) dt ------------- = T0 t( )  t mT0 - ( ) m  - =   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 100,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Problems a. Show that the Dirac comb is its own Fourier transform. That is, the Fourier transform of is\\nalso an infinitely long periodic train of delta functions, weighted by the factor f0 = (1/T0) and\\nregularly spaced by f0 along the frequency axis.\\nb. Hence, prove the pair of dual relations:\\nc. Finally, prove the validity of (2.38).\\nSignal Transmission through Linear Time-invariant Systems The periodic signal\\nis applied to a linear system of impulse response h(t). Show that the average power of the signal y(t)\\nproduced at the system output is defined by\\nwhere H(f) is the frequency response of the system, and f0 = 1/T0. According to the bounded input-bounded output stability criterion, the impulse response h(t) of a\\nlinear-invariant system must be absolutely integrable; that is,\\nProve that this condition is both necessary and sufficient for stability of the system.\\nHilbert Transform and Pre-envelopes Prove the three properties of the Hilbert transform itemized on pages 43 and 44. Let denote the Hilbert transform of g(t). Derive the set of Hilbert-transform pairs listed as\\nitems 5 to 8 in Table 2.3. Evaluate the inverse Fourier transform g(t) of the one-sided frequency function:\\nShow that g(t) is complex, and that its real and imaginary parts constitute a Hilbert-transform pair. Let denote the Hilbert transform of a Fourier transformable signal g(t). Show that is\\nequal to the Hilbert transform of\\n. T0 t( )  t mT0 - ( ) m  - =   f0 j2nf0t ( ) exp n  - =   = T0 j2mfT0 ( ) exp m  - =    f nf0 - ( ) n  - =   = x t( ) x nT0 ( ) t nT0 - ( ) m  - =   = Pav,y x nT0 ( ) 2 H nf0 ( ) 2 n  - =   = h t( ) t  < d  -   g t( ) G f( ) f - ( ), exp f 0 > 1 2---, f 0 = 0, f 0 <        = g t( ) d dt----g t( ) d dt----g t( )',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 101,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems In this problem, we revisit Problem 2.14, except that this time we use integration rather than\\ndifferentiation. Doing so, we find that, in general, the integral is not equal to the Hilbert\\ntransform of the integral\\n.\\na. Justify this statement.\\nb. Find the condition for which exact equality holds. Determine the pre-envelope g+(t) corresponding to each of the following two signals:\\na. g(t) = sinc(t)\\nb. g(t) = [1 + k cos(2fmt)]cos(2fct)\\nComplex Envelope Show that the complex envelope of the sum of two narrowband signals (with the same carrier\\nfrequency) is equal to the sum of their individual complex envelopes. The definition of the complex envelope of a band-pass signal given in (2.65) is based on the\\npre-envelope s+(t) for positive frequencies. How is the complex envelope defined in terms of the pre-\\nenvelope s-(t) for negative frequencies? Justify your answer. Consider the signal\\nwhose m(t) is a low-pass signal whose Fourier transform M(f) vanishes for | f | > W, and c(t) is a\\nhigh-pass signal whose Fourier transform C(f) vanishes for | f | < W. Show that the Hilbert transform\\nof s(t) is = , where is the Hilbert transform of c(t). a. Consider two real-valued signals s1(t) and s2(t) whose pre-envelopes are denoted by s1+(t) and\\ns2+(t), respectively. Show that\\nb. Suppose that s2(t) is replaced by s2(-t). Show that this modification has the effect of removing\\nthe complex conjugation in the right-hand side of the formula given in part a.\\nc. Assuming that s(t) is a narrowband signal with complex envelope and carrier frequency fc,\\nuse the result of part a to show that Let a narrow-band signal s(t) be expressed in the form\\nUsing S+(f) to denote the Fourier transform of the pre-envelope of s+(t), show that the Fourier\\ntransforms of the in-phase component sI(t) and quadrature component sQ(t) are given by',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 102,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Using S+(f) to denote the Fourier transform of the pre-envelope of s+(t), show that the Fourier\\ntransforms of the in-phase component sI(t) and quadrature component sQ(t) are given by\\nrespectively, where the asterisk denotes complex conjugation. The block diagram of Figure 2.20a illustrates a method for extracting the in-phase component sI(t)\\nand quadrature component sQ(t) of a narrowband signal s(t). Given that the spectrum of s(t) is\\nlimited to the interval fc - W  | f | fc + W, demonstrate the validity of this method. Hence, show that\\ng t( ) dt  -   g t( ) dt  -   s t( ) s t( ) c t( )m t( ) = s t( ) c t( )m t( ) c t( ) Re s1+ t( ) [ ]Re s2+ t( ) [ ] td  -   1 2---Re s1+ t( )s2+ * t( ) td  -   = s t( ) s2 t( ) td  -   1 2--- s t( ) 2 td  -   = s t( ) sI t( ) 2fct ( ) sQ t( ) 2fct ( ) sin - cos = SI f( ) 1 2--- S+ f fc + ( ) S+ * f - fc + ( ) + [ ] = SQ f( ) 1 2j ----- S+ f fc + ( ) S+ * - f - fc + ( ) [ ] =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 102,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems 83 and where SI(f), SQ(f), and S(f) are the Fourier transforms of sI(t), sQ(t), and s(t), respectively.\\nLow-Pass Equivalent Models of Band-Pass Systems Equations (2.82) and (2.83) define the in-phase component and the quadrature component of the frequency response of the complex low-pass equivalent model of a band-pass\\nsystem of impulse response h(t). Prove the validity of these two equations. Explain what happens to the low-pass equivalent model of Figure 2.21b when the amplitude\\nresponse of the corresponding bandpass filter has even symmetry and the phase response has odd\\nsymmetry with respect to the mid-band frequency fc. The rectangular RF pulse\\nis applied to a linear filter with impulse response\\nAssume that the frequency fc equals a large integer multiple of 1/T. Determine the response of the\\nfilter and sketch it. Figure P2.26 depicts the frequency response of an idealized band-pass filter in the receiver of a\\ncommunication system, namely H(f), which is characterized by a bandwidth of 2B centered on the\\ncarrier frequency fc. The signal applied to the band-pass filter is described by the modulated sinc\\nfunction:\\nwhere is frequency misalignment introduced due to the receivers imperfections, measured with\\nrespect to the carrier\\n.\\na. Find the complex low-pass equivalent models of the signal x(t) and the frequency response H(f).\\nSI f( ) S f fc - ( ) S f fc + ( ), + WfW  - 0, elsewhere      = SQ f( ) jSf fc - ( ) S - f fc + ( ) [ ], WfW  - 0, elsewhere      = H I f( ) H Q f( ) H f( ) x t( ) A 2fct ( ), cos 0 t T  0, elsewhere      = h t( ) xT t - ( ) = x t( ) 4AcB sinc 2Bt ( ) cos 2 fc f  ( )t [ ] = f Ac 2fct ( ) cos Figure P2.26 1.0 0 H(f) -f0 fc f 2B',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 103,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nFourier Analysis of Signals and Systems\\nb. Then, go on to find the complex low-pass response of the filter output, denoted by\\n, which\\nincludes distortion due to\\n.\\nc. Building on the formula derived for obtained in part b, explain how you would mitigate the\\nmisalignment distortion in the receiver.\\nNonlinear Modulations In analog communications, amplitude modulation is defined by\\nwhere is the carrier, m(t) is the message signal, and ka is a constant called amplitude\\nsensitivity of the modulator. Assume that for all time t.\\na. Justify the statement that, in a strict sense, sAM(t) violates the principle of superposition.\\nb. Formulate the complex envelope\\nand its spectrum.\\nc. Compare the result obtained in part b with the complex envelope of DSB-SC. Hence, comment\\non the advantages and disadvantages of amplitude modulation. Continuing on with analog communications, frequency modulation (FM) is defined by\\nwhere is the carrier, m(t) is the message signal, and kf is a constant called the\\nfrequency sensitivity of the modulator.\\na. Show that frequency modulation is nonlinear in that it violates the principle of superposition.\\nb. Formulate the complex envelope of the FM signal, namely\\n.\\nc. Consider the message signal to be in the form of a square wave as shown in Figure P2.28. The\\nmodulation frequencies used for the positive and negative amplitudes of the square wave, namely\\nf1 and f2, are defined as follows:\\nwhere Tb is the duration of each positive or negative amplitude in the square wave. Show that\\nunder these conditions the complex envelope maintains continuity for all time t,\\nincluding the switching times between positive and negative amplitudes.\\nd. Plot the real and imaginary parts of for the following values:\\nPhase and Group Delays The phase response of a band-pass communication channel is defined by.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 104,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'd. Plot the real and imaginary parts of for the following values:\\nPhase and Group Delays The phase response of a band-pass communication channel is defined by.\\ny t( )   f y t( ) sAM t( ) Ac 1 [ kam t( )] 2fct ( ) cos + = Ac 2fct ( ) cos kam t( ) 1 < sAM t( ) sFM t( ) Ac cos 2fct ( ) kf m ( ) d o t  + = Ac 2fct ( ) cos sFM t( ) f1 f2 2 Tb ----- = + f1 f2 - 1 Tb ----- = sFM t( ) sFM t( ) Tb 1 3--- s = f1 41 2--- Hz = f2 11 2--- Hz =  f( ) f 2 fc 2 - ffc ----------------       1 - tan - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 104,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Notes A sinusoidally modulated signal defined by\\nis transmitted through the channel; fc is the carrier frequency and fm is the modulation frequency.\\na. Determine the phase delay p.\\nb. Determine the group delay g.\\nc. Display the waveform produced at the channel output; hence, comment on the results obtained in\\nparts a and b.\\nNotes\\nFor a proof of convergence of the Fourier series, see Kammler (2000).\\nIf a time function g(t) is such that the value of the energy is defined and finite, then\\nthe Fourier transform G(f) of the function g(t) exists and\\nThis result is known as Plancherels theorem. For a proof of this theorem, see Titchmarsh (1950).\\nThe notation (t) for a delta function was first introduced into quantum mechanics by Dirac. This\\nnotation is now in general use in the signal processing literature. For detailed discussions of the delta\\nfunction, see Bracewell (1986).\\nIn a rigorous sense, the Dirac delta function is a distribution, not a function; for a rigorous treatment\\nof the subject, see the book by Lighthill (1958).\\nThe Paley-Wiener criterion is named in honor of the authors of the paper by Paley and Wiener\\n(1934).\\nThe integral in (2.54), defining the Hilbert transform of a signal, is an improper integral in that\\nthe integrand has a singularity at  = t. To avoid this singularity, the integration must be carried out in\\na symmetrical manner about the point  = t. For this purpose, we use the definition\\nwhere the symbol P denotes Cauchys principal value of the integral and\\nis incrementally\\nsmall. For notational simplicity, the symbol P has been omitted from (2.54) and (2.55).\\nThe complex representation of an arbitrary signal defined in (2.58) was first described by Gabor\\n(1946). Gabor used the term analytic signal. The term pre-envelope was used in Arens (1957)\\nand Dungundji (1958). For a review of the different envelopes, see the paper by Rice (1982).\\nThe FFT is ubiquitous in that it is applicable to a great variety of unrelated fields. For a detailed',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 105,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'and Dungundji (1958). For a review of the different envelopes, see the paper by Rice (1982).\\nThe FFT is ubiquitous in that it is applicable to a great variety of unrelated fields. For a detailed\\nmathematical treatment of this widely used tool and its applications, the reader is referred to\\nBrigham (1988). Figure P2.28 +1 0 m(t) t Tb Tb -1 s t( ) Ac 2fmt ( ) 2fct ( ) cos cos = g t( ) 2dt  -   g t( ) G f( )exp j2ft ( ) df A - A  - 2  -   A   lim 0 = P g  ( ) t  - ----------d  -   g  ( ) t  - ----------d g  ( ) t  - ----------d t+    +  - t=   g 0  lim =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 105,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '87 CHAPTER 3 Probability Theory and\\nBayesian Inference 3.1 Introduction The idea of a mathematical model used to describe a physical phenomenon is well\\nestablished in the physical sciences and engineering. In this context, we may distinguish\\ntwo classes of mathematical models: deterministic and probabilistic. A model is said to be\\ndeterministic if there is no uncertainty about its time-dependent behavior at any instant of\\ntime; linear time-invariant systems considered in Chapter 2 are examples of a\\ndeterministic model. However, in many real-world problems, the use of a deterministic\\nmodel is inappropriate because the underlying physical phenomenon involves too many\\nunknown factors. In such situations, we resort to a probabilistic model that accounts for\\nuncertainty in mathematical terms.\\nProbabilistic models are needed for the design of systems that are reliable in\\nperformance in the face of uncertainty, efficient in computational terms, and cost effective\\nin building them. Consider for example, a digital communication system that is required to\\nprovide practically error-free communication across a wireless channel. Unfortunately, the\\nwireless channel is subject to uncertainties, the sources of which include:\\n\\nnoise, internally generated due to thermal agitation of electrons in the conductors\\nand electronic devices at the front-end of the receiver;\\n\\nfading of the channel, due to the multipath phenomenon-an inherent characteristic\\nof wireless channels;\\n\\ninterference, representing spurious electromagnetic waves emitted by other\\ncommunication systems or microwave devices operating in the vicinity of the receiver.\\nTo account for these uncertainties in the design of a wireless communication system, we\\nneed a probabilistic model of the wireless channel.\\nThe objective of this chapter, devoted to probability theory, is twofold:\\n\\nthe formulation of a logical basis for the mathematical description of probabilistic\\nmodels and',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 107,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the formulation of a logical basis for the mathematical description of probabilistic\\nmodels and\\n\\nthe development of probabilistic reasoning procedures for handling uncertainty.\\nSince the probabilistic models are intended to assign probabilities to the collections (sets)\\nof possible outcomes of random experiments, we begin the study of probability theory\\nwith a review of set theory, which we do next.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 107,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\n2 Set Theory Definitions The objects constituting a set are called the elements of the set. Let A be a set and x be an\\nelement of the set A. To describe this statement, we write\\n; otherwise, we write\\n. If the set A is empty (i.e., it has no elements), we denote it by\\n.\\nIf x1, x2, ..., xN are all elements of the set A, we write\\nin which case we say that the set A is countably finite. Otherwise, the set is said to be\\ncountably infinite. Consider, for example, an experiment involving the throws of a die. In\\nthis experiment, there are six possible outcomes: the showing of one, two, three, four, five,\\nand six dots on the upper surface of the die; the set of possible outcomes of the experiment\\nis therefore countably finite. On the other hand, the set of all possible odd integers, written\\nas {1, 3, 5, }, is countably infinite.\\nIf every element of the set A is also an element of another set B, we say that A is a\\nsubset of B, which we describe by writing\\n.\\nIf two sets A and B satisfy the conditions and\\n, then the two sets are said\\nto be identical or equal, in which case we write A = B.\\nIn a discussion of set theory, we also find it expedient to think of a universal set,\\ndenoted by S. Such a set contains every possible element that could occur in the context of\\na random experiment.\\nBoolean Operations on Sets\\nTo illustrate the validity of Boolean operations on sets, the use of Venn diagrams can be\\nhelpful, as shown in what follows.\\nUnions and Intersections\\nThe union of two sets A and B is defined by the set of elements that belong to A or B, or to\\nboth. This operation, written as\\n, is illustrated in the Venn diagram of Figure 3.1.\\nThe intersection of two sets A and B is defined by the particular set of elements that belong\\nto both A and B, for which we write\\n. The shaded part of the Venn diagram in\\nFigure 3.1 represents this second operation.\\nFigure 3.1\\nIllustrating the union and intersection\\nof two sets, A and B.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 108,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'to both A and B, for which we write\\n. The shaded part of the Venn diagram in\\nFigure 3.1 represents this second operation.\\nFigure 3.1\\nIllustrating the union and intersection\\nof two sets, A and B.\\nx A  x A   A x1 x2 xN      = A B  A B  B A  A B  A B  Universal set SABAB',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 108,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '2 Set Theory Let x be an element of interest. Mathematically, the operations of union and\\nintersection are respectively described by\\nand\\nwhere the symbol | is shorthand for such that.\\nDisjoint and Partition Sets\\nTwo sets A and B are said to be disjoint if their intersection is empty; that is, they have no\\ncommon elements.\\nThe partition of a set A refers to a collection of disjoint subsets A1, A2, ..., AN of the set\\nA, the union of which equals A; that is,\\nThe Venn diagram illustrating the partition operation is depicted in Figure 3.2 for the\\nexample of N = 3.\\nComplements\\nThe set Ac is said to be the complement of the set A, with respect to the universal set S, if it\\nis made up of all the elements of S that do not belong to A, as depicted in Figure 3.3.\\nFigure 3.2\\nIllustrating the partition of set A into\\nthree subsets: A1, A2, and A3.\\nFigure 3.3\\nIllustrating the complement Ac of set A.\\nA1 A2 A3 Universal set SAB  xx A or x B     = A B  xx A and x B     = A A1 A2  AN   = A Universal set S Ac',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 109,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nThe Algebra of Sets\\nBoolean operations on sets have several properties, summarized here:\\nIdempotence property\\n(Ac)c = A\\nCommutative property\\nAssociative property\\nDistributive property\\nNote that the commutative and associative properties apply to both the union and\\nintersection, whereas the distributive property applies only to the intersection.\\nDe Morgans laws\\nThe complement of the union of two sets A and B is equal to the intersection of their\\nrespective complements; that is\\nThe complement of the intersection of two sets A and B is equal to the union of their\\nrespective complements; that is,\\nFor illustrations of these five properties and their confirmation, the reader is referred to\\nProblem 3.1. 3.3 Probability Theory Probabilistic Models The mathematical description of an experiment with uncertain outcomes is called a\\nprobabilistic model,1 the formulation of which rests on three fundamental ingredients:\\nSample space or universal set S, which is the set of all conceivable outcomes of a\\nrandom experiment under study.\\nA class E of events that are subsets of S.\\nProbability law, according to which a nonnegative measure or number \\x02[A] is\\nassigned to an event A. The measure \\x02[A] is called the probability of event A. In a\\nsense, \\x02[A] encodes our belief in the likelihood of event A occurring when the\\nexperiment is conducted.\\nThroughout the book, we will use the symbol \\x02[.] to denote the probability of occurrence\\nof the event that appears inside the square brackets.\\nA B  B A  = A B  B A  = ABC     A B    C  = ABC     A B    C  = ABC     A B    A C     = ABC     A B    A C     = A B   c Ac Bc  = A B   c Ac Bc  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 110,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '3 Probability Theory As illustrated in Figure 3.4, an event may involve a single outcome or a subset of\\npossible outcomes in the sample space S. These possibilities are exemplified by the way in\\nwhich three events, A, B, and C, are pictured in Figure 3.4. In light of such a reality, we\\nidentify two extreme cases:\\n\\nSure event, which embodies all the possible outcomes in the sample space S.\\n\\nNull or impossible event, which corresponds to the empty set or empty space\\n.\\nAxioms of Probability\\nFundamentally, the probability measure \\x02[A], assigned to event A in the class E, is\\ngoverned by three axioms:\\nAxiom I Nonnegativity\\nThe first axiom states that the probability of event A is a\\nnonnegative number bounded by unity, as shown by for any event A\\n(3.1)\\nAxiom II Additivity\\nThe second axiom states that if A and B are two disjoint events,\\nthen the probability of their union satisfies the equality\\n(3.2)\\nIn general, if the sample space has N elements and A1, A2, , AN is a sequence of disjoint\\nevents, then the probability of the union of these N events satisfies the equality\\nAxiom III Normalization\\nThe third and final axiom states that the probability of the\\nentire sample space S is equal to unity, as shown by\\n(3.3)\\nThese three axioms provide an implicit definition of probability. Indeed, we may use them\\nto develop some other basic properties of probability, as described next.\\nFigure 3.4 Illustration of the relationship between sample space, events, and probability\\ns1 sk A \\x02 Events Probability Sample space S 1 0 BCE  O \\x02 A  1   \\x02 A B    \\x02 A  \\x02 B  + = \\x02 A1 A2 AN     \\x02 A1   \\x02 A2    \\x02 AN   + + + = \\x02 S  1 =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 111,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nPROPERTY\\nThe probability of an impossible event is zero.\\nTo prove this property, we first use the axiom of normalization, then express the sample\\nspace S as the union of itself with the empty space\\n, and then use the axiom of\\nadditivity. We thus write\\nfrom which the property follows immediately.\\nPROPERTY\\nLet Ac denote the complement of event A; we may then write for any event A\\n(3.4)\\nTo prove this property, we first note that the sample space S is the union of the two\\nmutually exclusive events A and Ac. Hence, the use of the additivity and normalization\\naxioms yields\\nfrom which, after rearranging terms, (3.4) follows immediately.\\nPROPERTY\\nIf event A lies within the subspace of another event B, then for\\n(3.5)\\nTo prove this third property, consider the Venn diagram depicted in Figure 3.5. From this\\ndiagram, we observe that event B may be expressed as the union of two disjoint events, one\\ndefined by A and the other defined by the intersection of B with the complement of A; that is,\\n 1 \\x02 S  = \\x02 S     = \\x02 S  \\x02    + = 1 \\x02    + = \\x02    0 = \\x02 Ac   1 \\x02 A  - = 1 \\x02 S  = \\x02 A Ac    = \\x02 A  \\x02 Ac   + = Figure 3.5 The Venn diagram for proving (3.5).\\n\\x02 A  \\x02 B   A B  BAB Ac     = A Universal set S B',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 112,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '3 Probability Theory Therefore, applying the additivity axiom to this relation, we get\\nNext, invoking the nonnegativity axiom, we immediately find that the probability of event\\nB must be equal to or greater than the probability of event A, as indicated in (3.5).\\nPROPERTY\\nLet N disjoint events A1, A2, , AN satisfy the condition\\n(3.6) then (3.7) To prove this fourth property, we first apply the normalization axiom to (3.6) to write\\nNext, recalling the generalized form of the additivity axiom\\nFrom these two relations, (3.7) follows immediately.\\nFor the special case of N equally probable events, (3.7) reduces to\\n(3.8)\\nPROPERTY\\nIf two events A and B are not disjoint, then the probability of their union event is defined by (3.9) where is called the joint probability of A and B.\\nTo prove this last property, consider the Venn diagram of Figure 3.6. From this figure,\\nwe first observe that the union of A and B may be expressed as the union of two disjoint\\nevents: A itself and\\n, where Ac is the complement of A. We may therefore apply the\\nadditivity axiom to write\\n(3.10) \\x02 B  \\x02 A  \\x02 B Ac    + = A1 A2 AN   S = \\x02 A1   \\x02 A2    \\x02 AN   + + + 1 = \\x02 A1 A2  A  N     1 = \\x02 A1 A2 AN     \\x02 A1   \\x02 A2    \\x02 AN   + + + = \\x02 Ai   1 N---- for i 1 2, N   = = Figure 3.6 The Venn diagram for proving (3.9).\\n\\x02 A B    \\x02 A  \\x02 B  \\x02 A B    for any two events A and B\\n- + = \\x02 A B    Ac B  \\x02 A B    \\x02 A Ac B       = \\x02 A  \\x02 Ac B    + = ABAB Universal set S',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 113,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nFrom the Venn diagram of Figure 3.6, we next observe that the event B may be expressed as\\nThat is, B is the union of two disjoint events: and\\n; therefore, applying the\\nadditivity axiom to this second relation yields\\n(3.11)\\nSubtracting (3.11) from (3.10), canceling the common term and rearranging\\nterms, (3.9) follows and Property 4 is proved.\\nIt is of interest to note that the joint probability accounts for that part of the\\nsample space S where the events A and B coincide. If these two events are disjoint, then the\\njoint probability is zero, in which case (3.9) reduces to the additivity axiom of\\n(3.2).\\nConditional Probability\\nWhen an experiment is performed and we only obtain partial information on the outcome\\nof the experiment, we may reason about that particular outcome by invoking the notion of\\nconditional probability. Stated the other way round, we may make the statement:\\nConditional probability provides the premise for probabilistic reasoning.\\nTo be specific, suppose we perform an experiment that involves a pair of events A and B.\\nLet \\x02[A|B] denote the probability of event A given that event B has occurred. The\\nprobability \\x02[A|B] is called the conditional probability of A given B. Assuming that B has\\nnonzero probability, the conditional probability \\x02[A|B] is formally defined by\\n(3.12)\\nwhere is the joint probability of events A and B, and \\x02[B] is nonzero.\\nFor a fixed event B, the conditional probability \\x02[A|B] is a legitimate probability law\\nas it satisfies all three axioms of probability:\\nSince by definition, \\x02[A|B] is a probability, the nonnegativity axiom is clearly\\nsatisfied.\\nViewing the entire sample space S as event A and noting that\\n, we may\\nuse (3.12) to write\\nHence, the normalization axiom is also satisfied.\\nFinally, to verify the additivity axiom, assume that A1 and A2 are two mutually\\nexclusive events. We may then use (3.12) to write',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 114,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'use (3.12) to write\\nHence, the normalization axiom is also satisfied.\\nFinally, to verify the additivity axiom, assume that A1 and A2 are two mutually\\nexclusive events. We may then use (3.12) to write\\nBSB  = A Ac    B  = A B    Ac B     = A B  Ac B  \\x02 B  \\x02 A B    \\x02 Ac B    + = \\x02 Ac B    \\x02 A B    \\x02 A B    \\x02 A B   \\x02 A B    \\x02 B  ------------------------\\n= \\x02 A B    S B  B = \\x02 S B   \\x02 S B   \\x02 B  ------------------ \\x02 B  \\x02 B  ------------- 1 = = = \\x02 A1 A2 B    \\x02 A1 A2    B    \\x02 B  ---------------------------------------------\\n=',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 114,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '3 Probability Theory Applying the distributive property to the numerator on the right-hand side, we have\\nNext, recognizing that the two events and are actually disjoint, we\\nmay apply the additivity axiom to write\\n(3.13)\\nwhich proves that the conditional probability also satisfies the additivity axiom.\\nWe therefore conclude that all three axioms of probability (and therefore all known\\nproperties of probability laws) are equally valid for the conditional probability \\x02[A|B]. In\\na sense, this conditional probability captures the partial information that the occurrence of\\nevent B provides about event A; we may therefore view the conditional probability \\x02[A|B]\\nas a probability law concentrated on event B.\\nBayes Rule\\nSuppose we are confronted with a situation where the conditional probability \\x02[A|B] and\\nthe individual probabilities \\x02[A] and \\x02[B] are all easily determined directly, but the\\nconditional probability \\x02[B|A] is desired. To deal with this situation, we first rewrite\\n(3.12) in the form\\nClearly, we may equally write\\nThe left-hand parts of these two relations are identical; we therefore have\\nProvided that \\x02[A] is nonzero, we may determine the desired conditional probability\\n\\x02[B|A] by using the relation\\n(3.14)\\nThis relation is known as Bayes rule.\\nAs simple as it looks, Bayes rule provides the correct language for describing\\ninference, the formulation of which cannot be done without making assumptions.2 The\\nfollowing example illustrates an application of Bayes rule.\\nEXAMPLE\\nRadar Detection\\nRadar, a remote sensing system, operates by transmitting a sequence of pulses and has its\\nreceiver listen to echoes produced by a target (e.g., aircraft) that could be present in its\\nsurveillance area. \\x02 A1 A2 B    \\x02 A1 B    A2 B       \\x02 B  --------------------------------------------------------------\\n= A1 B  A2 B  \\x02 A1 A2 B    \\x02 A1 B    \\x02 A2 B    + \\x02 B  ------------------------------------------------------------\\n= \\x02 A1 B    \\x02 B  ---------------------------',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 115,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '= A1 B  A2 B  \\x02 A1 A2 B    \\x02 A1 B    \\x02 A2 B    + \\x02 B  ------------------------------------------------------------\\n= \\x02 A1 B    \\x02 B  ---------------------------\\n\\x02 A2 B    \\x02 B  ---------------------------\\n+ = \\x02 A B    \\x02 A B  \\x02 B  = \\x02 A B    \\x02 B A  \\x02 A  = \\x02 A B  \\x02 B  \\x02 B A  \\x02 A  = \\x02 B A   \\x02 A B  \\x02 B  \\x02 A  --------------------------------\\n=',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 115,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nLet the events A and B be defined as follows:\\nA = {a target is present in the area under surveillance}\\nAc = {there is no target in the area}\\nB = {the radar receiver detects a target}\\nIn the radar detection problem, there are three probabilities of particular interest:\\n\\x02[A]\\nprobability that a target is present in the area; this probability is called the\\nprior probability.\\n\\x02[B|A]\\nprobability that the radar receiver detects a target, given that a target is\\nactually present in the area; this second probability is called the probability\\nof detection.\\n\\x02[B|Ac]\\nprobability that the radar receiver detects a target in the area, given that there\\nis no target in the surveillance area; this third probability is called the\\nprobability of false alarm.\\nSuppose these three probabilities have the following values:\\n\\x02[A] = 0.02 \\x02[B|A] = 0.99 \\x02[B|Ac] = 0.01 The problem is to calculate the conditional probability \\x02[A|B] which defines the\\nprobability that a target is present in the surveillance area given that the radar receiver has\\nmade a target detection.\\nApplying Bayes rule, we write Independence Suppose that the occurrence of event A provides no information whatsoever about event B;\\nthat is,\\nThen, (3.14) also teaches us that\\n\\x02 A B   \\x02 B A  \\x02 A  \\x02 B  --------------------------------\\n= \\x02 B A  \\x02 A  \\x02 B A  \\x02 A  \\x02 B Ac  \\x02 Ac   + ----------------------------------------------------------------------------\\n= 0.99 0.02  0.99 0.02 0.01 0.98  +  -------------------------------------------------------------\\n= 0.0198 0.0296 ---------------- = 0.69  \\x02 B A   \\x02 B  = \\x02 A B   \\x02 A  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 116,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '4 Random Variables In this special case, we see that knowledge of the occurrence of either event, A or B, tells\\nus no more about the probability of occurrence of the other event than we knew without\\nthat knowledge. Events A and B that satisfy this condition are said to be independent.\\nFrom the definition of conditional probability given in (3.12), namely,\\nwe see that the condition \\x02[A|B]= \\x02[A] is equivalent to\\nWe therefore adopt this latter relation as the formal definition of independence. The\\nimportant point to note here is that the definition still holds even if the probability \\x02[B] is\\nzero, in which case the conditional probability \\x02[A|B] is undefined. Moreover, the\\ndefinition has a symmetric property, in light of which we can say the following:\\nIf an event A is independent of another event B, then B is independent of A, and\\nA and B are therefore independent events. Random Variables\\nIt is customary, particularly when using the language of sample space pertaining to an\\nexperiment, to describe the outcome of the experiment by using one or more real-valued\\nquantities or measurements that help us think in probabilistic terms. These quantities are\\ncalled random variables, for which we offer the following definition:\\nThe random variable is a function whose domain is a sample space and whose\\nrange is some set of real numbers.\\nThe following two examples illustrate the notion of a random variable embodied in this\\ndefinition.\\nConsider, for example, the sample space that represents the integers 1, 2, , 6, each\\none of which is the number of dots that shows uppermost when a die is thrown. Let the\\nsample point k denote the event that k dots show in one throw of the die. The random\\nvariable used to describe the probabilistic event k in this experiment is said to be a discrete\\nrandom variable.\\nFor an entirely different experiment, consider the noise being observed at the front end\\nof a communication receiver. In this new situation, the random variable, representing the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 117,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'random variable.\\nFor an entirely different experiment, consider the noise being observed at the front end\\nof a communication receiver. In this new situation, the random variable, representing the\\namplitude of the noise voltage at a particular instant of time, occupies a continuous range\\nof values, both positive and negative. Accordingly, the random variable representing the\\nnoise amplitude is said to be a continuous random variable.\\nThe concept of a continuous random variable is illustrated in Figure 3.7, which is a\\nmodified version of Figure 3.4. Specifically, for the sake of clarity, we have suppressed the\\nevents but show subsets of the sample space S being mapped directly to a subset of a real\\nline representing the random variable. The notion of the random variable depicted in\\nFigure 3.7 applies in exactly the same manner as it applies to the underlying events. The\\nbenefit of random variables, pictured in Figure 3.7, is that probability analysis can now be\\ndeveloped in terms of real-valued quantities, regardless of the form or shape of the\\nunderlying events of the random experiment under study.\\n\\x02 A B   \\x02 A B    \\x02 B  ------------------------\\n= \\x02 A B    \\x02 A \\x02 B  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 117,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nOne last comment is in order before we proceed further. Throughout the whole book,\\nwe will be using the following notation:\\nUppercase characters denote random variables and lowercase characters denote\\nreal values taken by random variables. Distribution Functions\\nTo proceed with the probability analysis in mathematical terms, we need a probabilistic\\ndescription of random variables that works equally well for discrete and continuous\\nrandom variables. Let us consider the random variable X and the probability of the event\\nX  x. We denote this probability by \\x02[X  x]. It is apparent that this probability is a\\nfunction of the dummy variable x. To simplify the notation, we write\\n(3.15)\\nThe function FX(x) is called the cumulative distribution function or simply the distribution\\nfunction of the random variable X. Note that FX(x) is a function of x, not of the random\\nvariable X. For any point x in the sample space, the distribution function FX(x) expresses\\nthe probability of an event.\\nThe distribution function FX(x), applicable to both continuous and discrete random\\nvariables, has two fundamental properties:\\nPROPERTY\\nBoundedness of the Distribution\\nThe distribution function FX(x) is a bounded function of the dummy variable x that lies\\nbetween zero and one.\\nSpecifically, FX(x) tends to zero as x tends to\\n, and it tends to one as x tends to\\n.\\nFigure 3.7 Illustration of the relationship between sample\\nspace, random variables, and probability.\\ns1 sk Sample space S Random variable  - Probability 1 0 0 FX x  \\x02 X x    for all x =  -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 118,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '5 Distribution Functions PROPERTY\\nMonotonicity of the Distribution\\nThe distribution function FX(x) is a monotone nondecreasing function of x.\\nIn mathematical terms, we write\\nBoth of these properties follow directly from (3.15).\\nThe random variable X is said to be continuous if the distribution function FX(x) is\\ndifferentiable with respect to the dummy variable x everywhere, as shown by\\n(3.16)\\nThe new function fX(x) is called the probability density function of the random variable X.\\nThe name, density function, arises from the fact that the probability of the event x1 < X  x2 is\\n(3.17)\\nThe probability of an interval is therefore the area under the probability density function in\\nthat interval. Putting in (3.17) and changing the notation somewhat, we readily\\nsee that the distribution function is defined in terms of the probability density function as\\n(3.18)\\nwhere is a dummy variable. Since = 1, corresponding to the probability of a\\nsure event, and = 0, corresponding to the probability of an impossible event, we\\nreadily find from (3.17) that\\n(3.19)\\nEarlier we mentioned that a distribution function must always be a monotone\\nnondecreasing function of its argument. It follows, therefore, that the probability density\\nfunction must always be nonnegative. Accordingly, we may now formally make the\\nstatement:\\nThe probability density function fX(x) of a continuous random variable X has\\ntwo defining properties: nonnegativity and normalization.\\nPROPERTY\\nNonnegativity\\nThe probability density function fX(x) is a nonnegative function of the sample value x of\\nthe random variable X.\\nPROPERTY\\nNormalization\\nThe total area under the graph of the probability density function fX(x) is equal to unity.\\nFX x1   FX x2   for x1 x2   fX x  d dx ------FX x  for all x = \\x02 x1 X x2     \\x02 X x2    \\x02 X x1    - = FX x2   FX x1   - = fX x dx x1 x2 = x1  - = FX x  fX  d  - x  =  FX    FX  -   fX x x d  -   1 =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 119,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nAn important point that should be stressed here is that the probability density function\\nfX(x) contains all the conceivable information needed for statistical characterization of the\\nrandom variable X. EXAMPLE 2 Uniform Distribution To illustrate the properties of the distribution function FX(x) and the probability density\\nfunction fX(x) for a continuous random variable, consider a uniformly distributed random\\nvariable, described by\\n(3.20)\\nIntegrating fX(x) with respect to x yields the associated distribution function\\n(3.21)\\nPlots of these two functions versus the dummy variable x are shown in Figure 3.8.\\nProbability Mass Function\\nConsider next the case of a discrete random variable, X, which is a real-valued function of\\nthe outcome of a probabilistic experiment that can take a finite or countably infinite\\nnumber of values. As mentioned previously, the distribution function FX(x) defined in\\n(3.15) also applies to discrete random variables. However, unlike a continuous random\\nvariable, the distribution function of a discrete random variable is not differentiable with\\nrespect to its dummy variable x.\\nFigure 3.8 Uniform distribution.\\nfX x  0, x a  1 b a - ------------, axb   0, x b         = FX x  0, x a  x a - b a - ------------, axb   0, x b         = fX(x) FX(x) 1 b - a 0 0 xxabab 1.0 (a) (b)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 120,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '5 Distribution Functions To get around this mathematical difficulty, we introduce the notion of the probability\\nmass function as another way of characterizing discrete random variables. Let X denote a\\ndiscrete random variable and let x be any possible value of X taken from a set of real\\nnumbers. We may then make the statement:\\nThe probability mass function of x, denoted by pX(x), is defined as the\\nprobability of the event X = x, which consists of all possible outcomes of\\nan experiment that lead to a value of X equal to x.\\nStated in mathematical terms, we write\\n(3.22)\\nwhich is illustrated in the next example.\\nEXAMPLE\\nThe Bernoulli Random Variable\\nConsider a probabilistic experiment involving the discrete random variable X that takes\\none of two possible values:\\n\\nthe value 1 with probability p;\\n\\nthe value 0 with probability 1 - p.\\nSuch a random variable is called the Bernoulli random variable, the probability mass\\nfunction of which is defined by\\n(3.23)\\nThis probability mass function is illustrated in Figure 3.9. The two delta functions, each of\\nweight 1/2, depicted in Figure 3.9 represent the probability mass function at each of the\\nsample points x = 0 and x = 1.\\nFrom here on, we will, largely but not exclusively, focus on the characterization of\\ncontinuous random variables. A parallel development and similar concepts are possible for\\ndiscrete random variables as well.3\\npX x  \\x02[X x] = = Figure 3.9 Illustrating the probability mass\\nfunction for a fair coin-tossing experiment.\\npX x  1 p - x 0 = p, x 1 = 0, otherwise      = Probability mass function \\x02[X = x] x 0 1 1 2 1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 121,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nMultiple Random Variables\\nThus far we have focused attention on situations involving a single random variable.\\nHowever, we frequently find that the outcome of an experiment requires several random\\nvariables for its description. In what follows, we consider situations involving two random\\nvariables. The probabilistic description developed in this way may be readily extended to\\nany number of random variables.\\nConsider two random variables X and Y. In this new situation, we say:\\nThe joint distribution function FX,Y(x,y) is the probability that the random\\nvariable X is less than or equal to a specified value x, and that the random\\nvariable Y is less than or equal to another specified value y.\\nThe variables X and Y may be two separate one-dimensional random variables or the\\ncomponents of a single two-dimensional random vector. In either case, the joint sample\\nspace is the xy-plane. The joint distribution function FX,Y(x,y) is the probability that the\\noutcome of an experiment will result in a sample point lying inside the quadrant of the joint sample space. That is,\\n(3.24)\\nSuppose that the joint distribution function FX,Y(x,y) is continuous everywhere and that the\\nsecond-order partial derivative\\n(3.25)\\nexists and is continuous everywhere too. We call the new function fX,Y(x,y) the joint\\nprobability density function of the random variables X and Y. The joint distribution\\nfunction FX,Y(x,y) is a monotone nondecreasing function of both x and y. Therefore, from\\n(3.25) it follows that the joint probability density function fX,Y(x,y) is always nonnegative.\\nAlso, the total volume under the graph of a joint probability density function must be\\nunity, as shown by the double integral (3.26)\\nThe so-called marginal probability density functions, fX(x) and fY(y), are obtained by\\ndifferentiating the corresponding marginal distribution functions\\nand\\nwith respect to the dummy variables x and y, respectively. We thus write',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 122,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'differentiating the corresponding marginal distribution functions\\nand\\nwith respect to the dummy variables x and y, respectively. We thus write\\n(3.27)  X x  Y y   -    -   FX Y  x y    \\x02 X xY y      = fX Y  x y    2FX Y  x y    xy -------------------------------\\n= fX Y  x y   xdyd  -    -   1 = FX x  fX Y  x     = FY y  fX Y  y    = fX x  d dx ------FX x  = d dx ------ fX Y  y   y d  -   d  - x  = fX Y  x y   y d  -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 122,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '5 Distribution Functions\\n103 Similarly, we write (3.28) In words, the first marginal probability density function fX(x), defined in (3.27), is\\nobtained from the joint probability density function fX,Y(x,y) by simply integrating it over\\nall possible values of the undesired random variable Y. Similarly, the second marginal\\nprobability density function fY(y), defined in (3.28), is obtained from fX,Y(x,y) by\\nintegrating it over all possible values of the undesired random variable; this time, the\\nundesirable random variable is X. Henceforth, we refer to fX(x) and fY(y), obtained in the\\nmanner described herein, as the marginal densities of the random variables X and Y, whose\\njoint probability density function is fX,Y(x,y). Here again, we conclude the discussion on a\\npair of random variables with the following statement:\\nThe joint probability density function fX,Y(x,y) contains all the conceivable\\ninformation on the two continuous random variables X and Y that is needed for\\nthe probability analysis of joint random variables.\\nThis statement can be generalized to cover the joint probability density function of many\\nrandom variables.\\nConditional Probability Density Function\\nSuppose that X and Y are two continuous random variables with their joint probability\\ndensity function fX,Y(x,y). The conditional probability density function of Y, such that\\nX = x, is defined by\\n(3.29)\\nprovided that fX(x) > 0, where fX(x) is the marginal density of X; fY(y|x) is a shortened\\nversion of fY|X(y|x), both of which are used interchangeably. The function fY(y|x) may be\\nthought of as a function of the variable Y, with the variable x arbitrary but fixed;\\naccordingly, it satisfies all the requirements of an ordinary probability density function for\\nany x, as shown by and (3.30) Cross-multiplying terms in (3.29) yields\\nwhich is referred to as the multiplication rule.\\nSuppose that knowledge of the outcome of X can, in no way, affect the distribution of Y.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 123,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'which is referred to as the multiplication rule.\\nSuppose that knowledge of the outcome of X can, in no way, affect the distribution of Y.\\nThen, the conditional probability density function fY(y|x) reduces to the marginal density\\nfY(y), as shown by fY y  fX Y  x y  x d  -   = fY y x   fX Y  x y    fX x  -----------------------\\n= fY y x   0  fY y x  y d  -   1 = fX Y  x y    fY y x  fX x  = fY y x   fY y  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 123,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nIn such a case, we may express the joint probability density function of the random\\nvariables X and Y as the product of their respective marginal densities; that is,\\nOn the basis of this relation, we may now make the following statement on the\\nindependence of random variables:\\nIf the joint probability density function of the random variables X and Y equals the\\nproduct of their marginal densities, then X and Y are statistically independent.\\nSum of Independent Random Variables: Convolution\\nLet X and Y be two continuous random variables that are statistically independent; their\\nrespective probability density functions are denoted by fX(x) and fY(y). Define the sum\\nThe issue of interest is to find the probability density function of the new random variable\\nZ, which is denoted by fZ(z).\\nTo proceed with this evaluation, we first use probabilistic arguments to write\\nwhere, in the second line, the given value x is used for the random variable X. Since X and\\nY are statistically independent, we may simplify matters by writing\\nEquivalently, in terms of the pertinent distribution functions, we may write\\nHence, differentiating both sides of this equation, we get the corresponding probability\\ndensity functions\\nUsing the multiplication rule described in (3.30), we have\\n(3.31)\\nNext, adapting the definition of the marginal density given in (3.27) to the problem at\\nhand, we write\\n(3.32)\\nFinally, substituting (3.31) into (3.32), we find that the desired fZ(z) is equal to the\\nconvolution of fX(x) and fY(y), as shown by\\n(3.33) fX Y  x y    fX x fY y  = ZXY + = \\x02 Z zX  x =   \\x02 X Y zX  + x =   = \\x02 x Y zX  + x =   = \\x02 Z zX  x =   \\x02 xYz  +   = \\x02 Yzx -    = FZ z x   FY z x -   = fz z x   fY z x -   = fZ X  z x    fY z x -  fX x  = fZ z fZ X  z x   x d  -   = fZ z fX x fY z x -  x d  -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 124,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '6 The Concept of Expectation In words, we may therefore state:\\nThe summation of two independent continuous random variables leads to the\\nconvolution of their respective probability density functions.\\nNote, however, that no assumptions were made in arriving at this statement except for the\\nrandom variables X and Y being continuous random variables. The Concept of Expectation\\nAs pointed out earlier, the probability density function fX(x) provides a complete statistical\\ndescription of a continuous random variable X. However, in many instances, we find that\\nthis description includes more detail than is deemed to be essential for practical\\napplications. In situations of this kind, simple statistical averages are usually considered\\nto be adequate for the statistical characterization of the random variable X.\\nIn this section, we focus attention on the first-order statistical average, called the\\nexpected value or mean of a random variable; second-order statistical averages are studied\\nin the next section. The rationale for focusing attention on the mean of a random variable\\nis its practical importance in statistical terms, as explained next.\\nMean\\nThe expected value or mean of a continuous random variable X is formally defined by\\n(3.34)\\nwhere \\x03 denotes the expectation or averaging operator. According to this definition, the\\nexpectation operator \\x03, applied to a continuous random variable x, produces a single\\nnumber that is derived uniquely from the probability density function fX(x).\\nTo describe the meaning of the defining equation (3.34), we may say the following:\\nThe mean X of a random variable X, defined by the expectation \\x03[x], locates\\nthe center of gravity of the area under the probability density curve of the\\nrandom variable X.\\nTo elaborate on this statement, we write the integral in (3.34) as the limit of an\\napproximating sum formulated as follows. Let {xk|k = 0, 1, 2, } denote a set of\\nuniformly spaced points on the real line\\n(3.35)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 125,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'approximating sum formulated as follows. Let {xk|k = 0, 1, 2, } denote a set of\\nuniformly spaced points on the real line\\n(3.35)\\nwhere  is the spacing between adjacent points on the line. We may thus rewrite (3.34) in\\nthe form of a limit as follows:\\nX \\x03 X  xfX x x d  -   = = xk k 1 2--- +    , k 0 1 2       = = \\x03 X   0  lim xk fX x xdk k+1    k  - =   =  0  lim xk\\x02 xk  2--- X xk  2--- +   - k  - =   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 125,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nFor a physical interpretation of the sum in the second line of the right-hand side of this\\nequation, suppose that we make n independent observations of the random variable X. Let\\nNn(k) denote the number of times that the random variable X falls inside the kth bin,\\ndefined by\\nArguing heuristically, we may say that, as the number of observations n is made large, the\\nratio Nn(k)/n approaches the probability \\x02[xk - /2 < X  xk + /2]. Accordingly, we may\\napproximate the expected value of the random variable X as\\n(3.36)\\nWe now recognize the quantity on the right-hand side of (3.36) simply as the sample\\naverage. The sum is taken over all the values xk, each of which is weighted by the number\\nof times it occurs; the sum is then divided by the total number of observations to give the\\nsample average. Indeed, (3.36) provides the basis for computing the expectation \\x03[X].\\nIn a loose sense, we may say that the discretization, introduced in (3.35), has changed\\nthe expectation of a continuous random variable to the sample averaging over a discrete\\nrandom variable. Indeed, in light of (3.36), we may formally define the expectation of a\\ndiscrete random variable X as\\n(3.37)\\nwhere pX(x) is the probability mass function of X, defined in (3.22), and where the\\nsummation extends over all possible discrete values of the dummy variable x. Comparing\\nthe summation in (3.37) with that of (3.36), we see that, roughly speaking, the ratio Nn(x)/n\\nplays a role similar to that of the probability mass function pX(x), which is intuitively\\nsatisfying.\\nJust as in the case of a continuous random variable, here again we see from the defining\\nequation (3.37) that the expectation operator \\x03, applied to a discrete random variable X,\\nproduces a single number derived uniquely from the probability mass function pX(x).\\nSimply put, the expectation operator \\x03 applies equally well to discrete and continuous\\nrandom variables.\\nProperties of the Expectation Operator',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 126,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Simply put, the expectation operator \\x03 applies equally well to discrete and continuous\\nrandom variables.\\nProperties of the Expectation Operator\\nThe expectation operator \\x03 plays a dominant role in the statistical analysis of random\\nvariables (as well as random processes studied in Chapter 4). It is therefore befitting that\\nwe study two important properties of this operation in this section; other properties are\\naddressed in the end-of-chapter Problem 3.13.\\nxk  2--- X xk  2---, k 0 1 2       = +   - \\x03 X  xk Nn k  n --------------     k=-    1 n--- xkNn k  k=-   for large n  = \\x03 X  xpX x  x =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 126,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 The Concept of Expectation\\n107 PROPERTY 1 Linearity Consider a random variable Z, defined by\\nwhere X and Y are two continuous random variables whose probability density functions\\nare respectively denoted by fX(x) and fY(y). Extending the definition of expectation\\nintroduced in (3.34) to the random variable Z, we write\\nwhere fZ(z) is defined by the convolution integral of (3.33). Accordingly, we may go on to\\nexpress the expectation \\x03[Z] as the double integral\\nwhere the joint probability density function\\nMaking the one-to-one change of variables\\nand\\nx = x\\nwe may now express the expectation \\x03[Z] in the expanded form\\nNext, we recall from (3.27) that the first marginal density of the random variable X is\\nand, similarly, for the second marginal density\\nThe formula for the expectation \\x03[Z] is therefore simplified as follows: ZXY + = \\x03 Z  zfZ zz d  -   = \\x03 Z  zfX x fY z x -  xdzd  -    -   = zfX Y  x z  x -  xdzd  -    -   = fX Y  x z  x -   fX x fY z x -   = yzx - = \\x03 Z  x y +  fX Y  x y   xdyd  -    -   = xfX Y  x y   xdyd yfX Y  x y   xdyd  -    -   +  -    -   = fX x  fX Y  x y   dy  -   = fY y  fX Y  x y   dx  -   = \\x03 Z  xfX x dx yfY y dy  -   +  -   = \\x03 X  \\x03 Y  + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 127,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nWe may extend this result to the sum of many random variables by the method of induction\\nand thus write that, in general,\\n(3.38)\\nIn words, we may therefore state:\\nThe expectation of a sum of random variables is equal to the sum of the\\nindividual expectations.\\nThis statement proves the linearity property of the expectation operator, which makes this\\noperator all the more appealing.\\nPROPERTY\\nStatistical Independence\\nConsider next the random variable Z, defined as the product of two independent random\\nvariables X and Y, whose probability density functions are respectively denoted by fX(x)\\nand fY(y). As before, the expectation of Z is defined by\\nexcept that, this time, we have\\nwhere, in the second line, we used the statistical independence of X and Y. With Z = XY, we\\nmay therefore recast the expectation \\x03[Z] as\\n(3.39)\\nIn words, we may therefore state:\\nThe expectation of the product of two statistically independent random\\nvariables is equal to the product of their individual expectations.\\nHere again, by induction, we may extend this statement to the product of many\\nindependent random variables. Second-Order Statistical Averages\\nFunction of a Random Variable\\nIn the previous section we studied the mean of random variables in some detail. In this\\nsection, we expand on the mean by studying different second-order statistical averages.\\n\\x03 Xi i=1 n  \\x03[Xi) i=1 n  = \\x03 Z  zfZ z z d  -   = fZ z fX Y  x y    = fX x fY y  = \\x03 XY   xyfX x fY y xdyd  -   = xfX x x d yfY y  y d  -    -   = \\x03 X \\x03 Y  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 128,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '7 Second-Order Statistical Averages These statistical averages, together with the mean, complete the partial characterization\\nof random variables.\\nTo this end, let X denote a random variable and let g(X) denote a real-valued function of\\nX defined on the real line. The quantity obtained by letting the argument of the function\\ng(X) be a random variable is also a random variable, which we denote as\\n(3.40)\\nTo find the expectation of the random variable Y, we could, of course, find the probability\\ndensity function fY(y) and then apply the standard formula\\nA simpler procedure, however, is to write\\n(3.41)\\nEquation (3.41) is called the expected value rule; validity of this rule for a continuous\\nrandom variable is addressed in Problem 3.14.\\nEXAMPLE\\nThe Cosine Transformation of a Random Variable\\nLet\\nwhere X is a random variable uniformly distributed in the interval (-,); that is,\\nAccording to (3.41), the expected value of Y is\\nThis result is intuitively satisfying in light of what we know about the dependence of a\\ncosine function on its argument.\\nSecond-order Moments\\nFor the special case of g(X) = Xn, the application of (3.41) leads to the nth moment of the\\nprobability distribution of a random variable X; that is,\\n(3.42) Y gX  = \\x03 Y  yfY y  y d  -   = \\x03 g X    g x fX x  x d  -   = Y gX  X  cos = = fX x  1 2 ------,  x    - 0, otherwise      = \\x03 Y  x cos   1 2 ------     x d  -   = 1 2 ------ x x  - =  sin - = 0 = \\x03 Xn   xnfX x  x d  -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 129,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nFrom an engineering perspective, however, the most important moments of X are the first\\ntwo moments. Putting n = 1 in (3.42) gives the mean of the random variable, which was\\ndiscussed in Section 3.6. Putting n = 2 gives the mean-square value of X, defined by\\n(3.43)\\nVariance\\nWe may also define central moments, which are simply the moments of the difference\\nbetween a random variable X and its mean X. Thus, the nth central moment of X is\\n(3.44)\\nFor n = 1, the central moment is, of course, zero. For n = 2, the second central moment is\\nreferred to as the variance of the random variable X, defined by\\n(3.45)\\nThe variance of a random variable X is commonly denoted by\\n. The square root of the\\nvariance, namely\\n, is called the standard deviation of the random variable X.\\nIn a sense, the variance of the random variable X is a measure of the variables\\nrandomness or volatility. By specifying the variance we essentially constrain the\\neffective width of the probability density function fX(x) of the random variable X about the\\nmean\\n. A precise statement of this constraint is contained in the Chebyshev inequality,\\nwhich states that for any positive number , we have the probability\\n(3.46)\\nFrom this inequality we see that the mean and variance of a random variable provide a\\nweak description of its probability distribution; hence the practical importance of these\\ntwo statistical averages.\\nUsing (3.43) and (3.45), we find that the variance and the mean-square value \\x03[X2]\\nare related by\\n(3.47)\\nwhere, in the second line, we used the linearity property of the statistical expectation\\noperator \\x03. Equation (3.47) shows that if the mean X is zero, then the variance and\\nthe mean-square value \\x03 [X2] of the random variable X are equal.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 130,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'operator \\x03. Equation (3.47) shows that if the mean X is zero, then the variance and\\nthe mean-square value \\x03 [X2] of the random variable X are equal.\\n\\x03 X2   x2fX x  x d  -   = \\x03 X X -  n   (x X - nfX x  x d  -   = var X  \\x03 X x -  2 = (x X - 2fX x  x d  -   = X 2 X X 2 X 2 X \\x02[ X X -  ] X 2 2 ------  X 2 X 2 \\x03 X2 2XX - X 2 +   = \\x03 X2   2X\\x03 X  - X 2 + = \\x03 X2   X 2 - = X',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 130,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Characteristic Function Covariance\\nThus far, we have considered the characterization of a single random variable. Consider\\nnext a pair of random variables X and Y. In this new setting, a set of statistical averages of\\nimportance is the joint moments, namely the expectation of XiYk, where i and k may\\nassume any positive integer values. Specifically, by definition, we have\\n(3.48)\\nA joint moment of particular importance is the correlation, defined by \\x03[XY], which\\ncorresponds to i = k = 1 in this equation.\\nMore specifically, the correlation of the centered random variables (X - \\x03[X]) and\\n(Y - \\x03[Y]), that is, the joint moment\\n(3.49)\\nis called the covariance of X and Y. Let X = \\x03[X] and Y = \\x03[Y]; we may then expand\\n(3.49) to obtain the result\\n(3.50)\\nwhere we have made use of the linearity property of the expectation operator \\x03. Let\\nand denote the variances of X and Y, respectively. Then, the covariance of X and Y,\\nnormalized with respect to the product XY, is called the correlation coefficient of X and\\nY, expressed as\\n(3.51)\\nThe two random variables X and Y are said to be uncorrelated if, and only if, their\\ncovariance is zero; that is,\\nThey are said to be orthogonal if and only if their correlation is zero; that is,\\nIn light of (3.50), we may therefore make the following statement:\\nIf one of the random variables X and Y or both have zero means, and if they are\\northogonal, then they are uncorrelated, and vice versa. Characteristic Function\\nIn the preceding section we showed that, given a continuous random variable X, we can\\nformulate the probability law defining the expectation of Xn (i.e., nth moment of X) in\\nterms of the probability density function fX(x), as shown in (3.42). We now introduce\\nanother way of formulating this probability law; we do so through the characteristic\\nfunction. \\x03 XiYk   xiykfX Y  x y   xdyd  -    -   = cov XY   \\x03 X \\x03 X  -  Y \\x03 Y  -     = cov XY   \\x03 XY   XY - = X 2 Y 2 X Y    cov XY   XY ---------------------\\n= cov XY   0 = \\x03 XY   0 =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 131,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nFor a formal definition of this new concept, we say:\\nThe characteristic function of a continuous random variable X, denoted by\\n, is defined as the expectation of the complex exponential function\\nexp(jX), that is (3.52) where is real and . According to the second expression on the right-hand side of (3.52), we may also view the\\ncharacteristic function of the random variable X as the Fourier transform of the\\nassociated probability density function fX(x), except for a sign change in the exponent. In\\nthis interpretation of the characteristic function we have used rather than so as to conform with the convention adopted in probability theory.\\nRecognizing that and x play roles analogous to the variables 2f and t respectively in\\nthe Fourier-transform theory, we may appeal to the Fourier transform theory of Chapter\\nto recover the probability density function fX(x) of the random variable X given the\\ncharacteristic function\\n. Specifically, we may use the inversion formula to write\\n(3.53)\\nThus, with fX(f) and X(f) forming a Fourier-transform pair, we may obtain the moments\\nof the random variable X from the function X(f). To pursue this issue, we differentiate\\nboth sides of (3.52) with respect to v a total of n times, and then set  = 0; we thus get the\\nresult\\n(3.54)\\nThe integral on the right-hand side of this relation is recognized as the nth moment of the\\nrandom variable X. Accordingly, we may recast (3.54) in the equivalent form\\n(3.55)\\nThis equation is a mathematical statement of the so-called moment theorem. Indeed, it is\\nbecause of (3.55) that the characteristic function is also referred to as a moment-\\ngenerating function.\\nEXAMPLE\\nExponential Distribution\\nThe exponential distribution is defined by',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 132,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'because of (3.55) that the characteristic function is also referred to as a moment-\\ngenerating function.\\nEXAMPLE\\nExponential Distribution\\nThe exponential distribution is defined by\\n(3.56) X  X  \\x03 jX   exp   = fX x  jx   exp x d  -   =  j 1 - = X  jx   exp j - x   exp  X  fX x  1 2 ------ X  j - x   exp dx  -   = dn dn --------X v=0 jn xnfX x dx  -   = \\x03 Xn   j -  n dn dn --------X  0 = = X  fX x   x -  , exp x 0  0, otherwise    =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 132,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '9 The Gaussian Distribution where  is the only parameter of the distribution. The characteristic function of the\\ndistribution is therefore\\nWe wish to use this result to find the mean of the exponentially distributed random\\nvariable X. To do this evaluation, we differentiate the characteristic function with respect to once, obtaining where the prime in signifies first-order differentiation with respect to the\\nargument\\n. Hence, applying the moment theorem of (3.55), we get the desired result\\n(3.57) The Gaussian Distribution\\nAmong the many distributions studied in the literature on probability theory, the Gaussian\\ndistribution stands out, by far, as the most commonly used distribution in the statistical\\nanalysis of communications systems, for reasons that will become apparent in Section\\nLet X denote a continuous random variable; the variable X is said to be Gaussian\\ndistributed if its probability density function has the general form\\n(3.58)\\nwhere  and  are two scalar parameters that characterize the distribution. The parameter\\n can assume both positive and negative values (including zero), whereas the parameter \\nis always positive. Under these two conditions, the fX(x) of (3.58) satisfies all the\\nproperties of a probability density function, including the normalization property; namely,\\n(3.59)\\nProperties of the Gaussian Distribution\\nA Gaussian random variable has many important properties, four of which are\\nsummarized on the next two pages.\\n   x -   jx   d exp exp x 0   =   j - -------------- =   X  j  j -  2 ---------------------\\n= X   \\x03 X  jX  0 = - = 1 --- = fX x  1 2 -------------- x  -  2 22 ------------------- - exp = 1 2 -------------- x  -  2 22 ------------------- - exp x d  -   1 =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 133,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nPROPERTY\\nMean and Variance\\nIn the defining (3.58), the parameter  is the mean of the Gaussian random variable X and is its variance. We may therefore state:\\nA Gaussian random variable is uniquely defined by specifying its mean and\\nvariance.\\nPROPERTY\\nLinear Function of a Gaussian Random Variable\\nLet X be a Gaussian random variable with mean  and variance\\n. Define a new random\\nvariable\\nwhere a and b are scalars and\\n. Then Y is also Gaussian with mean\\nand variance\\nIn words, we may state:\\nGaussianity is preserved by a linear transformation.\\nPROPERTY\\nSum of Independent Gaussian Random Variables\\nLet X and Y be independent Gaussian random variables with means X and Y,\\nrespectively, and variances and\\n, respectively. Define a new random variable\\nThe random variable Z is also Gaussian with mean\\n(3.60) and variance (3.61) In general, we may therefore state:\\nThe sum of independent Gaussian random variables is also a Gaussian random\\nvariable, whose mean and variance are respectively equal to the sum of the\\nmeans and the sum of the variances of the constituent random variables.\\nPROPERTY\\nJointly Gaussian Random Variables\\nLet X and Y be a pair of jointly Gaussian random variables with zero means and variances and\\n, respectively. The joint probability density function of X and Y is completely\\ndetermined by X, Y, and , where  is the correlation coefficient defined in (3.51).\\nSpecifically, we have\\n(3.62)\\nwhere the normalization constant c is defined by\\n(3.63) 2 2 Y aX b + = a 0  \\x03 Y  a b + = var Y  a22 = X 2 Y 2 ZXY + = \\x03 Z  X Y + = var Z  X 2 Y 2 + = X 2 Y 2 fX Y  x y    c qxy    -   exp = c 1 21 2 - XY ----------------------------------------\\n=',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 134,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '9 The Gaussian Distribution and the exponential term is defined by\\n(3.64)\\nIn the special case where the correlation coefficient  is zero, the joint probability density\\nfunction of X and Y assumes the simple form\\n(3.65)\\nAccordingly, we may make the statement:\\nIf the random variables X and Y are both Gaussian with zero mean and if they\\nare also orthogonal (that is, \\x03[XY] = 0), then they are statistically independent.\\nBy virtue of Gaussianity, this statement is stronger than the last statement made at the end\\nof the subsection on covariance.\\nCommonly Used Notation\\nIn light of Property 1, the notation (\\n) is commonly used as the shorthand\\ndescription of a Gaussian distribution parameterized in terms of its mean  and variance\\n. The symbol  is used in recognition of the fact that the Gaussian distribution is also\\nreferred to as the normal distribution, particularly in the mathematics literature.\\nThe Standard Gaussian Distribution\\nWhen  = 0 and\\n= 1, the probability density function of (3.58) reduces to the special\\nform:\\n(3.66)\\nA Gaussian random variable X so described is said to be in its standard form.4\\nCorrespondingly, the distribution function of the standard Gaussian random variable is\\ndefined by\\n(3.67)\\nOwing to the frequent use of integrals of the type described in (3.67), several related\\nfunctions have been defined and tabulated in the literature. The related function commonly\\nused in the context of communication systems is the Q-function, which is formally defined as\\n(3.68) qxy    1 2 1 2 -   ----------------------- x2\\nX 2 ------ 2xy XY ------------- - y2 Y 2 ------ +       = fX Y  x y    1 2XY -------------------- x2 2X 2 ---------- - y2 2Y 2 --------- -       exp = fX x fY y  = 2 2 2 fX x  1 2 ---------- x2 2----- -     exp = FX x  1 2 ---------- t2 2---- -     exp td  - x  = Q x  1 FX x  - = 1 2 ---------- t2 2---- -     exp td x   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 135,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nIn words, we may describe the Q-function as follows:\\nThe Q-function, Q(x), is equal to the area covered by the tail of the probability\\ndensity function of the standard Gaussian random variable X, extending from x\\nto infinity.\\nUnfortunately, the integral of (3.67) defining the standard Gaussian distribution FX(x) does\\nnot have a closed-form solution. Rather, with accuracy being an issue of importance, FX(x) is\\nusually presented in the form of a table for varying x. Table 3.1 is one such recording. To\\nutilize this table for calculating the Q-function, we build on two defining equations:\\nFor nonnegative values of x, the first line of (3.68) is used.\\nFor negative values of x, use is made of the symmetric property of the Q-function:\\n(3.69)\\nStandard Gaussian Graphics\\nTo visualize the graphical formats of the commonly used standard Gaussian functions,\\nFX(x), fX(x), and Q(x), three plots are presented at the bottom of this page:\\nFigure 3.10a plots the distribution function, FX(x), defined in (3.67).\\nFigure 3.10b plots the density function, fX(x), defined in (3.66).\\nFigure 3.11 plots the Q-function defined in (3.68).\\nQ x -   1 Q x  - = Figure 3.10 The normalized Gaussian (a) distribution\\nfunction and (b) probability density function.\\nFigure 3.11 The Q-function.\\n(b) 0 -1 -2 -3 3 2 1 0.2 0.6 0.4 (a) 0 -1 -2 -3 3 2 1 0.2 1.0 0.6 0.8 fX (x) x x FX (x) Q (x) x 0 1 2 3 4 5 10-6 10-5 10-4 10-3 0.01 0.1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 136,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '9 The Gaussian Distribution Table 3.1 The standard Gaussian distribution (Q-function) table5',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 137,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '.00 .01 .02 .03 .04 .05 .06 .07 .08 .09 0.0 .5000 .5040 .5080 .5120 .5160 .5199 .5239 .5279 .5319 .5359 0.1 .5398 .5438 .5478 .5517 .5557 .5596 .5636 .5675 .5714 .5753 0.2 .5793 .5832 .5871 .5910 .5948 .5987 .6026 .6064 .6103 .6141 0.3 .6179 .6217 .6255 .6293 .6331 .6368 .6406 .6443 .6460 .6517 0.4 .6554 .6591 .6628 .6664 .6700 .6736 .6772 .6808 .6844 .6879 0.5 .6915 .6950 .6985 .7019 .7054 .7088 .7123 .7157 .7190 .7224 0.6 .7257 .7291 .7324 .7357 .7389 .7422 .7454 .7485 .7517 .7549 0.7 .7580 .7611 .7642 .7673 .7704 .7734 .7764 .7794 .7823 .7852 0.8 .7881 .7910 .7939 .7967 .7995 .8023 .8051 .8078 .8106 .8133 0.9 .8159 .8186 .8212 .8238 .8264 .8289 .8315 .8340 .8365 .8389 1.0 .8413 .8438 .8461 .8485 .8508 .8531 .8554 .8577 .8599 .8621 1.1 .8643 .8665 .8686 .8708 .8729 .8749 .8770 .8790 .8810 .8830 1.2 .8849 .8869 .8888 .8907 .8925 .8944 .8962 .8980 .8997 .9015 1.3 .9032 .9049 .9066 .9082 .9099 .9115 .9131 .9149 .9162 .9177 1.4 .9192 .9207 .9222 .9236 .9251 .9265 .9279 .9292 .9306 .9319 1.5 .9332 .9345 .9357 .9370 .9382 .9394 .9406 .9418 .9429 .9441 1.6 .9452 .9463 .9474 .9484 .9495 .9505 .9515 .9525 .9535 .9545 1.7 .9554 .9564 .9573 .9582 .9591 .9599 .9608 .9616 .9625 .9633 1.8 .9641 .9649 .9656 .9664 .9671 .9678 .9686 .9693 .9699 .9706 1.9 .9713 .9719 .9726 .9732 .9738 .9744 .9750 .9756 .9761 .9767 2.0 .9772 .9778 .9783 .9788 .9793 .9798 .9803 .9808 .9812 .9817 2.1 .9821 .9826 .9830 .9834 .9838 .9842 .9846 .9850 .9854 .9857 2.2 .9861 .9864 .9868 .9871 .9875 .9878 .9881 .9884 .9887 .9890 2.3 .9893 .9896 .9898 .9901 .9904 .9906 .9909 .9911 .9913 .9916 2.4 .9918 .9920 .9922 .9925 .9927 .9929 .9931 .9932 .9934 .9936 2.5 .9938 .9940 .9941 .9943 .9945 .9946 .9948 .9949 .9951 .9952 2.6 .9953 .9955 .9956 .9957 .9959 .9960 .9961 .9962 .9963 .9964 2.7 .9965 .9966 .9967 .9968 .9969 .9970 .9971 .9972 .9973 .9974 2.8 .9974 .9975 .9976 .9977 .9977 .9978 .9979 9979 .9980 .9981 2.9 .9981 .9982 .9982 .9983 .9984 .9984 .9985 .9985 .9986 .9986 3.0 .9987 .9987 .9987 .9988 .9988 .9989',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 137,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '.9970 .9971 .9972 .9973 .9974 2.8 .9974 .9975 .9976 .9977 .9977 .9978 .9979 9979 .9980 .9981 2.9 .9981 .9982 .9982 .9983 .9984 .9984 .9985 .9985 .9986 .9986 3.0 .9987 .9987 .9987 .9988 .9988 .9989 .9989 .9989 .9990 .9990 3.1 .9990 .9991 .9991 .9991 .9992 .9992 .9992 .9992 .9993 .9993 3.2 .9993 .9993 .9994 .9994 .9994 .9994 .9994 .9995 .9995 .9995 3.3 .9995 .9995 .9995 .9996 .9996 .9996 .9996 .9996 .9996 .9997 3.4 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9998 1. The entries in this table, x say, occupy the range [0.0, 3.49]; the x is sample value of the random variable X.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 137,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': 'For each value of x, the table provides the corresponding value of the Q-function:\\nQ x  1 Fx x  1 2 ---------- t2 - 2    dt exp x   = - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 137,\n",
       "   'chunk_idx': 3}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference The Central Limit Theorem\\nThe central limit theorem occupies an important place in probability theory: it provides the\\nmathematical justification for using the Gaussian distribution as a model for an observed\\nrandom variable that is known to be the result of a large number of random events.\\nFor a formal statement of the central limit theorem, let X1, X2, ..., Xn denote a sequence of\\nindependently and identically distributed (iid) random variables with common mean  and\\nvariance\\n. Define the related random variable\\n(3.70)\\nThe subtraction of the product term n from the sum ensures that the random\\nvariable Yn has zero mean; the division by the factor ensures that Yn has unit variance.\\nGiven the setting described in (3.70), the central limit theorem formally states:\\nAs the number of random variables n in (3.70) approaches infinity, the\\nnormalized random variable Yn converges to the standard Gaussian random\\nvariable with the distribution function\\nin the sense that\\n(3.71)\\nwhere Q(y) is the Q-function.\\nTo appreciate the practical importance of the central limit theorem, suppose that we have a\\nphysical phenomenon whose occurrence is attributed to a large number of random events.\\nThe theorem, embodying (3.67)-(3.71), permits us to calculate certain probabilities\\nsimply by referring to a Q-function table (e.g., Table 3.1). Moreover, to perform the\\ncalculation, all that we need to know are means and variances.\\nHowever, a word of caution is in order here. The central limit theorem gives only the\\nlimiting form of the probability distribution of the standardized random variable Yn as n\\napproaches infinity. When n is finite, it is sometimes found that the Gaussian limit\\nprovides a relatively poor approximation for the actual probability distribution of Yn, even\\nthough n may be large.\\nEXAMPLE\\nSum of Uniformly Distributed Random Variables\\nConsider the random variable',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 138,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'provides a relatively poor approximation for the actual probability distribution of Yn, even\\nthough n may be large.\\nEXAMPLE\\nSum of Uniformly Distributed Random Variables\\nConsider the random variable\\n2 Yn 1 n ----------- Xi n - i 1 = n          = Xi i 1 = n  n FY y  1 2 ---------- = x2 2----- -     exp x d  - y  \\x02 Yn y    n   lim Q y  = Yn Xi i 1 = n  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 138,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '11 Bayesian Inference where the Xi are independent and uniformly distributed random variables on the interval\\nfrom -1 to +1. Suppose that we generate 20000 samples of the random variable Yn for\\nn = 10, and then compute the probability density function of Yn by forming a histogram of\\nthe results. Figure 3.11a compares the computed histogram (scaled for unit area) with the\\nprobability density function of a Gaussian random variable with the same mean and\\nvariance. The figure clearly illustrates that in this particular example the number of\\nindependent distributions n does not have to be large for the sum Yn to closely\\napproximate a Gaussian distribution. Indeed, the results of this example confirm how\\npowerful the central limit theorem is. Moreover, the results explain why Gaussian models\\nare so ubiquitous in the analysis of random signals not only in the study of communication\\nsystems, but also in so many other disciplines. Bayesian Inference\\nThe material covered up to this point in the chapter has largely addressed issues involved\\nin the mathematical description of probabilistic models. In the remaining part of the\\nchapter we will study the role of probability theory in probabilistic reasoning based on the\\nBayesian5 paradigm, which occupies a central place in statistical communication theory.\\nTo proceed with the discussion, consider Figure 3.12, which depicts two finite-\\ndimensional spaces: a parameter space and an observation space, with the parameter\\nspace being hidden from the observer. A parameter vector , drawn from the parameter\\nspace, is mapped probabilistically onto the observation space, producing the observation\\nvector x. The vector x is the sample value of a random vector X, which provides the\\nFigure 3.11 Simulation supporting validity of the central limit theorem.\\nProbability density fX (x)\\nx -4 -2 0 2 4 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 Simulated density of 5 uniforms\\nGaussian density with same mean\\nand variance',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 139,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nobserver information about Given the probabilistic scenario depicted in Figure 3.12, we\\nmay identify two different operations that are the dual of each other.6\\nProbabilistic modeling. The aim of this operation is to formulate the conditional\\nprobability density function\\n, which provides an adequate description of\\nthe underlying physical behavior of the observation space.\\nStatistical analysis. The aim of this second operation is the inverse of probabilistic\\nmodeling, for which we need the conditional probability density function\\n.\\nIn a fundamental sense, statistical analysis is more profound than probabilistic modeling.\\nWe may justify this assertion by viewing the unknown parameter vector  as the cause for\\nthe physical behavior of the observation space and viewing the observation vector x as the\\neffect. In essence, statistical analysis solves an inverse problem by retrieving the causes\\n(i.e., the parameter vector ) from the effects (i.e., the observation vector x). Indeed, we\\nmay go on to say that whereas probabilistic modeling helps us to characterize the future\\nbehavior of x conditional on , statistical analysis permits us to make inference about \\ngiven x.\\nTo formulate the conditional probability density function of\\n, we recast\\nBayes theorem of (3.14) in its continuous version, as shown by\\n(3.72)\\nThe denominator is itself defined in terms of the numerator as\\n(3.73)\\nFigure 3.12 Probabilistic model for Bayesian inference.\\nParameter space Observation space Observer x fX x    fX x   fX x    fX x   fX x   f  fX x  --------------------------------------\\n= fX x  fX x   f  d  = fX   x      d  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 140,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '11 Bayesian Inference which is the marginal density of X, obtained by integrating out the dependence of the joint\\nprobability density function\\n. In words, fX(x) is a marginal density of the joint\\nprobability density function\\n. The inversion formula of (3.72) is sometimes\\nreferred to as the principle of inverse probability.\\nIn light of this principle, we may now introduce four notions:\\nObservation density. This stands for the conditional probability density function\\n, referring to the observation vector x given the parameter vector .\\nPrior. This stands for the probability density function\\n, referring to the\\nparameter vector  prior to receiving the observation vector x.\\nPosterior. This stands for the conditional probability density function\\n,\\nreferring to the parameter vector  after receiving the observation vector x.\\nEvidence. This stands for the probability density function\\n, referring to the\\ninformation contained in the observation vector X for statistical analysis.\\nThe posterior is central to Bayesian inference. In particular, we may view it as\\nthe updating of information available on the parameter vector  in light of the information\\ncontained in the observation vector x, while the prior is the information available on\\n prior to receiving the observation vector x.\\nLikelihood\\nThe inversion aspect of statistics manifests itself in the notion of the likelihood function.7\\nIn a formal sense, the likelihood, denoted by l(|x), is just the observation density reformulated in a different order, as shown by\\n(3.74)\\nThe important point to note here is that the likelihood and the observation density are both\\ngoverned by exactly the same function that involves the parameter vector  and the obser-\\nvation vector x. There is, however, a difference in interpretation: the likelihood function\\nl(|x) is treated as a function of the parameter vector  given x, whereas the observation\\ndensity is treated as a function of the observation vector x given .\\nNote, however, unlike',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 141,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'l(|x) is treated as a function of the parameter vector  given x, whereas the observation\\ndensity is treated as a function of the observation vector x given .\\nNote, however, unlike\\n, the likelihood l(|x) is not a distribution; rather, it is\\na function of the parameter vector , given x.\\nIn light of the terminologies introduced, namely the posterior, prior, likelihood, and\\nevidence, we may now express Bayes rule of (3.72) in words as follows:\\nThe Likelihood Principle\\nFor convenience of presentation, let\\n(3.75)\\nThen, recognizing that the evidence defined in (3.73) plays merely the role of a\\nnormalizing function that is independent of , we may now sum up (3.72) on the principle\\nof inverse probability succinctly as follows:\\nfX x    fX   x     fX x    f  fX x   fX x  fX x   f  fX x    l x   fX x    = fX x    fX x    posterior likelihood prior  evidence ------------------------------------------\\n=   f  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 141,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nThe Bayesian statistical model is essentially made up of two components: the\\nlikelihood function l(|x) and the prior (), where  is an unknown parameter\\nvector and x is the observation vector.\\nTo elaborate on the significance of the defining equation (3.74), consider the likelihood\\nfunctions l(|x1) and l(|x2) on parameter vector . If, for a prescribed prior (), these\\ntwo likelihood functions are scaled versions of each other, then the corresponding\\nposterior densities of  are essentially identical, the validity of which is a straightforward\\nconsequence of Bayes theorem. In light of this result we may now formulate the so-called\\nlikelihood principle8 as follows:\\nIf x1 and x2 are two observation vectors depending on an unknown parameter vector ,\\nsuch that l(| x1) = c l(|x2) for all  where c is a scaling factor, then these two observation vectors lead to an identical\\ninference on  for any prescribed prior f().\\nSufficient Statistic\\nConsider a model, parameterized by the vector  and given the observation vector x. In\\nstatistical terms, the model is described by the posterior density\\n. In this context,\\nwe may now introduce a function t(x), which is said to be a sufficient statistic if the\\nprobability density function of the parameter vector  given t(x) satisfies the condition\\n(3.76)\\nThis condition imposed on t(x), for it to be a sufficient statistic, appears intuitively\\nappealing, as evidenced by the following statement:\\nThe function t(x) provides a sufficient summary of the whole information about\\nthe unknown parameter vector , which is contained in the observation vector x.\\nWe may thus view the notion of sufficient statistic as a tool for data reduction, the use of\\nwhich results in considerable simplification in analysis.9 The data reduction power of the\\nsufficient statistic t(x) is well illustrated in Example 7. Parameter Estimation\\nAs pointed out previously, the posterior density is central to the formulation of',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 142,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'sufficient statistic t(x) is well illustrated in Example 7. Parameter Estimation\\nAs pointed out previously, the posterior density is central to the formulation of\\na Bayesian probabilistic model, where  is an unknown parameter vector and x is the\\nobservation vector. It is logical, therefore, that we use this conditional probability density\\nfunction for parameter estimation.10 Accordingly, we define the maximum a posteriori\\n(MAP) estimate of  as\\n(3.77)\\nwhere l(|x) is the likelihood function defined in (3.74), and () is the prior defined in\\n(3.75). To compute the estimate\\n, we require availability of the prior ().\\nfX x   fX x   fT x t x    = fX x    MAP max  fX x   arg = max  l x    arg =  MAP',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 142,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '12 Parameter Estimation In words, the right-hand side of (3.77) reads as follows:\\nGiven the observation vector x, the estimate is that particular value of the\\nparameter vector  in the argument of the posterior density\\n, for\\nwhich this density attains its maximum value.\\nGeneralizing the statement made at the end of the discussion on multiple random variables\\nin Section 3.5, we may now go on to say that, for the problem at hand, the conditional\\nprobability density function contains all the conceivable information about the\\nmultidimensional parameter vector  given the observation vector x. The recognition of\\nthis fact leads us to make the follow-up important statement, illustrated in Figure 3.13 for\\nthe simple case of a one-dimensional parameter vector:\\nThe maximum a posterior estimate of the unknown parameter vector  is\\nthe globally optimal solution to the parameter-estimation problem, in the sense\\nthat there is no other estimator that can do better.\\nIn referring to as the MAP estimate, we have made a slight change in our\\nterminology: we have, in effect, referred to as the a posteriori density rather\\nthan the posterior density of . We have made this minor change so as to conform to the\\nMAP terminology that is well and truly embedded in the literature on statistical\\ncommunication theory.\\nIn another approach to parameter estimation, known as maximum likelihood estimation,\\nthe parameter vector  is estimated using the formula\\n(3.78)\\nThat is, the maximum likelihood estimate is that value of the parameter vector  that\\nmaximizes the conditional distribution at the observation vector x. Note that\\nthis second estimate ignores the prior () and, therefore, lies at the fringe of the Bayesian\\nparadigm. Nevertheless, maximum likelihood estimation is widely used in the literature on\\nstatistical communication theory, largely because in ignoring the prior (), it is less\\ndemanding than maximum posterior estimation in computational complexity.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 143,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'statistical communication theory, largely because in ignoring the prior (), it is less\\ndemanding than maximum posterior estimation in computational complexity.\\nFigure 3.13 Illustrating the a posteriori for the case of a one-dimensional\\nparameter space.  MAP fX x   fX x    MAP  MAP fX x    ML sup  l x   arg =  ML fX x    Maximum value Maximum 0 f |X( |x)   MAP  fX x',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 143,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nThe MAP and ML estimates do share a common possibility, in that the maximizations\\nin (3.77) and (3.78) may lead to more than one global maximum. However, they do differ\\nin one important result: the maximization indicated in (3.78) may not always be possible;\\nthat is, the procedure used to perform the maximization may diverge. To overcome this\\ndifficulty, the solution to (3.78) has to be stabilized by incorporating prior information on\\nthe parameter space, exemplified by the distribution (), into the solution, which brings\\nus back to the Bayesian approach and, therefore, (3.77). The most critical part in the\\nBayesian approach to statistical modeling and parameter estimation is how to choose the\\nprior (). There is also the possibility of the Bayesian approach requiring high-\\ndimensional computations. We should not, therefore, underestimate the challenges\\ninvolved in applying the Bayesian approach, on which note we may say the following:\\nThere is no free lunch: for every gain made, there is a price to be paid.\\nEXAMPLE\\nParameter Estimation in Additive Noise\\nConsider a set N of scalar observations, defined by\\n(3.79)\\nwhere the unknown parameter  is drawn from the Gaussian distribution\\n; that is,\\n(3.80)\\nEach ni is drawn from another Gaussian distribution\\n; that is,\\nIt is assumed that the random variables Ni are all independent of each other, and also\\nindependent from\\n. The issue of interest is to find the MAP of the parameter .\\nTo find the distribution of the random variable Xi, we invoke Property 2 of the Gaussian\\ndistribution, described in Section 3.9, in light of which we may say that Xi is also Gaussian\\nwith mean  and variance\\n. Furthermore, since the Ni are independent, by assumption,\\nit follows that the Xi are also independent. Hence, using the vector x to denote the N\\nobservations, we express the observation density of x as\\n(3.81)\\nThe problem is to determine the MAP estimate of the unknown parameter .',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 144,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'observations, we express the observation density of x as\\n(3.81)\\nThe problem is to determine the MAP estimate of the unknown parameter .\\nxi  ni + i  1 2 N   = = 0  2    f  1 2 ----------------- 2 2 2 --------- -       exp = 0 n 2    fNi ni   1 2n ----------------- ni 2 2n 2 --------- -       i 1, 2, N  =  exp =  n 2 fX x    1 2n ----------------- xi  -  2 2n 2 -------------------- - exp i=1 N  = 1 2n   N -------------------------\\n1 2n 2 --------- xi  -  2 i=1 N  - exp =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 144,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '12 Parameter Estimation To solve this problem, we need to know the posterior density\\n. Applying (3.72), we write (3.82) where (3.83) The normalization factor c(x) is independent of the parameter  and, therefore, has no\\nrelevance to the MAP of . We therefore need only pay attention to the exponent in (3.82).\\nRearranging terms and completing the square in the exponent in (3.82), and introducing\\na new normalization factor that absorbs all the terms involving\\n, we get (3.84) where (3.85) Equation (3.84) shows that the posterior density of the unknown parameter  is Gaussian\\nwith mean  and variance\\n. We therefore readily find that the MAP estimate of  is\\n(3.86)\\nwhich is the desired result.\\nExamining (3.84), we also see that the N observations enter the posterior density of \\nonly through the sum of the xi. It follows, therefore, that\\n(3.87)\\nis a sufficient statistic for the example at hand. This statement merely confirms that (3.84)\\nand (3.87) satisfy the condition of (3.76) for a sufficient statistic.\\nfX x   fX x   c x  1 2--- 2  2 ------ xi  -  2 i=1 N  n 2 ----------------------------\\n+                 - exp = c x  1 2 ----------------- 1 2n   N -------------------------\\n fX x  ---------------------------------------------------\\n= cx  xi 2 fX x   cx  1 2p 2 --------- n 2  2 n 2 N    + -------------------------------\\nN---- xi i=1 N           -        2 -           exp = p 2  2n 2 N 2 n 2 + -----------------------\\n= p 2  MAP n 2  2 n 2 N    + -------------------------------\\nN---- xi i=1 N          = t x  xi i=1 N  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 145,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference Hypothesis Testing\\nThe Bayesian paradigm discussed in Section 3.11 focused on two basic issues: predictive\\nmodeling of the observation space and statistical analysis aimed at parameter estimation.\\nAs mentioned previously in that section, these two issues are the dual of each other. In this\\nsection we discuss another facet of the Bayesian paradigm, aimed at hypothesis testing,11\\nwhich is basic to signal detection in digital communications, and beyond.\\nBinary Hypotheses\\nTo set the stage for the study of hypothesis testing, consider the model of Figure 3.14. A\\nsource of binary data emits a sequence of 0s and 1s, which are respectively denoted by\\nhypotheses H0 and H1. The source (e.g., digital communication transmitter) is followed by\\na probabilistic transition mechanism (e.g., communication channel). According to some\\nprobabilistic law, the transition mechanism generates an observation vector x that defines\\na specific point in the observation space.\\nThe mechanism responsible for probabilistic transition is hidden from the observer\\n(e.g., digital communication receiver). Given the observation vector x and knowledge of\\nthe probabilistic law characterizing the transition mechanism, the observer chooses\\nwhether hypothesis H0 or H1 is true. Assuming that a decision must be made, the observer\\nhas to have a decision rule that works on the observation vector x, thereby dividing the\\nobservation space Z into two regions: Z0 corresponding to H0 being true and Z1\\ncorresponding to H1 being true. To simplify matters, the decision rule is not shown in\\nFigure 3.14.\\nIn the context of a digital communication system, for example, the channel plays the\\nrole of the probabilistic transition mechanism. The observation space of some finite\\nFigure 3.14 Diagram illustrating the binary hypothesis-testing problem. Note: according to the\\nlikelihood ration test, the bottom observation vector x is incorrectly assigned to Z1.\\nWhen the observation',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 146,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'likelihood ration test, the bottom observation vector x is incorrectly assigned to Z1.\\nWhen the observation\\nvector is assigned to\\ndecision region Z1: say H1\\nObservation space partitioned into\\ndecision regions, Z0 and z1\\nSource of binary data Probabilistic transition mechanism Hypotheses: Likelihood ratios: When the observation vector x is assigned to\\ndecision region Z0: say H0 fX|H1(x|H1) fX|H0(x|H0) Z1 H1 H0 Z0 x x',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 146,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '13 Hypothesis Testing dimension corresponds to the ensemble of channel outputs. Finally, the receiver performs\\nthe decision rule.\\nLikelihood Receiver\\nTo proceed with the solution to the binary hypothesis-testing problem, we introduce the\\nfollowing notations:\\n, which denotes the conditional density of the observation vector x\\ngiven that hypothesis H0 is true.\\n, denotes the conditional density of x given that the other hypothesis\\nH1 is true.\\n0 and 1 denote the priors of hypotheses H0 and H1, respectively.\\nIn the context of hypothesis testing, the two conditional probability density functions and are referred to as likelihood functions, or just simply\\nlikelihoods.\\nSuppose we perform a measurement on the transition mechanisms output, obtaining\\nthe observation vector x. In processing x, there are two kinds of errors that can be made by\\nthe decision rule:\\nError of the first kind. This arises when hypothesis H0 is true but the rule makes a\\ndecision in favor of H1, as illustrated in Figure 3.14.\\nError of the second kind. This arises when hypothesis H1 is true but the rule makes a\\ndecision in favor of H0.\\nThe conditional probability of an error of the first kind is\\nwhere Z1 is part of the observation space that corresponds to hypothesis H1. Similarly, the\\nconditional probability of an error of the second kind is\\nBy definition, an optimum decision rule is one for which a prescribed cost function is\\nminimized. A logical choice for the cost function in digital communications is the average\\nprobability of symbol error, which, in a Bayesian context, is referred to as the Bayes risk.\\nThus, with the probable occurrence of the two kinds of errors identified above, we define\\nthe Bayes risk for the binary hypothesis-testing problem as\\n(3.88)\\nwhere we have accounted for the prior probabilities for which hypotheses H0 and H1 are\\nknown to occur. Using the language of set theory, let the union of the disjoint subspaces Z0',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 147,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '(3.88)\\nwhere we have accounted for the prior probabilities for which hypotheses H0 and H1 are\\nknown to occur. Using the language of set theory, let the union of the disjoint subspaces Z0\\nand Z1 be (3.89) fX H0 x H0   fX H1 x H1   fX H0 x H0   fX H1 x H1   fX H0 x H0  x d Z1 fX H1 x H1  x d Z0  0 fX H0 x H0  dx 1 fX H1 x H1  x d Z0 + Z1 = Z Z0 = Z1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 147,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nThen, recognizing that the subspace Z1 is the complement of the subspace Z0 with respect\\nto the total observation space Z, we may rewrite (3.88) in the equivalent form:\\n(3.90)\\nThe integral represents the total volume under the conditional density\\n, which, by definition, equals unity. Accordingly, we may reduce (3.90) to\\n(3.91)\\nThe term 0 on its own on the right-hand side of (3.91) represents a fixed cost. The integral\\nterm represents the cost controlled by how we assign the observation vector x to Z0.\\nRecognizing that the two terms inside the square brackets are both positive, we must\\ntherefore insist on the following plan of action for the average risk\\nto be minimized:\\nMake the integrand in (3.91) negative for the observation vector x to be\\nassigned to Z0.\\nIn light of this statement, the optimum decision rule proceeds as follows:\\nIf\\nthen the observation vector x should be assigned to Z0, because these two terms\\ncontribute a negative amount to the integral in (3.91). In this case, we say H0 is true.\\nIf, on the other hand,\\nthen the observation vector x should be excluded from Z0 (i.e., assigned to Z1),\\nbecause these two terms would contribute a positive amount to the integral in (3.91).\\nIn this second case, H1 is true.\\nWhen the two terms are equal, the integral would clearly have no effect on the average risk\\n; in such a situation, the observation vector x may be assigned arbitrarily.\\nThus, combining points (1) and (2) on the action plan into a single decision rule, we\\nmay write\\n(3.92)\\nThe observation-dependent quantity on the left-hand side of (3.92) is called the likelihood\\nratio; it is defined by\\n(3.93)\\nFrom this definition, we see that is the ratio of two functions of a random variable;\\ntherefore, it follows that is itself a random variable. Moreover, it is a one-',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 148,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'ratio; it is defined by\\n(3.93)\\nFrom this definition, we see that is the ratio of two functions of a random variable;\\ntherefore, it follows that is itself a random variable. Moreover, it is a one-\\n 0 fX H0 x H0  dx 1 fX H1 x H1  x d Z0 + Z Z - 0  = 0 fX H0 x H0  dx 1fX H1 x H1   0fX H0 x H0   -  x d Z0 + Z = fX H0 x H0  xdZ fX H0 x H0    0 [1fX H1 x H1  dx Z0 0fX H0 x H0  ]dx - + =  0fX H0 x H0   1fX H1 x H1    0fX H0 x H0   1fX H1 x H1     fX H1 x H1   fX H0 x H0   ------------------------------\\nH1 >< H0 0 1 ----- x  fX H1 x H1   fX H0 x H0   ------------------------------\\n= x  x',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 148,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '13 Hypothesis Testing dimensional variable, which holds regardless of the dimensionality of the observation\\nvector x. Most importantly, the likelihood ratio is a sufficient statistic.\\nThe scalar quantity on the right-hand side of (3.92), namely,\\n(3.94)\\nis called the threshold of the test. Thus, minimization of the Bayes risk leads to the\\nlikelihood ratio test, described by the combined form of two decisions: (3.95)\\nCorrespondingly, the hypothesis testing structure built on (3.93)-(3.95) is called the\\nlikelihood receiver; it is shown in the form of a block diagram in Figure 3.15a. An elegant\\ncharacteristic of this receiver is that all the necessary data processing is confined to\\ncomputing the likelihood ratio\\n. This characteristic is of considerable practical\\nimportance: adjustments to our knowledge of the priors 0 and 1 are made simply\\nthrough the assignment of an appropriate value to the threshold\\n.\\nThe natural logarithm is known to be a monotone function of its argument. Moreover,\\nboth sides of the likelihood ratio test in (3.95) are positive. Accordingly, we may express\\nthe test in its logarithmic form, as shown by\\n(3.96)\\nwhere ln is the symbol for the natural logarithm. Equation (3.96) leads to the equivalent\\nlog-likelihood ratio receiver, depicted in Figure 3.15b.\\n 0 1 ----- =  x  H1 >< H0  x    ln x  H1 >< H0  ln Figure 3.15 Two versions of the\\nlikelihood receiver: (a) based on the\\nlikelihood ratio\\n; (b) based on the\\nlog-likelihood ratio ln\\n. x  x  Likelihood ratio computer Comparator Otherwise, say H0 If the threshold is exceeded, say H1 Observation vector x (x) (a) Log-likelihood ratio computer Comparator Otherwise, say H0 If ln is exceeded,\\nsay H1 Threshold In Observation vector x In (x) (b)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 149,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nEXAMPLE\\nBinary Hypothesis Testing\\nConsider a binary hypothesis testing problem, described by the pair of equations:\\n(3.97)\\nThe term m is a constant that is nonzero only under hypothesis H1. As in Example 7, the ni\\nare independent and Gaussian\\n. The requirement is to formulate a likelihood ratio\\ntest for this example to come up with a decision rule.\\nFollowing the discussion presented in Example 7, under hypothesis H1 we write\\n(3.98)\\nAs in Example 7, let the vector x denote the set of N observations xi for i = 1, 2, ..., N.\\nThen, invoking the independence of the ni, we may express the joint density of the xi under\\nhypothesis H1 as\\n(3.99)\\nSetting m to zero in (3.99), we get the corresponding joint density of the xi under\\nhypothesis H0 as\\n(3.100)\\nHence, substituting (3.99) and (3.100) into the likelihood ratio of (3.93), we get (after\\ncanceling common terms)\\n(3.101)\\nEquivalently, we may express the likelihood ratio in its logarithmic form\\n(3.102)\\nUsing (3.102) in the log-likelihood ratio test of (3.96), we get\\nHypothesis H1: xi m ni, + = i 1 2 N   = Hypothesis H0: xi ni, i 1 2 N   = = 0 n 2    fxi H1 xi H1   1 2n ----------------- xi m -  2 2n 2 ----------------------\\n- exp = fX H1 x H1   1 2n ----------------- xi m -  2 2n 2 ----------------------\\n- exp i=1 N  = 1 2n   N -------------------------\\n1 2n 2 --------- xi m -  2 i=1 N  - exp = fX H0 x H0   1 2n   N -------------------------\\n1 2n 2 --------- xi 2 i=1 N  -         exp = x  m n 2 ------ xi Nm2 2n 2 ----------- - i=1 N          exp = x  ln m n 2 ------ xi Nm2 2n 2 ----------- - i 1 = N  = m n 2 ------ xi Nm2 2n 2 ----------- - i 1 = N          H1 >< H0  ln',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 150,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '13 Hypothesis Testing Dividing both sides of this test by and rearranging terms, we finally write\\n(3.103)\\nwhere the threshold  is itself defined by the ratio of priors, namely 0/1. Equation\\n(3.103) is the desired formula for the decision rule to solve the binary hypothesis-testing\\nproblem of (3.97).\\nOne last comment is in order. As with Example 7, the sum of the xi over the N\\nobservations; that is,\\nis a sufficient statistic for the problem at hand. We say so because the only way in which\\nthe observations can enter the likelihood ratio is in the sum; see (3.101).\\nMultiple Hypotheses\\nNow that we understand binary hypothesis testing, we are ready to consider the more\\ngeneral scenario where we have M possible source outputs to deal with. As before, we\\nassume that a decision must be made as to which one of the M possible source outputs was\\nactually emitted, given an observation vector x.\\nTo develop insight into how to construct a decision rule for testing multiple hypotheses,\\nwe consider first the case of M = 3 and then generalize the result. Moreover, in formulating\\nthe decision rule, we will use probabilistic reasoning that builds on the findings of the\\nbinary hypothesis-testing procedure. In this context, however, we find it more convenient\\nto work with likelihood functions rather than likelihood ratios.\\nTo proceed then, suppose we make a measurement on the probabilistic transition\\nmechanisms output, obtaining the observation vector x. We use this observation vector\\nand knowledge of the probability law characterizing the transition mechanism to construct\\nthree likelihood functions, one for each of the three possible hypotheses. For the sake of\\nillustrating what we have in mind, suppose further that in formulating the three possible\\nprobabilistic inequalities, each with its own inference, we get the following three results:\\nfrom which we infer that hypothesis H0 or H2 is true.\\nfrom which we infer that hypothesis H0 or H1 is true.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 151,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'from which we infer that hypothesis H0 or H2 is true.\\nfrom which we infer that hypothesis H0 or H1 is true.\\nfrom which we infer that hypothesis H1 or H0 is true.\\nExamining these three possible results for M = 3, we immediately see that hypothesis H0\\nis the only one that shows up in all three inferences. Accordingly, for the particular\\nscenario we have picked, the decision rule should say that hypothesis H0 is true. Moreover,\\nit is a straightforward matter for us to make similar statements pertaining to hypothesis H1\\nm n 2    xi i 1 = N  H1 >< H0 n 2 m ------  Nm2 2 ----------- + ln       t x  xi i 1 = N  = x  1 fX H1 x H1   0 fX H0 x H0    2 fX H2 x H2   0 fX H0 x H0    2 fX H2 x H2   1 fX H1 x H1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 151,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nor H2. The rationale just described for arriving at this test is an example of what we mean\\nby probabilistic reasoning: the use of multiple inferences to reach a specific decision.\\nFor an equivalent test, let both sides of each inequality under points 1, 2, and 3 be\\ndivided by the evidence fX(x). Let Hi, i = 1, 2, 3, denote the three hypotheses. We may then\\nuse the definition of joint probability density function to write\\n(3.104)\\nHence, recognizing that the conditional probability is actually the posterior\\nprobability of hypothesis Hi after receiving the observation vector x, we may now go on to\\ngeneralize the equivalent test for M possible source outputs as follows:\\nGiven an observation vector x in a multiple hypothesis test, the average\\nprobability of error is minimized by choosing the hypothesis Hi for which the\\nposterior probability has the largest value for i = 0, 1, ..., M - 1.\\nA processor based on this decision rule is frequently referred to as the MAP probability\\ncomputer. It is with this general hypothesis testing rule that earlier we made the\\nsupposition embodied under points 1, 2, and 3. Composite Hypothesis Testing\\nThroughout the discussion presented in Section 3.13, the hypotheses considered therein\\nwere all simple, in that the probability density function for each hypothesis was\\ncompletely specified. However, in practice, it is common to find that one or more of the\\nprobability density functions are not simple due to imperfections in the probabilistic\\ntransition mechanism. In situations of this kind, the hypotheses are said to be composite.\\nAs an illustrative example, let us revisit the binary hypothesis-testing problem\\nconsidered in Example 8. This time, however, we treat the mean m of the observable xi\\nunder hypothesis H1 not as a constant, but as a variable inside some interval [ma, mb]. If,\\nthen, we were to use the likelihood ratio test of (3.93) for simple binary hypothesis testing,',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 152,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'under hypothesis H1 not as a constant, but as a variable inside some interval [ma, mb]. If,\\nthen, we were to use the likelihood ratio test of (3.93) for simple binary hypothesis testing,\\nwe would find that the likelihood ratio involves the unknown mean m. We cannot\\ntherefore compute\\n, thereby negating applicability of the simple likelihood ratio test.\\nThe message to take from this illustrative example is that we have to modify the\\nlikelihood ratio test to make it applicable to composite hypotheses. To this end, consider the\\nmodel depicted in Figure 3.16, which is similar to that of Figure 3.14 for the simple case\\nexcept for one difference: the transition mechanism is now characterized by the conditional\\nprobability density function\\n, where  is a realization of the unknown\\nparameter vector , and the index i = 0, 1. It is the conditional dependence on  that makes\\ni fX Hi x Hi   fX x  ----------------------------------\\n\\x02 Hi  fX Hi x Hi   fX x  ---------------------------------------------\\nwhere \\x02 Hi   pi = = \\x02 Hi x    fX x  ----------------------\\n= \\x02 Hi x  fX x  fX x  -----------------------------------\\n= \\x02 Hi x   for i 0 1 M 1 -   = = \\x02 Hi x   \\x02 Hi x   xi   xi   fX Hi  x ,Hi',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 152,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '15 Summary and Discussion the hypotheses H0 and H1 to be of the composite kind. Unlike the simple model of Figure\\n14, we now have two spaces to deal with: an observation space and a parameter space. It\\nis assumed that the conditional probability density function of the unknown parameter\\nvector , that is,\\n, is known for i = 0, 1.\\nTo formulate the likelihood ratio for the composite hypotheses described in the model\\nof Figure 3.16, we require the likelihood function for i = 1, 2. We may satisfy\\nthis requirement by reducing the composite hypothesis-testing problem to a simple one by\\nintegrating over , as shown by\\n(3.105)\\nthe evaluation of which is contingent on knowing the conditional probability density\\nfunction of given the Hi for i = 1, 2. With this specification at hand, we may now\\nformulate the likelihood ratio for composite hypotheses as\\n(3.106)\\nAccordingly, we may now extend applicability of the likelihood ratio test described in\\n(3.95) to composite hypotheses.\\nFrom this discussion, it is clearly apparent that hypothesis testing for composite\\nhypotheses is computationally more demanding than it is for simple hypotheses. Chapter\\npresents applications of composite hypothesis testing to noncoherent detection, in the\\ncourse of which the phase information in the received signal is accounted for. Summary and Discussion\\nThe material presented in this chapter on probability theory is another mathematical pillar\\nin the study of communication systems. Herein, the emphasis has been on how to deal\\nwith uncertainty, which is a natural feature of every communication system in one form or\\nFigure 3.16 Model of composite hypothesis-testing for a binary scenario.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 153,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'with uncertainty, which is a natural feature of every communication system in one form or\\nFigure 3.16 Model of composite hypothesis-testing for a binary scenario.\\nSource Probabilistic transition mechanism Hypothesis H1 Hypothesis H0 Z1 Z0 x x  fX| , H1 (x| , H1)   fX| , H0 (x| , H0)  fHi Hi    fX Hi x Hi   fX Hi x Hi   fX Hi  x , Hi  fHi Hi  d  = x  fX H  1 x , H1  fH1 H1  d  fX H  0 x , H0  fH0 H0  d  ----------------------------------------------------------------------------------------\\n=',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 153,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nanother. Typically, uncertainties affect the behavior of channels connecting the transmitter\\nof a communication system to its receiver. Sources of uncertainty include noise, generated\\ninternally and externally, and interference from other transmitters.\\nIn this chapter, the emphasis has been on probabilistic modeling, in the context of\\nwhich we did the following:\\nStarting with set theory, we went on to state the three axioms of probability theory.\\nThis introductory material set the stage for the calculation of probabilities and\\nconditional probabilities of events of interest. When partial information is available\\non the outcome of an experiment, conditional probabilities permit us to reason in a\\nprobabilistic sense and thereby enrich our understanding of a random experiment.\\nWe discussed the notion of random variables, which provide the natural tools for\\nformulating probabilistic models of random experiments. In particular, we\\ncharacterized continuous random variables in terms of the cumulative distribution\\nfunction and probability density function; the latter contains all the conceivable\\ninformation about a random variable. Through focusing on the mean of a random\\nvariable, we studied the expectation or averaging operator, which occupies a\\ndominant role in probability theory. The mean and the variance, considered in that\\norder, provide a weak characterization of a random variable. We also introduced the\\ncharacteristic function as another way of describing the statistics of a random\\nvariable. Although much of the material in the early part of the chapter focused on\\ncontinuous random variables, we did emphasize important aspects of discrete\\nrandom variables by describing the concept of the probability mass function (unique\\nto discrete random variables) and the parallel development and similar concepts that\\nembody these two kinds of random variables.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 154,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'to discrete random variables) and the parallel development and similar concepts that\\nembody these two kinds of random variables.\\nTable 3.2 on summarizes the probabilistic descriptions of some important\\nrandom variances under two headings: discrete and random. Except for the Rayleigh\\nrandom variable, these random variables were discussed in the text or are given as\\nend-of-chapter problems; the Rayleigh random variable is discussed in Chapter 4.\\nAppendix A presents advanced probabilistic models that go beyond the contents of\\nTable 3.2.\\nWe discussed the characterization of a pair of random variables and introduced the\\nbasic concepts of covariance and correlation, and the independence of random\\nvariables.\\nWe provided a detailed description of the Gaussian distribution and discussed its\\nimportant properties. Gaussian random variables play a key role in the study of\\ncommunication systems.\\nThe second part of the chapter focused on the Bayesian paradigm, wherein inference may\\ntake one of two forms:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 154,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Probabilistic modeling, the aim of which is to develop a model for describing the\\nphysical behavior of an observation space.\\n\\nStatistical analysis, the aim of which is the inverse of probabilistic modeling.\\nIn a fundamental sense, statistical analysis is more profound than probabilistic modeling,\\nhence the focused attention on it in the chapter.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 154,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': '15 Summary and Discussion Table 3.2 Some important random variables\\nDiscrete random variables\\nBernoulli\\nPoisson\\nContinuous random variables\\nUniform 2. Exponential 3. Gaussian 4. Rayleigh 5. Laplacian pX x  1 p - if x 0 = p if x 1 = 0 otherwise      = \\x03 X  p = var X  p 1 p -   = pX k  k k! -----  -   k 0 1 2 and  0    = , exp = \\x03 X   = var X   = fX x  1 b a - ------------ axb   , = \\x03 X  1 2--- a b +   = var X  1 12 ------ b a -  2 = fX x   x -   x 0 and  0    exp = \\x03 X  1   = var X2   1 2  = fX x  1 2 -------------- x  -  2 22  -    x    -  exp = \\x03 X   = var X  2 = fX x  x 2 ------ x2 22  -   x 0 and  0    exp = \\x03 X  2  = var X  2  2--- -    2 = fX x   2--- x -    x    -  and  0  exp = \\x03 X  0 = var X  2 2  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 155,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nUnder statistical analysis, viewed from a digital communications perspective, we\\ndiscussed the following:\\nParameter estimation, where the requirement is to estimate an unknown parameter\\ngiven an observation vector; herein we covered:\\n\\nthe maximum a posteriori (MAP) rule that requires prior information, and',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 156,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the maximum likelihood procedure that by-passes the need for the prior and\\ntherefore sits on the fringe of the Bayesian paradigm.\\nHypothesis testing, where in a simple but important scenario, we have two\\nhypotheses to deal with, namely H1 and H0. In this case, the requirement is to make\\nan optimal decision in favor of hypothesis H1 or hypothesis H0 given an observation\\nvector. The likelihood ratio test plays the key role here.\\nTo summarize, the material on probability theory sets the stage for the study of stochastic\\nprocesses in Chapter 4. On the other hand, the material on Bayesian inference plays a key\\nrole in Chapters 7, 8, and 9 in one form or another.\\nProblems Set Theory 3.1 Using Venn diagrams, justify the five properties of the algebra of sets, which were stated (without\\nproofs) in Section 3.1:\\na. idempotence property\\nb. commutative property\\nc. associative property\\nd. distributive property\\ne. De Morgans laws. Let A and B denote two different sets. Validate the following three equalities:\\na. b. c. Probability Theory 3.3 Using the Bernoulli distribution of Table 3.2, develop an experiment that involves three independent\\ntosses of a fair coin. Irrespective of whether the toss is a head or tail, the probability of every toss is\\nto be conditioned on the results of preceding tosses. Display graphically the sequential evolution of\\nthe results. Use Bayes rule to convert the conditioning of event B given event Ai into the conditioning of event\\nAi given event B for the i = 1, 2, , N. A discrete memoryless channel is used to transmit binary data. The channel is discrete in that it is\\ndesigned to handle discrete messages and it is memoryless in that at any instant of time the channel\\noutput depends on the channel input only at that time. Owing to the unavoidable presence of noise in\\nthe channel, errors are made in the received binary data stream. The channel is symmetric in that the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 156,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'the channel, errors are made in the received binary data stream. The channel is symmetric in that the\\nprobability of receiving symbol 1 when symbol 0 is sent is the same as the probability of receiving\\nsymbol 0 when symbol 1 is sent.\\nAc Ac B    Ac Bc     = Bc A Bc    Ac Bc     = A B   c Ac B    Ac Bc    A Bc      =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 156,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': 'Problems The transmitter sends 0s across the channel with probability p0 and 1s with probability p1. The\\nreceiver occasionally makes random decision errors with probability p; that is, when symbol 0 is\\nsent across the channel, the receiver makes a decision in favor of symbol 1, and vice versa.\\nReferring to Figure P3.5, determine the following a posteriori probabilities:\\na. The conditional probability of sending symbol A0 given that symbol B0 was received.\\nb. The conditional probability of sending symbol A1 given that symbol B1 was received.\\nHint: Formulate expressions for the probability of receiving event B0, and likewise for event B1. Let B1, B2, , Bn denote a set of joint events whose union equals the sample space S, and assume\\nthat \\x02[Bi] > 0 for all i. Let A be any event in the sample space S.\\na. Show that\\nb. The total probability theorem states:\\nThis theorem is useful for finding the probability of event B when the conditional probabilities\\n\\x02[A|Bi] are known or easy to find for all i. Justify the theorem. Figure P3.7 shows the connectivity diagram of a computer network that connects node A to node B\\nalong different possible paths. The labeled branches of the diagram display the probabilities for\\nwhich the links in the network are up; for example, 0.8 is the probability that the link from node A to\\nintermediate node C is up, and so on for the other links. Link failures in the network are assumed to\\nbe independent of each other.\\na. When all the links in the network are up, find the probability that there is a path connecting node\\nA to node B.\\nb. What is the probability of complete failure in the network, with no connection from node A to\\nnode B? Figure P3.5 A0 1 - p 1 - ppp A1 B0 B1 A A B1    A B2     A Bn      = \\x02 A  \\x02 A B1  \\x02 B1   \\x02 A B2  \\x02 B2    \\x02 A Bn  \\x02 Bn   + + + = Figure P3.7 A 0.8 0.7 0.7 0.7 0.6 0.6 0.9 0.9 CEDBFG',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 157,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nDistribution Functions The probability density function of a continuous random variable X is defined by\\nDespite the fact that this function becomes infinitely large as x approaches zero, it may qualify to be a\\nlegitimate probability density function. Find the value of scalar c for which this condition is satisfied. The joint probability density function of two random variables X and Y is defined by the two-\\ndimensional uniform distribution\\nFind the scalar c for which fX,Y(x,y) satisfies the normalization property of a two-dimensional\\nprobability density function. In Table 3.2, the probability density function of a Rayleigh random variable is defined by\\na. Show that the mean of X is\\nb. Using the result of part a, show that the variance of X is\\nc. Use the results of a and b to determine the Rayleigh cumulative distribution function. The probability density function of an exponentially distributed random variable X is defined by\\nwhere  is a positive parameter.\\na. Show that fX(x) is a legitimate probability density function.\\nb. Determine the cumulative distribution function of X. Consider the one-sided conditional exponential distribution\\nwhere  > 0 and Z() is the normalizing constant required to make the area under fX(x|) equal\\nunity.\\na. Determine the normalizing constant Z().\\nb. Given N independent values of x, namely x1, x2, , xN, use Bayes rule to formulate the\\nconditional probability density function of the parameter , given this data set.\\nfX x  c x ------ for 0 x 1   0 otherwise      = fX Y  x y    c for axb and ayb     0 otherwise    = fX x  x 2 ------ x2 22 --------- -       for x 0 and  0   exp = X   2--- = var X  2  2--- -    2 = fX x   exp x -   for 0 x     0 otherwise     = fX x     Z   ----------- x -  , exp 1 x 20   0, otherwise      =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 158,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Problems 139 Expectation Operator 3.13 In Section 3.6 we described two properties of the expectation operator \\x03, one on linearity and the\\nother on statistical independence. In this problem, we address two other important properties of the\\nexpectation operator.\\na. Scaling property: Show that\\n\\x03(ax) = a\\x03[X]\\nwhere a is a constant scaling factor.\\nb. Linearity of conditional expectation: Show that\\n\\x03[X1 + X2|Y] = \\x03[X1|Y] + \\x03[X2|Y] Validate the expected value rule of (3.41) by building on two expressions:\\na. b. For any provided that 3.15 Let X be a discrete random variable with probability mass function pX(x) and let g(X) be a function\\nof the random variable X. Prove the following rule:\\nwhere the summation is over all possible discrete values of X. Continuing with the Bernoulli random variable X in (3.23), find the mean and variance of X. The mass probability function of the Poisson random variable X is defined by\\nFind the mean and variance of X. Find the mean and variance of the exponentially distributed random variable X in Problem 3.11. The probability density function of the Laplacian random variable X in Table 3.2 is defined by\\nfor the parameter  > 0. Find the mean and variance of X. In Example 5 we used the characteristic function to calculate the mean of an exponentially\\ndistributed random variable X. Continuing with that example, calculate the variance of X and check\\nyour result against that found in Problem 3.18. The characteristic function of a continuous random variable X, denoted by\\n, has some\\nimportant properties of its own:\\na. The transformed version of the random variable X, namely, aX + b, has the following\\ncharacteristic function\\nwhere a and b are constants.\\nb. The characteristic function is real if, and only if, the distribution function FX(x), pertaining\\nto the random variable X, is symmetric.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 159,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'characteristic function\\nwhere a and b are constants.\\nb. The characteristic function is real if, and only if, the distribution function FX(x), pertaining\\nto the random variable X, is symmetric.\\ng x  max g x 0    max g - x 0    - = a 0 g x  a    max g x 0    a  \\x03 g X    g x pX x  x = pX k  1 k!----k  -   k 0 1 2 and  0    =  exp = fX x  1 2--- x -   exp for x 0  1 2---  x   exp for x 0         = j     \\x03 jaX b +     exp   jb  X a    exp =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 159,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nProve the validity of these two properties, and demonstrate that property b is satisfied by the two-\\nsided exponential distribution described in Problem 3.19. Let X and Y be two continuous random variables. One version of the total expectation theorem states\\nJustify this theorem.\\nInequalities and Theorems Let X be a continuous random variable that can only assume nonnegative values. The Markov\\ninequality states\\nJustify this inequality. In (3.46) we stated the Chebyshev inequality without proof. Justify this inequality.\\nHint: consider the probability and then apply the Markov inequality, considered\\nin Problem 3.23, with a = 2. Consider a sequence X1, X2, ..., Xn of independent and identically distributed random variables with\\nmean  and variance\\n. The sample mean of this sequence is defined by\\nThe weak law of large numbers states\\nJustify this law. Hint: use the Chebyshev inequality. Let event A denote one of the possible outcomes of a random experiment. Suppose that in n\\nindependent trials of the experiment the event A occurs nA times. The ratio\\nis called the relative frequency or empirical frequency of the event A. Let p = \\x02[A] denote the\\nprobability of the event A. The experiment is said to exhibit statistical regularity if the relative\\nfrequency Mn is most likely to be within  of p for large n. Use the weak law of large numbers,\\nconsidered in Problem 3.25, to justify this statement.\\nThe Gaussian Distribution In the literature on signaling over additive white Gaussian noise (AWGN) channels, formulas are\\nderived for probabilistic error calculations using the complementary error function\\nShow that the erfc(x) is related to the Q-function as follows\\na. b. \\x03 X  \\x03[X Y  -   y] = = fY y dy \\x02 X a    1 a---\\x03 X  a 0    \\x02 X  -  2 2    2 Mn 1 n--- Xi i=1 n  = \\x02 Mn  -     n   lim 0 for  0  = Mn nA n------ = erfc x  1 1  ------- - t2 -   exp dt 0 x  = Q x  1 2--- erfc x 2 -------     = erfc x  2Q 2 x   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 160,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Problems 141 3.28 Equation (3.58) defines the probability density function of a Gaussian random variable X. Show that\\nthe area under this function is unity, in accordance with the normalization property described in\\n(3.59). Continuing with Problem3.28, justify the four properties of the Gaussian distribution stated in\\nSection 3.8 without proofs. a. Show that the characteristic function of a Gaussian random variable X of mean X and variance is\\nb. Using the result of part a, show that the nth central moment of this Gaussian random variable is\\nas follows: A Gaussian-distributed random variable X of zero mean and variance is transformed by a\\npiecewise-linear rectifier characterized by the input-output relation (see Figure P3.31): The probability density function of the new random variable Y is described by\\na. Explain the physical reasons for the functional form of this result.\\nb. Determine the value of the constant k by which the delta function (y) is weighted. In Section 3.9 we stated the central limit theorem embodied in (3.71) without proof. Justify this\\ntheorem. Bayesian Inference 3.33 Justify the likelihood principle stated (without proof) in Section 3.11. In this problem we address a procedure for estimating the mean of the random variable; the\\nprocedure was discussed in Section 3.6.\\nX 2 X   jX 1 2---2X 2 -     exp = \\x03 X X -  n   1 3 5 n 1 -  X n   for n even 0 for n odd    = X 2 Y X, X 0  0, X 0     = Figure P3.31 fY y  0, y 0  ky  y 0 = 1 2X ----------------- y2 2X 2 ---------- -       exp y 0           = Y Y = X X',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 161,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nConsider a Gaussian-distributed variable X with unknown mean X and unit variance. The mean X\\nis itself a random variable, uniformly distributed over the interval [a, b]. To do the estimation, we are\\ngiven N independent observations of the random variable X. Justify the estimator of (3.36). In this problem, we address the issue of estimating the standard deviation  of a Gaussian-\\ndistributed random variable X of zero mean. The standard deviation itself is uniformly distributed\\ninside the interval [1, 2]. For the estimation, we have N independent observations of the random\\nvariable X, namely, x1, x2, ..., xN.\\na. Derive a formula for the estimator using the MAP rule.\\nb. Repeat the estimation using the maximum likelihood criterion.\\nc. Comment on the results of parts a and b. A binary symbol X is transmitted over a noisy channel. Specifically, symbol X = 1 is transmitted with\\nprobability p and symbol X = 0 is transmitted with probability (1 - p). The received signals at the\\nchannel output are defined by\\nY = X + N\\nThe random variable N represents channel noise, modeled as a Gaussian-distributed random variable\\nwith zero mean and unit variance. The random variables X and N are independent.\\na. Describe how the conditional probability \\x02[X = 0|Y = y] varies with increasing y, all the way\\nfrom to . b. Repeat the problem for the conditional probability \\x02[X = 1|Y = y]. Consider an experiment involving the Poisson distribution, whose parameter  is unknown. Given\\nthat the distribution of  follows the exponential law\\nwhere a > 0, show that the MAP estimate of the parameter  is given by\\nwhere k is the number of events used in the observation. In this problem we investigate the use of analytic arguments to justify the optimality of the MAP\\nestimate for the simple case of a one-dimensional parameter vector.\\nDefine the estimation error\\nwhere\\nis the value of an unknown parameter, is the estimator to be optimized, and x is the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 162,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'estimate for the simple case of a one-dimensional parameter vector.\\nDefine the estimation error\\nwhere\\nis the value of an unknown parameter, is the estimator to be optimized, and x is the\\nobservation vector. Figure P3.38 shows a uniform cost function, C(e), for this problem, with zero\\ncost being incurred only when the absolute value of the estimation error is less than or equal\\nto /2.\\na. Formulate the Bayes risk for this parameter estimation problem, accounting for the joint\\nprobability density function fA,X(\\n,x).\\nb. Hence, determine the MAP estimate by minimizing the risk with respect to\\n. For\\nthis minimization, assume that  is an arbitrarily small number but nonzero.\\n  - + fn   a a -  , exp  0  0, otherwise    =  MAP k  k 1 a + ------------ = ex    x  - =   x  ex     MAP   x',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 162,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems 143 3.39 In this problem we generalize the likelihood ratio test for simple binary hypotheses by including\\ncosts incurred in the decision-making process. Let Cij denote the cost incurred in deciding in favor of\\nhypothesis Hi when hypothesis Hj is true. Hence, show that the likelihood ratio test of (3.95) still\\nholds, except for the fact that the threshold of the test is now defined by Consider a binary hypothesis-testing procedure where the two hypotheses H0 and H1 are described\\nby different Poisson distributions, characterized by the parameters 0 and 1, respectively. The\\nobservation is simply a number of events k, depending on whether H0 or H1 is true. Specifically, for\\nthese two hypotheses, the probability mass functions are defined by\\nwhere i = 0 for hypothesis H0 and i = 1 for hypothesis H1. Determine the log-likelihood ratio test for\\nthis problem. Consider the binary hypothesis-testing problem\\nH1 : X = M + N\\nH0 : X = N\\nThe M and N are independent exponentially distributed random variables, as shown by\\nDetermine the likelihood ratio test for this problem. In this problem we revisit Example 8. But this time we assume that the mean m under hypothesis H1\\nis Gaussian distributed, as shown by\\na. Derive the likelihood ratio test for the composite hypothesis scenario just described.\\nb. Compare your result with that derived in Example 8.\\nFigure P3.38 C(e)   0 1.0   0 C10 C00 -   1 C01 C11 -   ----------------------------------\\n= pXi k  i  k k! ----------- i -   k  exp 0, 1, 2,  = = pM m   m m -  , m 0  exp 0 otherwise     = pN n  n n -  , n 0  exp 0 otherwise     = fM H1 m H1   1 2m ------------------ m2 2m 2 ---------- -       exp =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 163,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nProbability Theory and Bayesian Inference\\nNotes\\nFor a readable account of probability theory, see Bertsekas and Tsitsiklis (2008). For an advanced\\ntreatment of probability theory aimed at electrical engineering, see the book by Fine (2006). For an\\nadvanced treatment of probability theory, see the two-volume book by Feller (1968, 1971).\\nFor an interesting account of inference, see the book by MacKay (2003).\\nFor a detailed treatment of the characterization of discrete random variables, see Chapter 2 of the\\nbook by Bertsekas and Tsitsiklis (2008).\\nIndeed, we may readily transform the probability density function of (3.58) into the standard\\nform by using the linear transformation\\nIn so doing, (3.58) is simplified as follows:\\nwhich has exactly the same mathematical form as (3.65), except for the use of y in place of x.\\nCalculations based on Bayes rule, presented previously as (3.14), are referred to as Bayesian.\\nIn actual fact, Bayes provided a continuous version of the rule; see (3.72). In a historical context, it is\\nalso of interest to note that the full generality of (3.72) was not actually perceived by Bayes; rather,\\nthe task of generalization was left to Laplace.\\nIt is because of this duality that the Bayesian paradigm is referred to as a principle of duality; see\\nRobert (2001). Roberts book presents a detailed and readable treatment of the Bayesian paradigm.\\nFor a more advanced treatment of the subject, see Bernardo and Smith (1998).\\nIn a paper published in 1912, R.A. Fisher moved away from the Bayesian approach. Then, in a\\nclassic paper published in 1922, he introduced the likelihood.\\nIn Appendix B of their book, Bernardo and Smith (1998) show that many non-Bayesian inference\\nprocedures do not lead to identical inferences when applied to such proportional likelihoods.\\nFor detailed discussion of the sufficient statistic, see Bernardo and Smith (1998).\\nA more detailed treatment of parameter-estimation theory is presented in the classic book by',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 164,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'For detailed discussion of the sufficient statistic, see Bernardo and Smith (1998).\\nA more detailed treatment of parameter-estimation theory is presented in the classic book by\\nVan Trees (1968); the notation used by Van Trees is somewhat different from that used in this\\nchapter. See also the book by McDonough and Whalen (1995).\\nFor a more detailed treatment and readable account of hypothesis testing, see the classic book\\nby Van Trees (1968). See also the book by McDonough and Whalen (1995).\\nY 1 --- X  -   = fY y  1 2 ---------- y2 2  -   exp =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 164,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '145 CHAPTER 4 Stochastic Processes 4.1 Introduction Stated in simple terms, we may say:\\nA stochastic process is a set of random variables indexed in time.\\nElaborating on this succinct statement, we find that in many of the real-life phenomena\\nencountered in practice, time features prominently in their description. Moreover, their\\nactual behavior has a random appearance. Referring back to the example of wireless\\ncommunications briefly described in Section 3.1, we find that the received signal at the\\nwireless channel output varies randomly with time. Processes of this kind are said to be\\nrandom or stochastic;1 hereafter, we will use the term stochastic. Although probability\\ntheory does not involve time, the study of stochastic processes naturally builds on\\nprobability theory.\\nThe way to think about the relationship between probability theory and stochastic\\nprocesses is as follows. When we consider the statistical characterization of a stochastic\\nprocess at a particular instant of time, we are basically dealing with the characterization of\\na random variable sampled (i.e., observed) at that instant of time. When, however, we\\nconsider a single realization of the process, we have a random waveform that evolves\\nacross time. The study of stochastic processes, therefore, embodies two approaches: one\\nbased on ensemble averaging and the other based on temporal averaging. Both\\napproaches and their characterizations are considered in this chapter.\\nAlthough it is not possible to predict the exact value of a signal drawn from a stochastic\\nprocess, it is possible to characterize the process in terms of statistical parameters such as\\naverage power, correlation functions, and power spectra. This chapter is devoted to the\\nmathematical definitions, properties, and measurements of these functions, and related issues. Mathematical Definition of a Stochastic Process\\nTo summarize the introduction: stochastic processes have two properties. First, they are',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 165,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'To summarize the introduction: stochastic processes have two properties. First, they are\\nfunctions of time. Second, they are random in the sense that, before conducting an experiment,\\nit is not possible to define the waveforms that will be observed in the future exactly.\\nIn describing a stochastic process, it is convenient to think in terms of a sample space.\\nSpecifically, each realization of the process is associated with a sample point. The totality\\nof sample points corresponding to the aggregate of all possible realizations of the\\nstochastic process is called the sample space. Unlike the sample space in probability',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 165,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '146 Chapter 4 Stochastic Processes theory, each sample point of the sample space pertaining to a stochastic process is a\\nfunction of time. We may therefore think of a stochastic process as the sample space or\\nensemble composed of functions of time. As an integral part of this way of thinking, we\\nassume the existence of a probability distribution defined over an appropriate class of sets\\nin the sample space, so that we may speak with confidence of the probability of various\\nevents observed at different points of time.2\\nConsider, then, a stochastic process specified by\\na. outcomes s observed from some sample space S;\\nb. events defined on the sample space S; and\\nc. probabilities of these events.\\nSuppose that we assign to each sample point s a function of time in accordance with the rule\\nwhere 2T is the total observation interval. For a fixed sample point sj, the graph of the\\nfunction X(t, sj) versus time t is called a realization or sample function of the stochastic\\nprocess. To simplify the notation, we denote this sample function as (4.1)\\nFigure 4.1 illustrates a set of sample functions {xj(t)| j = 1, 2, , n}. From this figure, we\\nsee that, for a fixed time tk inside the observation interval, the set of numbers\\nXts    TtT  -  xj t X t sj    T - t T   = x1 tk  x2 tk  xn tk        X tk s1   X tk s2   X tk sn         = Figure 4.1 An ensemble of sample functions.\\n0 Outcome of the first trial of the experiment Sample space S Outcome of the second trial of the experiment Outcome of the nth trial of the experiment 0 ... ... ... ... xn(tk) tk +T -T x2(tk) x1(tk) t xn(t) x2(t) x1(t) s1 s2 sn',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 166,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '3 Two Classes of Stochastic Processes: Strictly Stationary and Weakly Stationary constitutes a random variable. Thus, a stochastic process X(t, s) is represented by the time-\\nindexed ensemble (family) of random variables {X(t, s)}. To simplify the notation, the\\ncustomary practice is to suppress the s and simply use X(t) to denote a stochastic process.\\nWe may now formally introduce the definition:\\nA stochastic process X(t) is an ensemble of time functions, which, together with\\na probability rule, assigns a probability to any meaningful event associated with\\nan observation of one of the sample functions of the stochastic process.\\nMoreover, we may distinguish between a random variable and a random process as\\nfollows. For a random variable, the outcome of a stochastic experiment is mapped into a\\nnumber. On the other hand, for a stochastic process, the outcome of a stochastic\\nexperiment is mapped into a waveform that is a function of time. Two Classes of Stochastic Processes: Strictly Stationary and\\nWeakly Stationary\\nIn dealing with stochastic processes encountered in the real world, we often find that the\\nstatistical characterization of a process is independent of the time at which observation of\\nthe process is initiated. That is, if such a process is divided into a number of time intervals,\\nthe various sections of the process exhibit essentially the same statistical properties. Such\\na stochastic process is said to be stationary. Otherwise, it is said to be nonstationary.\\nGenerally speaking, we may say:\\nA stationary process arises from a stable phenomenon that has evolved into a\\nsteady-state mode of behavior, whereas a nonstationary process arises from an\\nunstable phenomenon.\\nTo be more precise, consider a stochastic process X(t) that is initiated at\\n. Let\\nX(t1), X(t2), , X(tk) denote the random variables obtained by sampling the process X(t) at\\ntimes t1, t2, , tk, respectively. The joint (cumulative) distribution function of this set of\\nrandom variables is',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 167,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'times t1, t2, , tk, respectively. The joint (cumulative) distribution function of this set of\\nrandom variables is\\n. Suppose next we shift all the sampling\\ntimes by a fixed amount  denoting the time shift, thereby obtaining the new set of random\\nvariables: X(t1), X(t2), , X(tk+). The joint distribution function of this latter set of\\nrandom variables is\\n. The stochastic process X(t) is said to\\nbe stationary in the strict sense, or strictly stationary, if the invariance condition\\n(4.2)\\nholds for all values of time shift , all positive integers k, and any possible choice of\\nsampling times t1, , tk. In other words, we may state:\\nA stochastic process X(t), initiated at time\\n, is strictly stationary if the\\njoint distribution of any set of random variables obtained by observing the\\nprocess X(t) is invariant with respect to the location of the origin t = 0.\\nNote that the finite-dimensional distributions in (4.2) depend on the relative time\\nseparation between random variables, but not on their absolute time. That is, the stochastic\\nprocess has the same probabilistic behavior throughout the global time t.\\nt  - = FX t1  X tk    x1 xk     FX t1  +  X tk  +     x1 xk     FX t1  +  X tk  +     x1 xk     FX t1  X tk    x1 xk     = t  - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 167,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '148 Chapter 4 Stochastic Processes Similarly, we may say that two stochastic processes X(t) and Y(t) are jointly strictly\\nstationary if the joint finite-dimensional distributions of the two sets of stochastic\\nvariables and are invariant with respect to the origin\\nt = 0 for all positive integers k and j, and all choices of the sampling times t1, , tk and\\n.\\nReturning to (4.2), we may identify two important properties:\\nFor k = 1, we have (4.3)\\nIn words, the first-order distribution function of a strictly stationary stochastic\\nprocess is independent of time t.\\nFor k = 2 and  = -t2, we have\\n(4.4)\\nIn words, the second-order distribution function of a strictly stationary stochastic\\nprocess depends only on the time difference between the sampling instants and not\\non the particular times at which the stochastic process is sampled.\\nThese two properties have profound practical implications for the statistical\\nparameterization of a strictly stationary stochastic process, as discussed in Section 4.4.\\nEXAMPLE\\nMultiple Spatial Windows for Illustrating Strict Stationarity\\nConsider Figure 4.2, depicting three spatial windows located at times t1, t2, t3. We wish to\\nevaluate the probability of obtaining a sample function x(t) of a stochastic process X(t) that\\npasses through this set of windows; that is, the probability of the joint event\\nSuppose now the stochastic process X(t) is known to be strictly stationary. An implication\\nof strict stationarity is that the probability of the set of sample functions of this process\\npassing through the windows of Figure 4.3a is equal to the probability of the set of sample\\nfunctions passing through the corresponding time-shifted windows of Figure 4.3b. Note,\\nhowever, that it is not necessary that these two sets consist of the same sample functions.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 168,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'functions passing through the corresponding time-shifted windows of Figure 4.3b. Note,\\nhowever, that it is not necessary that these two sets consist of the same sample functions.\\nX t1    X tk    Y t1    Y tj    t1 tj   FX tx  FX t  +  x  FX x  for all t and = = FX t1  X t2    x1 x2    FX 0 X t1 t2 -    x1 x2    for all t1 and t2 = Figure 4.2 Illustrating the probability of a joint event.\\n\\x02 A  FX t1  X t2  X t3     b1 b2 b3     FX t1  X t2  X t3     a1 a2 a3     = = b1 a1 b3 a3 t3 t1 a2 t2 b2 A possible sample function t',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 168,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Mean, Correlation, and Covariance Functions of Weakly Stationary Processes Another important class of stochastic processes is the so-called weakly stationary\\nprocesses. To be specific, a stochastic process X(t) is said to be weakly stationary if its\\nsecond-order moments satisfy the following two conditions:\\nThe mean of the process X(t) is constant for all time t.\\nThe autocorrelation function of the process X(t) depends solely on the difference\\nbetween any two times at which the process is sampled; the auto in autocorrelation\\nrefers to the correlation of the process with itself.\\nIn this book we focus on weakly stationary processes whose second-order statistics satisfy\\nconditions 1 and 2; both of them are easy to measure and considered to be adequate for\\npractical purposes. Such processes are also referred to as wide-sense stationary processes\\nin the literature. Henceforth, both terminologies are used interchangeably. Mean, Correlation, and Covariance Functions of\\nWeakly Stationary Processes\\nConsider a real-valued stochastic process X(t). We define the mean of the process X(t) as\\nthe expectation of the random variable obtained by sampling the process at some time t, as\\nshown by (4.5) Figure 4.3 Illustrating the concept of\\nstationarity in Example 1.\\n(a) b1 a1 b3 a3 t3 t t1 a2 t2 b2 (b) b1 a1 b3 a3 t3 + t t1 + a2 t2 + b2    X t \\x02 X t   = xfX tx dx  -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 169,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '150 Chapter 4 Stochastic Processes where is the first-order probability density function of the process X(t), observed\\nat time t; note also that the use of single X as subscript in X(t) is intended to emphasize\\nthe fact that X(t) is a first-order moment. For the mean X(t) to be a constant for all time t\\nso that the process X(t) satisfies the first condition of weak stationarity, we require that\\nfX(t)(x) be independent of time t. Consequently, (4.5) simplifies to\\n(4.6)\\nWe next define the autocorrelation function of the stochastic process X(t) as the\\nexpectation of the product of two random variables, X(t1) and X(t2), obtained by sampling\\nthe process X(t) at times t1 and t2, respectively. Specifically, we write\\n(4.7)\\nwhere is the joint probability density function of the process X(t)\\nsampled at times t1 and t2; here, again, note that the use of the double X subscripts is\\nintended to emphasize the fact that MXX(t1,t2) is a second-order moment. For MXX(t1,t2) to\\ndepend only on the time difference t2 - t1 so that the process X(t) satisfies the second\\ncondition of weak stationarity, it is necessary for to depend only on the\\ntime difference t2 - t1. Consequently, (4.7) reduces to\\n(4.8)\\nIn (4.8) we have purposely used two different symbols for the autocorrelation function:\\nMXX(t1, t2) for any stochastic process X(t) and RXX(t2 - t1) for a stochastic process that is\\nweakly stationary.\\nSimilarly, the autocovariance function of a weakly stationary process X(t) is defined by\\n(4.9)\\nEquation (4.9) shows that, like the autocorrelation function, the autocovariance function of\\na weakly stationary process X(t) depends only on the time difference (t2 - t1). This\\nequation also shows that if we know the mean and the autocorrelation function of the\\nprocess X(t), we can uniquely determine the autocovariance function. The mean and\\nautocorrelation function are therefore sufficient to describe the first two moments of the\\nprocess.\\nHowever, two important points should be carefully noted:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 170,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'autocorrelation function are therefore sufficient to describe the first two moments of the\\nprocess.\\nHowever, two important points should be carefully noted:\\nThe mean and autocorrelation function only provide a weak description of the\\ndistribution of the stochastic process X(t).\\nThe conditions involved in defining (4.6) and (4.8) are not sufficient to guarantee the\\nstochastic process X(t) to be strictly stationary, which emphasizes a remark that was\\nmade in the preceding section.\\nfX tx  X t X for all t = MXX t1 t2    \\x03 X t1  X t2     = x1x2 fX t1  X t2    x1 x2    dx1 dx2  -    -   = fX t1  X t2    x1 x2    fX t1  X t2    x1 x2    MXX t1 t2    \\x03 X t1  X t2     = R = XX t2 t1 -   for all t1 and t2 CXX t1 t2    \\x03 X t1   X -  X t2   X -     = RXX t2 t1 -   X 2 - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 170,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Mean, Correlation, and Covariance Functions of Weakly Stationary Processes Nevertheless, practical considerations often dictate that we simply limit ourselves to a\\nweak description of the process given by the mean and autocorrelation function because\\nthe computation of higher order moments can be computationally intractable.\\nHenceforth, the treatment of stochastic processes is confined to weakly stationary pro-\\ncesses, for which the definitions of the second-order moments in (4.6), (4.8), and (4.9) hold.\\nProperties of the Autocorrelation Function\\nFor convenience of notation, we reformulate the definition of the autocorrelation function\\nof a weakly stationary process X(t), presented in (4.8), as\\n(4.10)\\nwhere denotes a time shift; that is,\\n. This autocorrelation function\\nhas several important properties.\\nPROPERTY\\nMean-square Value\\nThe mean-square value of a weakly stationary process X(t) is obtained from RXX() simply\\nby putting  = 0 in (4.10), as shown by\\n(4.11) PROPERTY 2 Symmetry The autocorrelation function RXX() of a weakly stationary process X(t) is an even\\nfunction of the time shift ; that is,\\n(4.12)\\nThis property follows directly from (4.10). Accordingly, we may also define the\\nautocorrelation function RXX as\\nIn words, we may say that a graph of the autocorrelation function RXX(), plotted versus ,\\nis symmetric about the origin.\\nPROPERTY\\nBound on the Autocorrelation Function\\nThe autocorrelation function RXX() attains its maximum magnitude at  = 0; that is,\\n(4.13)\\nTo prove this property, consider the nonnegative quantity\\nExpanding terms and taking their individual expectations, we readily find that\\nwhich, in light of (4.11) and (4.12), reduces to\\nRXX   \\x03 X t  +  X t   for all t = t t2 and  t1 t2 - = = RXX 0  \\x03 X2 t   = RXX   RXX  -   =  RXX   \\x03 X tX t  -     = RXX  RXX 0   \\x03 X t  +   X t   2   0  \\x03 X2 t  +     2\\x03 X t  +     \\x03 X2 t   0  +  2RXX 0  2RXX   0',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 171,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '152 Chapter 4 Stochastic Processes Equivalently, we may write\\nfrom which (4.13) follows directly.\\nPROPERTY\\nNormalization\\nValues of the normalized autocorrelation function\\n(4.14)\\nare confined to the range [-1, 1].\\nThis last property follows directly from (4.13).\\nPhysical Significance of the Autocorrelation Function\\nThe autocorrelation function RXX is significant because it provides a means of\\ndescribing the interdependence of two random variables obtained by sampling the\\nstochastic process X(t) at times  seconds apart. It is apparent, therefore, that the more\\nrapidly the stochastic process X(t) changes with time, the more rapidly will the\\nautocorrelation function RXX() decrease from its maximum RXX(0) as  increases, as\\nillustrated in Figure 4.4. This behavior of the autocorrelation function may be\\ncharacterized by a decorrelation time dec, such that, for  > dec, the magnitude of the\\nautocorrelation function RXX() remains below some prescribed value. We may thus\\nintroduce the following definition:\\nThe decorrelation time dec of a weakly stationary process X(t) of zero mean is\\nthe time taken for the magnitude of the autocorrelation function RXX(t) to\\ndecrease, for example, to 1% of its maximum value RXX(0).\\nFor the example used in this definition, the parameter dec is referred to as the one-percent\\ndecorrelation time.\\nEXAMPLE\\nSinusoidal Wave with Random Phase\\nConsider a sinusoidal signal with random phase, defined by\\n(4.15) RXX 0  RXX   RXX 0    - XX   RXX   RXX 0  ------------------ =  Figure 4.4 Illustrating the autocorrelation functions of slowly and\\nrapidly fluctuating\\nstochastic processes.\\nX t A 2fct  +   cos = 0 Slowly fluctuating stochastic process Rapidly fluctuating stochastic process RXX( )',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 172,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '4 Mean, Correlation, and Covariance Functions of Weakly Stationary Processes where A and fc are constants and  is a random variable that is uniformly distributed over\\nthe interval [-]; that is,\\n(4.16)\\nAccording to (4.16), the random variable  is equally likely to have any value  in the\\ninterval [-]. Each value of  corresponds to a point in the sample space S of the\\nstochastic process X(t).\\nThe process X(t) defined by (4.15) and (4.16) may represent a locally generated carrier\\nin the receiver of a communication system, which is used in the demodulation of a\\nreceived signal. In such an application, the random variable  in (4.15) accounts for\\nuncertainties experienced in the course of signal transmission across the communication\\nchannel.\\nThe autocorrelation function of X(t) is\\nThe first term integrates to zero, so we simply have\\n(4.17)\\nwhich is plotted in Figure 4.5. From this figure we see that the autocorrelation function of\\na sinusoidal wave with random phase is another sinusoid at the same frequency in the\\nlocal time domain denoted by the time shift  rather than the global time domain\\ndenoted by t.\\nFigure 4.5 Autocorrelation function of a sine wave with random phase.\\nf  1 2 ------,      - 0, elsewhere      = RXX   \\x03 X t  +  X t   = \\x03 A2 2fct 2fc  + +   2fct  +   cos cos   = A2 2------\\x03 4fct 2fc 2 + +   cos   A2 2------\\x03 2fc   cos   + = A2 2------ 4fct 2fc 2 + +   cos d A2 2------ 2fc   cos +  -   = RXX   A2 2------ 2fc   cos = A2 2 0 1 fc RXX( )',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 173,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '154 Chapter 4 Stochastic Processes EXAMPLE 3 Random Binary Wave Figure 4.6 shows the sample function x(t) of a weakly stationary process X(t) consisting of\\na random sequence of binary symbols 1 and 0. Three assumptions are made:\\nThe symbols 1 and 0 are represented by pulses of amplitude +A and -A volts\\nrespectively and duration T seconds.\\nThe pulses are not synchronized, so the starting time td of the first complete pulse\\nfor positive time is equally likely to lie anywhere between zero and T seconds. That\\nis, td is the sample value of a uniformly distributed random variable Td, whose\\nprobability density function is defined by\\nDuring any time interval (n - 1)T < t - td < nT, where n is a positive integer, the\\npresence of a 1 or a 0 is determined by tossing a fair coin. Specifically, if the\\noutcome is heads, we have a 1; if the outcome is tails, we have a 0. These two\\nsymbols are thus equally likely, and the presence of a 1 or 0 in any one interval is\\nindependent of all other intervals.\\nSince the amplitude levels -A and +A occur with equal probability, it follows immediately\\nthat \\x03[X(t)] = 0 for all t and the mean of the process is therefore zero.\\nTo find the autocorrelation function RXX(tk,ti), we have to evaluate the expectation\\n\\x03[X(tk)X(ti)], where X(tk) and X(ti) are random variables obtained by sampling the\\nstochastic process X(t) at times tk and ti respectively. To proceed further, we need to\\nconsider two distinct conditions:\\nCondition 1:\\n|tk - ti| > T\\nUnder this condition, the random variables X(tk) and X(ti) occur in different pulse intervals\\nand are therefore independent. We thus have\\nFigure 4.6 Sample function of random binary wave.\\nfTd td   1 T---, 0 td T   0, elsewhere      = \\x03 X tk  X ti    \\x03 X tk    \\x03 X ti    0, tk ti - T  = = td T x(t) t +A -A',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 174,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '4 Mean, Correlation, and Covariance Functions of Weakly Stationary Processes Condition 2:\\n|tk - ti| > T, with tk = 0 and ti < tk\\nUnder this second condition, we observe from Figure 4.6 that the random variables X(tk)\\nand X(ti) occur in the same pulse interval if, and only if, the delay td satisfies the condition\\ntd < T - |tk - ti|. We thus have the conditional expectation\\nAveraging this result over all possible values of td, we get\\nBy similar reasoning for any other value of tk, we conclude that the autocorrelation\\nfunction of a random binary wave, represented by the sample function shown in Figure\\n6, is only a function of the time difference = tk - ti, as shown by\\n(4.18)\\nThis triangular result, described in (4.18), is plotted in Figure 4.7.\\nCross-correlation Functions\\nConsider next the more general case of two stochastic processes X(t) and Y(t) with\\nautocorrelation functions MXX(t, u) and MYY(t, u) respectively. There are two possible\\ncross-correlation functions of X(t) and Y(t) to be considered.\\nFigure 4.7 Autocorrelation function of random binary wave.\\n\\x03 X tk  X ti td   A2, td T tk ti - -  0, elsewhere      = \\x03 X tk  X ti    A2fTd td  dtd 0 T tk ti - -  = A2 T ------ dtd 0 T tk ti - -  = A2 1 tk ti - T ---------------- -     tk ti - T   = RXX   A2 1  T ------ -    ,  T  0,  T       = A2 0 T -T RXX( )',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 175,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '156 Chapter 4 Stochastic Processes Specifically, we have\\n(4.19) and (4.20) where t and u denote two values of the global time at which the processes are observed.\\nAll four correlation parameters of the two stochastic processes X(t) and Y(t) may now be\\ndisplayed conveniently in the form of the two-by-two matrix\\nwhich is called the cross-correlation matrix of the stochastic processes X(t) and Y(t). If the\\nstochastic processes X(t) and Y(t) are each weakly stationary and, in addition, they are\\njointly stationary, then the correlation matrix can be expressed by\\n(4.21)\\nwhere the time shift = u - t.\\nIn general, the cross-correlation function is not an even function of the time-shift  as\\nwas true for the autocorrelation function, nor does it have a maximum at the origin.\\nHowever, it does obey a certain symmetry relationship, described by\\n(4.22)\\nEXAMPLE\\nQuadrature-Modulated Processes\\nConsider a pair of quadrature-modulated processes X1(t) and X2(t) that are respectively\\nrelated to a weakly stationary process X(t) as follows:\\nwhere fc is a carrier frequency and the random variable  is uniformly distributed over the\\ninterval [0, 2]. Moreover,  is independent of X(t). One cross-correlation function of\\nX1(t) and X2(t) is given by\\n(4.23) MXY t u    \\x03 X tY u    = MYX t u    \\x03 Y tX u    = Mtu    MXX t u    MXY t u    MYX t u    MYY t u    = R  RXX   RXY  RYX   RYY  = RXY  RYX  -   = X1 t X t 2fct  +   cos = X2 t X t 2fct  +   sin = R12  \\x03 X1 tX2 t  -     = \\x03 X tX t  -   2fct  +   2fct 2fc  + -   sin cos   = \\x03 X tX t  -  ]\\x03[ 2fct  +   2fct 2fc  + -   sin cos   = 1 2---RXX  \\x03 4fc 2fct - 2 +   2fc   sin - sin   = 1 2---RXX  2fc   sin - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 176,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '5 Ergodic Processes where, in the last line, we have made use of the uniform distribution of the random\\nvariable , representing phase. Invoking (4.22), we find that the other cross-correlation\\nfunction of X1(t) and X2(t) is given by At  = 0, the factor sin(2fc) is zero, in which case we have\\nThis result shows that the random variables obtained by simultaneously sampling the\\nquadrature-modulated processes X1(t) and X2(t) at some fixed value of time t are\\northogonal to each other. Ergodic Processes\\nErgodic processes are subsets of weakly stationary processes. Most importantly, from a\\npractical perspective, the property of ergodicity permits us to substitute time averages for\\nensemble averages.\\nTo elaborate on these two succinct statements, we know that the expectations or\\nensemble averages of a stochastic process X(t) are averages across the process. For\\nexample, the mean of a stochastic process X(t) at some fixed time tk is the expectation of\\nthe random variable X(tk) that describes all possible values of sample functions of the\\nprocess X(t) sampled at time t = tk. Naturally, we may also define long-term sample\\naverages or time averages that are averages along the process. Whereas in ensemble\\naveraging we consider a set of independent realizations of the process X(t) sampled at\\nsome fixed time tk, in time averaging we focus on a single waveform evolving across time\\nt and representing one waveform realization of the process X(t).\\nWith time averages providing the basis of a practical method for possible estimation of\\nensemble averages of a stochastic process, we would like to explore the conditions under\\nwhich this estimation is justifiable. To address this important issue, consider the sample\\nfunction x(t) of a weakly stationary process X(t) observed over the interval -T  t  T. The\\ntime-average value of the sample function x(t) is defined by the definite integral\\n(4.24)\\nClearly, the time average x(T) is a random variable, as its value depends on the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 177,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'time-average value of the sample function x(t) is defined by the definite integral\\n(4.24)\\nClearly, the time average x(T) is a random variable, as its value depends on the\\nobservation interval and which particular sample function of the process X(t) is picked for\\nuse in (4.24). Since the process X(t) is assumed to be weakly stationary, the mean of the\\ntime average x(T) is given by (after interchanging the operations of expectation and\\nintegration, which is permissible because both operations are linear)\\nR21   1 2---RXX  -   2fc   sin = 1 2---RXX   2fc   sin = R12 0  R21 0  0 = = x T  1 2T ------ x t dt T - T  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 177,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '158 Chapter 4 Stochastic Processes (4.25) where X is the mean of the process X(t). Accordingly, the time average x(T) represents\\nan unbiased estimate of the ensemble-averaged mean X. Most importantly, we say that\\nthe process X(t) is ergodic in the mean if two conditions are satisfied:\\nThe time average x(T) approaches the ensemble average X in the limit as the\\nobservation interval approaches infinity; that is,\\nThe variance of x(T), treated as a random variable, approaches zero in the limit as\\nthe observation interval approaches infinity; that is,\\nThe other time average of particular interest is the autocorrelation function Rxx(, T),\\ndefined in terms of the sample function x(t) observed over the interval -T < t < T.\\nFollowing (4.24), we may formally define the time-averaged autocorrelation function of\\nx(t) as\\n(4.26)\\nThis second time average should also be viewed as a random variable with a mean and\\nvariance of its own. In a manner similar to ergodicity of the mean, we say that the process\\nx(t) is ergodic in the autocorrelation function if the following two limiting conditions are\\nsatisfied:\\nWith the property of ergodicity confined to the mean and autocorrelation functions, it\\nfollows that ergodic processes are subsets of weakly stationary processes. In other words,\\nall ergodic processes are weakly stationary; however, the converse is not necessarily true. Transmission of a Weakly Stationary Process through a\\nLinear Time-invariant Filter\\nSuppose that a stochastic process X(t) is applied as input to a linear time-invariant filter of\\nimpulse response h(t), producing a new stochastic process Y(t) at the filter output, as\\ndepicted in Figure 4.8. In general, it is difficult to describe the probability distribution of\\nthe output stochastic process Y(t), even when the probability distribution of the input\\nstochastic process X(t) is completely specified for the entire time interval',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 178,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the output stochastic process Y(t), even when the probability distribution of the input\\nstochastic process X(t) is completely specified for the entire time interval\\n. \\x03 x T    1 2T ------ \\x03 x t   dt T - T  = 1 2T ------ X dt T - T  = X = x T  T   lim X = var x T    T   lim 0 = Rxx T    1 2T ------ x t  +  x t dt T - T  = Rxx T    T   lim RXX  = var Rxx T      T   lim 0 =  t   -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 178,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 Transmission of a Weakly Stationary Process through a Linear Time-invariant Filter For the sake of mathematical tractability, we limit the discussion in this section to the\\ntime-domain form of the input-output relations of the filter for defining the mean and\\nautocorrelation functions of the output stochastic process Y(t) in terms of those of the\\ninput X(t), assuming that X(t) is a weakly stationary process.\\nThe transmission of a process through a linear time-invariant filter is governed by the\\nconvolution integral, which was discussed in Chapter 2. For the problem at hand, we may\\nthus express the output stochastic process Y(t) in terms of the input stochastic process X(t) as\\nwhere 1 is a local time. Hence, the mean of Y(t) is\\n(4.27)\\nProvided that the expectation \\x03[X(t)] is finite for all t and the filter is stable, we may\\ninterchange the order of expectation and integration in (4.27), in which case we obtain\\n(4.28)\\nWhen the input stochastic process X(t) is weakly stationary, the mean X(t) is a constant\\nX; therefore, we may simplify (4.28) as\\n(4.29)\\nwhere H(0) is the zero-frequency response of the system. Equation (4.29) states:\\nThe mean of the stochastic process Y(t) produced at the output of a linear\\ntime-invariant filter in response to a weakly stationary process X(t), acting as\\nthe input process, is equal to the mean of X(t) multiplied by the zero-frequency\\nresponse of the filter.\\nThis result is intuitively satisfying.\\nFigure 4.8 Transmission of a\\nstochastic process through a\\nlinear time-invariant filter.\\nImpulse response h(t) X(t) Y(t) Y t h 1  X t 1 -   d1  -   = Y t \\x03 Y t   = \\x03 h 1  X t 1 -   d1  -   = Y t h 1  \\x03 X t 1 -     d1  -   = h 1  X t 1 -   d1  -   = Y X h 1   d1  -   = XH 0  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 179,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '160 Chapter 4 Stochastic Processes Consider next the autocorrelation function of the output stochastic process Y(t). By\\ndefinition, we have\\nwhere t and u denote two values of the time at which the output process Y(t) is sampled.\\nWe may therefore apply the convolution integral twice to write\\n(4.30)\\nHere again, provided that the mean-square value \\x03[X2(t)] is finite for all t and the filter is\\nstable, we may interchange the order of the expectation and the integrations with respect\\nto 1 and 2 in (4.30), obtaining\\n(4.31)\\nWhen the input X(t) is a weakly stationary process, the autocorrelation function of X(t) is\\nonly a function of the difference between the sampling times t - 1 and u - 2. Thus,\\nputting = u - t in (4.31), we may go on to write\\n(4.32)\\nwhich depends only on the time difference .\\nOn combining the result of (4.32) with that involving the mean Y in (4.29), we may\\nnow make the following statement:\\nIf the input to a stable linear time-invariant filter is a weakly stationary process,\\nthen the output of the filter is also a weakly stationary process.\\nBy definition, we have RYY(0) = \\x03[Y2(t)]. In light of Property 1 of the autocorrelation\\nfunction RYY\\n, it follows, therefore, that the mean-square value of the output process\\nY(t) is obtained by putting  = 0 in (4.32), as shown by\\n(4.33)\\nwhich, of course, is a constant. Power Spectral Density of a Weakly Stationary Process\\nThus far we have considered the time-domain characterization of a weakly stationary\\nprocess applied to a linear filter. We next study the characterization of linearly filtered\\nweakly stationary processes by using frequency-domain ideas. In particular, we wish to\\nderive the frequency-domain equivalent to the result of (4.33), defining the mean-square\\nvalue of the filter output Y(t). The term filter used here should be viewed in a generic\\nsense; for example, it may represent the channel of a communication system.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 180,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'value of the filter output Y(t). The term filter used here should be viewed in a generic\\nsense; for example, it may represent the channel of a communication system.\\nMYY t u    \\x03 Y tY u    = MYY t u    \\x03 h 1  X t 1 -   d1 h 2  X u 2 -   d2  -    -   = MYY t u    h 1   d2 h 2  \\x03 X t 1 -  X u 2 -      -   d1  -   = h 1   d2 h 2  MXX t 1 u 2 -  -    -   d1  -   = RYY   h 1  h 2  RXX  1 2 - +   d1 d2  -    -   =  \\x03 Y2 t   h 1  h 2  RXX 1 2 -   d1 d2  -    -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 180,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '7 Power Spectral Density of a Weakly Stationary Process From Chapter 2, we recall that the impulse response of a linear time-invariant filter is\\nequal to the inverse Fourier transform of the frequency response of the filter. Using H(f) to\\ndenote the frequency response of the filter, we may thus write\\n(4.34)\\nSubstituting this expression for h(1) into (4.33) and then changing the order of\\nintegrations, we get the triple integral\\n(4.35)\\nAt first, the expression on the right-hand side of (4.35) looks rather overwhelming.\\nHowever, we may simplify it considerably by first introducing the variable\\nThen, we may rewrite (4.35) in the new form\\n(4.36)\\nThe middle integral involving the variable 2 inside the square brackets on the right-hand\\nside in (4.36) is simply H*(f), the complex conjugate of the frequency response of the\\nfilter. Hence, using |H(f)|2 = H(f)H*(f), where |H(f)| is the magnitude response of the\\nfilter, we may simplify (4.36) as\\n(4.37)\\nWe may further simplify (4.37) by recognizing that the integral inside the square brackets\\nin this equation with respect to the variable  is simply the Fourier transform of the\\nautocorrelation function RXX of the input process X(t). In particular, we may now\\ndefine a new function\\n(4.38)\\nThe new function SXX(f) is called the power spectral density, or power spectrum, of the\\nweakly stationary process X(t). Thus, substituting (4.38) into (4.37), we obtain the simple\\nformula\\n(4.39)\\nwhich is the desired frequency-domain equivalent to the time-domain relation of (4.33). In\\nwords, (4.39) states:\\nThe mean-square value of the output of a stable linear time-invariant filter\\nin response to a weakly stationary process is equal to the integral over all',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 181,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'words, (4.39) states:\\nThe mean-square value of the output of a stable linear time-invariant filter\\nin response to a weakly stationary process is equal to the integral over all\\nh 1   H f j2f1   exp df  -   = \\x03 Y2 t   H f j2f1   d exp f  -   h 2  RXX 1 2 -   d1 d2  -    -   = H f d2h 2   RXX 1 2 -   j2f1   d1 exp  -    -   df  -   =  1 2 - = \\x03 Y2 t   H f h 2   j2f2   exp d2 RXX  j - 2f   d exp  -    -   df  -   = \\x03 Y2 t   H f2 RXX  j - 2f   d exp  -   df  -   =  SXX f RXX  j - 2f   d exp  -   = \\x03 Y2 t   H f2SXX f df  -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 181,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '162 Chapter 4 Stochastic Processes frequencies of the power spectral density of the input process multiplied by\\nthe squared magnitude response of the filter.\\nPhysical Significance of the Power Spectral Density\\nTo investigate the physical significance of the power spectral density, suppose that the\\nweakly stationary process X(t) is passed through an ideal narrowband filter with a\\nmagnitude response |H(f)| centered about the frequency fc, depicted in Figure 4.9; we may\\nthus write\\n(4.40)\\nwhere f is the bandwidth of the filter. From (4.39) we readily find that if the bandwidth f\\nis made sufficiently small compared with the midband frequency fc of the filter and SXX(f)\\nis a continuous function of the frequency f, then the mean-square value of the filter output\\nis approximately given by\\n(4.41)\\nwhere, for the sake of generality, we have used f in place of fc. According to (4.41),\\nhowever, the filter passes only those frequency components of the input random process\\nX(t) that lie inside the narrow frequency band of width f. We may, therefore, say that\\nSX(f) represents the density of the average power in the weakly stationary process X(t),\\nevaluated at the frequency f. The power spectral density is therefore measured in watts per\\nhertz (W/Hz).\\nThe Wiener-Khintchine Relations\\nAccording to (4.38), the power spectral density SXX(f) of a weakly stationary process X(t)\\nis the Fourier transform of its autocorrelation function RXX\\n. Building on what we know\\nabout Fourier theory from Chapter 2, we may go on to say that the autocorrelation\\nfunction RXX is the inverse Fourier transform of the power spectral density SXX(f).\\nFigure 4.9 Magnitude response of ideal narrowband filter.\\nH f 1, f fc  1 2---f  0, f fc  1 2---f         = \\x03 Y2 t   2f  SXX f for all f    |H(f )|  f 1.0 0 fc f -fc  f',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 182,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '7 Power Spectral Density of a Weakly Stationary Process Simply put, RXX and SXX(f) form a Fourier-transform pair, as shown by the following\\npair of related equations:\\n(4.42)\\n(4.43)\\nThese two equations are known as the Wiener-Khintchine relations,3 which play a\\nfundamental role in the spectral analysis of weakly stationary processes.\\nThe Wiener-Khintchine relations show that if either the autocorrelation function or\\npower spectral density of a weakly stationary process is known, then the other can be\\nfound exactly. Naturally, these functions display different aspects of correlation-related\\ninformation about the process. Nevertheless, it is commonly accepted that, for practical\\npurposes, the power spectral density is the more useful function of the two for reasons that\\nwill become apparent as we progress forward in this chapter and the rest of the book.\\nProperties of the Power Spectral Density PROPERTY 1 Zero Correlation among Frequency Components\\nThe individual frequency components of the power spectral density SXX(f) of a weakly\\nstationary process X(t) are uncorrelated with each other.\\nTo justify this property, consider Figure 4.10, which shows two adjacent narrow bands\\nof the power spectral density SXX(f), with the width of each band being denoted by f.\\nFrom this figure, we see that there is no overlap, and therefore no correlation, between the\\ncontents of these two bands. As f approaches zero, the two narrow bands will\\ncorrespondingly evolve into two adjacent frequency components of SXX(f), remaining\\nuncorrelated with each other. This important property of the power spectral density SXX(f)\\nis attributed to the weak stationarity assumption of the stochastic process X(t).\\n SXX f RXX   j2f -   exp d  -   = RXX   SXX f j2f   exp df  -   = Figure 4.10 Illustration of zero correlation between two adjacent narrow\\nbands of an example power spectral density.\\nSXX(f ) f',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 183,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '164 Chapter 4 Stochastic Processes PROPERTY 2 Zero-frequency Value of Power Spectral Density\\nThe zero-frequency value of the power spectral density of a weakly stationary process\\nequals the total area under the graph of the autocorrelation function; that is,\\n(4.44)\\nThis second property follows directly from (4.42) by putting f = 0.\\nPROPERTY\\nMean-square Value of Stationary Process\\nThe mean-square value of a weakly stationary process X(t) equals the total area under the\\ngraph of the power spectral density of the process; that is,\\n(4.45)\\nThis third property follows directly from (4.43) by putting  = 0 and using Property 1 of\\nthe autocorrelation function described in (4.11) namely RX(0) = \\x02[X2(t)] for all t.\\nPROPERTY\\nNonnegativeness of Power Spectral Density\\nThe power spectral density of a stationary process X(t) is always nonnegative; that is,\\n(4.46)\\nThis property is an immediate consequence of the fact that, since the mean-square\\nvalue \\x02[Y2(t)] is always nonnegative in accordance with (4.41), it follows that must also be nonnegative.\\nPROPERTY\\nSymmetry\\nThe power spectral density of a real-valued weakly stationary process is an even function\\nof frequency; that is,\\n(4.47)\\nThis property is readily obtained by first substituting -f for the variable f in (4.42):\\nNext, substituting - for , and recognizing that RXX(-) = RXX in accordance with\\nProperty 2 of the autocorrelation function described in (4.12), we get\\nwhich is the desired result. It follows, therefore, that the graph of the power spectral\\ndensity SXX(f), plotted versus frequency f, is symmetric about the origin.\\nSXX 0  RXX  d  -   = \\x02 X2 t   SXX f df  -   = SXX f 0 for all f  SXX f \\x02 Y2 t   2f     SXX f -   SXX f = SXX f -   RXX   j2f   exp d  -   =  SXX f -   RXX   j - 2f   exp d  -   SXX f = =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 184,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '7 Power Spectral Density of a Weakly Stationary Process\\n165 PROPERTY 6 Normalization The power spectral density, appropriately normalized, has the properties associated with\\na probability density function in probability theory.\\nThe normalization we have in mind here is with respect to the total area under the graph\\nof the power spectral density (i.e., the mean-square value of the process). Consider then\\nthe function\\n(4.48)\\nIn light of Properties 3 and 4, we note that pXX(f)  0 for all f. Moreover, the total area\\nunder the function pXX(f) is unity. Hence, the normalized power spectral density, as\\ndefined in (4.48), behaves in a manner similar to a probability density function.\\nBuilding on Property 6, we may go on to define the spectral distribution function of a\\nweakly stationary process X(t) as\\n(4.49)\\nwhich has the following properties:\\n2. 3. is a nondecreasing function of the frequency f.\\nConversely, we may state that every nondecreasing and bounded function FXX(f) is the\\nspectral distribution function of a weakly stationary process.\\nJust as important, we may also state that the spectral distribution function FXX(f) has all\\nthe properties of the cumulative distribution function in probability theory, discussed in\\nChapter 3.\\nEXAMPLE\\nSinusoidal Wave with Random Phase (continued)\\nConsider the stochastic process X(t) = Acos(2fct + ), where  is a uniformly\\ndistributed random variable over the interval [-, ]. The autocorrelation function of this\\nstochastic process is given by (4.17), which is reproduced here for convenience:\\nLet (f) denote the delta function at f = 0. Taking the Fourier transform of both sides of the\\nformula defining RXX\\n, we find that the power spectral density of the sinusoidal process\\nX(t) is\\n(4.50)\\nwhich consists of a pair of delta functions weighted by the factor A2/4 and located at fc,\\nas illustrated in Figure 4.11. Since the total area under a delta function is one, it follows\\nthat the total area under SXX(f) is equal to A2/2, as expected.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 185,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'as illustrated in Figure 4.11. Since the total area under a delta function is one, it follows\\nthat the total area under SXX(f) is equal to A2/2, as expected.\\npXX f SXX f SXX f df  -   --------------------------------\\n= FXX f pXX   d  - f  = FXX  -   0 = FXX    1 = FXX f RXX   A2 2------ 2fc   cos =  SXX f A2 4------ f fc -   f fc +   +   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 185,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '166 Chapter 4 Stochastic Processes EXAMPLE 6 Random Binary Wave (continued)\\nConsider again a random binary wave consisting of a sequence of 1s and 0s represented by\\nthe values +A and -A respectively. In Example 3 we showed that the autocorrelation\\nfunction of this random process has the triangular form\\nThe power spectral density of the process is therefore\\nUsing the Fourier transform of a triangular function (see Table 2.2 of Chapter 2), we\\nobtain\\n(4.51)\\nwhich is plotted in Figure 4.12. Here again we see that the power spectral density is non-\\nnegative for all f and that it is an even function of f. Noting that RXX(0) = A2 and using\\nFigure 4.11 Power spectral density of sine wave with\\nrandom phase; (f) denotes the delta function at f = 0. ( f + fc) SXX( f ) 0 fc f -fc A2 4 ( f - fc) A2 4   Figure 4.12 Power spectral density of random binary wave.\\nRXX   A2 1  T ------ -    ,  T  0,  T       = SXX f A2 1  T ------ -     j2f -   exp d T - T  = SXX f A2T sinc2 fT   = SXX( f ) A2T 0 - - f 1 T 2 T 2 T 1 T',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 186,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '7 Power Spectral Density of a Weakly Stationary Process Property 2 of power spectral density, we find that the total area under SXX(f), or the aver-\\nage power of the random binary wave described here, is A2, which is intuitively satisfying.\\nGeneralization of Equation (4.51)\\nIt is informative to generalize (4.51) so that it assumes a more broadly applicable form.\\nWith this objective in mind, we first note that the energy spectral density (i.e., the squared\\nmagnitude of the Fourier transform) of a rectangular pulse g(t) of amplitude A and\\nduration T is given by\\n(4.52)\\nWe may therefore express (4.51) in terms of Eg(f) simply as\\n(4.53)\\nIn words, (4.53) states:\\nFor a random binary wave X(t) in which binary symbols 1 and 0 are represented\\nby pulses g(t) and -g(t) respectively, the power spectral density SXX(f) is equal\\nto the energy spectral density Eg(f) of the symbol-shaping pulse g(t) divided by\\nthe symbol duration T.\\nEXAMPLE\\nMixing of a Random Process with a Sinusoidal Process\\nA situation that often arises in practice is that of mixing (i.e., multiplication) of a weakly\\nstationary process X(t) with a sinusoidal wave cos(2fct + ),where the phase  is a\\nrandom variable that is uniformly distributed over the interval [0, 2]. The addition of the\\nrandom phase  in this manner merely recognizes the fact that the time origin is arbitrarily\\nchosen when both X(t) and cos(2fct + ) come from physically independent sources, as is\\nusually the case in practice. We are interested in determining the power spectral density of\\nthe stochastic process\\n(4.54)\\nUsing the definition of autocorrelation function of a weakly stationary process and noting\\nthat the random variable  is independent of X(t), we find that the autocorrelation function\\nof the process Y(t) is given by',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 187,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'that the random variable  is independent of X(t), we find that the autocorrelation function\\nof the process Y(t) is given by\\n(4.55) Eg f A2T 2 sinc2 fT   = SXX f Eg f T ------------- = Y t X t 2fct  +   cos = RYY   \\x03 Y t  +  Y t   = \\x03 X t  +   2fct 2fc  + +  X t 2fct  +   cos cos   = \\x03 X t  +  x t  \\x03 2fct 2fc  + +   cos 2fct  +   cos   = 1 2---RXX \\x03 2fct   4fct 2fc 2 + +   cos + cos   = 1 2---RXX  2fct   cos =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 187,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '168 Chapter 4 Stochastic Processes Since the power spectral density of a weakly stationary process is the Fourier transform of\\nits autocorrelation function, we may go on to express the relationship between the power\\nspectral densities of the processes X(t) and Y(t) as follows:\\n(4.56)\\nEquation (4.56) teaches us that the power spectral density of the stochastic process Y(t)\\ndefined in (4.54) can be obtained as follows:\\nShift the given power spectral density SXX(f) of the weakly stationary process\\nX(t) to the right by fc, shift it to the left by fc, add the two shifted power spectra,\\nand then divide the result by 4, thereby obtaining the desired power spectral\\ndensity SYY(f).\\nRelationship between the Power Spectral Densities of Input and\\nOutput Weakly Stationary Processes\\nLet SYY(f) denote the power spectral density of the output stochastic processes Y(t)\\nobtained by passing the weakly stationary process X(t) through a linear time-invariant\\nfilter of frequency response H(f). Then, by definition, recognizing that the power spectral\\ndensity of a weakly stationary process is equal to the Fourier transform of its\\nautocorrelation function and using (4.32), we obtain\\n(4.57)\\nLet  + 1 - 2 = 0, or equivalently  = 0 - 1 + 2. By making this substitution into\\n(4.57), we find that SYY(f) may be expressed as the product of three terms:\\n\\nthe frequency response H(f) of the filter;\\n\\nthe complex conjugate of H(f); and',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 188,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the frequency response H(f) of the filter;\\n\\nthe complex conjugate of H(f); and\\n\\nthe power spectral density SXX(f) of the input process X(t).\\nWe may thus simplify (4.57) as shown by\\n(4.58)\\nSince |H(f)|2 = H(f)H*(f), we finally find that the relationship among the power spectral\\ndensities of the input and output processes is expressed in the frequency domain by\\n(4.59)\\nEquation (4.59) states:\\nThe power spectral density of the output process Y(t) equals the power spectral\\ndensity of the input process X(t), multiplied by the squared magnitude response\\nof the filter.\\nBy using (4.59), we can therefore determine the effect of passing a weakly stationary\\nprocess through a stable, linear time-invariant filter. In computational terms, (4.59) is\\nSYY f 1 4--- SXX f fc -   SXX f fc +   +   = SYY f RYY  j2f -   exp d  -   = h 1  h 2  RXX  1 2 - +   j2f -   d1 d2 d exp  -    -    -   = SYY f H fH* fSXX f = SYY f H f2SXX f =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 188,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '7 Power Spectral Density of a Weakly Stationary Process obviously easier to handle than its time-domain counterpart of (4.32) that involves the\\nautocorrelation function.\\nThe Wiener-Khintchine Theorem\\nAt this point in the discussion, a basic question that comes to mind is the following:\\nGiven a function XX whose argument is some time shift , how do we know\\nthat XX is the legitimate normalized autocorrelation function of a weakly\\nstationary process X(t)?\\nThe answer to this question is embodied in a theorem that was first proved by Wiener\\n(1930) and at a later date by Khintchine (1934). Formally, the Wiener-Khintchine\\ntheorem4 states:\\nA necessary and sufficient condition for XX to be the normalized autocorrelation\\nfunction of a weakly stationary process X(t) is that there exists a distribution\\nfunction FXX(f) such that for all possible values of the time shift , the function\\nXX may be expressed in terms of the well-known Fourier-Stieltjes theorem,\\ndefined by\\n(4.60)\\nThe Wiener-Khintchine theorem described in (4.60) is of fundamental importance to a\\ntheoretical treatment of weakly stationary processes.\\nReferring back to the definition of the spectral distribution function FXX(f) given in\\n(4.49), we may express the integrated spectrum dFXX(f) as\\n(4.61)\\nwhich may be interpreted as the probability of X(t) contained in the frequency interval\\n[f, f + df]. Hence, we may rewrite (4.60) in the equivalent form\\n(4.62)\\nwhich expresses XX as the inverse Fourier transform of pXX(f). At this point, we\\nproceed by taking three steps:\\nSubstitute (4.14) for XX on the left-hand side of (4.62).\\nSubstitute (4.48) for pXX inside the integral on the right-hand side of (4.62).\\nUse Property 3 of power spectral density in Section 4.7.\\nThe end result of these three steps is the reformulation of (4.62) as shown by\\nHence, canceling out the common term RXX(0), we obtain',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 189,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Use Property 3 of power spectral density in Section 4.7.\\nThe end result of these three steps is the reformulation of (4.62) as shown by\\nHence, canceling out the common term RXX(0), we obtain\\n(4.63)     XX  j2f   exp dFXX f  -   = dFXX f pXX f df = XX  pXX f j2f   d exp f  -   =    RXX  RXX 0  ------------------ SXX f RXX 0  ------------------ j2f   df exp  -   = RXX  SXX f j2f   exp df  -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 189,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '170 Chapter 4 Stochastic Processes which is a rewrite of (4.43). We may argue, therefore, that basically the two Wiener-\\nKhintchine equations follow from either one of the following two approaches:\\nThe definition of the power spectral density as the Fourier transform of the\\nautocorrelation function, which was first derived in (4.38).\\nThe Wiener-Khintchine theorem described in (4.60). Another Definition of the Power Spectral Density\\nEquation (4.38) provides one definition of the power spectral density SXX(f) of a weakly\\nstationary process X(t); that is, SXX(f) is the Fourier transform of the autocorrelation\\nfunction RXX of the process X(t). We arrived at this definition by working on the mean-\\nsquare value (i.e., average power) of the process Y(t) produced at the output of a linear\\ntime-invariant filter, driven by a weakly stationary process X(t). In this section, we provide\\nanother definition of the power spectral density by working on the process X(t) directly.\\nThe definition so developed is not only mathematically satisfying, but it also provides\\nanother way of interpreting the power spectral density.\\nConsider, then, a stochastic process X(t), which is known to be weakly stationary. Let\\nx(t) represent a sample function of the process X(t). For the sample function to be Fourier\\ntransformable, it must be absolutely integrable; that is,\\nThis condition can never be satisfied by any sample function x(t) of infinite duration. To\\nget around this problem, we consider a truncated segment of x(t) defined over the\\nobservation interval -T t  T, as illustrated in Figure 4.13, as shown by (4.64)\\nClearly, the truncated signal xT(t) has finite energy; therefore, it is Fourier transformable.\\nLet XT(f) denote the Fourier transform of xT(t), as shown by the transform pair:\\nFigure 4.13 Illustration of the truncation of a sample x(t) for\\nFourier transformability; the actual function x(t) extends beyond\\nthe observation interval (-T, T) as shown by the dashed lines.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 190,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Figure 4.13 Illustration of the truncation of a sample x(t) for\\nFourier transformability; the actual function x(t) extends beyond\\nthe observation interval (-T, T) as shown by the dashed lines.\\n |  -   x t| dt   xT t x t, TtT  - 0, otherwise    = xT tXT f x(t) t 0 t = -T t = T',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 190,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Another Definition of the Power Spectral Density in light of which we may invoke Rayleighs energy theorem (Property 14 in Table 2.1) to\\nwrite\\nSince (4.64) implies that\\nwe may also apply Rayleighs energy theorem to the problem at hand as follows:\\n(4.65)\\nWith the two sides of (4.65) based on a single realization of the process X(t), they are both\\nsubject to numerical variability (i.e., instability) as we go from one sample function of the\\nprocess X(t) to another. To mitigate this difficulty, we take the ensemble average of (4.65),\\nand thus write\\n(4.66)\\nWhat we have in (4.66) are two energy-based quantities. However, in the weakly\\nstationary process X(t), we have a process with some finite power. To put matters right, we\\nmultiply both sides of (4.66) by the scaling factor 1/(2T) and take the limiting form of the\\nequation as the observation interval T approaches infinity. In so doing, we obtain\\n(4.67)\\nThe quantity on the left-hand side of (4.67) is now recognized as the average power of the\\nprocess X(t), denoted by Pav, which applies to all possible sample functions of the process\\nX(t). We may therefore recast (4.67) in the equivalent form\\n(4.68)\\nIn (4.68), we next recognize that there are two mathematical operations of fundamental\\ninterest:\\nIntegration with respect to the frequency f.\\nLimiting operation with respect to the total observation interval 2T followed by\\nensemble averaging.\\nThese two operations, viewed in a composite manner, result in a statistically stable\\nquantity defined by Pav. Therefore, it is permissible for us to interchange the order of the\\ntwo operations on the right-hand side of (4.68), recasting this equation in the desired form:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 191,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'quantity defined by Pav. Therefore, it is permissible for us to interchange the order of the\\ntwo operations on the right-hand side of (4.68), recasting this equation in the desired form:\\n(4.69) xT t2 dt  -   XT f  -   2 df = xT t2 dt  -   x t2 dt T - T  = x t2 dt T - T  XT f2 df  -   = \\x03 x t2 T - T  dt \\x03 XT f2  -   df = 1 2T ------ T   lim \\x03 x t2 dt T - T  \\x03 XT f 2T ---------------- 2 df  -   T   lim = Pav \\x03 XT f 2T ---------------- 2 df  -   T   lim = Pav \\x03 XT f2 2T ------------------- T   lim        -   df =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 191,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '172 Chapter 4 Stochastic Processes With (4.69) at hand, we are now ready to formulate another definition for the power\\nspectral density as5\\n(4.70)\\nThis new definition has the following interpretation:\\nSXX(f) df is the average of the contributions to the total power from components\\nin a weakly stationary process X(t) with frequencies extending from f to f + df,\\nand the average is taken over all possible realizations of the process X(t).\\nThis new interpretation of the power spectral density is all the more satisfying when (4.70)\\nis substituted into (4.68), yielding\\n(4.71)\\nwhich is immediately recognized as another way of describing Property 3 of the power\\nspectral density (i.e., (4.45). End-of-chapter Problem 4.8 invites the reader to prove other\\nproperties of the power spectral density, using the definition of (4.70).\\nOne last comment must be carefully noted: in the definition of the power spectral\\ndensity given in (4.70), it is not permissible to let the observation interval T approach\\ninfinity before taking the expectation; in other words, these two operations are not\\ncommutative. Cross-spectral Densities\\nJust as the power spectral density provides a measure of the frequency distribution of a\\nsingle weakly stationary process, cross-spectral densities provide measures of the\\nfrequency interrelationships between two such processes. To be specific, let X(t) and Y(t)\\nbe two jointly weakly stationary processes with their cross-correlation functions denoted\\nby RXY and RYX\\n. We define the corresponding cross-spectral densities SXY(f) and\\nSYX(f) of this pair of processes to be the Fourier transforms of their respective cross-\\ncorrelation functions, as shown by\\n(4.72) and (4.73) The cross-correlation functions and cross-spectral densities form Fourier-transform pairs.\\nAccordingly, using the formula for inverse Fourier transformation, we may also',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 192,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '(4.72) and (4.73) The cross-correlation functions and cross-spectral densities form Fourier-transform pairs.\\nAccordingly, using the formula for inverse Fourier transformation, we may also\\nrespectively write (4.74) SXX f \\x03 XT f2 2T ------------------- T   lim = Pav SXX f  -   df =   SXY f RXY  j2f -   exp d  -   = SYX f RYX  j2f -   exp d  -   = RXY  SXY f j2f   d exp f  -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 192,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '9 Cross-spectral Densities\\n173 and (4.75) The cross-spectral densities SXY(f) and SYX(f) are not necessarily real functions of the\\nfrequency f. However, substituting the following relationship (i.e., Property 2 of the\\nautocorrelation function)\\ninto (4.72) and then using (4.73), we find that SXY(f) and SYX(f) are related as follows:\\n(4.76)\\nwhere the asterisk denotes complex conjugation.\\nEXAMPLE\\nSum of Two Weakly Stationary Processes\\nSuppose that the stochastic processes X(t) and Y(t) have zero mean and let their sum be\\ndenoted by\\nThe problem is to determine the power spectral density of the process Z(t).\\nThe autocorrelation function of Z(t) is given by the second-order moment\\nDefining = t - u and assuming the joint weakly stationarity of the two processes, we\\nmay go on to write\\n(4.77)\\nAccordingly, taking the Fourier transform of both sides of (4.77), we get\\n(4.78)\\nThis equation shows that the cross-spectral densities SXY(f) and SYX(f) represent the\\nspectral components that must be added to the individual power spectral densities of a pair\\nof correlated weakly stationary processes in order to obtain the power spectral density of\\ntheir sum.\\nWhen the stationary processes X(t) and Y(t) are uncorrelated, the cross-spectral\\ndensities SXY(f) and SYX(f) are zero, in which case (4.78) reduces to\\n(4.79)\\nWe may generalize this latter result by stating:\\nWhen there is a multiplicity of zero-mean weakly stationary processes that are\\nuncorrelated with each other, the power spectral density of their sum is equal to\\nthe sum of their individual power spectral densities.\\nRYX  SYX f j2f   exp df  -   = RXY  RYX  -   = SXY f SYX f -   SYX * f = = Z t X t Y t + = MZZ t u    \\x03 Z tZ u    = \\x03 X t Y t +  X u  Y u  +     = \\x03 X tX u    \\x03 X tY u    \\x03 Y tX u    \\x03 Y tY u    + + + = MXX t u    MXY t u    MYX t u    MYY t u    + + + = RZZ  RXX  RXY  RYX  RYY  + + + = SZZ f SXX f SXY f SYX f SYY f + + + = SZZ f SXX f SYY f + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 193,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '174 Chapter 4 Stochastic Processes EXAMPLE 9 Filtering of Two Jointly Weakly Stationary Processes\\nConsider next the problem of passing two jointly weakly stationary processes through a\\npair of separate, stable, linear time-invariant filters, as shown in Figure 4.14. The\\nstochastic process X(t) is the input to the filter of impulse response h1(t), and the stochastic\\nprocess Y(t) is the input to the filter of the impulse response h2(t). Let V(t) and Z(t) denote\\nthe processes at the respective filter outputs. The cross-correlation function of the output\\nprocesses V(t) and Z(t) is therefore defined by the second-order moment\\n(4.80)\\nwhere MXY(t, u) is the cross-correlation function of X(t) and Y(t). Because the input\\nstochastic processes are jointly weakly stationary, by hypothesis, we may set = t - u, and\\nthereby rewrite (4.80) as\\n(4.81)\\nTaking the Fourier transform of both sides of (4.81) and using a procedure similar to that\\nwhich led to the development of (4.39), we finally get\\n(4.82)\\nwhere H1(f) and H2(f) are the frequency responses of the respective filters in Figure 4.14\\nand is the complex conjugate of H2(f). This is the desired relationship between the\\ncross-spectral density of the output processes and that of the input processes. Note that\\n(4.82) includes (4.59) as a special case. The Poisson Process\\nHaving covered the basics of stochastic process theory, we now turn our attention to\\ndifferent kinds of stochastic processes that are commonly encountered in the study of\\ncommunication systems. We begin the study with the Poisson process,6 which is the\\nsimplest process dealing with the issue of counting the number of occurrences of random\\nevents.\\nFigure 4.14\\nA pair of separate linear\\ntime-invariant filters.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 194,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'simplest process dealing with the issue of counting the number of occurrences of random\\nevents.\\nFigure 4.14\\nA pair of separate linear\\ntime-invariant filters.\\nMVZ t u    \\x03 V tZ u    = \\x03 h1 1  X t 1 -   d1 h2 2  Y u 2 -   d2  -    -   = h1 1  h2 2  \\x03 X t 1 -  Y u 2 -     d1 d2  -    -   = h1 1  h2 2  MXY t 1 - u 2 -    d1 d2  -    - = RVZ   h1 1  h2 2  RXY  1 - 2 +   d1 d2  -    -   = SVZ f H1 fH2* fSXY f = H2* f h2(t) Y(t) Z(t) h1(t) X(t) V(t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 194,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '10 The Poisson Process Consider, for example, a situation in which events occur at random instants of time,\\nsuch that the average rate of events per second is equal to . The sample path of such a\\nrandom process is illustrated in Figure 4.15, where i denotes the occurrence time of the\\nith event with i = 1, 2, . Let N(t) be the number of event occurrences in the time interval\\n[0, t]. As illustrated in Figure 4.15, we see that N(t) is a nondecreasing, integer-valued,\\ncontinuous process. Let pk, denote the probability that exactly k events occur during an\\ninterval of duration ; that is,\\n(4.83)\\nWith this background, we may now formally define the Poisson process:\\nA random counting process is said to be a Poisson process with average rate  if\\nit satisfies the three basic properties listed below.\\nPROPERTY\\nTime Homogeneity\\nThe probability pk, of k event occurrences is the same for all intervals of the same\\nduration .\\nThe essence of Property 1 is that the events are equally likely at all times.\\nPROPERTY\\nDistribution Function\\nThe number of event occurrences, N0,t in the interval [0, t] has a distribution function with\\nmean t, defined by\\n(4.84)\\nThat is, the time between events is exponentially distributed.\\nFrom Chapter 3, this distribution function is recognized to be the Poisson distribution.\\nIt is for this reason that N(t) is called the Poisson process.\\nFigure 4.15 Sample function of a Poisson counting process.\\n0 1  2  3  4  5  6 t 1 2 3 4 5 6 N(t)  pk  \\x02 Ntt  +    k =   = \\x02 N t k =   t  k k! ------------ t -  , k 0 1 2   = exp =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 195,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '176 Chapter 4 Stochastic Processes PROPERTY 3 Independence The numbers of events in nonoverlapping time intervals are statistically independent,\\nregardless of how small or large the intervals happen to be and no matter how close or\\ndistant they could be.\\nProperty 3 is the most distinguishing property of the Poisson process. To illustrate the\\nsignificance of this property, let [ti,ui] for i = 1, 2, , k denote k disjoint intervals on the\\nline . We may then write (4.85) The important point to take from this discussion is that these three properties provide a\\ncomplete characterization of the Poisson process.\\nThis kind of stochastic process arises, for example, in the statistical characterization of\\na special kind of noise called shot noise in electronic devices (e.g., diodes and transistors),\\nwhich arises due to the discrete nature of current flow. The Gaussian Process\\nThe second stochastic process of interest is the Gaussian process, which builds on the\\nGaussian distribution discussed in Chapter 3. The Gaussian process is by far the most\\nfrequently encountered random process in the study of communication systems. We say so\\nfor two reasons: practical applicability and mathematical tractability.7\\nLet us suppose that we observe a stochastic process X(t) for an interval that starts at\\ntime t = 0 and lasts until t = T. Suppose also that we weight the process X(t) by some\\nfunction g(t) and then integrate the product g(t)X(t) over the observation interval [0, T],\\nthereby obtaining the random variable\\n(4.86)\\nWe refer to Y as a linear functional of X(t). The distinction between a function and a\\nfunctional should be carefully noted. For example, the sum\\n, where the ai\\nare constants and the Xi are random variables, is a linear function of the Xi; for each\\nobserved set of values for the random variable Xi, we have a corresponding value for the\\nrandom variable Y. On the other hand, the value of the random variable Y in (4.86) depends',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 196,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'observed set of values for the random variable Xi, we have a corresponding value for the\\nrandom variable Y. On the other hand, the value of the random variable Y in (4.86) depends\\non the course of the integrand function g(t)X(t) over the entire observation interval from\\nto T. Thus, a functional is a quantity that depends on the entire course of one or more\\nfunctions rather than on a number of discrete variables. In other words, the domain of a\\nfunctional is a space of admissible functions rather than a region of coordinate space.\\nIf, in (4.86), the weighting function g(t) is such that the mean-square value of the\\nrandom variable Y is finite and if the random variable Y is a Gaussian-distributed random\\nvariable for every g(t) in this class of functions, then the process X(t) is said to be a\\nGaussian process. In words, we may state:\\nA process X(t) is said to be a Gaussian process if every linear functional of X(t)\\nis a Gaussian random variable.\\n0     \\x02[N t1 u1    n1 N ; t2 u2    = n2  ; N ; tk uk    = tk] \\x02 N ti ui    ni =   i 1 = k  = = Y gtX t dt 0 T  = Y i=1 N aiXi =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 196,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '11 The Gaussian Process From Chapter 3 we recall that the random variable Y has a Gaussian distribution if its\\nprobability density function has the form\\n(4.87)\\nwhere  is the mean and is the variance of the random variable Y. The distribution of a\\nGaussian process X(t), sampled at some fixed time tk, say, satisfies (4.87).\\nFrom a theoretical as well as practical perspective, a Gaussian process has two main\\nvirtues:\\nThe Gaussian process has many properties that make analytic results possible; we\\nwill discuss these properties later in the section.\\nThe stochastic processes produced by physical phenomena are often such that a\\nGaussian model is appropriate. Furthermore, the use of a Gaussian model to describe\\nphysical phenomena is often confirmed by experiments. Last, but by no means least,\\nthe central limit theorem (discussed in Chapter 3) provides mathematical justification\\nfor the Gaussian distribution.\\nThus, the frequent occurrence of physical phenomena for which a Gaussian model is\\nappropriate and the ease with which a Gaussian process is handled mathematically make\\nthe Gaussian process very important in the study of communication systems.\\nProperties of a Gaussian Process\\nPROPERTY\\nLinear Filtering\\nIf a Gaussian process X(t) is applied to a stable linear filter, then the stochastic process\\nY(t) developed at the output of the filter is also Gaussian.\\nThis property is readily derived by using the definition of a Gaussian process based on\\n(4.86). Consider the situation depicted in Figure 4.8, where we have a linear time-invariant\\nfilter of impulse response h(t), with the stochastic process X(t) as input and the stochastic\\nprocess Y(t) as output. We assume that X(t) is a Gaussian process. The process Y(t) is\\nrelated to X(t) by the convolution integral\\n(4.88)\\nWe assume that the impulse response h(t) is such that the mean-square value of the output\\nrandom process Y(t) is finite for all time t in the range\\n, for which the process',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 197,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '(4.88)\\nWe assume that the impulse response h(t) is such that the mean-square value of the output\\nrandom process Y(t) is finite for all time t in the range\\n, for which the process\\nY(t) is defined. To demonstrate that the output process Y(t) is Gaussian, we must show that\\nany linear functional of it is also a Gaussian random variable. That is, if we define the\\nrandom variable\\n(4.89)\\nthen Z must be a Gaussian random variable for every function gY(t), such that the mean-\\nsquare value of Z is finite. The two operations performed in the right-hand side of (4.89)\\nfY y  1 2 ----------------- y  -  2 22 -------------------    -    exp = 2 Y t h t  -  X   d 0 t     0 T  = 0 t    Z gY t h t  -  X d 0 T  dt 0   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 197,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '178 Chapter 4 Stochastic Processes are both linear; therefore, it is permissible to interchange the order of integrations,\\nobtaining\\n(4.90)\\nwhere the new function\\n(4.91)\\nSince X(t) is a Gaussian process by hypothesis, it follows from (4.91) that Z must also be a\\nGaussian random variable. We have thus shown that if the input X(t) to a linear filter is a\\nGaussian process, then the output Y(t) is also a Gaussian process. Note, however, that\\nalthough our proof was carried out assuming a time-invariant linear filter, this property is\\nalso true for any arbitrary stable linear filter.\\nPROPERTY\\nMultivariate Distribution\\nConsider the set of random variables X(t1), X(t2), , X(tn), obtained by sampling a\\nstochastic process X(t) at times t1, t2, , tn. If the process X(t) is Gaussian, then this set of\\nrandom variables is jointly Gaussian for any n, with their n-fold joint probability density\\nfunction being completely determined by specifying the set of means\\n(4.92)\\nand the set of covariance functions\\n(4.93)\\nLet the n-by-1 vector X denote the set of random variables X(t1), X(t2), , X(tn) derived\\nfrom the Gaussian process X(t) by sampling it at times t1, t2, , tn. Let the vector x denote\\na sample value of X. According to Property 2, the random vector X has a multivariate\\nGaussian distribution, defined in matrix form as\\n(4.94)\\nwhere the superscript T denotes matrix transposition, the mean vector\\n= [1, 2,, n]T\\nthe covariance matrix\\n =\\n-1 is the inverse of the covariance matrix and  is the determinant of the covariance\\nmatrix \\nProperty 2 is frequently used as the definition of a Gaussian process. However, this\\ndefinition is more difficult to use than that based on (4.86) for evaluating the effects of\\nfiltering on a Gaussian process.\\nNote also that the covariance matrix  is a symmetric nonnegative definite matrix. For a\\nnondegenerate Gaussian process,  is positive definite, in which case the covariance\\nmatrix is invertible.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 198,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Note also that the covariance matrix  is a symmetric nonnegative definite matrix. For a\\nnondegenerate Gaussian process,  is positive definite, in which case the covariance\\nmatrix is invertible.\\nZ gtX  d 0 T  = g   gY th t  -   d 0 T  = X ti  \\x03 X ti    i  1 2 n   = = CX tk ti    \\x03 X tk   X tk  -  X  ti X ti ) -   k i 1 2 n   =   = fX t1  X t2  X tn      x1 x2 xn      1 2  n 2 1 2  --------------------------------\\n1 2--- x  -  T1 - x  -   - exp = CX tk ti     k i 1 =  n',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 198,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '12 Noise 179 PROPERTY 3 Stationarity If a Gaussian process is weakly stationary, then the process is also strictly stationary.\\nThis follows directly from Property 2.\\nPROPERTY\\nIndependence\\nIf the random variables X(t1), X(t2), , X(tn), obtained by respectively sampling a\\nGaussian process X(t) at times t1, t2, , tn, are uncorrelated, that is\\n(4.95)\\nthen these random variables are statistically independent.\\nThe uncorrelatedness of X(t1), , X(tn) means that the covariance matrix  is reduced\\nto a diagonal matrix, as shown by\\n(4.96)\\nwhere the 0s denote two sets of elements whose values are all zero, and the diagonal terms\\n(4.97)\\nUnder this special condition, the multivariate Gaussian distribution described in (4.94)\\nsimplifies to (4.98) where Xi = X(ti) and (4.99) In words, if the Gaussian random variables X(t1), X(t2), , X(tn) are uncorrelated, then\\nthey are statistically independent, which, in turn, means that the joint probability density\\nfunction of this set of random variables is expressed as the product of the probability\\ndensity functions of the individual random variables in the set. Noise\\nThe term noise is used customarily to designate unwanted signals that tend to disturb the\\ntransmission and processing of signals in communication systems, and over which we\\nhave incomplete control. In practice, we find that there are many potential sources of noise\\nin a communication system. The sources of noise may be external to the system (e.g.,\\n\\x03 X tk   X tk  -  (X ti  X ti ) -   0 = i k   1 2 0 0 n 2 = . . . i 2 \\x03 X ti  \\x03 X ti    -  2 i 1, 2, n  =  = fX x  fXi xi   i 1 = n  = fXi xi   1 2i ---------------- xi Xi -  2 2i 2 -------------------------\\n- i 1, 2, n  =  exp =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 199,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '180 Chapter 4 Stochastic Processes atmospheric noise, galactic noise, man-made noise) or internal to the system. The second\\ncategory includes an important type of noise that arises from the phenomenon of\\nspontaneous fluctuations of current flow that is experienced in all electrical circuits. In a\\nphysical context, the most common examples of the spontaneous fluctuation phenomenon\\nare shot noise, which, as stated in Section 4.10, arises because of the discrete nature of\\ncurrent flow in electronic devices; and thermal noise, which is attributed to the random\\nmotion of electrons in a conductor.8 However, insofar as the noise analysis of\\ncommunication systems is concerned, be they analog or digital, the analysis is customarily\\nbased on a source of noise called white-noise, which is discussed next.\\nWhite Noise\\nThis source of noise is idealized, in that its power spectral density is assumed to be\\nconstant and, therefore, independent of the operating frequency. The adjective white is\\nused in the sense that white light contains equal amounts of all frequencies within the\\nvisible band of electromagnetic radiation. We may thus make the statement:\\nWhite noise, denoted by W(t), is a stationary process whose power spectral density\\nSW(f) has a constant value across the entire frequency interval\\n.\\nClearly, white-noise can only be meaningful as an abstract mathematical concept; we say\\nso because a constant power spectral density corresponds to an unbounded spectral\\ndistribution function and, therefore, infinite average power, which is physically\\nnonrealizable. Nevertheless, the utility of white-noise is justified in the study of\\ncommunication theory by virtue of the fact that it is used to model channel noise at the\\nfront end of a receiver. Typically, the receiver includes a filter whose frequency response is\\nessentially zero outside a frequency band of some finite value. Consequently, when white-',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 200,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'front end of a receiver. Typically, the receiver includes a filter whose frequency response is\\nessentially zero outside a frequency band of some finite value. Consequently, when white-\\nnoise is applied to the model of such a receiver, there is no need to describe how the power\\nspectral density SWW(f) falls off outside the usable frequency band of the receiver.9\\nLet\\n(4.100)\\nas illustrated in Figure 4.16a. Since the autocorrelation function is the inverse Fourier\\ntransform of the power spectral density in accordance with the Wiener-Khintchine\\nrelations, it follows that for white-noise the autocorrelation function is\\n(4.101)\\nHence, the autocorrelation function of white noise consists of a delta function weighted by\\nthe factor N02 and occurring at the time shift = 0, as shown in Figure 4.16b.\\nSince RWW is zero for\\n, it follows that any two different samples of white noise\\nare uncorrelated no matter how closely together in time those two samples are taken. If the\\nwhite noise is also Gaussian, then the two samples are statistically independent in\\naccordance with Property 4 of the Gaussian process. In a sense, then, white Gaussian\\nnoise represents the ultimate in randomness.\\nThe utility of a white-noise process in the noise analysis of communication systems is\\nparallel to that of an impulse function or delta function in the analysis of linear systems.\\n f   - SWW f N0 2 ------ for all f = RWW  N0 2 ------ =   0',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 200,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '12 Noise Just as we may observe the effect of an impulse only after it has been passed through a\\nlinear system with a finite bandwidth, so it is with white noise whose effect is observed\\nonly after passing through a similar system. We may therefore state:\\nAs long as the bandwidth of a noise process at the input of a system is\\nappreciably larger than the bandwidth of the system itself, then we may model\\nthe noise process as white noise.\\nEXAMPLE\\nIdeal Low-pass Filtered White Noise\\nSuppose that a white Gaussian noise of zero mean and power spectral density N02 is\\napplied to an ideal low-pass filter of bandwidth B and passband magnitude response of\\none. The power spectral density of the noise N(t) appearing at the filter output, as shown in\\nFigure 4.17a, is therefore\\n(4.102)\\nSince the autocorrelation function is the inverse Fourier transform of the power spectral\\ndensity, it follows that\\n(4.103)\\nwhose dependence on  is plotted in Figure 4.17b. From this figure, we see that RNN\\nhas the maximum value N0B at the origin and it passes through zero at  =k(2B), where\\nk = 1, 2, 3, .\\nSince the input noise W(t) is Gaussian (by hypothesis), it follows that the band-limited\\nnoise N(t) at the filter output is also Gaussian. Suppose, then, that N(t) is sampled at the\\nrate of 2B times per second. From Figure 4.17b, we see that the resulting noise samples are\\nuncorrelated and, being Gaussian, they are statistically independent. Accordingly, the joint\\nprobability density function of a set of noise samples obtained in this way is equal to the\\nproduct of the individual probability density functions. Note that each such noise sample\\nhas a mean of zero and variance of N0B.\\nFigure 4.16 Characteristics of white-noise: (a) power spectral density; (b) autocorrelation function.\\nN0 2 (b) (a) ( ) 0 N0 2 SW ( f ) f 0 RW( )    SNN f N0 2 ------, BfB  - 0, f B       = RNN   N0 2 ------ j2f   exp df B - B  = N0B sinc 2B   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 201,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '182 Chapter 4 Stochastic Processes EXAMPLE 11 Correlation of White Noise with Sinusoidal Wave\\nConsider the sample function\\n(4.104)\\nwhich is the output of a correlator with white Gaussian noise sample function w(t) and\\nsinusoidal wave as its two inputs; the scaling factor is included\\nin (4.104) to make the sinusoidal wave input have unit energy over the interval 0 t  T.\\nWith w(t) having zero mean, it immediately follows that the correlator output has\\nzero mean too. The variance of the correlator output is therefore defined by\\n(4.105)\\nwhere, in the last line, we made use of (4.101). We now invoke the sifting property of the\\ndelta function, namely\\n(4.106)\\nwhere g(t) is a continuous function of time that has the value g(0) at time t = 0. Hence, we\\nmay further simplify the expression for the noise variance as\\n(4.107)\\nFigure 4.17 Characteristics of low-pass filtered white noise; (a) power spectral density;\\n(b) autocorrelation function.\\nN0B N0 2 0 B -B 0 (a) (b) - 3 2B  SN(f) f - 1 B - 1 2B 1 2B 1 B 3 2B RN( ) wt 2 T--- w t 2fct   cos dt 0 T  = 2 T 2fct   cos 2 T  wt W 2 \\x03 2 T--- w t1   2fct1  w t2   2fct2   cos cos dt1 dt2 0 T  0 T  = 2 T--- \\x03 w t1  w t2     2fct1   2fct2   cos cos dt1 dt2 0 T  0 T  = 2 T--- N0 2 ------t1 t2 -   2fct1   2fct2   cos cos dt1 dt2 0 T  0 T  = g tt dt  -   g 0  = W 2 N0 2 ------2 T--- 2fct   2 cos dt T - T  = N0 2T ------ 1 4fct   cos +   dt 0 T  = N0 2 ------ =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 202,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '13 Narrowband Noise where, in the last line, it is assumed that the frequency fc of the sinusoidal wave input is an\\ninteger multiple of the reciprocal of T for mathematical convenience. Narrowband Noise\\nThe receiver of a communication system usually includes some provision for\\npreprocessing the received signal. Typically, the preprocessing takes the form of a\\nnarrowband filter whose bandwidth is just large enough to pass the modulated component\\nof the received signal essentially undistorted, so as to limit the effect of channel noise\\npassing through the receiver. The noise process appearing at the output of such a filter is\\ncalled narrowband noise. With the spectral components of narrowband noise concentrated\\nabout some midband frequency fc as in Figure 4.18a, we find that a sample function n(t)\\nof such a process appears somewhat similar to a sine wave of frequency fc. The sample\\nfunction n(t) may, therefore, undulate slowly in both amplitude and phase, as illustrated in\\nFigure 4.18b.\\nConsider, then, the n(t) produced at the output of a narrowband filter in response to the\\nsample function w(t) of a white Gaussian noise process of zero mean and unit power spec-\\ntral density applied to the filter input; w(t) and n(t) are sample functions of the respective\\nprocesses W(t) and N(t). Let H(f) denote the transfer function of this filter. Accordingly,\\nwe may express the power spectral density SN(f) of the noise N(t) in terms of H(f) as\\n(4.108)\\nOn the basis of this equation, we may now make the following statement:\\nAny narrowband noise encountered in practice may be modeled by applying a\\nwhite-noise to a suitable filter in the manner described in (4.108).\\nIn this section we wish to represent the narrowband noise n(t) in terms of its in-phase and\\nquadrature components in a manner similar to that described for a narrowband signal in\\nSection 2.10. The derivation presented here is based on the idea of pre-envelope and related',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 203,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'quadrature components in a manner similar to that described for a narrowband signal in\\nSection 2.10. The derivation presented here is based on the idea of pre-envelope and related\\nconcepts, which were discussed in Chapter 2 on Fourier analysis of signals and systems.\\nSNN f H f2 = Figure 4.18 (a) Power spectral density of narrowband noise. (b) Sample function of\\nnarrowband noise. 0 fc - B fc + Bfft -fc - B -fc + B -fc 0 n(t) SNN( f ) 1 B 1 fc (a) (b)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 203,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '184 Chapter 4 Stochastic Processes Let n+(t) and , respectively, denote the pre-envelope and complex envelope of the\\nnarrowband noise n(t). We assume that the power spectrum of n(t) is centered about the\\nfrequency fc. Then we may write\\n(4.109) and (4.110) where is the Hilbert transform of n(t). The complex envelope may itself be expressed as (4.111) Hence, combining (4.109) through (4.111), we find that the in-phase component nI(t) and\\nthe quadrature component nQ(t) of the narrowband noise n(t) are\\n(4.112) and (4.113) respectively. Eliminating between (4.112) and (4.113), we get the desired canonical\\nform for representing the narrowband noise n(t), as shown by\\n(4.114)\\nUsing (4.112) to (4.114), we may now derive some important properties of the in-phase\\nand quadrature components of a narrowband noise, as described next.\\nPROPERTY 1 The in-phase component nI(t) and quadrature component nQ(t) of narrowband noise n(t)\\nhave zero mean.\\nTo prove this property, we first observe that the noise is obtained by passing n(t)\\nthrough a linear filter (i.e., Hilbert transformer). Accordingly, will have zero mean\\nbecause n(t) has zero mean by virtue of its narrowband nature. Furthermore, from (4.112)\\nand (4.113), we see that nI(t) and nQ(t) are weighted sums of n(t) and\\n. It follows,\\ntherefore, that the in-phase and quadrature components, nI(t) and nQ(t), both have zero\\nmean.\\nPROPERTY\\nIf the narrowband noise n(t) is Gaussian, then its in-phase component and quadra-\\nture component are jointly Gaussian.\\nTo prove this property, we observe that is derived from n(t) by a linear filtering\\noperation. Hence, if is Gaussian, the Hilbert transform is also Gaussian, and and are jointly Gaussian. It follows, therefore, that the in-phase and quadrature\\ncomponents, nI(t) and nQ(t), are jointly Gaussian, since they are weighted sums of jointly\\nGaussian processes.\\nPROPERTY\\nIf the narrowband noise is weakly stationary, then its in-phase component and',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 204,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'components, nI(t) and nQ(t), are jointly Gaussian, since they are weighted sums of jointly\\nGaussian processes.\\nPROPERTY\\nIf the narrowband noise is weakly stationary, then its in-phase component and\\nquadrature component are jointly weakly stationary.\\nIf n(t) is weakly stationary, so is its Hilbert transform\\n. However, since the in-phase\\nand quadrature components, and\\n, are both weighted sums of and n t n+ t n t jn t + = n t n+ t j2fct -   exp = n t n t n t nI t jnQ t + = nI t n t 2fct   cos = n t j2fct   sin + nQ t n t 2fct   n t 2fct   sin - cos = n t n t nI t 2fct   nQ t 2fct   sin - cos = n t n t n t nI t nQ t n t n t n t n t n t n t nI t nQ t n t nI t nQ t n t n t',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 204,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '13 Narrowband Noise and the weighting functions, cos(2fct) and sin(2fct), vary with time, we cannot directly\\nassert that and are weakly stationary. To prove Property 3, we have to\\nevaluate their correlation functions.\\nUsing (4.112) and (4.113), we find that the in-phase and quadrature components,\\nand\\n, of a narrowband noise have the same autocorrelation function, as shown by\\n(4.115)\\nand their cross-correlation functions are given by\\n(4.116)\\nwhere RNN is the autocorrelation function of\\n, and is the Hilbert transform\\nof RNN(). From (4.115) and (4.116), we readily see that the correlation functions\\n,\\n, and of the in-phase and quadrature components and depend only on the time shift . This dependence, in conjunction with Property 1,\\nproves that and are weakly stationary if the original narrowband noise is weakly stationary. PROPERTY 4 Both the in-phase noise and quadrature noise have the same power spectral\\ndensity, which is related to the power spectral density SNN(f) of the original narrowband\\nnoise as follows: (4.117) where it is assumed that SNN(f) occupies the frequency interval and\\nfc  B.\\nTo prove this fourth property, we take the Fourier transforms of both sides of (4.115),\\nand use the fact that\\n(4.118)\\nWe thus obtain the result\\n(4.119)\\nNow, with the power spectral density SNN(f) of the original narrowband noise n(t)\\noccupying the frequency interval\\n, where fc > B, as illustrated in\\nFigure 4.19, we find that the corresponding shapes of SNN(f - fc) and SNN(f + fc) are as in\\nFigures 4.19b and 4.19c respectively. Figures 4.19d, 4.19e, and 4.19f show the shapes of',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 205,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Figure 4.19, we find that the corresponding shapes of SNN(f - fc) and SNN(f + fc) are as in\\nFigures 4.19b and 4.19c respectively. Figures 4.19d, 4.19e, and 4.19f show the shapes of\\nnI t nQ t nI t nQ t n t RNINI  RNQNQ  RNN  2fc   R NN  2fc   sin + cos = = RNINQ  R - NQNI  RNN  2fc   R NN  2fc   cos - sin = =  n t R NN  RNINI RNQNQ  RNINQ  nI t nQ t nI t nQ t n t nI t nQ t n t SNINI f SNQNQ f SNN f fc -   SNN f fc +  , + BfB  - 0, otherwise    = = fc B f fc B +   - F R NN    j fF RNN    sgn - = j fSNN f sgn - = SNINI f SNQNQ f = 1 2--- SNN f fc -   SNN f fc +   +   = 1 2--- SNN f fc -   f fc -   SNN f fc +   f fc +   sgn - sgn   - 1 2---SNN f fc -  1 f fc -   sgn -   1 2---SNN f fc +  1 f fc +   sgn +   + = fc B f fc B +   -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 205,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '186 Chapter 4 Stochastic Processes sgn(f), sgn(f - fc), and sgn(f + fc) respectively. Accordingly, we may make the following\\nobservation from Figure 4.19:\\nFor frequencies defined by -B f  B, we have\\nsgn(f - fc) = -1 and sgn(f  fc) = +1 Hence, substituting these results into (4.119), we obtain\\nSNINI f SNQNQ f = SNN f fc -   SNN f fc +   B - f B   + = Figure 4.19 (a) Power spectral density SNN(f)\\npertaining to narrowband noise n(t).\\n(b), (c) Frequency-shifted versions\\nof SNN(f) in opposite directions.\\n(d) Signum function sgn(f).\\n(e), (f) Frequency-shifted versions\\nof sgn(f) in opposite directions.\\n0 SNN(f) f (a) fc fc 2B -fc 2B -B B -B B 0 SNN(f - fc) SNN(f + fc) fff (b) 2fc 2B 0 0 sgn(f) sgn(f + fc) sgn(f - fc) (c) (d) (e) -2fc 2B f 0 -1 -1 -fc (f) f 0 +1 +1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 206,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '13 Narrowband Noise\\nFor 2fc - B f  2fc  B, we have\\nsgn(f - fc) = 1 and sgn(f + fc) = 0 with the result that and are both zero.\\nFor -2fc - B f  -2fc B, we have\\nsgn(f - fc) = 0 and sgn(f + fc) = -1 with the result that, here also, and are both zero.\\nOutside the frequency intervals defined in points 1, 2, and 3, both SNN(f - fc) and\\nSNN(f + fc) are zero, and in a corresponding way, SNN(f - fc) and are also\\nzero.\\nCombining these results, we obtain the simple relationship defined in (4.117).\\nAs a consequence of this property, we may extract the in-phase component and\\nquadrature component\\n, except for scaling factors, from the narrowband noise n(t)\\nby using the scheme shown in Figure 4.20a, where both low-pass filters have a cutoff\\nfrequency at B. The scheme shown in Figure 4.20a may be viewed as an analyzer. Given\\nthe in-phase component and the quadrature component\\n, we may generate the\\nnarrowband noise n(t) using the scheme shown in Figure 4.20b, which may be viewed as a\\nsynthesizer.\\nPROPERTY\\nThe in-phase and quadrature components nI(t) and nQ(t) have the same variance as the\\nnarrowband noise n(t).\\nThis property follows directly from (4.117), according to which the total area under the\\npower spectral density curve or is the same as the total area under the power\\nspectral density curve of n(t). Hence, and have the same mean-square value\\nas n(t). Earlier we showed that since n(t) has zero mean, then and have zero\\nmean, too. It follows, therefore, that and have the same variance as the\\nnarrowband noise n(t).\\nSNINI f SNQNQ f SNINI f SNQNQ f SNQNQ f nI t nQ t nI t nQ t Figure 4.20 (a) Extraction of in-phase and quadrature components of a narrowband process.\\n(b) Generation of a narrowband process from its in-phase and quadrature components.\\nnI t nQ t nI t nQ t nI t nQ t nI t nQ t Low-pass filter Low-pass filter n(t) nI(t) nI(t) n1(t) nQ(t) nQ(t) n(t) + _ (a) cos(2fc t) sin(2fc t) (b) cos(2fc t) sin(2fc t) 1 2 1 2 -  n2(t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 207,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '188 Chapter 4 Stochastic Processes PROPERTY 6 The cross-spectral densities of the in-phase and quadrature components of a narrowband\\nnoise are purely imaginary, as shown by\\n(4.120)\\nTo prove this property, we take the Fourier transforms of both sides of (4.116), and use the\\nrelation of (4.118), obtaining\\n(4.121)\\nFollowing a procedure similar to that described for proving Property 4, we find that\\n(4.121) reduces to the form shown in (4.120).\\nPROPERTY\\nIf a narrowband noise n(t) is Gaussian with zero mean and a power spectral density\\nSNN(f) that is locally symmetric about the midband frequency fc, then the in-phase noise and the quadrature noise are statistically independent.\\nTo prove this property, we observe that if SNN(f) is locally symmetric about fc, then\\n(4.122)\\nConsequently, we find from (4.120) that the cross-spectral densities of the in-phase and\\nquadrature components, and\\n, are zero for all frequencies. This, in turn, means\\nthat the cross-correlation functions and are zero for all , as shown by\\n(4.123)\\nwhich implies that the random variables NI(tk + ) and NQ(tk) (obtained by observing the\\nin-phase component at time tk +  and observing the quadrature component at time tk\\nrespectively) are orthogonal for all .\\nThe narrowband noise n(t) is assumed to be Gaussian with zero mean; hence, from\\nProperties 1 and 2 it follows that both NI(tk + ) and NQ(tk) are also Gaussian with zero\\nmean. We thus conclude that because NI(tk + ) and NQ(tk) are orthogonal and have zero\\nmean, they are uncorrelated, and being Gaussian, they are statistically independent for all\\n. In other words, the in-phase component and the quadrature component are\\nstatistically independent.\\nIn light of Property 7, we may express the joint probability density function of the\\nrandom variables NI(tk + ) and NQ(tk) (for any time shift ) as the product of their\\nindividual probability density functions, as shown by',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 208,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'random variables NI(tk + ) and NQ(tk) (for any time shift ) as the product of their\\nindividual probability density functions, as shown by\\nSNINQ f S - NQNI f = j SN f fc +   SN f fc -   -  , BfB  - 0, otherwise    = SNINQ f S - NQNI f = j 2--- SNN f fc -   SNN f fc +   -   - = j 2--- SNN f fc -   f fc -   SNN f fc +   f fc +   sgn + sgn   + j 2---SNN f fc +  1 f fc +   sgn +   j 2---SNN f fc -  1 f fc -   sgn -   - = nI t nQ t SNN f fc -   SNN f fc +   BfB  -  = nI t nQ t SNINQ f SNQNI f \\x03 NI tk  +  NQ tk  +     0 = nI t nQ t',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 208,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '13 Narrowband Noise\\n189 (4.124) where is the variance of the original narrowband noise n(t). Equation (4.124) holds if,\\nand only if, the spectral density SNN(f) or n(t) is locally symmetric about fc. Otherwise,\\nthis relation holds only for  = 0 or those values of  for which and are uncorrelated. Summarizing Remarks To sum up, if the narrowband noise n(t) is zero mean, weakly stationary, and Gaussian,\\nthen its in-phase and quadrature components and are both zero mean, jointly\\nstationary, and jointly Gaussian. To evaluate the power spectral density of or\\n,\\nwe may proceed as follows:\\nShift the positive frequency portion of the power spectral density SNN(f) of the\\noriginal narrowband noise n(t) to the left by fc.\\nShift the negative frequency portion of SNN(f) to the right by fc.\\nAdd these two shifted spectra to obtain the desired or . EXAMPLE 12 Ideal Band-pass Filtered White Noise\\nConsider a white Gaussian noise of zero mean and power spectral density N02, which is\\npassed through an ideal band-pass filter of passband magnitude response equal to one,\\nmidband frequency fc, and bandwidth 2B. The power spectral density characteristic of the\\nfiltered noise n(t) is, therefore, as shown in Figure 4.21a. The problem is to determine the\\nautocorrelation functions of n(t) and those of its in-phase and quadrature components.\\nThe autocorrelation function of n(t) is the inverse Fourier transform of the power\\nspectral density characteristic shown in Figure 4.21a, as shown by\\n(4.125)\\nwhich is plotted in Figure 4.21b.\\nThe spectral density characteristic of Figure 4.21a is symmetric about fc. The\\ncorresponding spectral density characteristics of the in-phase noise component and\\nthe quadrature noise component are equal, as shown in Figure 4.21c. Scaling the\\nresult of Example 10 by a factor of two in accordance with the spectral characteristics of',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 209,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the quadrature noise component are equal, as shown in Figure 4.21c. Scaling the\\nresult of Example 10 by a factor of two in accordance with the spectral characteristics of\\nfNI tk  +  NQ tk   nI n, Q   fNI tk  +  nI  fNQ tk nQ   = 1 2 -------------- nI 2 22 --------- -       1 2 -------------- nQ 2 22 --------- -       exp exp = 1 22 ------------- nI 2 nQ 2 + 22 ------------------ -       = 2 nI t nQ t nI t nQ t nI t nQ t SNINI f SNQNQ f RNN   N0 2 ------ j2f   exp df N0 2 ------ j2f   exp df fc B - fc+B  + fc B - - fc - +B  = N0B sinc 2B   j2fc -   j2fc   exp + exp   = 2 = N0B sinc 2B   2fc   cos nI t nQ t',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 209,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '190 Chapter 4 Stochastic Processes Figure 4.21a and 4.21c, we find that the autocorrelation function of or is given by (4.126) Figure 4.21 Characteristics of ideal band-pass filtered white noise: (a) power\\nspectral density, (b) autocorrelation function, (c) power spectral density of in-phase\\nand quadrature components.\\nnI t nQ t RNINI  RNQNQ  2N0B sinc 2B   = = 2N0B fc f f SNN( f ) (a) 2B -fc 1 fc 0 1 B N0 2 N0 (c) SNINI( f ) = SNQNQ( f )\\n0 B -B (b) RNN( )',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 210,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '13 Narrowband Noise Representation of Narrowband Noise in Terms of Envelope and\\nPhase Components\\nIn the preceding subsection we used the Cartesian representation of a narrowband noise\\nn(t) in terms of its in-phase and quadrature components. In this subsection we use the\\npolar representation of the noise n(t) in terms of its envelope and phase components, as\\nshown by (4.127) where (4.128) and (4.129) The function r(t) is the envelope of n(t) and the function is the phase of n(t).\\nThe probability density functions of r(t) and may be obtained from those of\\nand as follows. Let NI and NQ denote the random variables obtained by\\nsampling (at some fixed time) the stochastic processes represented by the sample\\nfunctions and respectively. We note that NI and NQ are independent Gaussian\\nrandom variables of zero mean and variance\\n, so we may express their joint probability\\ndensity function as\\n(4.130)\\nAccordingly, the probability of the joint event that NI lies between nI and nI + dnI and NQ\\nlies between nQ + dnQ (i.e., the pair of random variables NI and NQ lies jointly inside the\\nshaded area of Figure 4.22a) is given by\\n(4.131) n t r t 2fct t +   cos = r t nI 2 t nQ 2 t +   1 2  = t nQ t nI t ------------- 1 - tan = t t nI t nQ t nI t nQ t 2 fNI NQ  nI nQ    1 22 ------------- nI 2 nQ 2 + 22 ------------------ -       exp = fNI NQ  nI nQ    dnI dnQ 1 22 ------------- nI 2 nQ 2 + 22 ------------------ -       dnIdnQ exp = Figure 4.22 Illustrating the coordinate system\\nfor representation of narrowband\\nnoise: (a) in terms of in-phase and\\nquadrature components; (b) in\\nterms of envelope and phase.\\nnI r r dnQ nQ dnI  (a) 0 nI nQ dr  d (b) 0',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 211,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '192 Chapter 4 Stochastic Processes where dnI and dnQ are incrementally small. Now, define the transformations (see Figure 4.22b)\\n(4.132)\\n(4.133)\\nIn a limiting sense, we may equate the two incremental areas shown shaded in parts a and\\nb of Figure 4.22 and thus write\\n(4.134)\\nNow, let R and denote the random variables obtained by observing (at some fixed time\\nt) the stochastic processes represented by the envelope r(t) and phase respectively.\\nThen substituting (4.132)-(4.134) into (4.131), we find that the probability of the random\\nvariables R and lying jointly inside the shaded area of Figure 4.22b is equal to the\\nexpression\\nThat is, the joint probability density function of R and is given by\\n(4.135)\\nThis probability density function is independent of the angle\\n, which means that the\\nrandom variables R and are statistically independent. We may thus express\\nas the product of the two probability density functions: and . In particular, the random variable representing the phase is uniformly distributed inside the interval\\n[0, 2], as shown by\\n(4.136)\\nThis result leaves the probability density function of the random variable R as\\n(4.137)\\nwhere is the variance of the original narrowband noise n(t). A random variable having\\nthe probability density function of (4.137) is said to be Rayleigh distributed.10\\nFor convenience of graphical presentation, let\\n(4.138) (4.139) nI r  cos = nQ r  sin = dnIdnQ r dr d =  t  r 22 ------------- r2 22 --------- -       dr d exp  fR   r     r 22 ------------- r2 22 --------- -       exp =   fR   r     fR r f    f   1 2 ------, 0  2   0, elsewhere      = fR r r 2 ------ r2 22 --------- -      , exp r 0  0, elsewhere        = 2 v r --- = fV v  fR r =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 212,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '14 Sine Wave Plus Narrowband Noise Then, we may rewrite the Rayleigh distribution of (4.137) in the normalized form\\n(4.140)\\nEquation (4.140) is plotted in Figure 4.23. The peak value of the distribution fV(v) occurs\\nat v = 1 and is equal to 0.607. Note also that, unlike the Gaussian distribution, the Rayleigh\\ndistribution is zero for negative values of v, which follows naturally from the fact that the\\nenvelope r(t) of the narrowband noise n(t) can only assume nonnegative values. Sine Wave Plus Narrowband Noise\\nSuppose next that we add the sinusoidal wave to the narrowband noise n(t),\\nwhere A and fc are both constants. We assume that the frequency of the sinusoidal wave is\\nthe same as the nominal carrier frequency of the noise. A sample function of the sinusoidal\\nwave plus noise is then expressed by\\n(4.141)\\nRepresenting the narrowband noise n(t) in terms of its in-phase and quadrature\\ncomponents, we may write\\n(4.142) where (4.143) We assume that n(t) is Gaussian with zero mean and variance\\n. Accordingly, we may\\nstate the following:\\nBoth and nQ(t) are Gaussian and statistically independent.\\nThe mean of is A and that of nQ(t) is zero.\\nThe variance of both and nQ(t) is\\n.\\nFigure 4.23 Normalized Rayleigh distribution.\\nfV (v) 0 1 2 3 0.2 0.4 0.6 0.8 v fV v  v v2 2----- -    , exp v 0  0, elsewhere      = A 2fct   cos x t A 2fct   n t + cos = x t nI t 2fct   nQ t 2fct   sin - cos = nI t A nl t + = 2 nI t nI t nI t 2',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 213,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '194 Chapter 4 Stochastic Processes We may, therefore, express the joint probability density function of the random variables and NQ, corresponding to and , as follows: (4.144) Let r(t) denote the envelope of x(t) and denote its phase. From (4.142), we thus find\\nthat (4.145) and (4.146) Following a procedure similar to that described in Section 4.12 for the derivation of the\\nRayleigh distribution, we find that the joint probability density function of the random\\nvariables R and\\n, corresponding to r(t) and for some fixed time t, is given by\\n(4.147)\\nWe see that in this case, however, we cannot express the joint probability density function as a product\\n, because we now have a term involving the values of\\nboth random variables multiplied together as\\n. Hence, R and are dependent\\nrandom variables for nonzero values of the amplitude A of the sinusoidal component.\\nWe are interested, in particular, in the probability density function of R. To determine\\nthis probability density function, we integrate (4.147) over all possible values of\\n,\\nobtaining the desired marginal density (4.148)\\nAn integral similar to that in the right-hand side of (4.148) is referred to in the literature as\\nthe modified Bessel function of the first kind of zero order (see Appendix C); that is,\\n(4.149)\\nThus, letting x = Ar\\n, we may rewrite (4.148) in the compact form\\n(4.150)\\nThis new distribution is called the Rician distribution.11\\nNI  nI t nQ t fNI NQ  nI nQ    1 22 ------------- nI A -  2 nQ 2 + 22 ------------------------------------\\n- exp = t r t nI t  2 nQ 2 t +       1 2  = t nQ t nI t ------------- 1 - tan =  t fR   r     r 22 ------------- r2 A2 2Ar  cos - + 22 ------------------------------------------------',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 214,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '- exp = t r t nI t  2 nQ 2 t +       1 2  = t nQ t nI t ------------- 1 - tan =  t fR   r     r 22 ------------- r2 A2 2Ar  cos - + 22 ------------------------------------------------\\n-       exp = fR   r     fR rf   r  cos   fR r fR   r     d 0 2  = r 22 ------------- r2 A2 + 22 ----------------- -       Ar  cos 2 -------------------     d exp 0 2  exp = I0 x  1 2 ------ x  cos   d exp 0 2  = 2 fR r r 2 ------ r2 A2 + 22 ----------------- -      I0 Ar 2 ------     r 0   exp =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 214,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '14 Sine Wave Plus Narrowband Noise As with the Rayleigh distribution, the graphical presentation of the Rician distribution\\nis simplified by putting\\n(4.151) (4.152) (4.153) Then we may express the Rician distribution of (4.150) in the normalized form\\n(4.154)\\nwhich is plotted in Figure 4.24 for the values 0, 1, 2, 3, 5, of the parameter a.12 Based on\\nthese curves, we may make two observations:\\nWhen the parameter a = 0, and therefore I0(0) = 1, the Rician distribution reduces to\\nthe Rayleigh distribution.\\nThe envelope distribution is approximately Gaussian in the vicinity of v = a when a\\nis large; that is, when the sine-wave amplitude A is large compared with , the\\nsquare root of the average power of the noise n(t). Summary and Discussion\\nMuch of the material presented in this chapter has dealt with the characterization of a\\nparticular class of stochastic processes known to be weakly stationary. The implication of\\nweak stationarity is that we may develop a partial description of a stochastic process in\\nterms of two ensemble-averaged parameters: (1) a mean that is independent of time and\\n(2) an autocorrelation function that depends only on the difference between the times at\\nwhich two samples of the process are drawn. We also discussed ergodicity, which enables\\nFigure 4.24 Normalized Rician distribution.\\nfV (v) 0 1 2 5 3 2 1 3 4 5 6 7 8 0.1 0.2 0.3 0.4 0.5 0.6 a = 0 vvr --- = a A --- = fV v  fR r = fV v  v v2 a2 + 2 ------------------ -    I0 av   exp =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 215,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '196 Chapter 4 Stochastic Processes us to use time averages as estimates of these parameters. The time averages are\\ncomputed using a sample function (i.e., single waveform realization) of the stochastic\\nprocess, evolving as a function of time.\\nThe autocorrelation function RXX\\n, expressed in terms of the time shift , is one way\\nof describing the second-order statistic of a weakly (wide-sense) stationary process X(t).\\nAnother equally important parameter, if not more so, for describing the second-order\\nstatistic of X(t) is the power spectral density SXX(f), expressed in terms of the frequency f.\\nThe Fourier transform and the inverse Fourier transform formulas that relate these two\\nparameters to each other constitute the celebrated Wiener-Khintchine equations. The first\\nof these two equations, namely (4.42), provides the basis for a definition of the power\\nspectral density SXX(f) as the Fourier transform of the autocorrelation function RXX\\n,\\ngiven that RXX is known. This definition was arrived at by working on the output of a\\nlinear time-invariant filter, driven by a weakly stationary process X(t). We also described\\nanother definition for the power spectral density SXX(f), described in (4.70); this second\\ndefinition was derived by working directly on the process X(t).\\nAnother celebrated theorem discussed in the chapter is the Wiener-Khintchine\\ntheorem, which provides the necessary and sufficient condition for confirming the\\nfunction XX as the normalized autocorrelation function of a weakly stationary process\\nX(t), provided that it satisfies the Fourier-Stieltjes transform, described in (4.60).\\nThe stochastic-process theory described in this chapter also included the topic of cross-\\npower spectral densities SXY(f) and SYX(f), involving a pair of jointly weakly stationary\\nprocesses X(t) and Y(t), and how these two frequency-dependent parameters are related to\\nthe respective cross-correlation functions RXY and RYX\\n.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 216,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'processes X(t) and Y(t), and how these two frequency-dependent parameters are related to\\nthe respective cross-correlation functions RXY and RYX\\n.\\nThe remaining part of the chapter was devoted to the statistical characterization of\\ndifferent kinds of stochastic processes:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 216,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'The Poisson process, which is well-suited for the characterization of random-\\ncounting processes.\\n\\nThe ubiquitous Gaussian process, which is widely used in the statistical study of\\ncommunication systems.\\n\\nThe two kinds of electrical noise, namely shot noise and thermal noise.\\n\\nWhite noise, which plays a fundamental role in the noise analysis of communication\\nsystems similar to that of the impulse function in the study of linear systems.\\n\\nNarrowband noise, which is produced by passing white noise through a linear band-\\npass filter. Two different methods for the description of narrowband noise were\\npresented: one in terms of the in-phase and quadrature components and the other in\\nterms of the envelope and phase.\\n\\nThe Rayleigh distribution, which is described by the envelope of a narrowband noise\\nprocess.\\n\\nThe Rician distribution, which is described by the envelope of narrowband noise\\nplus a sinusoidal component, with the midband frequency of the narrowband noise\\nand the frequency of the sinusoidal component being coincident.\\nWe conclude this chapter on stochastic processes by including Table 4.1, where we present\\na graphical summary of the autocorrelation functions and power spectral densities of\\nimportant stochastic processes. All the processes described in this table are assumed to\\nhave zero mean and unit variance. This table should give the reader a feeling for (1) the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 216,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': 'Summary and Discussion interplay between the autocorrelation function and power spectral density of a stochastic\\nprocess and (2) the role of linear filtering in shaping the autocorrelation function or,\\nequivalently, the power spectral density of a white-noise process.\\nTable 4.1 Graphical summary of autocorrelation functions and power\\nspectral densities of random processes of zero mean and unit variance\\n-1 -1 0 1 1 -0.5 0.5  0 -1.0 1.0 f -4 -2 0 2 4 1  -2 -1 0 1.0 2.0 1 f -4 -2 0 2 4 1  -0.5 0 2.0 0.5 f -4 -2 2 4 1  0 1 f -1.05 0 0.05 -1.0 -4  -2 2 4 1 0 -1.0 f 0 1.0 0.5 -1.0 -4  -2 2 4 1 0 -1.0 f 0 1.0',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 217,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '198 Chapter 4 Stochastic Processes Problems Stationarity and Ergodicity Consider a pair of stochastic processes X(t) and Y(t). In the strictly stationary world of stochastic\\nprocesses, the statistical independence of X(t) and Y(t) corresponds to their uncorrelatedness in the\\nworld of weakly stationary processes. Justify this statement. Let X1, X2, , Xk denote a sequence obtained by uniformly sampling a stochastic process X(t). The\\nsequence consists of statistically independent and identically distributed (iid) random variables, with\\na common cumulative distribution function FX(x), mean  and variance 2. Show that this sequence\\nis strictly stationary. A stochastic process X(t) is defined by\\nwhere A is a Gaussian-distributed random variable of zero mean and variance\\n. The process X(t) is\\napplied to an ideal integrator, producing the output\\na. Determine the probability density function of the output Y(t) at a particular time tk.\\nb. Determine whether or not Y(t) is strictly stationary. Continuing with Problem 4.3, determine whether or not the integrator output Y(t) produced in\\nresponse to the input process X(t) is ergodic.\\nAutocorrelation Function and Power Spectral Density The square wave x(t) of Figure P4.5, having constant amplitude A, period T0, and time shift td,\\nrepresents the sample function of a stochastic process X(t). The time shift td is a random variable,\\ndescribed by the probability density function\\na. Determine the probability density function of the random variable X(tk), obtained by sampling\\nthe stochastic process X(t) at time tk.\\nb. Determine the mean and autocorrelation function of X(t) using ensemble averaging.\\nc. Determine the mean and autocorrelation function of X(t) using time averaging.\\nd. Establish whether or not X(t) is weakly stationary. In what sense is it ergodic? A binary wave consists of a random sequence of symbols 1 and 0, similar to that described in',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 218,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'd. Establish whether or not X(t) is weakly stationary. In what sense is it ergodic? A binary wave consists of a random sequence of symbols 1 and 0, similar to that described in\\nExample 6, with one basic difference: symbol 1 is now represented by a pulse of amplitude A volts,\\nX t A 2fct   cos = A 2 Y t X   d 0 t  = Figure P4.5 fTd td   1 T0 -----, 1 2---T0 td 1 2---T0   - 0, otherwise      = td A x(t) t 0 T0',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 218,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems and symbol 0 is represented by zero volts. All other parameters are the same as before. Show that\\nthis new random binary wave X(t) is characterized as follows:\\na. The autocorrelation function is\\nb. The power spectral density is\\nWhat is the percentage power contained in the dc component of the binary wave? The output of an oscillator is described by\\nwhere the amplitude A is constant, and F and  are independent random variables. The probability\\ndensity function of  is defined by\\nFind the power spectral density of X(t) in terms of the probability density function of the frequency F.\\nWhat happens to this power spectral density when the randomized frequency F assumes a constant\\nvalue? Equation (4.70) presents the second of two definitions introduced in the chapter for the power\\nspectral density function, SXX(f), pertaining to a weakly stationary process X(t). This definition\\nreconfirms Property 3 of SXX(f), as shown in (4.71).\\na. Using (4.70), prove the other properties of SXX(f): zero correlation among frequency\\ncomponents, zero-frequency value, nonnegativity, symmetry, and normalization, which were\\ndiscussed in Section 4.8.\\nb. Starting with (4.70), derive (4.43) that defines the autocorrelation function RXX of the\\nstationary process X(t) in terms of SXX(f). In the definition of (4.70) for the power spectral density of a weakly stationary process X(t), it is not\\npermissible to interchange the order of expectation and limiting operations. Justify the validity of\\nthis statement.\\nThe Wiener-Khintchine Theorem\\nIn the next four problems we explore the application of the Wiener-Khintchine theorem of (4.60) to\\nsee whether a given function \\n, expressed in terms of the time shift , is a legitimate normalized\\nautocorrelation function or not. Consider the Fourier transformable function\\nBy inspection, we see that f is an odd function of . It cannot, therefore, be a legitimate',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 219,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'autocorrelation function or not. Consider the Fourier transformable function\\nBy inspection, we see that f is an odd function of . It cannot, therefore, be a legitimate\\nautocorrelation function as it violates a fundamental property of the autocorrelation function. Apply\\nthe Wiener-Khintchine theorem to arrive at this same conclusion.\\nRXX   A2 4------ A2 4------ 1  T------ -    , +  T  A2 4------,  T         = SXX f A2 4------f A2T 4 ---------sinc2 fT   + = X t A Ft  +   cos = f  1 2 ------, 0  2   0, otherwise      =   f   A2 2------ 2fc   for all  sin =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 219,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '200 Chapter 4 Stochastic Processes 4.11 Consider the infinite series which is an even function of , thereby satisfying the symmetry property of the autocorrelation\\nfunction. Apply the Wiener-Khintchine theorem to confirm that f is indeed a legitimate\\nautocorrelation function of a weakly stationary process. Consider the Gaussian function\\nwhich is Fourier transformable. Moreover, it is an even function of , thereby satisfying the\\nsymmetry property of the autocorrelation function around the origin  = 0. Apply the Wiener-\\nKhintchine theorem to confirm that f is indeed a legitimate normalized autocorrelation function\\nof a weakly stationary process. Consider the Fourier transformable function\\nwhich is an odd function of . It cannot, therefore, be a legitimate autocorrelation function. Apply\\nthe Wiener-Khintchine theorem to arrive at this same conclusion.\\nCross-correlation Functions and Cross-spectral Densities Consider a pair of weakly stationary processes X(t) and Y(t). Show that the cross-correlations\\nRXY and RYX of these two processes have the following properties:\\na. RXY = RYX(-) b. where RXX and RYY are the autocorrelation functions of X(t) and Y(t) respectively. A weakly stationary process X(t), with zero mean and autocorrelation function RXX\\n, is passed\\nthrough a differentiator, yielding the new process\\na. Determine the autocorrelation function of Y(t).\\nb. Determine the cross-correlation function between X(t) and Y(t). Consider two linear filters connected in cascade as in Figure P4.16. Let X(t) be a weakly stationary\\nprocess with autocorrelation function RXX\\n. The weakly stationary process appearing at the first\\nfilter output is denoted by V(t) and that at the second filter output is denoted by Y(t).\\na. Find the autocorrelation function of Y(t).',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 220,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '. The weakly stationary process appearing at the first\\nfilter output is denoted by V(t) and that at the second filter output is denoted by Y(t).\\na. Find the autocorrelation function of Y(t).\\nb. Find the cross-correlation function RVY of V(t) and Y(t). f  A2 2------ 1 1 2! ----- 2fc  2 - 1 4! ----- 2fc  4  - + for all  =  f  2 -   for all  exp =  f    1 2--- -    ,  1 2--- =  1 2--- +    , -  1 2--- - = 0, otherwise          =    RXY  1 2--- RXX 0  RYY 0  +       Y t d dt -----X t =  Figure P4.16  h1(t) V(t) h2(t) X(t) Y(t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 220,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems 201 4.17 A weakly stationary process X(t) is applied to a linear time-invariant filter of impulse response h(t),\\nproducing the output Y(t).\\na. Show that the cross-correlation function RYX of the output Y(t) and the input X(t) is equal to\\nthe impulse response h convolved with the autocorrelation function RXX of the input, as\\nshown by\\nShow that the second cross-correlation function RXY is b. Find the cross-spectral densities SYX(f) and SXY(f).\\nc. Assuming that X(t) is a white-noise process with zero mean and power spectral density N02,\\nshow that\\nComment on the practical significance of this result.\\nPoisson Process The sample function of a stochastic process X(t) is shown in Figure P4.18a, where we see that the\\nsample function x(t) assumes the values 1 in a random manner. It is assumed that at time t = 0, the\\nvalues X(0) = -1 and X(1) = 1 are equiprobable. From there on, the changes in X(t) occur in\\naccordance with a Poisson process of average rate . The process X(t), described herein, is\\nsometimes referred to as a telegraph signal.\\na. Show that, for any time t  0, the values X(t) = -1 and X(t) = +1 are equiprobable.\\nb. Building on the result of part a, show that the mean of X(t) is zero and its variance is unity.\\nc. Show that the autocorrelation function of X(t) is given by\\nd. The process X(t) is applied to the simple low-pass filter of Figure P4.18b. Determine the power\\nspectral density of the process Y(t) produced at the filter output.\\n   RYX   h u RXX  u -   du  -   =  RXY   h u -  RXX  u -   du  -   = RYX   N0 2 ------h   = RXX  2 -   exp = Figure P4.18 Poisson process X(t) Output process Y(t) T1 +1 0 -1 T2 T3 T4 T5 T6 T7 T8 Low-pass filter H(f) x(t) Time t (b) (a)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 221,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '202 Chapter 4 Stochastic Processes Gaussian Process 4.19 Consider the pair of integrals\\nand\\nwhere X(t) is a Gaussian process and h1(t) and h2(t) are two different weighting functions. Show\\nthat the two random variables Y1 and Y2, resulting from the integrations, are jointly Gaussian. A Gaussian process X(t), with zero mean and variance\\n, is passed through a full-wave rectifier,\\nwhich is described by the input-output relationship of Figure P4.20. Show that the probability\\ndensity function of the random variable Y(tk), obtained by observing the stochastic process Y(t)\\nproduced at the rectifier output at time tk, is one sided, as shown by\\nConfirm that the total area under the graph of is unity. A stationary Gaussian process X(t), with mean X and variance\\n, is passed through two linear\\nfilters with impulse responses h1(t) and h2(t), yielding the processes Y(t) and Z(t), as shown in\\nFigure P4.21. Determine the necessary and sufficient conditions, for which Y(t1) and Z(t2) are\\nstatistically independent Gaussian processes.\\nWhite Noise Consider the stochastic process\\nwhere W(t) is a white-noise process of power spectral density N02 and the parameters a and t0 are\\nconstants. Y1 h1 tX t dt  -   = Y2 h2 tX t dt  -   = X 2 fY tk y  2 --- 1 X ------ y2 2X 2 ---------- -      , exp y 0  0, y 0         = Figure P4.20 Figure P4.21 fY tk y  Y 0 X Y = X Y = -X X 2 h2(t) X(t) Z(t) h1(t) Y(t) X t W t a + W t t0 -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 222,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Problems a. Determine the autocorrelation function of the process X(t), and sketch it.\\nb. Determine the power spectral density of the process X(t), and sketch it. The process\\ndescribes a sinusoidal process that is corrupted by an additive white-noise process W(t) of known\\npower spectral density N02. The phase of the sinusoidal process, denoted by\\n, is a uniformly\\ndistributed random variable, defined by\\nThe amplitude A and frequency f0 are both constant but unknown.\\na. Determine the autocorrelation function of the process X(t) and its power spectral density.\\nb. How would you use the two results of part a to measure the unknown parameters A and f0? A white Gaussian noise process of zero mean and power spectral density N02 is applied to the\\nfiltering scheme shown in Figure P4.24. The noise at the low-pass filter output is denoted by n(t).\\na. Find the power spectral density and the autocorrelation function of n(t).\\nb. Find the mean and variance of n(t).\\nc. What is the maximum rate at which n(t) can be sampled so that the resulting samples are\\nessentially uncorrelated? Let X(t) be a weakly stationary process with zero mean, autocorrelation function RXX\\n, and power\\nspectral density SXX(f). We are required to find a linear filter with impulse response h(t), such that\\nthe filter output is X(t) when the input is white-noise of power spectral density N02.\\na. Determine the condition that the impulse response h(t) must satisfy in order to achieve this\\nrequirement.\\nb. What is the corresponding condition on the transfer function H(f) of the filter?\\nc. Using the Paley-Wiener criterion discussed in Chapter 2, find the requirement on SXX(f) for the\\nfilter to be causal. X t A 2f0t  +   W t + cos =  f  1 2 ------ for      - 0 otherwise      = Figure P4.24 |H1( f )| 1.0 cos (2fct) White noise Output Band-pass filter H1( f ) Low-pass filter H2( f ) fc - fc f f 0 (b) (a) |H2( f )| 1.0 2B 2B 0',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 223,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '204 Chapter 4 Stochastic Processes Narrowband Noise 4.26 Consider a narrowband noise n(t) with its Hilbert transform denoted by\\n.\\na. Show that the cross-correlation functions of n(t) and are given by and where is the Hilbert transform of the autocorrelation function RNN of n(t).\\nHint: use the formula\\nb. Show that, for  = 0, we have\\n. A narrowband noise n(t) has zero mean and autocorrelation function RNN(). Its power spectral\\ndensity SNN(f) is centered about fc. The in-phase and quadrature components, and\\n, of\\nn(t) are defined by the weighted sums\\nand\\nwhere is the Hilbert transform of the noise n(t). Using the result obtained in part a of Problem\\n26, show that nI(t) and nQ(t) have the following autocorrelation functions:\\nand\\nRayleigh and Rician Distributions Consider the problem of propagating signals through so-called random or fading communications\\nchannels. Examples of such channels include the ionosphere from which short-wave (high-\\nfrequency) signals are reflected back to the earth producing long-range radio transmission, and\\nunderwater communications. A simple model of such a channel is shown in Figure P4.28, which\\nconsists of a large collection of random scatterers, with the result that a single incident beam is\\nconverted into a correspondingly large number of scattered beams at the receiver. The transmitted\\nsignal is equal to\\n. Assume that all scattered beams travel at the same mean velocity.\\nHowever, each scattered beam differs in amplitude and phase from the incident beam, so that the kth\\nscattered beam is given by\\n, where the amplitude Ak and the phase\\nvary\\nslowly and randomly with time. In particular, assume that the are all independent of one another\\nand uniformly distributed random variables.\\na. With the received signal denoted by\\nshow that the random variable R, obtained by observing the envelope of the received signal at\\ntime t, is Rayleigh-distributed, and that the random variable\\n, obtained by observing the phase',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 224,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'show that the random variable R, obtained by observing the envelope of the received signal at\\ntime t, is Rayleigh-distributed, and that the random variable\\n, obtained by observing the phase\\nat some fixed time, is uniformly distributed.\\nb. Assuming that the channel includes a line-of-sight path, so that the received signal contains a\\nsinusoidal component of frequency fc, show that in this case the envelope of the received signal is\\nRician distributed. n t n t RNN  R - NN  = RN N  R NN  = R NN   n t 1 --- n   t  - ----------- d  -   = RNN 0  RN N 0 = = nI t nQ t nI t n t 2fct   n t 2fct   sin + cos = nQ t n t 2fct   n t 2fct   sin - cos = n t RNINI  RNQNQ  RNN   2fc   R NN   2fc   sin + cos = = RNINQ  RNQNI  - RNN  2fc   R NN   2fc   cos - sin = = A j2fct   exp Ak j2fct jk +   exp k k xt r t j2fct t +   exp =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 224,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Notes 205 4.29 Referring back to the graphical plots of Figure 4.23, describing the Rician envelope distribution for\\nvarying parameter a, we see that for the parameter a = 5, this distribution is approximately Gaussian.\\nJustify the validity of this statement.\\nNotes\\nStochastic is of Greek origin.\\nFor rigorous treatment of stochastic processes, see the classic books by Doob (1953), Love\\n(1963), and Cramr and Leadbetter (1967).\\nTraditionally, (4.42) and (4.43) have been referred to in the literature as the Wiener-Khintchine\\nrelations in recognition of pioneering work done by Norbert Wiener and A.I. Khintchine; for their\\noriginal papers, see Wiener (1930) and Khintchine (1934). The discovery of a forgotten paper by\\nAlbert Einstein on time-series analysis (delivered at the Swiss Physical Societys February\\nmeeting in Basel) reveals that Einstein had discussed the autocorrelation function and its relationship\\nto the spectral content of a time series many years before Wiener and Khintchine. An English\\ntranslation of Einsteins paper is reproduced in the IEEE ASSP Magazine, vol. 4, October 1987. This\\nparticular issue also contains articles by W.A. Gardner and A.M. Yaglom, which elaborate on\\nEinsteins original work.\\nFor a mathematical proof of the Wiener-Khintchine theorem, see Priestley (1981).\\nEquation (4.70) provides the mathematical basis for estimating the power spectral density of a\\nweakly stationary process. There is a plethora of procedures that have been formulated for\\nperforming this estimation. For a detailed treatment of reliable procedures to do the estimation, see\\nthe book by Percival and Walden (1993).\\nThe Poisson process is named in honor of S.D. Poisson. The distribution bearing his name first\\nappeared in an exposition by Poisson on the role of probability in the administration of justice. The\\nclassic book on Poisson processes is Snyder (1975). For an introductory treatment of the subject, see\\nBertsekas and Tsitsiklis (2008: Chapter 6).',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 225,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'classic book on Poisson processes is Snyder (1975). For an introductory treatment of the subject, see\\nBertsekas and Tsitsiklis (2008: Chapter 6).\\nThe Gaussian distribution and the associated Gaussian process are named after the great\\nmathematician C.F. Gauss. At age 18, Gauss invented the method of least squares for finding the\\nbest value of a sequence of measurements of some quantity. Gauss later used the method of least\\nsquares in fitting orbits of planets to data measurements, a procedure that was published in 1809 in\\nhis book entitled Theory of Motion of the Heavenly Bodies. In connection with the error of\\nobservation, he developed the Gaussian distribution.\\nFigure P4.28 Transmitting antenna Receiving antenna Random medium Scattered beams Incident beam',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 225,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '206 Chapter 4 Stochastic Processes 8. Thermal noise was first studied experimentally by J.B. Johnson in 1928, and for this reason it is\\nsometimes referred to as the Johnson noise. Johnsons experiments were confirmed theoretically by\\nNyquist (1928a).\\nFor further insight into white noise, see Appendix I on generalized random processes in the book\\nby Yaglom (1962).\\nThe Rayleigh distribution is named in honor of the English physicist J.W. Strutt, Lord Rayleigh.\\nThe Rician distribution is named in honor of S.O. Rice (1945).\\nIn mobile wireless communications to be covered in Chapter 9, the sinusoidal term in (4.141) is viewed as a line-of-sight (LOS) component of average power A22 and the\\nadditive noise term n(t) is viewed as a Gaussian diffuse component of average power\\n, with both\\nbeing assumed to have zero mean. In such an environment, it is the Rice factor K that is used to\\ncharacterize the Rician distribution. Formally, we write\\nIn effect,\\n. Thus for the graphical plots of Figure 4.23, the running parameter K would\\nassume the values 0, 0.5, 2, 4.5, 12.5.\\nA 2fct   cos 2 K Average power of the LOS component\\nAverage power of the diffuse component = A2 22 --------- = K a2 2----- =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 226,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '207 CHAPTER 5 Information Theory 5.1 Introduction As mentioned in Chapter 1 and reiterated along the way, the purpose of a communication\\nsystem is to facilitate the transmission of signals generated by a source of information over a\\ncommunication channel. But, in basic terms, what do we mean by the term information? To\\naddress this important issue, we need to understand the fundamentals of information theory.1\\nThe rationale for studying the fundamentals of information theory at this early stage in\\nthe book is threefold:\\nInformation theory makes extensive use of probability theory, which we studied in\\nChapter 3; it is, therefore, a logical follow-up to that chapter.\\nIt adds meaning to the term information used in previous chapters of the book.\\nMost importantly, information theory paves the way for many important concepts\\nand topics discussed in subsequent chapters.\\nIn the context of communications, information theory deals with mathematical modeling\\nand analysis of a communication system rather than with physical sources and physical\\nchannels. In particular, it provides answers to two fundamental questions (among others):\\nWhat is the irreducible complexity, below which a signal cannot be compressed?\\nWhat is the ultimate transmission rate for reliable communication over a noisy channel?\\nThe answers to these two questions lie in the entropy of a source and the capacity of a\\nchannel, respectively:\\nEntropy is defined in terms of the probabilistic behavior of a source of information;\\nit is so named in deference to the parallel use of this concept in thermodynamics.\\nCapacity is defined as the intrinsic ability of a channel to convey information; it is\\nnaturally related to the noise characteristics of the channel.\\nA remarkable result that emerges from information theory is that if the entropy of the\\nsource is less than the capacity of the channel, then, ideally, error-free communication over',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 227,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'A remarkable result that emerges from information theory is that if the entropy of the\\nsource is less than the capacity of the channel, then, ideally, error-free communication over\\nthe channel can be achieved. It is, therefore, fitting that we begin our study of information\\ntheory by discussing the relationships among uncertainty, information, and entropy. Entropy\\nSuppose that a probabilistic experiment involves observation of the output emitted by a\\ndiscrete source during every signaling interval. The source output is modeled as a',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 227,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '208 Chapter 5 Information Theory stochastic process, a sample of which is denoted by the discrete random variable S. This\\nrandom variable takes on symbols from the fixed finite alphabet\\n(5.1) with probabilities (5.2) Of course, this set of probabilities must satisfy the normalization property\\n(5.3)\\nWe assume that the symbols emitted by the source during successive signaling intervals\\nare statistically independent. Given such a scenario, can we find a measure of how much\\ninformation is produced by such a source? To answer this question, we recognize that the\\nidea of information is closely related to that of uncertainty or surprise, as described next.\\nConsider the event S = sk, describing the emission of symbol sk by the source with\\nprobability pk, as defined in (5.2). Clearly, if the probability pk = 1 and pi = 0 for all\\n,\\nthen there is no surprise and, therefore, no information when symbol sk is emitted,\\nbecause we know what the message from the source must be. If, on the other hand, the\\nsource symbols occur with different probabilities and the probability pk is low, then there\\nis more surprise and, therefore, information when symbol sk is emitted by the source than\\nwhen another symbol si,\\n, with higher probability is emitted. Thus, the words uncer-\\ntainty, surprise, and information are all related. Before the event S = sk occurs, there is an\\namount of uncertainty. When the event S = sk occurs, there is an amount of surprise. After\\nthe occurrence of the event S = sk, there is gain in the amount of information, the essence\\nof which may be viewed as the resolution of uncertainty. Most importantly, the amount of\\ninformation is related to the inverse of the probability of occurrence of the event S = sk.\\nWe define the amount of information gained after observing the event S = sk, which\\noccurs with probability pk, as the logarithmic function2\\n(5.4)\\nwhich is often termed self-information of the event S = sk. This definition exhibits the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 228,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'occurs with probability pk, as the logarithmic function2\\n(5.4)\\nwhich is often termed self-information of the event S = sk. This definition exhibits the\\nfollowing important properties that are intuitively satisfying:\\nPROPERTY\\n(5.5)\\nObviously, if we are absolutely certain of the outcome of an event, even before it occurs,\\nthere is no information gained.\\nPROPERTY\\n(5.6)\\nThat is to say, the occurrence of an event S = sk either provides some or no information,\\nbut never brings about a loss of information.\\nPROPERTY\\n(5.7)\\nThat is, the less probable an event is, the more information we gain when it occurs.\\n s0 s1 , sK 1 -     = \\x02 S=sk   pk, = k 0 1 K 1 -   = pk k 0 = K 1 -  1, pk 0  = i k  i k  I sk   1 pk -----     log = I sk   0 for pk 1 = = I sk   0 for 0 pk 1    I sk   I si  for pk p  i',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 228,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '2 Entropy 209 PROPERTY 4 if sk and sl are statistically independent\\nThis additive property follows from the logarithmic definition described in (5.4).\\nThe base of the logarithm in (5.4) specifies the units of information measure.\\nNevertheless, it is standard practice in information theory to use a logarithm to base 2 with\\nbinary signaling in mind. The resulting unit of information is called the bit, which is a\\ncontraction of the words binary digit. We thus write\\n(5.8)\\nWhen pk = 12, we have I(sk) = 1 bit. We may, therefore, state:\\nOne bit is the amount of information that we gain when one of two possible and\\nequally likely (i.e., equiprobable) events occurs.\\nNote that the information I(sk) is positive, because the logarithm of a number less than\\none, such as a probability, is negative. Note also that if pk is zero, then the self-information assumes an unbounded value.\\nThe amount of information I(sk) produced by the source during an arbitrary signaling\\ninterval depends on the symbol sk emitted by the source at the time. The self-information\\nI(sk) is a discrete random variable that takes on the values I(s0), I(s1), , I(sK - 1) with\\nprobabilities p0, p1, , pK - 1 respectively. The expectation of I(sk) over all the probable\\nvalues taken by the random variable S is given by\\n(5.9)\\nThe quantity H(S) is called the entropy,3 formally defined as follows:\\nThe entropy of a discrete random variable, representing the output of a\\nsource of information, is a measure of the average information content per\\nsource symbol.\\nNote that the entropy H(S) is independent of the alphabet ; it depends only on the\\nprobabilities of the symbols in the alphabet  of the source.\\nProperties of Entropy\\nBuilding on the definition of entropy given in (5.9), we find that entropy of the discrete\\nrandom variable S is bounded as follows:\\n(5.10)\\nwhere K is the number of symbols in the alphabet .',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 229,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Building on the definition of entropy given in (5.9), we find that entropy of the discrete\\nrandom variable S is bounded as follows:\\n(5.10)\\nwhere K is the number of symbols in the alphabet .\\nI sk sl    I sk   I sl  + = I sk   1 pk -----     2 log = p 2 k for log - k 0 1, K 1 -   = = Isk H S  \\x03 I sk     = pkI sk   k 0 = K 1 -  = pk 1 pk -----     2 log k 0 = K 1 -  = 0 H S  K 2 log',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 229,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '210 Chapter 5 Information Theory Elaborating on the two bounds on entropy in (5.10), we now make two statements:\\nH(S) = 0, if, and only if, the probability pk = 1 for some k, and the remaining\\nprobabilities in the set are all zero; this lower bound on entropy corresponds to no\\nuncertainty.\\nH(S) = log K, if, and only if, pk = 1/K for all k (i.e., all the symbols in the source\\nalphabet  are equiprobable); this upper bound on entropy corresponds to maximum\\nuncertainty.\\nTo prove these properties of H(S), we proceed as follows. First, since each probability pk is\\nless than or equal to unity, it follows that each term pklog2(1/pk) in (5.9) is always\\nnonnegative, so H(S) > 0. Next, we note that the product term pk log2(1/pk) is zero if, and\\nonly if, pk = 0 or 1. We therefore deduce that H(S) = 0 if, and only if, pk = 0 or 1 for some\\nk and all the rest are zero. This completes the proofs of the lower bound in (5.10) and\\nstatement 1.\\nTo prove the upper bound in (5.10) and statement 2, we make use of a property of the\\nnatural logarithm:\\n(5.11)\\nwhere loge is another way of describing the natural logarithm, commonly denoted by ln;\\nboth notations are used interchangeably. This inequality can be readily verified by plotting\\nthe functions lnx and (x - 1) versus x, as shown in Figure 5.1. Here we see that the line\\ny = x - 1 always lies above the curve y = logex. The equality holds only at the point x = 1,\\nwhere the line is tangential to the curve.\\nTo proceed with the proof, consider first any two different probability distributions\\ndenoted by p0, p1, , pK - 1 and q0, q1, , qK - 1 on the alphabet  = {s0, s1, , sK - 1) of a\\ndiscrete source. We may then define the relative entropy of these two distributions:\\n(5.12)\\nFigure 5.1 Graphs of the functions x - 1 and log x versus x.\\nloge x x 1 x 0   -  Dpq   pk pk qk -----     2 log k 0 = K 1 -  = 0 1.0 -1.0 1.0 2.0 x y = x - 1 y = loge x',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 230,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '2 Entropy Hence, changing to the natural logarithm and using the inequality of (5.11), we may\\nexpress the summation on the right-hand side of (5.12) as follows:\\nwhere, in the third line of the equation, it is noted that the sums over pk and qk are both\\nequal to unity in accordance with (5.3). We thus have the fundamental property of\\nprobability theory:\\n(5.13)\\nIn words, (5.13) states:\\nThe relative entropy of a pair of different discrete distributions is always\\nnonnegative; it is zero only when the two distributions are identical.\\nSuppose we next put\\nwhich corresponds to a source alphabet  with equiprobable symbols. Using this\\ndistribution in (5.12) yields\\nwhere we have made use of (5.3) and (5.9). Hence, invoking the fundamental inequality of\\n(5.13), we may finally write\\n(5.14)\\nThus, H(S) is always less than or equal to log2 K. The equality holds if, and only if, the\\nsymbols in the alphabet  are equiprobable. This completes the proof of (5.10) and with it\\nthe accompanying statements 1 and 2.\\nEXAMPLE\\nEntropy of Bernoulli Random Variable\\nTo illustrate the properties of H(S) summed up in (5.10), consider the Bernoulli random\\nvariable for which symbol 0 occurs with probability p0 and symbol 1 with probability\\np1 = 1 - p0. pk pk qk -----     2 log k 0 = K 1 -  pk qk pk -----     2 log k 0 = K 1 -  - = 1 2 ln -------- pk qk pk ----- 1 -     k 0 = K 1 -   1 loge 2 -------------- qk pk -   k 0 = K 1 -  = 0 = Dpq   0  qk 1 K---- k 0 1 K 1 -   =  = Dpq   pk log2 pk log2 K pk k 0 = K 1 -  + k 0 = K 1 -  = H S  - K 2 log + = H S  K 2 log',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 231,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '212 Chapter 5 Information Theory The entropy of this random variable is\\n(5.15)\\nfrom which we observe the following:\\nWhen p0 = 0, the entropy H(S) = 0; this follows from the fact that as\\n.\\nWhen p0 = 1, the entropy H(S) = 0.\\nThe entropy H(S) attains its maximum value Hmax = 1 bit when p1 = p0 = 12; that is,\\nwhen symbols 1 and 0 are equally probable.\\nIn other words, H(S) is symmetric about p0 = 12.\\nThe function of p0 given on the right-hand side of (5.15) is frequently encountered in\\ninformation-theoretic problems. It is customary, therefore, to assign a special symbol to\\nthis function. Specifically, we define\\n(5.16)\\nWe refer to H(p0) as the entropy function. The distinction between (5.15) and (5.16)\\nshould be carefully noted. The H(S) of (5.15) gives the entropy of the Bernoulli random\\nvariable S. The H(p0) of (5.16), on the other hand, is a function of the prior probability p0\\ndefined on the interval [0, 1]. Accordingly, we may plot the entropy function H(p0) versus\\np0, defined on the interval [0, 1], as shown in Figure 5.2. The curve in Figure 5.2\\nhighlights the observations made under points 1, 2, and 3.\\nExtension of a Discrete Memoryless Source\\nTo add specificity to the discrete source of symbols that has been the focus of attention up\\nuntil now, we now assume it to be memoryless in the sense that the symbol emitted by the\\nsource at any time is independent of previous and future emissions.\\nIn this context, we often find it useful to consider blocks rather than individual symbols,\\nwith each block consisting of n successive source symbols. We may view each such block\\nFigure 5.2\\nEntropy function H(p0).\\nH S  p0 p0 2 log p1 log2p1 - - = p0 p0 2 log 1 p0 -   log2 1 p0 -   - bits - = x loge x 0  x 0  H p0   p0 p0 1 p0 -   1 p0 -   2 log - 2 log - = 0 0.2 0.4 0.6 0.5 Symbol probability, p0',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 232,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '2 Entropy as being produced by an extended source with a source alphabet described by the Cartesian\\nproduct of a set Sn that has Kn distinct blocks, where K is the number of distinct symbols in\\nthe source alphabet S of the original source. With the source symbols being statistically\\nindependent, it follows that the probability of a source symbol in Sn is equal to the product\\nof the probabilities of the n source symbols in S that constitute a particular source symbol of\\nSn. We may thus intuitively expect that H(Sn), the entropy of the extended source, is equal\\nto n times H(S), the entropy of the original source. That is, we may write\\n(5.17)\\nWe illustrate the validity of this relationship by way of an example.\\nEXAMPLE\\nEntropy of Extended Source\\nConsider a discrete memoryless source with source alphabet  = {s0, s1, s2}, whose three\\ndistinct symbols have the following probabilities:\\nHence, the use of (5.9) yields the entropy of the discrete random variable S representing\\nthe source as\\nConsider next the second-order extension of the source. With the source alphabet \\nconsisting of three symbols, it follows that the source alphabet of the extended source S(2)\\nhas nine symbols. The first row of Table 5.1 presents the nine symbols of S(2), denoted by\\n0, 1, , 8. The second row of the table presents the composition of these nine symbols\\nin terms of the corresponding sequences of source symbols s0, s1, and s2, taken two at a\\nHSn    nH S  = p0 1 4--- = p1 1 4--- = p2 1 2--- = H S  p0 1 p0 -----     p1 1 p1 -----     p2 1 p2 -----     2 log + 2 log + 2 log = 1 4--- log2 4  1 4--- 4  2 log 1 2--- 2  2 log + + = 3 2--- = bits Table 5.1 Alphabets of second-order extension of a discrete memoryless source\\nSymbols of S(2) 0 1 2 3 4 5 6 7 8 Corresponding sequences of\\nsymbols of S s0s0 s0s1 s0s2 s1s0 s1s1 s1s2 s2s0 s2s1 s2s2 Probability \\x02(i), i 0 1 8   = 1 16 ------ 1 16 ------ 1 8--- 1 16 ------ 1 16 ------ 1 8--- 1 8--- 1 8--- 1 4---',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 233,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '214 Chapter 5 Information Theory time. The probabilities of the nine source symbols of the extended source are presented in\\nthe last row of the table. Accordingly, the use of (5.9) yields the entropy of the extended\\nsource as\\nWe thus see that H(S (2)) = 2H(S) in accordance with (5.17). Source-coding Theorem\\nNow that we understand the meaning of entropy of a random variable, we are equipped to\\naddress an important issue in communication theory: the representation of data generated\\nby a discrete source of information.\\nThe process by which this representation is accomplished is called source encoding.\\nThe device that performs the representation is called a source encoder. For reasons to be\\ndescribed, it may be desirable to know the statistics of the source. In particular, if some\\nsource symbols are known to be more probable than others, then we may exploit this\\nfeature in the generation of a source code by assigning short codewords to frequent source\\nsymbols, and long codewords to rare source symbols. We refer to such a source code as a\\nvariable-length code. The Morse code, used in telegraphy in the past, is an example of a\\nvariable-length code. Our primary interest is in the formulation of a source encoder that\\nsatisfies two requirements:\\nThe codewords produced by the encoder are in binary form.\\nThe source code is uniquely decodable, so that the original source sequence can be\\nreconstructed perfectly from the encoded binary sequence.\\nThe second requirement is particularly important: it constitutes the basis for a perfect\\nsource code.\\nConsider then the scheme shown in Figure 5.3 that depicts a discrete memoryless\\nsource whose output sk is converted by the source encoder into a sequence of 0s and 1s,\\ndenoted by bk. We assume that the source has an alphabet with K different symbols and\\nthat the kth symbol sk occurs with probability pk, k = 0, 1, , K - 1. Let the binary',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 234,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'denoted by bk. We assume that the source has an alphabet with K different symbols and\\nthat the kth symbol sk occurs with probability pk, k = 0, 1, , K - 1. Let the binary\\nH S 2    p i   1 p i   -------------     2 log i 0 = 8  = 1 16 ------log2 16   1 16 ------log2 16   1 8---log2 8  1 16 ------ 16   2 log + + + = + 1 16 ------log2 16   1 8---log2 8  1 8---log2 8  1 8---log2 8  1 4--- 4  2 log + + + + 3 bits = Figure 5.3 Source encoding.\\nsk bk Discrete memoryless source Source encoder Binary sequence',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 234,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Lossless Data Compression Algorithms codeword assigned to symbol sk by the encoder have length lk, measured in bits. We define\\nthe average codeword length of the source encoder as\\n(5.18)\\nIn physical terms, the parameter represents the average number of bits per source\\nsymbol used in the source encoding process. Let Lmin denote the minimum possible value\\nof L. We then define the coding efficiency of the source encoder as\\n(5.19)\\nWith\\n, we clearly have   1. The source encoder is said to be efficient when \\napproaches unity.\\nBut how is the minimum value Lmin determined? The answer to this fundamental\\nquestion is embodied in Shannons first theorem: the source-coding theorem,4 which may\\nbe stated as follows:\\nGiven a discrete memoryless source whose output is denoted by the random\\nvariable S, the entropy H(S) imposes the following bound on the average\\ncodeword length for any source encoding scheme: (5.20)\\nAccording to this theorem, the entropy H(S) represents a fundamental limit on the average\\nnumber of bits per source symbol necessary to represent a discrete memoryless source, in\\nthat it can be made as small as but no smaller than the entropy H(S). Thus, setting\\nLmin = H(S), we may rewrite (5.19), defining the efficiency of a source encoder in terms of\\nthe entropy H(S) as shown by\\n(5.21)\\nwhere as before we have  < 1. Lossless Data Compression Algorithms\\nA common characteristic of signals generated by physical sources is that, in their natural\\nform, they contain a significant amount of redundant information, the transmission of\\nwhich is therefore wasteful of primary communication resources. For example, the output\\nof a computer used for business transactions constitutes a redundant sequence in the sense\\nthat any two adjacent symbols are typically correlated with each other.\\nFor efficient signal transmission, the redundant information should, therefore, be\\nremoved from the signal prior to transmission. This operation, with no loss of information,',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 235,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'For efficient signal transmission, the redundant information should, therefore, be\\nremoved from the signal prior to transmission. This operation, with no loss of information,\\nis ordinarily performed on a signal in digital form, in which case we refer to the operation\\nas lossless data compression. The code resulting from such an operation provides a\\nrepresentation of the source output that is not only efficient in terms of the average number\\nof bits per symbol, but also exact in the sense that the original data can be reconstructed\\nwith no loss of information. The entropy of the source establishes the fundamental limit on\\nthe removal of redundancy from the data. Basically, lossless data compression is achieved\\nL L pklk k 0 = K 1 -  = L  Lmin L ----------- = L Lmin  L L HS    H S  L ------------ =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 235,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '216 Chapter 5 Information Theory by assigning short descriptions to the most frequent outcomes of the source output and\\nlonger descriptions to the less frequent ones.\\nIn this section we discuss some source-coding schemes for lossless data compression.\\nWe begin the discussion by describing a type of source code known as a prefix code,\\nwhich not only is uniquely decodable, but also offers the possibility of realizing an\\naverage codeword length that can be made arbitrarily close to the source entropy.\\nPrefix Coding\\nConsider a discrete memoryless source of alphabet {s0, s1, , sK - 1} and respective\\nprobabilities {p0, p1, , pK - 1}. For a source code representing the output of this source to\\nbe of practical use, the code has to be uniquely decodable. This restriction ensures that, for\\neach finite sequence of symbols emitted by the source, the corresponding sequence of\\ncodewords is different from the sequence of codewords corresponding to any other source\\nsequence. We are specifically interested in a special class of codes satisfying a restriction\\nknown as the prefix condition. To define the prefix condition, let the codeword assigned to\\nsource symbol sk be denoted by\\n, where the individual elements are 0s and 1s and n is the codeword length. The initial part of the codeword\\nis represented by the elements for some i  n. Any sequence made up of the\\ninitial part of the codeword is called a prefix of the codeword. We thus say:\\nA prefix code is defined as a code in which no codeword is the prefix of any\\nother codeword.\\nPrefix codes are distinguished from other uniquely decodable codes by the fact that the\\nend of a codeword is always recognizable. Hence, the decoding of a prefix can be\\naccomplished as soon as the binary sequence representing a source symbol is fully\\nreceived. For this reason, prefix codes are also referred to as instantaneous codes.\\nEXAMPLE\\nIllustrative Example of Prefix Coding',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 236,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'received. For this reason, prefix codes are also referred to as instantaneous codes.\\nEXAMPLE\\nIllustrative Example of Prefix Coding\\nTo illustrate the meaning of a prefix code, consider the three source codes described in\\nTable 5.2. Code I is not a prefix code because the bit 0, the codeword for s0, is a prefix of\\n00, the codeword for s2. Likewise, the bit 1, the codeword for s1, is a prefix of 11, the\\ncodeword for s3. Similarly, we may show that code III is not a prefix code but code II is.\\nmk1 mk2 mkn      mk1 mkn   mk1 mki   Table 5.2 Illustrating the definition of a prefix code\\nSymbol source\\nProbability of occurrence\\nCode I Code II Code III s0 0.5 0 0 0 s1 0.25 1 10 01 s2 0.125 00 110 011 s3 0.125 11 111',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 236,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Lossless Data Compression Algorithms Decoding of Prefix Code\\nTo decode a sequence of codewords generated from a prefix source code, the source\\ndecoder simply starts at the beginning of the sequence and decodes one codeword at a\\ntime. Specifically, it sets up what is equivalent to a decision tree, which is a graphical\\nportrayal of the codewords in the particular source code. For example, Figure 5.4 depicts\\nthe decision tree corresponding to code II in Table 5.2. The tree has an initial state and\\nfour terminal states corresponding to source symbols s0, s1, s2, and s3. The decoder always\\nstarts at the initial state. The first received bit moves the decoder to the terminal state s0 if\\nit is 0 or else to a second decision point if it is 1. In the latter case, the second bit moves the\\ndecoder one step further down the tree, either to terminal state s1 if it is 0 or else to a third\\ndecision point if it is 1, and so on. Once each terminal state emits its symbol, the decoder\\nis reset to its initial state. Note also that each bit in the received encoded sequence is\\nexamined only once. Consider, for example, the following encoded sequence:\\nThis sequence is readily decoded as the source sequence s1s3s2s0s0 The reader is\\ninvited to carry out this decoding.\\nAs mentioned previously, a prefix code has the important property that it is\\ninstantaneously decodable. But the converse is not necessarily true. For example, code III\\nin Table 5.2 does not satisfy the prefix condition, yet it is uniquely decodable because the\\nbit 0 indicates the beginning of each codeword in the code.\\nTo probe more deeply into prefix codes, exemplified by that in Table 5.2, we resort to\\nan inequality, which is considered next.\\nKraft Inequality\\nConsider a discrete memoryless source with source alphabet {s0, s1, , sK - 1} and source\\nprobabilities {p0, p1, , pK - 1}, with the codeword of symbol sk having length lk, k = 0, 1,\\nFigure 5.4 Decision tree for code II of Table 5.2.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 237,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'probabilities {p0, p1, , pK - 1}, with the codeword of symbol sk having length lk, k = 0, 1,\\nFigure 5.4 Decision tree for code II of Table 5.2.\\n10 111 110 0 0  s1 s3 s2 s0 s0        s1 s0 s2 s3 0 0 Initial state 0 1 1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 237,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '218 Chapter 5 Information Theory , K - 1. Then, according to the Kraft inequality,5 the codeword lengths always satisfy the\\nfollowing inequality:\\n(5.22)\\nwhere the factor 2 refers to the number of symbols in the binary alphabet. The Kraft\\ninequality is a necessary but not sufficient condition for a source code to be a prefix code.\\nIn other words, the inequality of (5.22) is merely a condition on the codeword lengths of a\\nprefix code and not on the codewords themselves. For example, referring to the three\\ncodes listed in Table 5.2, we see:\\n\\nCode I violates the Kraft inequality; it cannot, therefore, be a prefix code.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 238,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Code I violates the Kraft inequality; it cannot, therefore, be a prefix code.\\n\\nThe Kraft inequality is satisfied by both codes II and III, but only code II is a\\nprefix code.\\nGiven a discrete memoryless source of entropy H(S), a prefix code can be constructed with\\nan average codeword length , which is bounded as follows:\\n(5.23)\\nThe left-hand bound of (5.23) is satisfied with equality under the condition that symbol sk\\nis emitted by the source with probability\\n(5.24)\\nwhere lk is the length of the codeword assigned to source symbol sk. A distribution governed\\nby (5.24) is said to be a dyadic distribution. For this distribution, we naturally have\\nUnder this condition, the Kraft inequality of (5.22) confirms that we can construct a prefix\\ncode, such that the length of the codeword assigned to source symbol sk is\\n. For\\nsuch a code, the average codeword length is\\n(5.25)\\nand the corresponding entropy of the source is\\n(5.26)\\nHence, in this special (rather meretricious) case, we find from (5.25) and (5.26) that the\\nprefix code is matched to the source in that\\n.\\nBut how do we match the prefix code to an arbitrary discrete memoryless source? The\\nanswer to this basic problem lies in the use of an extended code. Let denote the 2 lk - 1  k 0 = K 1 -  L HS  L HS  1 +   pk 2 lk - = 2 lk - k 0 = K 1 -  pk k 0 = K 1 -  1 = = log2 pk - L lk 2 lk ------ k 0 = K 1 -  = H S  1 2 lk ------      log2 2 lk   k 0 = K 1 -  = lk 2 lk ------       k 0 = K 1 -  = L HS  = Ln',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 238,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Lossless Data Compression Algorithms average codeword length of the extended prefix code. For a uniquely decodable code,\\nis the smallest possible. From (5.23), we find that\\n(5.27) or, equivalently, (5.28) In the limit, as n approaches infinity, the lower and upper bounds in (5.28) converge as\\nshown by\\n(5.29)\\nWe may, therefore, make the statement:\\nBy making the order n of an extended prefix source encoder large enough,\\nwe can make the code faithfully represent the discrete memoryless source S\\nas closely as desired.\\nIn other words, the average codeword length of an extended prefix code can be made as\\nsmall as the entropy of the source, provided that the extended code has a high enough\\norder in accordance with the source-coding theorem. However, the price we have to pay\\nfor decreasing the average codeword length is increased decoding complexity, which is\\nbrought about by the high order of the extended prefix code.\\nHuffman Coding\\nWe next describe an important class of prefix codes known as Huffman codes. The basic\\nidea behind Huffman coding6 is the construction of a simple algorithm that computes an\\noptimal prefix code for a given distribution, optimal in the sense that the code has the\\nshortest expected length. The end result is a source code whose average codeword length\\napproaches the fundamental limit set by the entropy of a discrete memoryless source,\\nnamely H(S). The essence of the algorithm used to synthesize the Huffman code is to\\nreplace the prescribed set of source statistics of a discrete memoryless source with a\\nsimpler one. This reduction process is continued in a step-by-step manner until we are left\\nwith a final set of only two source statistics (symbols), for which (0, 1) is an optimal code.\\nStarting from this trivial code, we then work backward and thereby construct the Huffman\\ncode for the given source.\\nTo be specific, the Huffman encoding algorithm proceeds as follows:\\nThe source symbols are listed in order of decreasing probability. The two source',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 239,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'code for the given source.\\nTo be specific, the Huffman encoding algorithm proceeds as follows:\\nThe source symbols are listed in order of decreasing probability. The two source\\nsymbols of lowest probability are assigned 0 and 1. This part of the step is referred\\nto as the splitting stage.\\nThese two source symbols are then combined into a new source symbol with\\nprobability equal to the sum of the two original probabilities. (The list of source\\nsymbols, and, therefore, source statistics, is thereby reduced in size by one.) The\\nprobability of the new symbol is placed in the list in accordance with its value.\\nThe procedure is repeated until we are left with a final list of source statistics\\n(symbols) of only two for which the symbols 0 and 1 are assigned.\\nLn nH S  Ln nH S  1 +   H S  Ln n----- H S  1 n--- +   1 n--- n   lim Ln H S  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 239,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '220 Chapter 5 Information Theory The code for each (original) source is found by working backward and tracing the\\nsequence of 0s and 1s assigned to that symbol as well as its successors.\\nEXAMPLE\\nHuffman Tree\\nTo illustrate the construction of a Huffman code, consider the five symbols of the alphabet\\nof a discrete memoryless source and their probabilities, which are shown in the two\\nleftmost columns of Figure 5.5b. Following through the Huffman algorithm, we reach the\\nend of the computation in four steps, resulting in a Huffman tree similar to that shown in\\nFigure 5.5; the Huffman tree is not to be confused with the decision tree discussed\\npreviously in Figure 5.4. The codewords of the Huffman code for the source are tabulated\\nin Figure 5.5a. The average codeword length is, therefore,\\nThe entropy of the specified discrete memoryless source is calculated as follows (see (5.9)):\\nFor this example, we may make two observations:\\nThe average codeword length exceeds the entropy H(S) by only 3.67%.\\nThe average codeword length does indeed satisfy (5.23).\\nIt is noteworthy that the Huffman encoding process (i.e., the Huffman tree) is not unique.\\nIn particular, we may cite two variations in the process that are responsible for the\\nnonuniqueness of the Huffman code. First, at each splitting stage in the construction of a\\nHuffman code, there is arbitrariness in the way the symbols 0 and 1 are assigned to the last\\ntwo source symbols. Whichever way the assignments are made, however, the resulting\\ndifferences are trivial. Second, ambiguity arises when the probability of a combined\\nL 0.4 2  0.2 2  0.2 2  0.1 3  0.1 3  + + + + = 2.2 binary symbols = H S  0.4 log2 1 0.4 -------     0.2 log2 1 0.2 -------     0.2 + + log2 1 0.2 -------     0.1 log2 1 0.1 -------     0.1 + log2 1 0.1 -------     + = 0.529 0.464 0.464 0.332 0.332 + + + + = 2.121 bits = L L Figure 5.5 (a) Example of the Huffman encoding algorithm. (b) Source code.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 240,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '(b) (a) Symbol s0 s1 s2 s3 s4 Symbol s0 s1 s2 s3 s4 Probability 0.4 0.2 0.2 0.1 0.1 Code word 00 10 11 010 011 Stage I 0.4 0.2 0.2 0.1 0.1 Stage II 0.4 0.2 0.2 0.2 Stage III 0.4 0.4 0.2 Stage IV 0.6 0.4 0 1 0 1 0 1 0',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 240,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Lossless Data Compression Algorithms symbol (obtained by adding the last two probabilities pertinent to a particular step) is\\nfound to equal another probability in the list. We may proceed by placing the probability\\nof the new symbol as high as possible, as in Example 4. Alternatively, we may place it as\\nlow as possible. (It is presumed that whichever way the placement is made, high or low, it\\nis consistently adhered to throughout the encoding process.) By this time, noticeable\\ndifferences arise in that the codewords in the resulting source code can have different\\nlengths. Nevertheless, the average codeword length remains the same.\\nAs a measure of the variability in codeword lengths of a source code, we define the\\nvariance of the average codeword length over the ensemble of source symbols as\\n(5.30)\\nwhere p0, p1, , pK - 1 are the source statistics and lk is the length of the codeword\\nassigned to source symbol sk. It is usually found that when a combined symbol is moved\\nas high as possible, the resulting Huffman code has a significantly smaller variance\\nthan when it is moved as low as possible. On this basis, it is reasonable to choose the\\nformer Huffman code over the latter.\\nLempel-Ziv Coding\\nA drawback of the Huffman code is that it requires knowledge of a probabilistic model of\\nthe source; unfortunately, in practice, source statistics are not always known a priori.\\nMoreover, in the modeling of text we find that storage requirements prevent the Huffman\\ncode from capturing the higher-order relationships between words and phrases because the\\ncodebook grows exponentially fast in the size of each super-symbol of letters (i.e.,\\ngrouping of letters); the efficiency of the code is therefore compromised. To overcome\\nthese practical limitations of Huffman codes, we may use the Lempel-Ziv algorithm,7\\nwhich is intrinsically adaptive and simpler to implement than Huffman coding.\\nBasically, the idea behind encoding in the Lempel-Ziv algorithm is described as\\nfollows:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 241,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'which is intrinsically adaptive and simpler to implement than Huffman coding.\\nBasically, the idea behind encoding in the Lempel-Ziv algorithm is described as\\nfollows:\\nThe source data stream is parsed into segments that are the shortest\\nsubsequences not encountered previously.\\nTo illustrate this simple yet elegant idea, consider the example of the binary sequence\\n000101110010100101 \\nIt is assumed that the binary symbols 0 and 1 are already stored in that order in the code\\nbook. We thus write Subsequences stored: 0, 1 Data to be parsed: 000101110010100101  The encoding process begins at the left. With symbols 0 and 1 already stored, the shortest\\nsubsequence of the data stream encountered for the first time and not seen before is 00; so\\nwe write Subsequences stored: 0, 1, 00 Data to be parsed: 0101110010100101  L 2 pk lk L -   2 k 0 = K 1 -  = 2',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 241,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '222 Chapter 5 Information Theory The second shortest subsequence not seen before is 01; accordingly, we go on to write\\nSubsequences stored: 0, 0, 00, 01 Data to be parsed: 01110010100101  The next shortest subsequence not encountered previously is 011; hence, we write\\nSubsequences stored: 0, 1, 00, 01, 011 Data to be parsed: 10010100101  We continue in the manner described here until the given data stream has been completely\\nparsed. Thus, for the example at hand, we get the code book of binary subsequences\\nshown in the second row of Figure 5.6.8\\nThe first row shown in this figure merely indicates the numerical positions of the\\nindividual subsequences in the code book. We now recognize that the first subsequence of\\nthe data stream, 00, is made up of the concatenation of the first code book entry, 0, with\\nitself; it is, therefore, represented by the number 11. The second subsequence of the data\\nstream, 01, consists of the first code book entry, 0, concatenated with the second code book\\nentry, 1; it is, therefore, represented by the number 12. The remaining subsequences are\\ntreated in a similar fashion. The complete set of numerical representations for the various\\nsubsequences in the code book is shown in the third row of Figure 5.6. As a further example\\nillustrating the composition of this row, we note that the subsequence 010 consists of the\\nconcatenation of the subsequence 01 in position 4 and symbol 0 in position 1; hence, the\\nnumerical representation is 41. The last row shown in Figure 5.6 is the binary encoded\\nrepresentation of the different subsequences of the data stream.\\nThe last symbol of each subsequence in the code book (i.e., the second row of Figure\\n6) is an innovation symbol, which is so called in recognition of the fact that its\\nappendage to a particular subsequence distinguishes it from all previous subsequences\\nstored in the code book. Correspondingly, the last bit of each uniform block of bits in the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 242,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'appendage to a particular subsequence distinguishes it from all previous subsequences\\nstored in the code book. Correspondingly, the last bit of each uniform block of bits in the\\nbinary encoded representation of the data stream (i.e., the fourth row in Figure 5.6)\\nrepresents the innovation symbol for the particular subsequence under consideration. The\\nremaining bits provide the equivalent binary representation of the pointer to the root\\nsubsequence that matches the one in question, except for the innovation symbol.\\nThe Lempel-Ziv decoder is just as simple as the encoder. Specifically, it uses the\\npointer to identify the root subsequence and then appends the innovation symbol.\\nConsider, for example, the binary encoded block 1101 in position 9. The last bit, 1, is the\\ninnovation symbol. The remaining bits, 110, point to the root subsequence 10 in position\\nHence, the block 1101 is decoded into 101, which is correct.\\nFrom the example described here, we note that, in contrast to Huffman coding, the\\nLempel-Ziv algorithm uses fixed-length codes to represent a variable number of source\\nsymbols; this feature makes the Lempel-Ziv code suitable for synchronous transmission.\\nFigure 5.6 Illustrating the encoding process performed by the Lempel-Ziv algorithm\\non the binary sequence 000101110010100101 \\nNumerical positions 1 2 3 4 5 6 7 8 9 Subsequences 0 1 00 01 011 10 010 100 101 Numerical representations 11 12 42 21 41 61 62 Binary encoded blocks 0010 0011 1001 0100 1000 1100',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 242,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '5 Discrete Memoryless Channels In practice, fixed blocks of 12 bits long are used, which implies a code book of 212 =\\nentries.\\nFor a long time, Huffman coding was unchallenged as the algorithm of choice for\\nlossless data compression; Huffman coding is still optimal, but in practice it is hard to\\nimplement. It is on account of practical implementation that the Lempel-Ziv algorithm\\nhas taken over almost completely from the Huffman algorithm. The Lempel-Ziv\\nalgorithm is now the standard algorithm for file compression. Discrete Memoryless Channels\\nUp to this point in the chapter we have been preoccupied with discrete memoryless\\nsources responsible for information generation. We next consider the related issue of\\ninformation transmission. To this end, we start the discussion by considering a discrete\\nmemoryless channel, the counterpart of a discrete memoryless source.\\nA discrete memoryless channel is a statistical model with an input X and an output Y that\\nis a noisy version of X; both X and Y are random variables. Every unit of time, the channel\\naccepts an input symbol X selected from an alphabet and, in response, it emits an output\\nsymbol Y from an alphabet . The channel is said to be discrete when both of the alphabets\\n and  have finite sizes. It is said to be memoryless when the current output symbol\\ndepends only on the current input symbol and not any previous or future symbol.\\nFigure 5.7a shows a view of a discrete memoryless channel. The channel is described in\\nterms of an input alphabet\\n(5.31)\\nand an output alphabet\\n(5.32)\\nFigure 5.7 (a) Discrete memoryless channel; (b) Simplified\\ngraphical representation of the channel.\\n x0 x1 xJ 1 -      =  y0 y1 yK 1 -      = y0 y1 . . . yK - 1 x0 x1 . . . xJ - 1 p(yk | xj) X   Y p(y|x) Input X with sample value x Output Y with sample value y (a) (b)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 243,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '224 Chapter 5 Information Theory The cardinality of the alphabets  and , or any other alphabet for that matter, is defined\\nas the number of elements in the alphabet. Moreover, the channel is characterized by a set\\nof transition probabilities\\n(5.33)\\nfor which, according to probability theory, we naturally have\\n(5.34) and (5.35) When the number of input symbols J and the number of output symbols K are not large, we\\nmay depict the discrete memoryless channel graphically in another way, as shown in Figure\\n7b. In this latter depiction, each input-output symbol pair (x, y), characterized by the\\ntransition probability p(y|x)  0, is joined together by a line labeled with the number p(y|x).\\nAlso, the input alphabet  and output alphabet  need not have the same size; hence\\nthe use of J for the size of  and K for the size of . For example, in channel coding, the\\nsize K of the output alphabet  may be larger than the size J of the input alphabet ; thus,\\nK J. On the other hand, we may have a situation in which the channel emits the same\\nsymbol when either one of two input symbols is sent, in which case we have K J.\\nA convenient way of describing a discrete memoryless channel is to arrange the various\\ntransition probabilities of the channel in the form of a matrix\\n(5.36)\\nThe J-by-K matrix P is called the channel matrix, or stochastic matrix. Note that each row\\nof the channel matrix P corresponds to a fixed channel input, whereas each column of the\\nmatrix corresponds to a fixed channel output. Note also that a fundamental property of the\\nchannel matrix P, as defined here, is that the sum of the elements along any row of the\\nstochastic matrix is always equal to one, according to (5.35).\\nSuppose now that the inputs to a discrete memoryless channel are selected according to\\nthe probability distribution {p(xj), j = 0, 1, , J - 1}. In other words, the event that the\\nchannel input X = xj occurs with probability\\n(5.37)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 244,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the probability distribution {p(xj), j = 0, 1, , J - 1}. In other words, the event that the\\nchannel input X = xj occurs with probability\\n(5.37)\\nHaving specified the random variable X denoting the channel input, we may now specify\\nthe second random variable Y denoting the channel output. The joint probability\\ndistribution of the random variables X and Y is given by\\n(5.38) p yk xj   \\x02 Y yk X xj = =   for all j and k = 0 p yk xj   1 for all j and k   p yk xj   k 1 for fixed j = P p y0 x0   p y1 x0    p yK 1 - x0   p y0 x1   p y1 x1    p yK 1 - x1   p y0 xJ 1 -  p y1 xJ 1 -  p yK 1 - xJ 1 -   = . . . . . . . . . p xj   \\x02(X xj) for j 0 1, J 1 -   = = = p xj yk    \\x02 X xj Y  yk = =   = \\x02 Y yk X xj = =  \\x02(X xj) = = p yk xj  p xj   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 244,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '5 Discrete Memoryless Channels The marginal probability distribution of the output random variable Y is obtained by\\naveraging out the dependence of p(xj, yk) on xj, obtaining\\n(5.39)\\nThe probabilities p(xj) for j = 0, 1, , J - 1, are known as the prior probabilities of the\\nvarious input symbols. Equation (5.39) states:\\nIf we are given the input prior probabilities p(xj) and the stochastic matrix\\n(i.e., the matrix of transition probabilities p(yk|xj)), then we may calculate the\\nprobabilities of the various output symbols, the p(yk).\\nEXAMPLE\\nBinary Symmetric Channel\\nThe binary symmetric channel is of theoretical interest and practical importance. It is a\\nspecial case of the discrete memoryless channel with J = K = 2. The channel has two input\\nsymbols (x0 = 0, x1 = 1) and two output symbols (y0 = 0, y1 = 1). The channel is symmetric\\nbecause the probability of receiving 1 if 0 is sent is the same as the probability of receiving\\n0 if 1 is sent. This conditional probability of error is denoted by p (i.e., the probability of a\\nbit flipping). The transition probability diagram of a binary symmetric channel is as\\nshown in Figure 5.8. Correspondingly, we may express the stochastic matrix as\\np yk   \\x02(Y yk) = = \\x02 Y yk X xj = =  \\x02(X j 0 = J 1 -  xj) = = p yk xj  p xj   for k 0 1 K 1 -   = j 0 = J 1 -  = Figure 5.8 Transition probability diagram of binary symmetric channel.\\nP 1 p - p p 1 p - = x0 = 0 y0 = 0 x1 = 1 y1 = 1 1 - p 1 - ppp Input Output',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 245,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '226 Chapter 5 Information Theory 5.6 Mutual Information Given that we think of the channel output Y (selected from alphabet ) as a noisy version\\nof the channel input X (selected from alphabet ) and that the entropy H(X) is a measure of\\nthe prior uncertainty about X, how can we measure the uncertainty about X after observing\\nY? To answer this basic question, we extend the ideas developed in Section 5.2 by defining\\nthe conditional entropy of X selected from alphabet , given Y = yk. Specifically, we write\\n(5.40)\\nThis quantity is itself a random variable that takes on the values H(X|Y = y0), ,\\nH(X|Y = yK - 1) with probabilities p(y0), , p(yK - 1), respectively. The expectation of\\nentropy H(X|Y = yk) over the output alphabet  is therefore given by\\n(5.41)\\nwhere, in the last line, we used the definition of the probability of the joint event (X = xj,\\nY = yk) as shown by\\n(5.42)\\nThe quantity H(X|Y) in (5.41) is called the conditional entropy, formally defined as\\nfollows:\\nThe conditional entropy, H(X|Y), is the average amount of uncertainty\\nremaining about the channel input after the channel output has been observed.\\nThe conditional entropy H(X|Y) relates the channel output Y to the channel input X. The\\nentropy H(X) defines the entropy of the channel input X by itself. Given these two\\nentropies, we now introduce the definition\\n(5.43)\\nwhich is called the mutual information of the channel. To add meaning to this new\\nconcept, we recognize that the entropy H(X) accounts for the uncertainty about the\\nchannel input before observing the channel output and the conditional entropy H(X|Y)\\naccounts for the uncertainty about the channel input after observing the channel output.\\nWe may, therefore, go on to make the statement:\\nThe mutual information I(X;Y) is a measure of the uncertainty about the\\nchannel input, which is resolved by observing the channel output.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 246,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'We may, therefore, go on to make the statement:\\nThe mutual information I(X;Y) is a measure of the uncertainty about the\\nchannel input, which is resolved by observing the channel output.\\nH(X Y yk) p xj yk   1 p xj yk   -------------------     2 log j 0 = J 1 -  = = H(X Y) H(X Y yk) = k 0 = K 1 -  = p yk   p xj yk  p yk   1 p xj yk   -------------------     2 log j 0 = J 1 -  k 0 = K 1 -  = p xj yk    1 p xj yk   -------------------     2 log j 0 = J 1 -  k 0 = K 1 -  = p xj yk    p xj yk  p yk   = IXY ;   H X  HXY   - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 246,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 Mutual Information Equation (5.43) is not the only way of defining the mutual information of a channel.\\nRather, we may define it in another way, as shown by\\n(5.44)\\non the basis of which we may make the next statement:\\nThe mutual information I(Y;X) is a measure of the uncertainty about the\\nchannel output that is resolved by sending the channel input.\\nOn first sight, the two definitions of (5.43) and (5.44) look different. In reality, however,\\nthey embody equivalent statements on the mutual information of the channel that are\\nworded differently. More specifically, they could be used interchangeably, as\\ndemonstrated next.\\nProperties of Mutual Information PROPERTY 1 Symmetry\\nThe mutual information of a channel is symmetric in the sense that\\n(5.45)\\nTo prove this property, we first use the formula for entropy and then use (5.35) and (5.38),\\nin that order, obtaining\\n(5.46)\\nwhere, in going from the third to the final line, we made use of the definition of a joint\\nprobability. Hence, substituting (5.41) and (5.46) into (5.43) and then combining terms,\\nwe obtain\\n(5.47)\\nNote that the double summation on the right-hand side of (5.47) is invariant with respect\\nto swapping the x and y. In other words, the symmetry of the mutual information I(X;Y) is\\nalready evident from (5.47).\\nIYX ;   H Y  HYX   - = IXY ;   IYX ;   = H X  p xj   1 p xj   ------------     2 log j 0 = J 1 -  = p xj   log2 1 p xj   ------------     p yk xj   k 0 = K 1 -  j 0 = J 1 -  = p yk xj  p xj   1 p xj   ------------     2 log k 0 = K 1 -  j 0 = J 1 -  = p xj yk    1 p xj   ------------     2 log k 0 = K 1 -  j 0 = J 1 -  = IXY ;   p xj yk    log2 p xj yk   p xj   -------------------       k 0 = K 1 -  j 0 = J 1 -  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 247,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '228 Chapter 5 Information Theory To further confirm this property, we may use Bayes rule for conditional probabilities,\\npreviously discussed in Chapter 3, to write\\n(5.48)\\nHence, substituting (5.48) into (5.47) and interchanging the order of summation, we get\\n(5.49)\\nwhich proves Property1.\\nPROPERTY\\nNonnegativity\\nThe mutual information is always nonnegative; that is;\\n(5.50)\\nTo prove this property, we first note from (5.42) that\\n(5.51)\\nHence, substituting (5.51) into (5.47), we may express the mutual information of the\\nchannel as\\n(5.52)\\nNext, a direct application of the fundamental inequality of (5.12) on relative entropy\\nconfirms (5.50), with equality if, and only if,\\n(5.53)\\nIn words, Property 2 states the following:\\nWe cannot lose information, on the average, by observing the output\\nof a channel.\\nMoreover, the mutual information is zero if, and only if, the input and output symbols of\\nthe channel are statistically independent; that is, when (5.53) is satisfied.\\nPROPERTY\\nExpansion of the Mutual Information\\nThe mutual information of a channel is related to the joint entropy of the channel input\\nand channel output by\\n(5.54)\\nwhere the joint entropy H(X, Y) is defined by\\n(5.55) p xj yk   p xj   ------------------- p yk xj   p yk   ------------------- = IXY ;   p xj yk    log2 p yk xj   p yk   -------------------       j 0 = J 1 -  k 0 = K 1 -  = IYX ;   = IXY ;   0  p xj yk   p xj yk    p yk   -------------------- = IXY ;   p xj yk    p xj yk    p xj  p yk   --------------------------\\n    2 log k 0 = K 1 -  j 0 = J 1 -  = p xj yk    p xj  p yk   for all j and k = IXY ;   H X  H Y  HXY    - + = HXY    p xj yk    1 p xj yk    --------------------     2 log k 0 = K 1 -  j 0 = J 1 -  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 248,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '6 Mutual Information To prove (5.54), we first rewrite the joint entropy in the equivalent form\\n(5.56)\\nThe first double summation term on the right-hand side of (5.56) is recognized as the\\nnegative of the mutual information of the channel, I(X;Y), previously given in (5.52). As\\nfor the second summation term, we manipulate it as follows:\\n(5.57)\\nwhere, in the first line, we made use of the following relationship from probability theory:\\nand a similar relationship holds for the second line of the equation.\\nAccordingly, using (5.52) and (5.57) in (5.56), we get the result\\n(5.58)\\nwhich, on rearrangement, proves Property 3.\\nWe conclude our discussion of the mutual information of a channel by providing a\\ndiagramatic interpretation in Figure 5.9 of (5.43), (5.44), and (5.54).\\nHXY    p xj yk    2 p xj  p yk   p xj yk    --------------------------\\n    p xj yk    1 p xj  p yk   --------------------------\\n    2 log k 0 = K 1 -  j 0 = J 1 -  + log k 0 = K 1 -  j 0 = J 1 -  = p xj yk    1 p xj  p yk   --------------------------\\n    2 log k 0 = K 1 -  j 0 = J 1 -  log2 1 p xj   ------------     p xj yk    k 0 = K 1 -  j 0 = J 1 -  = log2 1 p yk   -------------     p xj yk    j 0 = J 1 -  k 0 = K 1 -  + p xj  log2 1 p xj   ------------     p yk   1 p yk   -------------     2 log k 0 = K 1 -  + j 0 = J 1 -  = H X  H Y  + = Figure 5.9 Illustrating the relations among various channel entropies.\\np xj yk    k 0 = K 1 -  p yk   = HXY    IXY ;   - H X  H Y  + + = H(X, Y) = H(Y, X) H(X) H(Y) H(Y|X) H(Y|X) H(X|Y) H(X|Y) I(X;Y)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 249,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '230 Chapter 5 Information Theory 5.7 Channel Capacity The concept of entropy introduced in Section 5.2 prepared us for formulating Shannons\\nfirst theorem: the source-coding theorem. To set the stage for formulating Shannons\\nsecond theorem, namely the channel-coding theorem, this section introduces the concept of\\ncapacity, which, as mentioned previously, defines the intrinsic ability of a communication\\nchannel to convey information.\\nTo proceed, consider a discrete memoryless channel with input alphabet , output\\nalphabet , and transition probabilities p(yk|xj), where j = 0, 1, , J - 1 and k = 0, 1, ,\\nK - 1. The mutual information of the channel is defined by the first line of (5.49), which is\\nreproduced here for convenience:\\nwhere, according to (5.38),\\nAlso, from (5.39), we have\\nPutting these three equations into a single equation, we write\\nCareful examination of the double summation in this equation reveals two different\\nprobabilities, on which the essence of mutual information I(X;Y) depends:\\n\\nthe probability distribution that characterizes the channel input and',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 250,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the probability distribution that characterizes the channel input and\\n\\nthe transition probability distribution that characterizes the\\nchannel itself.\\nThese two probability distributions are obviously independent of each other. Thus, given a\\nchannel characterized by the transition probability distribution {p(yk|xj}, we may now\\nintroduce the channel capacity, which is formally defined in terms of the mutual\\ninformation between the channel input and output as follows:\\n(5.59)\\nThe maximization in (5.59) is performed, subject to two input probabilistic constraints:\\nIXY ;   p xj yk    log2 p yk xj   p yk   -------------------       j 0 = J 1 -  k 0 = K 1 -  = p xj yk    p yk xj  p xj   = p yk   p yk xj  p xj   j 0 = J 1 -  = IXY ;   p yk xj   p xj   log2 p yk xj   p yk xj  p xj   j 0 = J 1 -  ------------------------------------------\\n                j 0 = J 1 -  k 0 = K 1 -  = p xj    j 0 = J 1 - p yk xj    j 0 = , k 0 = j J 1 - = K 1 -  C max p xj     = IXY ;   bits per channel use p xj   0 for all j',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 250,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '7 Channel Capacity 231 and Accordingly, we make the following statement:\\nThe channel capacity of a discrete memoryless channel, commonly denoted by\\nC, is defined as the maximum mutual information I(X;Y) in any single use of the\\nchannel (i.e., signaling interval), where the maximization is over all possible\\ninput probability distributions {p(xj)} on X.\\nThe channel capacity is clearly an intrinsic property of the channel.\\nEXAMPLE\\nBinary Symmetric Channel (Revisited)\\nConsider again the binary symmetric channel, which is described by the transition\\nprobability diagram of Figure 5.8. This diagram is uniquely defined by the conditional\\nprobability of error p.\\nFrom Example 1 we recall that the entropy H(X) is maximized when the channel input\\nprobability p(x0) = p(x1) = 12, where x0 and x1 are each 0 or 1. Hence, invoking the\\ndefining equation (5.59), we find that the mutual information I(X;Y) is similarly\\nmaximized and thus write\\nFrom Figure 5.8 we have\\nand\\nTherefore, substituting these channel transition probabilities into (5.49) with J = K = 2 and\\nthen setting the input probability p(x0) = p(x1) = 12 in (5.59), we find that the capacity of\\nthe binary symmetric channel is\\n(5.60)\\nMoreover, using the definition of the entropy function introduced in (5.16), we may reduce\\n(5.60) to\\nThe channel capacity C varies with the probability of error (i.e., transition probability) p in\\na convex manner as shown in Figure 5.10, which is symmetric about p = 12. Comparing\\nthe curve in this figure with that in Figure 5.2, we make two observations:\\nWhen the channel is noise free, permitting us to set p = 0, the channel capacity C\\nattains its maximum value of one bit per channel use, which is exactly the\\ninformation in each channel input. At this value of p, the entropy function H(p)\\nattains its minimum value of zero.\\nWhen the conditional probability of error p = 12 due to channel noise, the channel\\ncapacity C attains its minimum value of zero, whereas the entropy function H(p)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 251,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'attains its minimum value of zero.\\nWhen the conditional probability of error p = 12 due to channel noise, the channel\\ncapacity C attains its minimum value of zero, whereas the entropy function H(p)\\np xj   j 0 = J 1 -  1 = C IXY ;  p x0   p x1   1 2  = = = p y0 x1   p y1 x0   p = = p y0 x0   p y1 x1   1 p - = = C 1 p p 1 p -   + 2 1 p -   2 log log + = C 1 H p  - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 251,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '232 Chapter 5 Information Theory attains its maximum value of unity; in such a case, the channel is said to be useless in\\nthe sense that the channel input and output assume statistically independent structures. Channel-coding Theorem\\nWith the entropy of a discrete memoryless source and the corresponding capacity of a\\ndiscrete memoryless channel at hand, we are now equipped with the concepts needed for\\nformulating Shannons second theorem: the channel-coding theorem.\\nTo this end, we first recognize that the inevitable presence of noise in a channel causes\\ndiscrepancies (errors) between the output and input data sequences of a digital\\ncommunication system. For a relatively noisy channel (e.g., wireless communication\\nchannel), the probability of error may reach a value as high as 10-1, which means that (on the\\naverage) only 9 out of 10 transmitted bits are received correctly. For many applications, this\\nlevel of reliability is utterly unacceptable. Indeed, a probability of error equal to 10-6 or even\\nlower is often a necessary practical requirement. To achieve such a high level of\\nperformance, we resort to the use of channel coding.\\nThe design goal of channel coding is to increase the resistance of a digital communication\\nsystem to channel noise. Specifically, channel coding consists of mapping the incoming data\\nsequence into a channel input sequence and inverse mapping the channel output sequence\\ninto an output data sequence in such a way that the overall effect of channel noise on the\\nsystem is minimized. The first mapping operation is performed in the transmitter by a\\nchannel encoder, whereas the inverse mapping operation is performed in the receiver by a\\nchannel decoder, as shown in the block diagram of Figure 5.11; to simplify the exposition,\\nwe have not included source encoding (before channel encoding) and source decoding (after\\nchannel decoding) in this figure.9\\nFigure 5.10\\nVariation of channel capacity of a\\nbinary symmetric channel with',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 252,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'channel decoding) in this figure.9\\nFigure 5.10\\nVariation of channel capacity of a\\nbinary symmetric channel with\\ntransition probability p.\\n0 0.5 0.5 1.0 Channel capacity C Transition probability p Figure 5.11\\nBlock diagram of digital\\ncommunication system.\\nDiscrete memoryless source Channel encoder Discrete memoryless channel Noise Receiver Transmitter Destination Channel decoder',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 252,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Channel-coding Theorem The channel encoder and channel decoder in Figure 5.11 are both under the designers\\ncontrol and should be designed to optimize the overall reliability of the communication\\nsystem. The approach taken is to introduce redundancy in the channel encoder in a\\ncontrolled manner, so as to reconstruct the original source sequence as accurately as\\npossible. In a rather loose sense, we may thus view channel coding as the dual of source\\ncoding, in that the former introduces controlled redundancy to improve reliability whereas\\nthe latter reduces redundancy to improve efficiency.\\nTreatment of the channel-coding techniques is deferred to Chapter 10. For the purpose\\nof our present discussion, it suffices to confine our attention to block codes. In this class of\\ncodes, the message sequence is subdivided into sequential blocks each k bits long, and\\neach k-bit block is mapped into an n-bit block, where n > k. The number of redundant bits\\nadded by the encoder to each transmitted block is n - k bits. The ratio kn is called the code\\nrate. Using r to denote the code rate, we write\\n(5.61)\\nwhere, of course, r is less than unity. For a prescribed k, the code rate r (and, therefore, the\\nsystems coding efficiency) approaches zero as the block length n approaches infinity.\\nThe accurate reconstruction of the original source sequence at the destination requires\\nthat the average probability of symbol error be arbitrarily low. This raises the following\\nimportant question:\\nDoes a channel-coding scheme exist such that the probability that a message bit\\nwill be in error is less than any positive number  (i.e., as small as we want it),\\nand yet the channel-coding scheme is efficient in that the code rate need not be\\ntoo small?\\nThe answer to this fundamental question is an emphatic yes. Indeed, the answer to the\\nquestion is provided by Shannons second theorem in terms of the channel capacity C, as\\ndescribed in what follows.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 253,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The answer to this fundamental question is an emphatic yes. Indeed, the answer to the\\nquestion is provided by Shannons second theorem in terms of the channel capacity C, as\\ndescribed in what follows.\\nUp until this point, time has not played an important role in our discussion of channel\\ncapacity. Suppose then the discrete memoryless source in Figure 5.11 has the source\\nalphabet  and entropy H(S) bits per source symbol. We assume that the source emits\\nsymbols once every Ts seconds. Hence, the average information rate of the source is H(S)Ts\\nbits per second. The decoder delivers decoded symbols to the destination from the source\\nalphabet S and at the same source rate of one symbol every Ts seconds. The discrete\\nmemoryless channel has a channel capacity equal to C bits per use of the channel. We\\nassume that the channel is capable of being used once every Tc seconds. Hence, the\\nchannel capacity per unit time is CTc bits per second, which represents the maximum rate\\nof information transfer over the channel. With this background, we are now ready to state\\nShannons second theorem, the channel-coding theorem,10 in two parts as follows:\\nLet a discrete memoryless source with an alphabet  have entropy H(S) for random\\nvariable S and produce symbols once every Ts seconds. Let a discrete memoryless\\nchannel have capacity C and be used once every Tc seconds, Then, if\\n(5.62) rkn--- = H S  Ts ------------ C Tc -----',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 253,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '234 Chapter 5 Information Theory there exists a coding scheme for which the source output can be transmitted over the\\nchannel and be reconstructed with an arbitrarily small probability of error. The\\nparameter CTc is called the critical rate; when (5.62) is satisfied with the equality\\nsign, the system is said to be signaling at the critical rate.\\nConversely, if\\nit is not possible to transmit information over the channel and reconstruct it with an\\narbitrarily small probability of error.\\nThe channel-coding theorem is the single most important result of information theory. The\\ntheorem specifies the channel capacity C as a fundamental limit on the rate at which the\\ntransmission of reliable error-free messages can take place over a discrete memoryless\\nchannel. However, it is important to note two limitations of the theorem:\\nThe channel-coding theorem does not show us how to construct a good code. Rather,\\nthe theorem should be viewed as an existence proof in the sense that it tells us that if\\nthe condition of (5.62) is satisfied, then good codes do exist. Later, in Chapter10, we\\ndescribe good codes for discrete memoryless channels.\\nThe theorem does not have a precise result for the probability of symbol error after\\ndecoding the channel output. Rather, it tells us that the probability of symbol error\\ntends to zero as the length of the code increases, again provided that the condition of\\n(5.62) is satisfied.\\nApplication of the Channel-coding Theorem to Binary\\nSymmetric Channels\\nConsider a discrete memoryless source that emits equally likely binary symbols (0s and\\n1s) once every Ts seconds. With the source entropy equal to one bit per source symbol (see\\nExample 1), the information rate of the source is (1Ts) bits per second. The source\\nsequence is applied to a channel encoder with code rate r. The channel encoder produces a\\nsymbol once every Tc seconds. Hence, the encoded symbol transmission rate is (1Tc)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 254,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'sequence is applied to a channel encoder with code rate r. The channel encoder produces a\\nsymbol once every Tc seconds. Hence, the encoded symbol transmission rate is (1Tc)\\nsymbols per second. The channel encoder engages a binary symmetric channel once every\\nTc seconds. Hence, the channel capacity per unit time is (CTc) bits per second, where C is\\ndetermined by the prescribed channel transition probability p in accordance with (5.60).\\nAccordingly, part (1) of the channel-coding theorem implies that if\\n(5.63)\\nthen the probability of error can be made arbitrarily low by the use of a suitable channel-\\nencoding scheme. But the ratio TcTs equals the code rate of the channel encoder:\\n(5.64)\\nHence, we may restate the condition of (5.63) simply as\\nH S  Ts ------------ C Tc -----  1 Ts ----- C Tc -----  r Tc Ts ----- = r C',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 254,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Channel-coding Theorem That is, for r  C, there exists a code (with code rate less than or equal to channel capacity\\nC) capable of achieving an arbitrarily low probability of error.\\nEXAMPLE\\nRepetition Code\\nIn this example we present a graphical interpretation of the channel-coding theorem. We also\\nbring out a surprising aspect of the theorem by taking a look at a simple coding scheme.\\nConsider first a binary symmetric channel with transition probability p = 10-2. For this\\nvalue of p, we find from (5.60) that the channel capacity C = 0.9192. Hence, from the\\nchannel-coding theorem, we may state that, for any   0 and r  0.9192, there exists a\\ncode of large enough length n, code rate r, and an appropriate decoding algorithm such\\nthat, when the coded bit stream is sent over the given channel, the average probability of\\nchannel decoding error is less than . This result is depicted in Figure 5.12 for the limiting\\nvalue  = 10-8.\\nTo put the significance of this result in perspective, consider next a simple coding\\nscheme that involves the use of a repetition code, in which each bit of the message is\\nrepeated several times. Let each bit (0 or 1) be repeated n times, where n = 2m + 1 is an\\nodd integer. For example, for n = 3, we transmit 0 and 1 as 000 and 111, respectively.\\nFigure 5.12 Illustrating the significance of the channel-coding theorem.\\n01 0.1 Channel capacity C Code rate, r Limiting value = 10-8 1.0 Repetition code Average probability of error, Pe\\n10-1 10-2 10-3 10-4 10-5 10-6 10-7 10-8 1.0',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 255,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '236 Chapter 5 Information Theory Intuitively, it would seem logical to use a majority rule for decoding, which operates as\\nfollows:\\nIf in a block of n repeated bits (representing one bit of the message) the number\\nof 0s exceeds the number of 1s, the decoder decides in favor of a 0; otherwise, it\\ndecides in favor of a 1.\\nHence, an error occurs when m + 1 or more bits out of n = 2m + 1 bits are received\\nincorrectly. Because of the assumed symmetric nature of the channel, the average\\nprobability of error, denoted by Pe, is independent of the prior probabilities of 0 and 1.\\nAccordingly, we find that Pe is given by\\n(5.65)\\nwhere p is the transition probability of the channel.\\nTable 5.3 gives the average probability of error Pe for a repetition code that is\\ncalculated by using (5.65) for different values of the code rate r. The values given here\\nassume the use of a binary symmetric channel with transition probability p = 10-2. The\\nimprovement in reliability displayed in Table 5.3 is achieved at the cost of decreasing code\\nrate. The results of this table are also shown plotted as the curve labeled repetition code\\nin Figure 5.12. This curve illustrates the exchange of code rate for message reliability,\\nwhich is a characteristic of repetition codes.\\nThis example highlights the unexpected result presented to us by the channel-coding\\ntheorem. The result is that it is not necessary to have the code rate r approach zero (as in\\nthe case of repetition codes) to achieve more and more reliable operation of the\\ncommunication link. The theorem merely requires that the code rate be less than the\\nchannel capacity C.\\nTable 5.3 Average probability of error for repetition code\\nCode rate, r = 1/n\\nAverage probability of error, Pe\\n1 10-2 3 10-4 10-6 4  10-7 10-8 5  10-10 Pe n i  pi 1 p -  n i - i m 1 + = n  = 1 3--- 1 5--- 1 7--- 1 9--- 1 11 ------',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 256,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '9 Differential Entropy and Mutual Information for Continuous Random Ensembles Differential Entropy and Mutual Information for Continuous\\nRandom Ensembles\\nThe sources and channels considered in our discussion of information-theoretic concepts\\nthus far have involved ensembles of random variables that are discrete in amplitude. In this\\nsection, we extend these concepts to continuous random variables. The motivation for\\ndoing so is to pave the way for the description of another fundamental limit in information\\ntheory, which we take up in Section 5.10.\\nConsider a continuous random variable X with the probability density function fX(x).\\nBy analogy with the entropy of a discrete random variable, we introduce the following\\ndefinition:\\n(5.66)\\nWe refer to the new term h(X) as the differential entropy of X to distinguish it from the\\nordinary or absolute entropy. We do so in recognition of the fact that, although h(X) is a\\nuseful mathematical quantity to know, it is not in any sense a measure of the randomness\\nof X. Nevertheless, we justify the use of (5.66) in what follows. We begin by viewing the\\ncontinuous random variable X as the limiting form of a discrete random variable that\\nassumes the value xk = kx, where k = 0, 1, 2, , and x approaches zero. By\\ndefinition, the continuous random variable X assumes a value in the interval [xk, xk  x]\\nwith probability fX(xk)x. Hence, permitting x to approach zero, the ordinary entropy of\\nthe continuous random variable X takes the limiting form\\n(5.67)\\nIn the last line of (5.67), use has been made of (5.66) and the fact that the total area under\\nthe curve of the probability density function fX(x) is unity. In the limit as x approaches\\nzero, the term -log2x approaches infinity. This means that the entropy of a continuous\\nrandom variable is infinitely large. Intuitively, we would expect this to be true because a\\ncontinuous random variable may assume a value anywhere in the interval\\n; we',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 257,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'random variable is infinitely large. Intuitively, we would expect this to be true because a\\ncontinuous random variable may assume a value anywhere in the interval\\n; we\\nmay, therefore, encounter uncountable infinite numbers of probable outcomes. To avoid the\\nproblem associated with the term log2x, we adopt h(X) as a differential entropy, with the\\nterm -log2x serving merely as a reference. Moreover, since the information transmitted\\nover a channel is actually the difference between two entropy terms that have a common\\nreference, the information will be the same as the difference between the corresponding\\nh X  fX x  1 fX x  ------------ 2 log dx  -   = H X  fX xk   k  - =   x 0  lim x 1 fX xk  x ----------------------\\n    2 log = fX xk   log2 1 fX xk   ---------------    x log2x fX xk  x k  - =   - k  - =           x 0  lim = fX x  1 fX x  ------------     2 log dx log2x fX xk   dx  -       x 0  lim -  -   = h X  x 2 log x 0  lim - =   -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 257,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '238 Chapter 5 Information Theory differential entropy terms. We are, therefore, perfectly justified in using the term h(X),\\ndefined in (5.66), as the differential entropy of the continuous random variable X.\\nWhen we have a continuous random vector X consisting of n random variables X1, X2,\\n, Xn, we define the differential entropy of X as the n-fold integral\\n(5.68)\\nwhere fX(x) is the joint probability density function of X.\\nEXAMPLE\\nUniform Distribution\\nTo illustrate the notion of differential entropy, consider a random variable X uniformly\\ndistributed over the interval (0, a). The probability density function of X is\\nApplying (5.66) to this distribution, we get\\n(5.69)\\nNote that loga < 0 for a < 1. Thus, this example shows that, unlike a discrete random vari-\\nable, the differential entropy of a continuous random variable can assume a negative value.\\nRelative Entropy of Continuous Distributions\\nIn (5.12) we defined the relative entropy of a pair of different discrete distributions. To\\nextend that definition to a pair of continuous distributions, consider the continuous random\\nvariables X and Y whose respective probability density functions are denoted by fX(x) and\\nfY(x) for the same sample value (argument) x. The relative entropy11 of the random\\nvariables X and Y is defined by\\n(5.70)\\nwhere fX(x) is viewed as the reference distribution. In a corresponding way to the\\nfundamental property of (5.13), we have\\n(5.71)\\nCombining (5.70) and (5.71) into a single inequality, we may thus write\\nh X   fX x  1 fX x  ------------- 2 log dx  -   = fX x  1 a---, 0 x a   0, otherwise      = h X  1 a--- a  log dx 0 a  = a log = D fY fX   fY x  log2 fY x  fX x  ------------     dx  -   = D fY fX   0  fY x  log2 1 fY x  ------------     dx fY x  log2 1 fX x  ------------     dx  -     -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 258,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '9 Differential Entropy and Mutual Information for Continuous Random Ensembles The expression on the left-hand side of this inequality is recognized as the differential\\nentropy of the random variable Y, namely h(Y). Accordingly,\\n(5.72)\\nThe next example illustrates an insightful application of (5.72).\\nEXAMPLE\\nGaussian Distribution\\nSuppose two random variables, X and Y, are described as follows:\\n\\nthe random variables X and Y have the common mean  and variance\\n;\\n\\nthe random variable X is Gaussian distributed (see Section 3.9) as shown by\\n(5.73)\\nHence, substituting (5.73) into (5.72) and changing the base of the logarithm from 2 to\\ne = 2.7183, we get\\n(5.74)\\nwhere e is the base of the natural algorithm. We now recognize the following\\ncharacterizations of the random variable Y (given that its mean is  and its variance is\\n):\\nWe may, therefore, simplify (5.74) as\\n(5.75)\\nThe quantity on the right-hand side of (5.75) is, in fact, the differential entropy of the\\nGaussian random variable X:\\n(5.76)\\nFinally, combining (5.75) and (5.76), we may write\\n(5.77)\\nwhere equality holds if, and only if, Y = X.\\nWe may now summarize the results of this important example by describing two\\nentropic properties of a random variable:\\nh Y  fY x  log2 1 fY x  ------------     dx  -    2 fX x  1 2 -------------- x  -  2 22 ------------------- - exp = h Y  log2e fY x  x  -  2 22 ------------------- - 2   log - dx  -   -  2 fY x  dx  -   1 = x  -  2fY x  dx  -   2 = h Y  1 2--- 2 2e2   log  h X  1 2--- 2 2e2   log = h Y  h X  X: Gaussian random variable\\nY: nonGaussian random variable',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 259,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '240 Chapter 5 Information Theory PROPERTY 1 For any finite variance, a Gaussian random variable has the largest differential entropy\\nattainable by any other random variable.\\nPROPERTY\\nThe entropy of a Gaussian random variable is uniquely determined by its variance (i.e.,\\nthe entropy is independent of the mean).\\nIndeed, it is because of Property1 that the Gaussian channel model is so widely used as a\\nconservative model in the study of digital communication systems.\\nMutual Information\\nContinuing with the information-theoretic characterization of continuous random\\nvariables, we may use analogy with (5.47) to define the mutual information between the\\npair of continuous random variables X and Y as follows:\\n(5.78)\\nwhere fX,Y(x,y) is the joint probability density function of X and Y and fX(x|y) is the\\nconditional probability density function of X given Y = y. Also, by analogy with (5.45),\\n(5.50), (5.43), and (5.44), we find that the mutual information between the pair of Gausian\\nrandom variables has the following properties:\\n(5.79) (5.80) (5.81) The parameter h(X) is the differential entropy of X; likewise for h(Y). The parameter\\nh(X|Y) is the conditional differential entropy of X given Y; it is defined by the double\\nintegral (see (5.41))\\n(5.82)\\nThe parameter h(Y|X) is the conditional differential entropy of Y given X; it is defined in a\\nmanner similar to h(X|Y). Information Capacity Law\\nIn this section we use our knowledge of probability theory to expand Shannons channel-\\ncoding theorem, so as to formulate the information capacity for a band-limited, power-\\nlimited Gaussian channel, depicted in Figure 5.13. To be specific, consider a zero-mean\\nstationary process X(t) that is band-limited to B hertz. Let Xk, k = 1, 2, , K, denote the\\ncontinuous random variables obtained by uniform sampling of the process X(t) at a rate of\\n2B samples per second. The rate 2B samples per second is the smallest permissible rate for',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 260,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'continuous random variables obtained by uniform sampling of the process X(t) at a rate of\\n2B samples per second. The rate 2B samples per second is the smallest permissible rate for\\na bandwidth B that would not result in a loss of information in accordance with the\\nsampling theorem; this is discussed in Chapter 6. Suppose that these samples are\\nIXY ;   fX Y  x y    log2 fX x y   fX x  ----------------- dx dy\\n -    -   = IXY ;   IYX ;   = IXY ;   0  IXY ;   h X  hXY   - = h Y  hYX   - = hXY   fX Y  x y    1 fX x y   ----------------- 2 log dx dy  -    -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 260,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '10 Information Capacity Law transmitted in T seconds over a noisy channel, also band-limited to B hertz. Hence, the\\ntotal number of samples K is given by\\nK = 2BT\\n(5.83)\\nWe refer to Xk as a sample of the transmitted signal. The channel output is perturbed by\\nadditive white Gaussian noise (AWGN) of zero mean and power spectral density N02.\\nThe noise is band-limited to B hertz. Let the continuous random variables Yk, k = 1, 2, ,\\nK, denote the corresponding samples of the channel output, as shown by\\nYk = Xk + Nk, k = 1, 2, , K (5.84) The noise sample Nk in (5.84) is Gaussian with zero mean and variance = N0B\\n(5.85)\\nWe assume that the samples Yk, k = 1, 2, , K, are statistically independent.\\nA channel for which the noise and the received signal are as described in (5.84) and\\n(5.85) is called a discrete-time, memoryless Gaussian channel, modeled as shown in\\nFigure 5.13. To make meaningful statements about the channel, however, we have to\\nassign a cost to each channel input. Typically, the transmitter is power limited; therefore, it\\nis reasonable to define the cost as\\n(5.86)\\nwhere P is the average transmitted power. The power-limited Gaussian channel described\\nherein is not only of theoretical importance but also of practical importance, in that it\\nmodels many communication channels, including line-of-sight radio and satellite links.\\nThe information capacity of the channel is defined as the maximum of the mutual\\ninformation between the channel input Xk and the channel output Yk over all distributions\\nof the input Xk that satisfy the power constraint of (5.86). Let I(Xk;Yk) denote the mutual\\ninformation between Xk and Yk. We may then define the information capacity of the\\nchannel as\\n,\\nsubject to the constraint\\nfor all k\\n(5.87)\\nIn words, maximization of the mutual information I(Xk;Yk) is done with respect to all prob-\\nability distributions of the channel input Xk, satisfying the power constraint\\n.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 261,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'for all k\\n(5.87)\\nIn words, maximization of the mutual information I(Xk;Yk) is done with respect to all prob-\\nability distributions of the channel input Xk, satisfying the power constraint\\n.\\nThe mutual information I(Xk;Yk) can be expressed in one of the two equivalent forms\\nshown in (5.81). For the purpose at hand, we use the second line of this equation to write\\n(5.88)\\nFigure 5.13 Model of discrete-time, memoryless Gaussian channel.\\nXk Nk Yk \\x02 Input Output Noise 2 \\x03 Xk 2   P k 1 2 K   =   C max fXk x  = I Xk Yk ;   \\x03 Xk 2   P = \\x03 Xk 2   P = I Xk Yk ;   h Yk   h Yk Xk   - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 261,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '242 Chapter 5 Information Theory Since Xk and Nk are independent random variables and their sum equals Yk in accordance\\nwith (5.84), we find that the conditional differential entropy of Yk given Xk is equal to the\\ndifferential entropy of Nk, as shown by\\n(5.89)\\nHence, we may rewrite (5.88) as\\n(5.90)\\nWith h(Nk) being independent of the distribution of Xk, it follows that maximizing I(Xk;Yk)\\nin accordance with (5.87) requires maximizing the differential entropy h(Yk). For h(Yk) to\\nbe maximum, Yk has to be a Gaussian random variable. That is to say, samples of the\\nchannel output represent a noiselike process. Next, we observe that since Nk is Gaussian\\nby assumption, the sample Xk of the channel input must be Gaussian too. We may\\ntherefore state that the maximization specified in (5.87) is attained by choosing samples of\\nthe channel input from a noiselike Gaussian-distributed process of average power P.\\nCorrespondingly, we may reformulate (5.87) as and for all k (5.91) where the mutual information I(Xk;Yk) is defined in accordance with (5.90).\\nFor evaluation of the information capacity C, we now proceed in three stages:\\nThe variance of sample Yk of the channel output equals P +\\n, which is a\\nconsequence of the fact that the random variables X and N are statistically\\nindependent; hence, the use of (5.76) yields the differential entropy\\n(5.92)\\nThe variance of the noisy sample Nk equals\\n; hence, the use of (5.76) yields the\\ndifferential entropy\\n(5.93)\\nSubstituting (5.92) and (5.93) into (5.90), and recognizing the definition of\\ninformation capacity given in (5.91), we get the formula:\\nbits per channel use\\n(5.94)\\nWith the channel used K times for the transmission of K samples of the process X(t) in\\nT seconds, we find that the information capacity per unit time is (KT) times the result\\ngiven in (5.94). The number K equals 2BT, as in (5.83). Accordingly, we may express the\\ninformation capacity of the channel in the following equivalent form:\\nbits per second\\n(5.95)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 262,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'given in (5.94). The number K equals 2BT, as in (5.83). Accordingly, we may express the\\ninformation capacity of the channel in the following equivalent form:\\nbits per second\\n(5.95)\\nwhere N0B is the total noise power at the channel output, defined in accordance with (5.85).\\nBased on the formula of (5.95), we may now make the following statement\\nThe information capacity of a continuous channel of bandwidth B hertz,\\nperturbed by AWGN of power spectral density N02 and limited in bandwidth\\nh Yk Xk   h Nk   = I Xk Yk ;   h Yk   h Nk   - = C I Xk Yk ;  : for Gaussian Xk = \\x03 Xk 2   P = 2 h Yk   1 2---log2 2e P 2 +     = 2 h Nk   1 2---log2 2e2   = C 1 2---log2 1 P 2 ------ +     = C B log2 1 P N0B ---------- +     =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 262,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '10 Information Capacity Law to B, is given by the formula\\nbits per second\\nwhere P is the average transmitted power.\\nThe information capacity law12 of (5.95) is one of the most remarkable results of\\nShannons information theory. In a single formula, it highlights most vividly the interplay\\namong three key system parameters: channel bandwidth, average transmitted power, and\\npower spectral density of channel noise. Note, however, that the dependence of\\ninformation capacity C on channel bandwidth B is linear, whereas its dependence on\\nsignal-to-noise ratio P(N0B) is logarithmic. Accordingly, we may make another insightful\\nstatement:\\nIt is easier to increase the information capacity of a continuous communication\\nchannel by expanding its bandwidth than by increasing the transmitted power\\nfor a prescribed noise variance.\\nThe information capacity formula implies that, for given average transmitted power P and\\nchannel bandwidth B, we can transmit information at the rate of C bits per second, as\\ndefined in (5.95), with arbitrarily small probability of error by employing a sufficiently\\ncomplex encoding system. It is not possible to transmit at a rate higher than C bits per\\nsecond by any encoding system without a definite probability of error. Hence, the channel\\ncapacity law defines the fundamental limit on the permissible rate of error-free\\ntransmission for a power-limited, band-limited Gaussian channel. To approach this limit,\\nhowever, the transmitted signal must have statistical properties approximating those of\\nwhite Gaussian noise.\\nSphere Packing\\nTo provide a plausible argument supporting the information capacity law, suppose that we\\nuse an encoding scheme that yields K codewords, one for each sample of the transmitted\\nsignal. Let n denote the length (i.e., the number of bits) of each codeword. It is presumed\\nthat the coding scheme is designed to produce an acceptably low probability of symbol',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 263,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'signal. Let n denote the length (i.e., the number of bits) of each codeword. It is presumed\\nthat the coding scheme is designed to produce an acceptably low probability of symbol\\nerror. Furthermore, the codewords satisfy the power constraint; that is, the average power\\ncontained in the transmission of each codeword with n bits is nP, where P is the average\\npower per bit.\\nSuppose that any codeword in the code is transmitted. The received vector of n bits is\\nGaussian distributed with a mean equal to the transmitted codeword and a variance equal\\nto n\\n, where is the noise variance. With a high probability, we may say that the\\nreceived signal vector at the channel output lies inside a sphere of radius\\n; that is,\\ncentered on the transmitted codeword. This sphere is itself contained in a larger sphere of\\nradius\\n, where n(P + 2) is the average power of the received signal vector.\\nWe may thus visualize the sphere packing13 as portrayed in Figure 5.14. With\\neverything inside a small sphere of radius assigned to the codeword on which it is\\nC B log2 1 P N0B ---------- +     = 2 2 n2 n P 2 +   n2',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 263,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '244 Chapter 5 Information Theory centered. It is therefore reasonable to say that, when this particular codeword is\\ntransmitted, the probability that the received signal vector will lie inside the correct\\ndecoding sphere is high. The key question is:\\nHow many decoding spheres can be packed inside the larger sphere of received\\nsignal vectors? In other words, how many codewords can we in fact choose?\\nTo answer this question, we want to eliminate the overlap between the decoding spheres as\\ndepicted in Figure 5.14. Moreover, expressing the volume of an n-dimensional sphere of\\nradius r as Anrn, where An is a scaling factor, we may go on to make two statements:\\nThe volume of the sphere of received signal vectors is An[n(P +\\n)]n/2.\\nThe volume of the decoding sphere is An(n\\n)n/2.\\nAccordingly, it follows that the maximum number of nonintersecting decoding spheres\\nthat can be packed inside the sphere of possible received signal vectors is given by\\n(5.96)\\nTaking the logarithm of this result to base 2, we readily see that the maximum number of\\nbits per transmission for a low probability of error is indeed as defined previously in (5.94).\\nA final comment is in order: (5.94) is an idealized manifestation of Shannons channel-\\ncoding theorem, in that it provides an upper bound on the physically realizable\\ninformation capacity of a communication channel. Implications of the Information Capacity Law\\nNow that we have a good understanding of the information capacity law, we may go on to\\ndiscuss its implications in the context of a Gaussian channel that is limited in both power\\nFigure 5.14 The sphere-packing problem.\\n2 2 An n P 2 +     n 2  An n2   n 2  --------------------------------------------\\n1 P 2 ------ +    n 2  = 2 n 2    1 P 2  +   2 log =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 264,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '11 Implications of the Information Capacity Law and bandwidth. For the discussion to be useful, however, we need an ideal framework\\nagainst which the performance of a practical communication system can be assessed. To\\nthis end, we introduce the notion of an ideal system, defined as a system that transmits\\ndata at a bit rate Rb equal to the information capacity C. We may then express the average\\ntransmitted power as P = EbC (5.97) where Eb is the transmitted energy per bit. Accordingly, the ideal system is defined by the\\nequation\\n(5.98)\\nRearranging this formula, we may define the signal energy-per-bit to noise power spectral\\ndensity ratio, Eb/N0, in terms of the ratio CB for the ideal system as follows:\\n(5.99)\\nA plot of the bandwidth efficiency Rb/B versus Eb/N0 is called the bandwidth-efficiency\\ndiagram. A generic form of this diagram is displayed in Figure 5.15, where the curve\\nlabeled capacity boundary corresponds to the ideal system for which Rb = C.\\nFigure 5.15 Bandwidth-efficiency diagram.\\nC B---- 1 Eb N0 ------C B---- +     2 log = Eb N0 ------ 2C B  1 - C B  ---------------------\\n= 20 30 10 0 0.1 0 -1.6 6 12 18 Region for which Rb < C Bandwidth efficiency,\\nCapacity boundary for which Rb = C 24 30 36 Eb N0 Rb B Shannon limit Region for which Rb > C , dB',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 265,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '246 Chapter 5 Information Theory Based on Figure 5.15, we can make three observations:\\nFor infinite channel bandwidth, the ratio Eb/N0 approaches the limiting value\\n(5.100)\\nwhere loge stands for the natural logarithm ln. The value defined in (5.100) is called\\nthe Shannon limit for an AWGN channel, assuming a code rate of zero. Expressed in\\ndecibels, the Shannon limit equals -1.6 dB. The corresponding limiting value of the\\nchannel capacity is obtained by letting the channel bandwidth B in (5.95) approach\\ninfinity, in which case we obtain\\n(5.101)\\nThe capacity boundary is defined by the curve for the critical bit rate Rb = C. For\\nany point on this boundary, we may flip a fair coin (with probability of 12) whether\\nwe have error-free transmission or not. As such, the boundary separates\\ncombinations of system parameters that have the potential for supporting error-free\\ntransmission (Rb < C) from those for which error-free transmission is not possible\\n(Rb > C). The latter region is shown shaded in Figure 5.15.\\nThe diagram highlights potential trade-offs among three quantities: the EbN0, the\\nratio RbB, and the probability of symbol error Pe. In particular, we may view\\nmovement of the operating point along a horizontal line as trading Pe versus EbN0\\nfor a fixed RbB. On the other hand, we may view movement of the operating point\\nalong a vertical line as trading Pe versus RbB for a fixed EbN0.\\nEXAMPLE\\nCapacity of Binary-Input AWGN Channel\\nIn this example, we investigate the capacity of an AWGN channel using encoded binary\\nantipodal signaling (i.e., levels -1 and 1 for binary symbols 0 and 1, respectively). In\\nparticular, we address the issue of determining the minimum achievable bit error rate as a\\nfunction of EbN0 for varying code rate r. It is assumed that the binary symbols 0 and 1 are\\nequiprobable.\\nLet the random variables X and Y denote the channel input and channel output\\nrespectively; X is a discrete variable, whereas Y is a continuous variable. In light of the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 266,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'equiprobable.\\nLet the random variables X and Y denote the channel input and channel output\\nrespectively; X is a discrete variable, whereas Y is a continuous variable. In light of the\\nsecond line of (5.81), we may express the mutual information between the channel input\\nand channel output as\\nThe second term, h(Y|X), is the conditional differential entropy of the channel output Y,\\ngiven the channel input X. By virtue of (5.89) and (5.93), this term is just the entropy of a\\nGaussian distribution. Hence, using to denote the variance of the channel noise, we write\\nEb N0 ------      Eb N0 ------     B   lim = 2 e log 0.693 = = C C B   lim = P N0 ------     e 2 log = IXY ;   h Y  h - Y X   = 2 hYX   1 2--- log2 2e2   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 266,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '11 Implications of the Information Capacity Law Next, the first term, h(Y), is the differential entropy of the channel output Y. With the use of\\nbinary antipodal signaling, the probability density function of Y, given X = x, is a mixture\\nof two Gaussian distributions with common variance and mean values -1 and +1, as\\nshown by\\n(5.102)\\nHence, we may determine the differential entropy of Y using the formula\\nwhere fY(yi | x) is defined by (5.102). From the formulas of h(Y|X) and h(Y), it is clear that\\nthe mutual information is solely a function of the noise variance\\n. Using M(\\n) to\\ndenote this functional dependence, we may thus write\\nUnfortunately, there is no closed formula that we can derive for M(\\n) because of the\\ndifficulty of determining h(Y). Nevertheless, the differential entropy h(Y) can be well\\napproximated using Monte Carlo integration; see Appendix E for details.\\nBecause symbols 0 and 1 are equiprobable, it follows that the channel capacity C is\\nequal to the mutual information between X and Y. Hence, for error-free data transmission\\nover the AWGN channel, the code rate r must satisfy the condition\\n(5.103)\\nA robust measure of the ratio Eb/N0, is\\nwhere P is the average transmitted power and N0/2 is the two-sided power spectral density\\nof the channel noise. Without loss of generality, we may set P = 1. We may then express\\nthe noise variance as\\n(5.104)\\nSubstituting Equation (5.104) into (5.103) and rearranging terms, we get the desired\\nrelation:\\n(5.105)\\nwhere M-1(r) is the inverse of the mutual information between the channel input and\\nputput, expressed as a function of the code rate r.\\nUsing the Monte Carlo method to estimate the differential entropy h(Y) and therefore\\nM-1(r), the plots of Figure 5.16 are computed.14 Figure 5.16a plots the minimum Eb/N0\\nversus the code rate r for error-free transmission. Figure 5.16b plots the minimum\\n2 fY yi x   1 2--- exp yi 1 +  2/22 -   2 --------------------------------------------------',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 267,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'versus the code rate r for error-free transmission. Figure 5.16b plots the minimum\\n2 fY yi x   1 2--- exp yi 1 +  2/22 -   2 --------------------------------------------------\\nexp yi 1 -  2/22 -   2 -------------------------------------------------\\n+       = h Y  fY yi x  log2 fY yi x     dyi  -   - = 2 2 IXY ;   M 2   = 2 r M 2    Eb N0 ------ P N0r --------- P 22r ------------ = = 2 N0 2Ebr ------------ = Eb N0 ------ 1 2rM 1 - r -----------------------\\n=',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 267,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '248 Chapter 5 Information Theory achievable bit error rate versus Eb/N0 with the code rate r as a running parameter. From\\nFigure 5.16 we may draw the following conclusions:\\n\\nFor uncoded binary signaling (i.e., r = 1), an infinite Eb/N0 is required for error-free\\ncommunication, which agrees with what we know about uncoded data transmission\\nover an AWGN channel.\\n\\nThe minimum Eb/N0, decreases with decreasing code rate r, which is intuitively\\nsatisfying. For example, for r = 1/2, the minimum value of Eb/N0 is slightly less than\\n2 dB.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 268,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The minimum Eb/N0, decreases with decreasing code rate r, which is intuitively\\nsatisfying. For example, for r = 1/2, the minimum value of Eb/N0 is slightly less than\\n2 dB.\\n\\nAs r approaches zero, the minimum Eb/N0 approaches the limiting value of -1.6 dB,\\nwhich agrees with the Shannon limit derived earlier; see (5.100). Information Capacity of Colored Noisy Channel\\nThe information capacity theorem as formulated in (5.95) applies to a band-limited white\\nnoise channel. In this section we extend Shannons information capacity law to the more\\ngeneral case of a nonwhite, or colored, noisy channel.15 To be specific, consider the\\nchannel model shown in Figure 5.17a where the transfer function of the channel is denoted\\nby H(f). The channel noise n(t), which appears additively at the channel output, is\\nmodeled as the sample function of a stationary Gaussian process of zero mean and power\\nspectral density SN(f). The requirement is twofold:\\nFind the input ensemble, described by the power spectral density Sxx(f), that\\nmaximizes the mutual information between the channel output y(t) and the channel\\ninput x(t), subject to the constraint that the average power of x(t) is fixed at a\\nconstant value P.\\nHence, determine the optimum information capacity of the channel.\\nFigure 5.16 Binary antipodal signaling over an AWGN channel. (a) Minimum Eb/N0 versus the\\ncode rate r. (b) Minimum bit error rate versus Eb/N0 for varying code rate r.\\nCode rate r, bits/transmission\\n(a) Eb/N0, dB (b) 1 -2 -1.5 -1 -0.5 0 0.5 1 1/32 1/16 1/8 1/5 1/4 1/3 1/2 Minimum Eb/N0, dB 4 3 2 1 0 -1 -2 Minimum bit error rate\\nr = 1 10-1 10-2 10-3 10-4 10-5 r = 1/2 r = 1/3 r = 1/4 r = 1/5 r = 1/8',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 268,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': \"12 Information Capacity of Colored Noisy Channel This problem is a constrained optimization problem. To solve it, we proceed as follows:\\n\\nBecause the channel is linear, we may replace the model of Figure 5.17a with the\\nequivalent model shown in Figure 5.17b. From the viewpoint of the spectral\\ncharacteristics of the signal plus noise measured at the channel output, the two\\nmodels of Figure 5.17 are equivalent, provided that the power spectral density of the\\nnoise n'(t) in Figure 5.17b is defined in terms of the power spectral density of the\\nnoise n(t) in Figure 5.17a as\\n(5.106)\\nwhere |H(f)| is the magnitude response of the channel.\\n\\nTo simplify the analysis, we use the principle of divide and conquer to\\napproximate the continuous |H(f)| described as a function of frequency f in the form\\nof a staircase, as illustrated in Figure 5.18. Specifically, the channel is divided into a\\nlarge number of adjoining frequency slots. The smaller we make the incremental\\nfrequency interval of each subchannel, the better this approximation is.\\nThe net result of these two points is that the original model of Figure 5.17a is replaced by\\nthe parallel combination of a finite number of subchannels, N, each of which is corrupted\\nessentially by band-limited white Gaussian noise.\\nFigure 5.17 (a) Model of band-limited, power-limited noisy channel. (b) Equivalent\\nmodel of the channel.\\nFigure 5.18 Staricase approximation of an arbitrary magnitude response\\n|H(f)|; only the positive frequency portion of the response is shown.\\n H( f ) Output y(t) Colored noise n(t) Input x(t)  H( f ) Output y(t) Modified colored noise n'(t) x(t) (a) (b) SNNf SNN f H f2 ------------------ = f 0 Actual response Staircase approximation f |H(f)| f\",\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 269,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '250 Chapter 5 Information Theory The kth subchannel in the approximation to the model of Figure 5.17b is described by\\n(5.107)\\nThe average power of the signal component xk(t) is\\n(5.108)\\nwhere SX(fk) is the power spectral density of the input signal evaluated at the frequency\\nf = fk. The variance of the noise component nk(t) is\\n(5.109)\\nwhere SN(fk) and |H(fk)| are the noise spectral density and the channels magnitude response\\nevaluated at the frequency fk, respectively. The information capacity of the kth subchannel is\\n(5.110)\\nwhere the factor 1/2 accounts for the fact that applies to both positive and negative\\nfrequencies. All the N subchannels are independent of one another. Hence, the total\\ncapacity of the overall channel is approximately given by the summation\\n(5.111)\\nThe problem we have to address is to maximize the overall information capacity C subject\\nto the constraint\\n(5.112)\\nThe usual procedure to solve a constrained optimization problem is to use the method of\\nLagrange multipliers (see Appendix D for a discussion of this method). To proceed with\\nthis optimization, we first define an objective function that incorporates both the\\ninformation capacity C and the constraint (i.e., (5.111) and (5.112)), as shown by\\n(5.113)\\nwhere  is the Lagrange multiplier. Next, differentiating the objective function J(Pk) with\\nrespect to Pk and setting the result equal to zero, we obtain\\nyk t xk t nk t k 1 2 N   =  + = Pk SXX fk  f k 1 2 N   =  = k 2 SNN fk   H fk  2 ------------------- = f k 1 2 N   =  Ck 1 2---f log2 1 Pk k 2 ------ +       k 1 2 N   =  = f C Ck k 1 = N   1 2--- = f log2 1 Pk k 2 ------ +       k 1 = N  Pk k 1 = N  P constant = = J Pk   1 2--- = f log2 1 Pk k 2 ------ +       P Pk k 1 = N  -         + k 1 = N  f log2e Pk k 2 + ---------------------\\n - 0 =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 270,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '12 Information Capacity of Colored Noisy Channel To satisfy this optimizing solution, we impose the following requirement:\\n(5.114)\\nwhere K is a constant that is the same for all k. The constant K is chosen to satisfy the\\naverage power constraint.\\nInserting the defining values of (5.108) and (5.109) in the optimizing condition of\\n(5.114), simplifying, and rearranging terms we get\\n(5.115)\\nLet denote the frequency range for which the constant K satisfies the condition\\nThen, as the incremental frequency interval is allowed to approach zero and the\\nnumber of subchannels N goes to infinity, we may use (5.115) to formally state that the\\npower spectral density of the input ensemble that achieves the optimum information\\ncapacity is a nonnegative quantity defined by\\n(5.116)\\nBecause the average power of a random process is the total area under the curve of the\\npower spectral density of the process, we may express the average power of the channel\\ninput x(t) as\\n(5.117)\\nFor a prescribed P and specified SN(f) and H(f), the constant K is the solution to (5.117).\\nThe only thing that remains for us to do is to find the optimum information capacity.\\nSubstituting the optimizing solution of (5.114) into (5.111) and then using the defining\\nvalues of (5.108) and (5.109), we obtain\\nWhen the incremental frequency interval is allowed to approach zero, this equation\\ntakes the limiting form\\n(5.118)\\nwhere the constant K is chosen as the solution to (5.117) for a prescribed input signal\\npower P. Pk  + k 2 K = f for k 1 2 N   = SXX fk   K = SNN fk   H fk  2 ------------------- - k 1 2 N   =  K SNN fk   H fk  2 -------------------  f SXX f K SNN f H f2 ----------------- f A  - 0 otherwise       = P K SNN f H f2 ----------------- -       df f A   = C 1 2--- f log2 K H fk  2 SNN fk   -------------------       k 1 = N   f C 1 2--- log2 KHf2 SNN f ----------------       df  -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 271,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '252 Chapter 5 Information Theory Water-filling Interpretation of the Information Capacity Law\\nEquations (5.116) and (5.117) suggest the picture portrayed in Figure 5.19. Specifically,\\nwe make the following observations:\\n\\nThe appropriate input power spectral density SX(f) is described as the bottom\\nregions of the function SN(f)/|H(f)|2 that lie below the constant level K, which are\\nshown shaded.\\n\\nThe input power P is defined by the total area of these shaded regions.\\nThe spectral-domain picture portrayed here is called the water-filling (pouring)\\ninterpretation, in the sense that the process by which the input power is distributed across\\nthe function SN(f)/ |H(f)|2 is identical to the way in which water distributes itself in a vessel.\\nConsider now the idealized case of a band-limited signal in AWGN channel of power spectral\\ndensity N(f) = N0/2. The transfer function H(f) is that of an ideal band-pass filter defined by\\nwhere fc is the midband frequency and B is the channel bandwidth. For this special case,\\n(5.117) and (5.118) reduce respectively to\\nand\\nHence, eliminating K between these two equations, we get the standard form of Shannons\\ncapacity theorem, defined by (5.95).\\nEXAMPLE\\nCapacity of NEXT-Dominated Channel\\nDigital subscriber lines (DSLs) refer to a family of different technologies that operate\\nover a closed transmission loop; they will be discussed in Chapter 8, Section 8.11. For the\\nH f 1 0 fc B 2--- - f fc B 2--- +    0 otherwise      = P 2B K N0 2 ------ -     = C B log2 2K N0 -------     = Figure 5.19 Water-filling interpretation of information-capacity\\ntheorem for a colored noisy channel.\\nSN( f ) |H( f )|2 0 K f SX( f )',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 272,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '13 Rate Distortion Theory present, it suffices to say that a DSL is designed to provide for data transmission between\\na user terminal (e.g., computer) and the central office of a telephone company. A major\\nchannel impairment that arises in the deployment of a DSL is the near-end cross-talk\\n(NEXT). The power spectral density of this crosstalk may be taken as\\n(5.119)\\nwhere SX(f) is the power spectral density of the transmitted signal and HNEXT(f) is the\\ntransfer function that couples adjacent twisted pairs. The only constraint we have to satisfy\\nin this example is that the power spectral density function SX(f) be nonnegative for all f.\\nSubstituting (5.119) into (5.116), we readily find that this condition is satisfied by solving\\nfor K as\\nFinally, using this result in (5.118), we find that the capacity of the NEXT-dominated\\ndigital subscriber channel is given by\\nwhere A is the set of positive and negative frequencies for which SX(f) > 0. Rate Distortion Theory\\nIn Section 5.3 we introduced the source-coding theorem for a discrete memoryless source,\\naccording to which the average codeword length must be at least as large as the source\\nentropy for perfect coding (i.e., perfect representation of the source). However, in many\\npractical situations there are constraints that force the coding to be imperfect, thereby\\nresulting in unavoidable distortion. For example, constraints imposed by a communication\\nchannel may place an upper limit on the permissible code rate and, therefore, on average\\ncodeword length assigned to the information source. As another example, the information\\nsource may have a continuous amplitude as in the case of speech, and the requirement is to\\nquantize the amplitude of each sample generated by the source to permit its representation\\nby a codeword of finite length as in pulse-code modulation to be discussed in Chapter 6. In\\nsuch cases, the problem is referred to as source coding with a fidelity criterion, and the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 273,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'by a codeword of finite length as in pulse-code modulation to be discussed in Chapter 6. In\\nsuch cases, the problem is referred to as source coding with a fidelity criterion, and the\\nbranch of information theory that deals with it is called rate distortion theory.16 Rate\\ndistortion theory finds applications in two types of situations:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 273,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Source coding where the permitted coding alphabet cannot exactly represent the\\ninformation source, in which case we are forced to do lossy data compression.\\n\\nInformation transmission at a rate greater than channel capacity.\\nAccordingly, rate distortion theory may be viewed as a natural extension of Shannons\\ncoding theorem. SN f HNEXT f2SX f = K 1 HNEXT f2 H f2 -----------------------------\\n+      SX f = C 1 2--- log2 1 H f2 HNEXT f2 -----------------------------\\n+       df A  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 273,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': '254 Chapter 5 Information Theory Rate Distortion Function\\nConsider a discrete memoryless source defined by an M-ary alphabet  : {xi|i = 1, 2, M},\\nwhich consists of a set of statistically independent symbols together with the associated sym-\\nbol probabilities {pi|i = 1, 2,  M}. Let R be the average code rate in bits per codeword.\\nThe representation codewords are taken from another alphabet  :{yj| j = 1, 2,  N}. The\\nsource-coding theorem states that this second alphabet provides a perfect representation of\\nthe source provided that R H, where H is the source entropy. But if we are forced to have\\nR H, then there is unavoidable distortion and, therefore, loss of information.\\nLet p(xi, yj) denote the joint probability of occurrence of source symbol xi and\\nrepresentation symbol yj. From probability theory, we have\\n(5.120)\\nwhere p(yj|xi) is a transition probability. Let d(xi, yj) denote a measure of the cost incurred\\nin representing the source symbol xi by the symbol yj; the quantity d(xi, yj) is referred to as\\na single-letter distortion measure. The statistical average of d(xi, yj) over all possible\\nsource symbols and representation symbols is given by\\n(5.121)\\nNote that the average distortion is a nonnegative continuous function of the transition\\nprobabilities p(yj|xi) that are determined by the source encoder-decoder pair.\\nA conditional probability assignment p(yj|xi) is said to be D-admissible if, and only if,\\nthe average distortion is less than or equal to some acceptable value D. The set of all\\nD-admissible conditional probability assignments is denoted by\\n(5.122)\\nFor each set of transition probabilities, we have a mutual information\\n(5.123)\\nA rate distortion function R(D) is defined as the smallest coding rate possible for which\\nthe average distortion is guaranteed not to exceed D. Let D denote the set to which the\\nconditional probability p(yj|xi) belongs for a prescribed D. Then, for a fixed D we write17\\n(5.124)\\nsubject to the constraint\\n(5.125)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 274,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'conditional probability p(yj|xi) belongs for a prescribed D. Then, for a fixed D we write17\\n(5.124)\\nsubject to the constraint\\n(5.125)\\nThe rate distortion function R(D) is measured in units of bits if the base-2 logarithm is\\nused in (5.123). Intuitively, we expect the distortion D to decrease as the rate distortion\\nfunction R(D) is increased. We may say conversely that tolerating a large distortion D\\npermits the use of a smaller rate for coding and/or transmission of information.\\np xi yj    p yj xi  p xi   = d p xi  p yj xi  d xi yj   j 1 = N  i 1 = M  = d d D p yj xi  : d D    = IXY ;   p xi  p yj xi   log p yj xi   p yj   -------------------       j 1 = N  i 1 = M  = R D   min p yj xi   D  = IXY ;   p yj xi   1 for i 1 2 M   = = j 1 = N',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 274,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '13 Rate Distortion Theory Figure 5.20 summarizes the main parameters of rate distortion theory. In particular,\\ngiven the source symbols {xi} and their probabilities {pi}, and given a definition of the\\nsingle-letter distortion measure d(xi,yj), the calculation of the rate distortion function R(D)\\ninvolves finding the conditional probability assignment p(yj | xi) subject to certain\\nconstraints imposed on p(yj | xi). This is a variational problem, the solution of which is\\nunfortunately not straightforward in general.\\nEXAMPLE\\nGaussian Source\\nConsider a discrete-time, memoryless Gaussian source with zero mean and variance\\n.\\nLet x denote the value of a sample generated by such a source. Let y denote a quantized\\nversion of x that permits a finite representation of it. The square-error distortion\\nprovides a distortion measure that is widely used for continuous alphabets. The rate\\ndistortion function for the Gaussian source with square-error distortion, as described\\nherein, is given by\\n(5.126)\\nIn this case, we see that R(D)   as D  0, and R(D) = 0 for D =\\n.\\nEXAMPLE\\nSet of Parallel Gaussian Sources\\nConsider next a set of N independent Gaussian random variables\\n, where Xi has\\nzero mean and variance i2.Using the distortion measure\\nand building on the result of Example 12, we may express the rate distortion function for\\nthe set of parallel Gaussian sources described here as\\n(5.127) Figure 5.20 Summary of rate distortion theory. Transition probability p(yj | xi) xi yj d(xi, yj) Probability of occurrence = pi Distortion measure 2 dxy    x y -  2 = R D   1 2--- log 2 D ------     0 D 2    0 D 2        = 2 Xi  i 1 = N d xi xi -  2 xi estimate of xi =  i 1 = N  = R D   1 2--- log i 2 Di ------       i 1 = N  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 275,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '256 Chapter 5 Information Theory where Di is itself defined by\\n(5.128)\\nand the constant  is chosen so as to satisfy the condition\\n(5.129)\\nCompared to Figure 5.19, (5.128) and (5.129) may be interpreted as a kind of water-\\nfilling in reverse, as illustrated in Figure 5.21. First, we choose a constant  and only the\\nsubset of random variables whose variances exceed the constant . No bits are used to\\ndescribe the remaining subset of random variables whose variances are less than the\\nconstant . Summary and Discussion\\nIn this chapter we established two fundamental limits on different aspects of a communi-\\ncation system, which are embodied in the source-coding theorem and the channel-coding\\ntheorem.\\nThe source-coding theorem, Shannons first theorem, provides the mathematical tool\\nfor assessing data compaction; that is, lossless compression of data generated by a\\ndiscrete memoryless source. The theorem teaches us that we can make the average number\\nof binary code elements (bits) per source symbol as small as, but no smaller than, the\\nentropy of the source measured in bits. The entropy of a source is a function of the\\nprobabilities of the source symbols that constitute the alphabet of the source. Since\\nFigure 5.21 Reverse water-filling picture for a set of\\nparallel Gaussian processes.\\nDi   i 2   i 2  i 2        = Di D = i 1 = N  1 Variance i 2 2 3 4 5 6 Source index i 1 2 4 2 6 2',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 276,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Problems entropy is a measure of uncertainty, the entropy is maximum when the associated\\nprobability distribution generates maximum uncertainty.\\nThe channel-coding theorem, Shannons second theorem, is both the most surprising\\nand the single most important result of information theory. For a binary symmetric\\nchannel, the channel-coding theorem teaches us that, for any code rate r less than or equal\\nto the channel capacity C, codes do exist such that the average probability of error is as\\nsmall as we want it. A binary symmetric channel is the simplest form of a discrete\\nmemoryless channel. It is symmetric, because the probability of receiving symbol 1 if\\nsymbol 0 is sent is the same as the probability of receiving symbol 0 if symbol 1 is sent.\\nThis probability, the probability that an error will occur, is termed a transition probability.\\nThe transition probability p is determined not only by the additive noise at the channel\\noutput, but also by the kind of receiver used. The value of p uniquely defines the channel\\ncapacity C.\\nThe information capacity law, an application of the channel-coding theorem, teaches us\\nthat there is an upper limit to the rate at which any communication system can operate\\nreliably (i.e., free of errors) when the system is constrained in power. This maximum rate,\\ncalled the information capacity, is measured in bits per second. When the system operates\\nat a rate greater than the information capacity, it is condemned to a high probability of\\nerror, regardless of the choice of signal set used for transmission or the receiver used for\\nprocessing the channel output.\\nWhen the output of a source of information is compressed in a lossless manner, the\\nresulting data stream usually contains redundant bits. These redundant bits can be\\nremoved by using a lossless algorithm such as Huffman coding or the Lempel-Ziv\\nalgorithm for data compaction. We may thus speak of data compression followed by data',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 277,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'removed by using a lossless algorithm such as Huffman coding or the Lempel-Ziv\\nalgorithm for data compaction. We may thus speak of data compression followed by data\\ncompaction as two constituents of the dissection of source coding, which is so called\\nbecause it refers exclusively to the sources of information.\\nWe conclude this chapter on Shannons information theory by pointing out that, in\\nmany practical situations, there are constraints that force source coding to be imperfect,\\nthereby resulting in unavoidable distortion. For example, constraints imposed by a\\ncommunication channel may place an upper limit on the permissible code rate and,\\ntherefore, average codeword length assigned to the information source. As another\\nexample, the information source may have a continuous amplitude, as in the case of\\nspeech, and the requirement is to quantize the amplitude of each sample generated by the\\nsource to permit its representation by a codeword of finite length, as in pulse-code\\nmodulation discussed in Chapter 6. In such cases, the information-theoretic problem is\\nreferred to as source coding with a fidelity criterion, and the branch of information theory\\nthat deals with it is called rate distortion theory, which may be viewed as a natural\\nextension of Shannons coding theorem.\\nProblems Entropy 5.1 Let p denote the probability of some event. Plot the amount of information gained by the occurrence\\nof this event for 0  p  1.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 277,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '258 Chapter 5 Information Theory 5.2 A source emits one of four possible symbols during each signaling interval. The symbols occur with\\nthe probabilities p0 = 0.4 p1 = 0.3 p2 = 0.2 p3 = 0.1 which sum to unity as they should. Find the amount of information gained by observing the source\\nemitting each of these symbols. A source emits one of four symbols s0, s1, s2, and s3 with probabilities 1/3, 1/6, 1/4 and 1/4,\\nrespectively. The successive symbols emitted by the source are statistically independent. Calculate\\nthe entropy of the source. Let X represent the outcome of a single roll of a fair die. What is the entropy of X? The sample function of a Gaussian process of zero mean and unit variance is uniformly sampled and\\nthen applied to a uniform quantizer having the input-output amplitude characteristic shown in\\nFigure P5.5. Calculate the entropy of the quantizer output. Consider a discrete memoryless source with source alphabet S = {s0, s1, , sK - 1} and source statistics\\n{p0, p1, , pK - 1}. The nth extension of this source is another discrete memoryless source with source\\nalphabet S(n) = {0, 1, , M - 1}, where M = Kn. Let P(i) denote the probability ofi.\\na. Show that, as expected,\\nb. Show that\\nwhere is the probability of symbol and H(S) is the entropy of the original source.\\nc. Hence, show that Figure P5.5 1.5 0.5 0 -1.5 -0.5 1 -1 Output Input P i   i 0 = M 1 -  1 = P i   i 0 = M 1 -  1 pik ------     2 log H S  k 1 2 n   =  = pik sik HSn    P i   i 0 = M 1 -  1 P i   -------------     2 log = nH S  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 278,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Problems 259 5.7 Consider a discrete memoryless source with source alphabet S = {s0, s1, s2} and source statistics\\n{0.7, 0.15, 0.15}.\\na. Calculate the entropy of the source.\\nb. Calculate the entropy of the second-order extension of the source. It may come as a surprise, but the number of bits needed to store text is much less than that required\\nto store its spoken equivalent. Can you explain the reason for this statement? Let a discrete random variable X assume values in the set {x1, x2, , xn}. Show that the entropy of X\\nsatisfies the inequality\\nand with equality if, and only if, the probability pi = 1/n for all i.\\nLossless Data Compression Consider a discrete memoryless source whose alphabet consists of K equiprobable symbols.\\na. Explain why the use of a fixed-length code for the representation of such a source is about as\\nefficient as any code can be.\\nb. What conditions have to be satisfied by K and the codeword length for the coding efficiency to\\nbe 100%? Consider the four codes listed below:\\na. Two of these four codes are prefix codes. Identify them and construct their individual decision\\ntrees.\\nb. Apply the Kraft inequality to codes I, II, III, and IV. Discuss your results in light of those\\nobtained in part a. Consider a sequence of letters of the English alphabet with their probabilities of occurrence\\nCompute two different Huffman codes for this alphabet. In one case, move a combined symbol in\\nthe coding procedure as high as possible; in the second case, move it as low as possible. Hence, for\\neach of the two codes, find the average codeword length and the variance of the average codeword\\nlength over the ensemble of letters. Comment on your results. A discrete memoryless source has an alphabet of seven symbols whose probabilities of occurrence\\nare as described here:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 279,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'length over the ensemble of letters. Comment on your results. A discrete memoryless source has an alphabet of seven symbols whose probabilities of occurrence\\nare as described here:\\nH X  n log  Symbol Code I Code II Code III Code IV s0 0 0 0 00 s1 10 01 01 01 s2 110 001 011 10 s3 1110 0010 110 110 s4 1111 0011 111 111 Letter Probability a 0.1 i 0.1 l 0.2 m 0.1 n 0.1 o 0.2 p 0.1 y 0.1 Symbol Probability s0 0.25 s1 0.25 s2 0.125 s3 0.125 s4 0.125 s5 0.0625 s6 0.0625',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 279,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '260 Chapter 5 Information Theory Compute the Huffman code for this source, moving a combined symbol as high as possible.\\nExplain why the computed source code has an efficiency of 100%. Consider a discrete memoryless source with alphabet {s0, s1, s2} and statistics {0.7, 0.15, 0.15} for\\nits output.\\na. Apply the Huffman algorithm to this source. Hence, show that the average codeword length of\\nthe Huffman code equals 1.3 bits/symbol.\\nb. Let the source be extended to order two. Apply the Huffman algorithm to the resulting extended\\nsource and show that the average codeword length of the new code equals 1.1975 bits/symbol.\\nc. Extend the order of the extended source to three and reapply the Huffman algorithm; hence,\\ncalculate the average codeword length.\\nd. Compare the average codeword length calculated in parts b and c with the entropy of the original\\nsource. Figure P5.15 shows a Huffman tree. What is the codeword for each of the symbols A, B, C, D, E, F,\\nand G represented by this Huffman tree? What are their individual codeword lengths? A computer executes four instructions that are designated by the codewords (00, 01, 10, 11).\\nAssuming that the instructions are used independently with probabilities (12, 18, 18, 14),\\ncalculate the percentage by which the number of bits used for the instructions may be reduced by the\\nuse of an optimum source code. Construct a Huffman code to realize the reduction. Consider the following binary sequence\\n11101001100010110100 \\nUse the Lempel-Ziv algorithm to encode this sequence, assuming that the binary symbols 0 and\\nare already in the cookbook.\\nBinary Symmetric Channel Consider the transition probability diagram of a binary symmetric channel shown in Figure 5.8. The\\ninput binary symbols 0 and 1 occur with equal probability. Find the probabilities of the binary\\nsymbols 0 and 1 appearing at the channel output. Repeat the calculation in Problem 5.18, assuming that the input binary symbols 0 and 1 occur with',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 280,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'symbols 0 and 1 appearing at the channel output. Repeat the calculation in Problem 5.18, assuming that the input binary symbols 0 and 1 occur with\\nprobabilities 14 and 34, respectively.\\nFigure P5.15 ABCDEFG 1 1 1 1 1 1 0 0 0 0 0 0 3/8 3/16 3/16 1/8 1/16 1/32 1/32',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 280,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems Mutual Information and Channel Capacity Consider a binary symmetric channel characterized by the transition probability p. Plot the mutual\\ninformation of the channel as a function of p1, the a priori probability of symbol 1 at the channel\\ninput. Do your calculations for the transition probability p = 0, 0.1, 0.2, 0.3, 0.5. Revisiting (5.12), express the mutual information I(X;Y) in terms of the relative entropy\\nD(p(x,y)||p(x)p(y)) Figure 5.10 depicts the variation of the channel capacity of a binary symmetric channel with the\\ntransition probability p. Use the results of Problem 5.19 to explain this variation. Consider the binary symmetric channel described in Figure 5.8. Let p0 denote the probability of\\nsending binary symbol x0 = 0 and let p1 = 1 - p0 denote the probability of sending binary symbol\\nx1 = 1. Let p denote the transition probability of the channel.\\na. Show that the mutual information between the channel input and channel output is given by\\nwhere the two entropy functions\\nand\\nb. Show that the value of p0 that maximizes I(X;Y) is equal to 1/2.\\nc. Hence, show that the channel capacity equals Two binary symmetric channels are connected in cascade as shown in Figure P5.24. Find the overall\\nchannel capacity of the cascaded connection, assuming that both channels have the same transition\\nprobability diagram of Figure 5.8. The binary erasure channel has two inputs and three outputs as described in Figure P5.25. The\\ninputs are labeled 0 and 1 and the outputs are labeled 0, 1, and e. A fraction  of the incoming bits is\\nerased by the channel. Find the capacity of the channel.\\nIXY ;   H z H p  - = H z z 1 z---   1 z -   1 1 z - -----------     2 log + 2 log = z p0p 1 p0 -  1 p -   + = H p  p 1 p---   1 p -   1 1 p - ------------     2 log + 2 log = C 1 H p  - = Figure P5.24 Figure P5.25 Output Input Binary symmetric channel 1 Binary symmetric channel 2 0 0 1 1 e 1 - 1 -     Input Output',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 281,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '262 Chapter 5 Information Theory 5.26 Consider a digital communication system that uses a repetition code for the channel encoding/decoding.\\nIn particular, each transmission is repeated n times, where n = 2m + 1 is an odd integer. The decoder\\noperates as follows. If in a block of n received bits the number of 0s exceeds the number of 1s, then\\nthe decoder decides in favor of a 0; otherwise, it decides in favor of a 1. An error occurs when m +\\nor more transmissions out of n = 2m + 1 are incorrect. Assume a binary symmetric channel.\\na. For n = 3, show that the average probability of error is given by\\nwhere p is the transition probability of the channel.\\nb. For n = 5, show that the average probability of error is given by\\nc. Hence, for the general case, deduce that the average probability of error is given by Let X, Y, and Z be three discrete random variables. For each value of the random variable Z,\\nrepresented by sample z, define\\nShow that the conditional entropy H(X | Y) satisfies the inequality\\nwhere \\x03 is the expectation operator. Consider two correlated discrete random variables X and Y, each of which takes a value in the set\\n. Suppose that the value taken by Y is known. The requirement is to guess the value of X. Let\\nPe denote the probability of error, defined by\\nShow that Pe is related to the conditional entropy of X given Y by the inequality\\nThis inequality is known as Fanos inequality. Hint: Use the result derived in Problem 5.27. In this problem we explore the convexity of the mutual information I(X;Y), involving the pair of\\ndiscrete random variables X and Y.\\nConsider a discrete memoryless channel, for which the transition probability p(y|x) is fixed for all x\\nand y. Let X1 and X2 be two input random variables, whose input probability distributions are\\nrespectively denoted by p(x1) and p(x2). The corresponding probability distribution of X is defined\\nby the convex combination\\np(x) = a1p(x1) + a2 p(x2)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 282,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'respectively denoted by p(x1) and p(x2). The corresponding probability distribution of X is defined\\nby the convex combination\\np(x) = a1p(x1) + a2 p(x2)\\nwhere a1 and a2 are arbitrary constants. Prove the inequality\\nI(X;Y)  a1I(X1;Y1) + a2I(X2;Y2)\\nwhere X1, X2, and X are the channel inputs, and Y1, Y2, and Y are the corresponding channel outputs.\\nFor the proof, you may use the following form of Jensens inequality:\\nPe 3p2 1 p -   p3 + = Pe 10p3 1 p -  2 5p4 1 p -   p5 + + = Pe n i  pi 1 p -  n i - i m+1 = n  = A z p y pzxy    y x = HXY   H z \\x03 A log   +  xi  i=1 n Pe \\x02 X Y    = HXY   H Pe   Pe n 1 -   log +  y p1 x y    p y  p1 y  -------------     y p1 x y    p y  p1 y  -------------     y x log  log y x',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 282,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems 263 Differential Entropy 5.30 The differential entropy of a continuous random variable X is defined by the integral of (5.66).\\nSimilarly, the differential entropy of a continuous random vector X is defined by the integral of\\n(5.68). These two integrals may not exist. Justify this statement. Show that the differential entropy of a continuous random variable X is invariant to translation; that is,\\nh(X + c) = h(X) for some constant c. 5.32 Let X1, X2, , Xn denote the elements of a Gaussian vector X. The Xi are independent with mean mi\\nand variance\\n, i = 1, 2, , n. Show that the differential entropy of the vector X is given by\\nwhere e is the base of the natural logarithm.What does h(X) reduce to if the variances are all equal? A continuous random variable X is constrained to a peak magnitude M; that is,\\n-M < X < M\\na. Show that the differential entropy of X is maximum when it is uniformly distributed, as shown by\\nb. Determine the maximum differential entropy of X. Referring to (5.75), do the following:\\na. Verify that the differential entropy of a Gaussian random variable of mean  and variance is\\ngiven by\\n, where e is the base of the natural algorithm.\\nb. Hence, confirm the inequality of (5.75). Demonstrate the properties of symmetry, nonnegativity, and expansion of the mutual information\\nI(X;Y) described in Section 5.6. Consider the continuous random variable Y, defined by\\nY = X + N\\nwhere the random variables X and N are statistically independent. Show that the conditional\\ndifferential entropy of Y, given X, equals\\nh(Y | X) = h(N)\\nwhere h(N) is the differential entropy of N.\\nInformation Capacity Law A voice-grade channel of the telephone network has a bandwidth of 3.4 kHz.\\na. Calculate the information capacity of the telephone channel for a signal-to-noise ratio of 30 dB.\\nb. Calculate the minimum signal-to-noise ratio required to support information transmission',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 283,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'a. Calculate the information capacity of the telephone channel for a signal-to-noise ratio of 30 dB.\\nb. Calculate the minimum signal-to-noise ratio required to support information transmission\\nthrough the telephone channel at the rate of 9600 bits/s. Alphanumeric data are entered into a computer from a remote terminal through a voice-grade\\ntelephone channel. The channel has a bandwidth of 3.4 kHz and output signal-to-noise ratio of\\n20 dB. The terminal has a total of 128 symbols. Assume that the symbols are equiprobable and the\\nsuccessive transmissions are statistically independent.\\na. Calculate the information capacity of the channel.\\nb. Calculate the maximum symbol rate for which error-free transmission over the channel is\\npossible. i 2 h X   n 2--- 2e 1 22 2n 2   1 n    2 log = fX x  1 2M  , MxM   -  0 otherwise     = 2 1 2  2 e2   2 log',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 283,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '264 Chapter 5 Information Theory 5.39 A black-and-white television picture may be viewed as consisting of approximately\\nelements, each of which may occupy one of 10 distinct brightness levels with equal probability.\\nAssume that (1) the rate of transmission is 30 picture frames per second and (2) the signal-to-noise\\nratio is 30 dB.\\nUsing the information capacity law, calculate the minimum bandwidth required to support the\\ntransmission of the resulting video signal. In Section 5.10 we made the statement that it is easier to increase the information capacity of a\\ncommunication channel by expanding its bandwidth B than increasing the transmitted power for a\\nprescribed noise variance N0B. This statement assumes that the noise spectral density N0 varies\\ninversely with B. Why is this inverse relationship the case? In this problem, we revisit Example 5.10, which deals with coded binary antipodal signaling over an\\nadditive white Gaussian noise (AWGN) channel. Starting with (5.105) and the underlying theory,\\ndevelop a software package for computing the minimum EbN0 required for a given bit error rate,\\nwhere Eb is the signal energy per bit, and N02 is the noise spectral density. Hence, compute the\\nresults plotted in parts a and b of Figure 5.16.\\nAs mentioned in Example 5.10, the computation of the mutual information between the channel input\\nand channel output is well approximated using Monte Carlo integration. To explain how this method\\nworks, consider a function g(y) that is difficult to sample randomly, which is indeed the case for the\\nproblem at hand. (For this problem, the function g(y) represents the complicated integrand in the for-\\nmula for the differential entropy of the channel output.) For the computation, proceed as follows:\\n\\nFind an area A that includes the region of interest and that is easily sampled.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 284,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Find an area A that includes the region of interest and that is easily sampled.\\n\\nChoose N points, uniformly randomly inside the area A.\\nThen the Monte Carlo integration theorem states that the integral of the function g(y) with respect to\\ny is approximately equal to the area A multiplied by the fraction of points that reside below the curve\\nof g, as illustrated in Figure P5.41. The accuracy of the approximation improves with increasing N.\\nNotes\\nAccording to Lucky (1989), the first mention of the term information theory by Shannon\\noccurred in a 1945 memorandum entitled A mathematical theory of cryptography. It is rather\\ncurious that the term was never used in Shannons (1948) classic paper, which laid down the\\nfoundations of information theory. For an introductory treatment of information theory, see Part 1 of\\nthe book by McEliece (2004), Chapters 1-6. For an advanced treatment of this subject, viewed in a\\nrather broad context and treated with rigor, and clarity of presentation, see Cover and Thomas\\n(2006). 3 105  Figure P5.41 y Area A Shaded area =  g(y) dy A where is the fraction of\\nrandomly chosen points that\\nlie under the curve of g(y).\\ng(y)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 284,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Notes For a collection of papers on the development of information theory (including the 1948 classic\\npaper by Shannon), see Slepian (1974). For a collection of the original papers published by Shannon,\\nsee Sloane and Wyner (1993).\\nThe use of a logarithmic measure of information was first suggested by Hartley (1928); however,\\nHartley used logarithms to base 10.\\nIn statistical physics, the entropy of a physical system is defined by (Rief, 1965: 147)\\nwhere kB is Boltzmanns constant, is the number of states accessible to the system, and ln denotes\\nthe natural logarithm. This entropy has the dimensions of energy, because its definition involves the\\nconstant kB. In particular, it provides a quantitative measure of the degree of randomness of the\\nsystem. Comparing the entropy of statistical physics with that of information theory, we see that they\\nhave a similar form.\\nFor the original proof of the source coding theorem, see Shannon (1948). A general proof of the\\nsource coding theorem is also given in Cover and Thomas (2006). The source coding theorem is also\\nreferred to in the literature as the noiseless coding theorem, noiseless in the sense that it establishes\\nthe condition for error-free encoding to be possible.\\nFor proof of the Kraft inequality, see Cover and Thomas (2006). The Kraft inequality is also\\nreferred to as the Kraft-McMillan inequality in the literature.\\nThe Huffman code is named after its inventor D.A. Huffman (1952). For a detailed account of\\nHuffman coding and its use in data compaction, see Cover and Thomas (2006).\\nThe original papers on the Lempel-Ziv algorithm are Ziv and Lempel (1977, 1978). For detailed\\ntreatment of the algorithm, see Cover and Thomas (2006).\\nIt is also of interest to note that once a parent subsequence is joined by its two children, that\\nparent subsequence can be replaced in constructing the Lempel-Ziv algorithm. To illustrate this nice\\nfeature of the algorithm, suppose we have the following example sequence:\\n01, 010, 011,',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 285,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'parent subsequence can be replaced in constructing the Lempel-Ziv algorithm. To illustrate this nice\\nfeature of the algorithm, suppose we have the following example sequence:\\n01, 010, 011, \\nwhere 01 plays the role of a parent and 010 and 011 play the roles of the parents children. In this\\nexample, the algorithm removes the 01, thereby reducing the length of the table through the use of a\\npointer.\\nIn Cover and Thomas (2006), it is proved that the two-stage method, where the source coding and\\nchannel coding are considered separately as depicted in Figure 5.11, is as good as any other method\\nof transmitting information across a noisy channel. This result has practical implications, in that the\\ndesign of a communication system may be approached in two separate parts: source coding followed\\nby channel coding. Specifically, we may proceed as follows:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 285,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Design a source code for the most efficient representation of data generated by a discrete\\nmemoryless source of information.\\n\\nSeparately and independently, design a channel code that is appropriate for a discrete\\nmemoryless channel.\\nThe combination of source coding and channel coding designed in this manner will be as efficient as\\nanything that could be designed by considering the two coding problems jointly.\\nTo prove the channel-coding theorem, Shannon used several ideas that were new at the time;\\nhowever, it was some time later when the proof was made rigorous (Cover and Thomas, 2006: 199).\\nPerhaps the most thoroughly rigorous proof of this basic theorem of information theory is presented\\nin Chapter 7 of the book by Cover and Thomas (2006). Our statement of the theorem, though\\nslightly different from that presented by Cover and Thomas, in essence is the same.\\nIn the literature, the relative entropy is also referred to as the Kullback-Leibler divergence (KLD).\\nL kB  ln =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 285,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': '266 Chapter 5 Information Theory 12. Equation (5.95) is also referred to in the literature as the Shannon-Hartley law in recognition of\\nthe early work by Hartley on information transmission (Hartley, 1928). In particular, Hartley showed\\nthat the amount of information that can be transmitted over a given channel is proportional to the\\nproduct of the channel bandwidth and the time of operation.\\nA lucid exposition of sphere packing is presented in Cover and Thomas (2006); see also\\nWozencraft and Jacobs (1965).\\nParts a and b of Figure 5.16 follow the corresponding parts of Figure 6.2 in the book by Frey\\n(1998).\\nFor a rigorous treatment of information capacity of a colored noisy channel, see Gallager\\n(1968). The idea of replacing the channel model of Figure 5.17a with that of Figure 5.17b is\\ndiscussed in Gitlin, Hayes, and Weinstein (1992)\\nFor a complete treatment of rate distortion theory, see the classic book by Berger (1971); this\\nsubject is also treated in somewhat less detail in Cover and Thomas (1991), McEliece (1977), and\\nGallager (1968).\\nFor the derivation of (5.124), see Cover and Thomas (2006). An algorithm for computation of\\nthe rate distortion function R(D) defined in (5.124) is described in Blahut (1987) and Cover and\\nThomas (2006).',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 286,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '267 CHAPTER 6 Conversion of Analog\\nWaveforms into Coded Pulses In continuous-wave (CW) modulation, which was studied briefly in Chapter 2, some\\nparameter of a sinusoidal carrier wave is varied continuously in accordance with the\\nmessage signal. This is in direct contrast to pulse modulation, which we study in this\\nchapter. In pulse modulation, some parameter of a pulse train is varied in accordance with\\nthe message signal. On this basis, we may distinguish two families of pulse modulation:\\nAnalog pulse modulation, in which a periodic pulse train is used as the carrier wave\\nand some characteristic feature of each pulse (e.g., amplitude, duration, or position)\\nis varied in a continuous manner in accordance with the corresponding sample value\\nof the message signal. Thus, in analog pulse modulation, information is transmitted\\nbasically in analog form but the transmission takes place at discrete times.\\nDigital pulse modulation, in which the message signal is represented in a form that\\nis discrete in both time and amplitude, thereby permitting transmission of the\\nmessage in digital form as a sequence of coded pulses; this form of signal\\ntransmission has no CW counterpart.\\nThe use of coded pulses for the transmission of analog information-bearing signals\\nrepresents a basic ingredient in digital communications. In this chapter, we focus attention\\non digital pulse modulation, which, in basic terms, is described as the conversion of\\nanalog waveforms into coded pulses. As such, the conversion may be viewed as the\\ntransition from analog to digital communications.\\nThree different kinds of digital pulse modulation are studied in the chapter:\\nPulse-code modulation (PCM), which has emerged as the most favored scheme for\\nthe digital transmission of analog information-bearing signals (e.g., voice and video\\nsignals). The important advantages of PCM are summarized thus:\\n\\nrobustness to channel noise and interference;',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 287,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'robustness to channel noise and interference;\\n\\nefficient regeneration of the coded signal along the transmission path;\\n\\nefficient exchange of increased channel bandwidth for improved signal-to-\\nquantization noise ratio, obeying an exponential law;\\n\\na uniform format for the transmission of different kinds of baseband signals,\\nhence their integration with other forms of digital data in a common network;',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 287,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\n\\ncomparative ease with which message sources may be dropped or reinserted in a\\nmultiplex system;\\n\\nsecure communication through the use of special modulation schemes or\\nencryption.\\nThese advantages, however, are attained at the cost of increased system complexity\\nand increased transmission bandwidth. Simply stated:\\nThere is no free lunch.\\nFor every gain we make, there is a price to pay.\\nDifferential pulse-code modulation (DPCM), which exploits the use of lossy data\\ncompression to remove the redundancy inherent in a message signal, such as voice or\\nvideo, so as to reduce the bit rate of the transmitted data without serious degradation\\nin overall system response. In effect, increased system complexity is traded off for\\nreduced bit rate, therefore reducing the bandwidth requirement of PCM.\\nDelta modulation (DM), which addresses another practical limitation of PCM: the\\nneed for simplicity of implementation when it is a necessary requirement. DM\\nsatisfies this requirement by intentionally oversampling the message signal. In\\neffect, increased transmission bandwidth is traded off for reduced system\\ncomplexity. DM may therefore be viewed as the dual of DPCM.\\nAlthough, indeed, these three methods of analog-to-digital conversion are quite different,\\nthey do share two basic signal-processing operations, namely sampling and quantization:\\n\\nthe process of sampling, followed by\\n\\npulse-amplitude modulation (PAM) and finally',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 288,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the process of sampling, followed by\\n\\npulse-amplitude modulation (PAM) and finally\\n\\namplitude quantization\\nare studied in what follows in this order. Sampling Theory\\nThe sampling process is usually described in the time domain. As such, it is an operation\\nthat is basic to digital signal processing and digital communications. Through use of the\\nsampling process, an analog signal is converted into a corresponding sequence of samples\\nthat are usually spaced uniformly in time. Clearly, for such a procedure to have practical\\nutility, it is necessary that we choose the sampling rate properly in relation to the bandwidth\\nof the message signal, so that the sequence of samples uniquely defines the original analog\\nsignal. This is the essence of the sampling theorem, which is derived in what follows.\\nFrequency-Domain Description of Sampling\\nConsider an arbitrary signal g(t) of finite energy, which is specified for all time t. A\\nsegment of the signal g(t) is shown in Figure 6.1a. Suppose that we sample the signal g(t)\\ninstantaneously and at a uniform rate, once every Ts seconds. Consequently, we obtain an\\ninfinite sequence of samples spaced Ts seconds apart and denoted by {g(nTs)}, where n\\ntakes on all possible integer values, positive as well as negative. We refer to Ts as the\\nsampling period, and to its reciprocal fs = 1/Ts as the sampling rate. For obvious reasons,\\nthis ideal form of sampling is called instantaneous sampling.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 288,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '2 Sampling Theory Let g(t) denote the signal obtained by individually weighting the elements of a\\nperiodic sequence of delta functions spaced Ts seconds apart by the sequence of numbers\\n{g(nTs)}, as shown by (see Figure 6.1b):\\n(6.1)\\nWe refer to g(t) as the ideal sampled signal. The term (t - nTs) represents a delta func-\\ntion positioned at time t = nTs. From the definition of the delta function, we recall from\\nChapter 2 that such an idealized function has unit area. We may therefore view the multi-\\nplying factor g(nTs) in (6.1) as a mass assigned to the delta function (t - nTs). A delta\\nfunction weighted in this manner is closely approximated by a rectangular pulse of dura-\\ntion t and amplitude g(nTs)/t; the smaller we make t the better the approximation will\\nbe.\\nReferring to the table of Fourier-transform pairs in Table 2.2, we have\\n(6.2)\\nwhere G(f) is the Fourier transform of the original signal g(t) and fs is the sampling rate.\\nEquation (6.2) states:\\nThe process of uniformly sampling a continuous-time signal of finite energy\\nresults in a periodic spectrum with a frequency equal to the sampling rate.\\nAnother useful expression for the Fourier transform of the ideal sampled signal g(t) may\\nbe obtained by taking the Fourier transform of both sides of (6.1) and noting that the\\nFourier transform of the delta function (t - nTs) is equal to exp(-j2nfTs). Letting G(f)\\ndenote the Fourier transform of g(t), we may write\\n(6.3)\\nEquation (6.3) describes the discrete-time Fourier transform. It may be viewed as a\\ncomplex Fourier series representation of the periodic frequency function G(f), with the\\nsequence of samples {g(nTs)} defining the coefficients of the expansion.\\nFigure 6.1 The sampling process. (a) Analog signal. (b) Instantaneously sampled version of the\\nanalog signal. g(t) 0 0 Ts t t (b) (a) g (t)  gt g nTs  t nTs -   n  - =   = gtfs G f mfs -   m  - =   Gf g nTs   j2nfTs -   exp n  - =   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 289,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nThe discussion presented thus far applies to any continuous-time signal g(t) of finite\\nenergy and infinite duration. Suppose, however, that the signal g(t) is strictly band limited,\\nwith no frequency components higher than W hertz. That is, the Fourier transform G(f) of\\nthe signal g(t) has the property that G(f) is zero for | f | W, as illustrated in Figure 6.2a;\\nthe shape of the spectrum shown in this figure is merely intended for the purpose of\\nillustration. Suppose also that we choose the sampling period Ts = 12W. Then the\\ncorresponding spectrum G(f) of the sampled signal g(t) is as shown in Figure 6.2b.\\nPutting Ts = 1/2W in (6.3) yields\\n(6.4)\\nIsolating the term on the right-hand side of (6.2), corresponding to m = 0, we readily see\\nthat the Fourier transform of g(t) may also be expressed as\\n(6.5)\\nSuppose, now, we impose the following two conditions:\\nG(f) = 0 for | f | W.\\nfs = 2W.\\nWe may then reduce (6.5) to\\n(6.6)\\nSubstituting (6.4) into (6.6), we may also write\\n(6.7)\\nEquation (6.7) is the desired formula for the frequency-domain description of sampling.\\nThis formula reveals that if the sample values g(n/2W) of the signal g(t) are specified for\\nall n, then the Fourier transform G(f) of that signal is uniquely determined. Because g(t) is\\nrelated to G(f) by the inverse Fourier transform, it follows, therefore, that g(t) is itself\\nuniquely determined by the sample values g(n/2W) for\\n. In other words, the\\nsequence {g(n/2W)} has all the information contained in the original signal g(t).\\nFigure 6.2 (a) Spectrum of a strictly band-limited signal g(t). (b) Spectrum of the sampled version\\nof g(t) for a sampling period Ts = 1/2W.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 290,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Figure 6.2 (a) Spectrum of a strictly band-limited signal g(t). (b) Spectrum of the sampled version\\nof g(t) for a sampling period Ts = 1/2W.\\nGf g n 2W --------     jnf W ---------- -     exp n  - =   = Gf fsG f fs G f mfs -   m  - = m 0    + = G f 1 2W --------Gf WfW  -  = G f 1 2W -------- g n 2W --------     jnf W ---------- -     WfW  -  exp n  - =   =  n    - f (b) G ( f ) 0 2WG(0) 2fs -2fs fs -fs W -W (a) G( f ) 0 G(0) f W -W',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 290,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '2 Sampling Theory Consider next the problem of reconstructing the signal g(t) from the sequence of\\nsample values {g(n/2W)}. Substituting (6.7) in the formula for the inverse Fourier\\ntransform\\nand interchanging the order of summation and integration, which is permissible because\\nboth operations are linear, we may go on to write\\n(6.8)\\nThe definite integral in (6.8), including the multiplying factor 1/2W, is readily evaluated in\\nterms of the sinc function, as shown by\\nAccordingly, (6.8) reduces to the infinite-series expansion\\n(6.9)\\nEquation (6.9) is the desired reconstruction formula. This formula provides the basis for\\nreconstructing the original signal g(t) from the sequence of sample values {g(n/2W)}, with\\nthe sinc function sinc(2Wt) playing the role of a basis function of the expansion. Each\\nsample, g(n/2W), is multiplied by a delayed version of the basis function, sinc(2Wt - n),\\nand all the resulting individual waveforms in the expansion are added to reconstruct the\\noriginal signal g(t).\\nThe Sampling Theorem\\nEquipped with the frequency-domain description of sampling given in (6.7) and the\\nreconstruction formula of (6.9), we may now state the sampling theorem for strictly band-\\nlimited signals of finite energy in two equivalent parts:\\nA band-limited signal of finite energy that has no frequency components higher than\\nW hertz is completely described by specifying the values of the signal instants of\\ntime separated by 1/2W seconds.\\nA band-limited signal of finite energy that has no frequency components higher than\\nW hertz is completely recovered from a knowledge of its samples taken at the rate of\\n2W samples per second.\\nPart 1 of the theorem, following from (6.7), is performed in the transmitter. Part 2 of the\\ntheorem, following from (6.9), is performed in the receiver. For a signal bandwidth of\\nW hertz, the sampling rate of 2W samples per second, for a signal bandwidth of W hertz, is',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 291,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'theorem, following from (6.9), is performed in the receiver. For a signal bandwidth of\\nW hertz, the sampling rate of 2W samples per second, for a signal bandwidth of W hertz, is\\ncalled the Nyquist rate; its reciprocal 1/2W (measured in seconds) is called the Nyquist\\ninterval; see the classic paper (Nyquist, 1928b).\\ng t G f j2ft   exp df  -   = g t g n 2W --------    1 2W -------- j2ft n 2W -------- -     df exp W - W  n  - =   = 1 2W -------- j2ft n 2W -------- -     df exp W - W  2Wt n -   sin 2Wt n - ---------------------------------------\\n= sinc 2Wt n -   = g t g n 2W --------    sinc 2Wt n -    t   - n  - =   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 291,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nAliasing Phenomenon\\nDerivation of the sampling theorem just described is based on the assumption that the\\nsignal g(t) is strictly band limited. In practice, however, a message signal is not strictly band\\nlimited, with the result that some degree of undersampling is encountered, as a consequence\\nof which aliasing is produced by the sampling process. Aliasing refers to the phenomenon\\nof a high-frequency component in the spectrum of the signal seemingly taking on the\\nidentity of a lower frequency in the spectrum of its sampled version, as illustrated in Figure\\nThe aliased spectrum, shown by the solid curve in Figure 6.3b, pertains to the\\nundersampled version of the message signal represented by the spectrum of Figure 6.3a.\\nTo combat the effects of aliasing in practice, we may use two corrective measures:\\nPrior to sampling, a low-pass anti-aliasing filter is used to attenuate those high-\\nfrequency components of the signal that are not essential to the information being\\nconveyed by the message signal g(t).\\nThe filtered signal is sampled at a rate slightly higher than the Nyquist rate.\\nThe use of a sampling rate higher than the Nyquist rate also has the beneficial effect of\\neasing the design of the reconstruction filter used to recover the original signal from its\\nsampled version. Consider the example of a message signal that has been anti-alias (low-\\npass) filtered, resulting in the spectrum shown in Figure 6.4a. The corresponding spectrum\\nof the instantaneously sampled version of the signal is shown in Figure 6.4b, assuming a\\nsampling rate higher than the Nyquist rate. According to Figure 6.4b, we readily see that\\ndesign of the reconstruction filter may be specified as follows:\\n\\nThe reconstruction filter is low-pass with a passband extending from -W to W,\\nwhich is itself determined by the anti-aliasing filter.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 292,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The reconstruction filter is low-pass with a passband extending from -W to W,\\nwhich is itself determined by the anti-aliasing filter.\\n\\nThe reconstruction filter has a transition band extending (for positive frequencies)\\nfrom W to (fs - W), where fs is the sampling rate.\\nFigure 6.3 (a) Spectrum of a signal. (b) Spectrum of an under-sampled version\\nof the signal exhibiting the aliasing phenomenon.\\n(b) (a) f 0 0 f 2fs fs -fs -2fs G ( f )  G( f )',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 292,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '2 Sampling Theory 273 EXAMPLE 1 Sampling of Voice Signals\\nAs an illustrative example, consider the sampling of voice signals for waveform coding.\\nTypically, the frequency band, extending from 100 Hz to 3.1 kHz, is considered to be\\nadequate for telephonic communication. This limited frequency band is accomplished by\\npassing the voice signal through a low-pass filter with its cutoff frequency set at 3.1 kHz;\\nsuch a filter may be viewed as an anti-aliasing filter. With such a cutoff frequency, the\\nNyquist rate is fs = 2 3.1 = 6.2 kHz. The standard sampling rate for the waveform coding\\nof voice signals is 8 kHz. Putting these numbers together, design specifications for the\\nreconstruction (low-pass) filter in the receiver are as follows:\\nCutoff frequency 3.1 kHz Transition band 6.2 to 8 kHz Transition-band width\\n8 kHz.\\nFigure 6.4 (a) Anti-alias filtered spectrum of an information-bearing signal. (b) Spectrum of\\ninstantaneously sampled version of the signal, assuming the use of a sampling rate greater than the\\nNyquist rate. (c) Magnitude response of reconstruction filter.\\n(b) (c) (a) f 0 0 f -fs f -W W -fs - W -fs + W -W W fs + W fs - W fs 0 -fs + W -W W fs - W Magnitude G ( f )  G( f )',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 293,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses Pulse-Amplitude Modulation\\nNow that we understand the essence of the sampling process, we are ready to formally\\ndefine PAM, which is the simplest and most basic form of analog pulse modulation. It is\\nformally defined as follows:\\nPAM is a linear modulation process where the amplitudes of regularly spaced\\npulses are varied in proportion to the corresponding sample values of a\\ncontinuous message signal.\\nThe pulses themselves can be of a rectangular form or some other appropriate shape.\\nThe waveform of a PAM signal is illustrated in Figure 6.5. The dashed curve in this\\nfigure depicts the waveform of a message signal m(t), and the sequence of amplitude-\\nmodulated rectangular pulses shown as solid lines represents the corresponding PAM\\nsignal s(t). There are two operations involved in the generation of the PAM signal:\\nInstantaneous sampling of the message signal m(t) every Ts seconds, where the\\nsampling rate fs = 1Ts is chosen in accordance with the sampling theorem.\\nLengthening the duration of each sample so obtained to some constant value T.\\nIn digital circuit technology, these two operations are jointly referred to as sample and\\nhold. One important reason for intentionally lengthening the duration of each sample is to\\navoid the use of an excessive channel bandwidth, because bandwidth is inversely\\nproportional to pulse duration. However, care has to be exercised in how long we make the\\nsample duration T, as the following analysis reveals.\\nLet s(t) denote the sequence of flat-top pulses generated in the manner described in\\nFigure 6.5. We may express the PAM signal as a discrete convolution sum:\\n(6.10)\\nwhere Ts is the sampling period and m(nTs) is the sample value of m(t) obtained at time\\nt = nTs. The h(t) is a Fourier-transformal pulse. With spectral analysis of s(t) in mind, we\\nwould like to recast (6.10) in the form of a convolution integral. To this end, we begin by',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 294,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 't = nTs. The h(t) is a Fourier-transformal pulse. With spectral analysis of s(t) in mind, we\\nwould like to recast (6.10) in the form of a convolution integral. To this end, we begin by\\ninvoking the sifting property of a delta function (discussed in Chapter 2) to express the\\ndelayed version of the pulse shape h(t) in (6.10) as\\n(6.11)\\nFigure 6.5 Flat-top samples, representing an analog signal.\\ns t m nTs  h t nTs -   n  - =   = h t nTs -   h t  -  t nTs -   d  -   = Ts s(t) m(t) t 0 T',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 294,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '3 Pulse-Amplitude Modulation Hence, substituting (6.11) into (6.10), and interchanging the order of summation and\\nintegration, we get\\n(6.12)\\nReferring to (6.1), we recognize that the expression inside the brackets in (6.12) is simply\\nthe instantaneously sampled version of the message signal m(t), as shown by\\n(6.13)\\nAccordingly, substituting (6.13) into (6.12), we may reformulate the PAM signal s(t) in the\\ndesired form\\n(6.14)\\nwhich is the convolution of the two time functions; and\\n.\\nThe stage is now set for taking the Fourier transform of both sides of (6.14) and\\nrecognizing that the convolution of two time functions is transformed into the\\nmultiplication of their respective Fourier transforms; we get the simple result\\n(6.15)\\nwhere S(f) = F[s(t)], M(f) = F[m(t)], and H(f) = F[h(t)]. Adapting (6.2) to the problem\\nat hand, we note that the Fourier transform M(f) is related to the Fourier transform M(f)\\nof the original message signal m(t) as follows:\\n(6.16)\\nwhere fs is the sampling rate. Therefore, the substitution of (6.16) into (6.15) yields the\\ndesired formula for the Fourier transform of the PAM signal s(t), as shown by\\n(6.17)\\nGiven this formula, how do we recover the original message signal m(t)? As a first step in\\nthis reconstruction, we may pass s(t) through a low-pass filter whose frequency response is\\ndefined in Figure 6.4c; here, it is assumed that the message signal is limited to bandwidth\\nW and the sampling rate fs is larger than the Nyquist rate 2W. Then, from (6.17) we find\\nthat the spectrum of the resulting filter output is equal to M(f)H(f). This output is\\nequivalent to passing the original message signal m(t) through another low-pass filter of\\nfrequency response H(f).\\nEquation (6.17) applies to any Fourier-transformable pulse shape h(t).\\ns t m nTs  t nTs -   n  - =   h t  -   d  -   = mt m nTs  t nTs -   n  - =   = s t mth t  -   d  -   = mth t = mt h t S f MfH f = Mf fs M f kfs -   k  - =   = S f fs M f kfs -  H f k  - =   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 295,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nConsider now the special case of a rectangular pulse of unit amplitude and duration T,\\nas shown in Figure 6.6a; specifically:\\n(6.18)\\nCorrespondingly, the Fourier transform of h(t) is given by\\n(6.19)\\nwhich is plotted in Figure 6.6b. We therefore find from (6.17) that by using flat-top\\nsamples to generate a PAM signal we have introduced amplitude distortion as well as a\\ndelay of T/2. This effect is rather similar to the variation in transmission with frequency\\nthat is caused by the finite size of the scanning aperture in television. Accordingly, the\\ndistortion caused by the use of PAM to transmit an analog information-bearing signal is\\nreferred to as the aperture effect.\\nTo correct for this distortion, we connect an equalizer in cascade with the low-pass\\nreconstruction filter, as shown in Figure 6.7. The equalizer has the effect of decreasing the\\nin-band loss of the reconstruction filter as the frequency increases in such a manner as to\\nh t 1, 0 t T  1 2---, t 0 t T =  = 0, otherwise          = H f Tsinc fT   jfT -   exp = Figure 6.6 (a) Rectangular pulse h(t). (b) Transfer function H(f), made up of the magnitude |H(f)|\\nand phase arg[H(f)].  - (a) (b) f h(t) 0 1 |H( f )| arg [H( f )] 0 0 TfTt 3 T - 3 T - 2 T 1 T 2 T 3 T - 2 T - 1 T 1 T 2 T 3 T - 1 T -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 296,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '3 Pulse-Amplitude Modulation compensate for the aperture effect. In light of (6.19), the magnitude response of the\\nequalizer should ideally be\\nThe amount of equalization needed in practice is usually small. Indeed, for a duty cycle\\ndefined by the ratio TTs 0.1, the amplitude distortion is less than 0.5%. In such a\\nsituation, the need for equalization may be omitted altogether.\\nPractical Considerations\\nThe transmission of a PAM signal imposes rather stringent requirements on the frequency\\nresponse of the channel, because of the relatively short duration of the transmitted pulses.\\nOne other point that should be noted: relying on amplitude as the parameter subject to\\nmodulation, the noise performance of a PAM system can never be better than baseband-\\nsignal transmission. Accordingly, in practice, we find that for transmission over a\\ncommunication channel PAM is used only as the preliminary means of message\\nprocessing, whereafter the PAM signal is changed to some other more appropriate form of\\npulse modulation.\\nWith analog-to-digital conversion as the aim, what would be the appropriate form of\\nmodulation to build on PAM? Basically, there are three potential candidates, each with its\\nown advantages and disadvantages, as summarized here:\\nPCM, which, as remarked previously in Section 6.1, is robust but demanding in both\\ntransmission bandwidth and computational requirements. Indeed, PCM has\\nestablished itself as the standard method for the conversion of speech and video\\nsignals into digital form.\\nDPCM, which provides a method for the reduction in transmission bandwidth but at\\nthe expense of increased computational complexity.\\nDM, which is relatively simple to implement but requires a significant increase in\\ntransmission bandwidth.\\nBefore we go on, a comment on terminology is in order. The term modulation used\\nherein is a misnomer. In reality, PCM, DM, and DPCM are different forms of source\\ncoding, with source coding being understood in the sense described in Chapter 5 on',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 297,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'herein is a misnomer. In reality, PCM, DM, and DPCM are different forms of source\\ncoding, with source coding being understood in the sense described in Chapter 5 on\\ninformation theory. Nevertheless, the terminologies used to describe them have become\\nembedded in the digital communications literature, so much so that we just have to live\\nwith them.\\nDespite their basic differences, PCM, DPCM and DM do share an important feature:\\nthe message signal is represented in discrete form in both time and amplitude. PAM takes\\ncare of the discrete-time representation. As for the discrete-amplitude representation, we\\nresort to a process known as quantization, which is discussed next.\\nFigure 6.7 System for recovering message signal m(t) from PAM signal s(t).\\nReconstruction filter Equalizer PAM signal s(t) Message signal m(t) 1 H f -------------- 1 Tsinc fT   -----------------------\\nf fT   sin ---------------------\\n=\\n=',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 297,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses Quantization and its Statistical Characterization\\nTypically, an analog message signal (e.g., voice) has a continuous range of amplitudes\\nand, therefore, its samples have a continuous amplitude range. In other words, within the\\nfinite amplitude range of the signal, we find an infinite number of amplitude levels. In\\nactual fact, however, it is not necessary to transmit the exact amplitudes of the samples for\\nthe following reason: any human sense (the ear or the eye) as ultimate receiver can detect\\nonly finite intensity differences. This means that the message signal may be approximated\\nby a signal constructed of discrete amplitudes selected on a minimum error basis from an\\navailable set. The existence of a finite number of discrete amplitude levels is a basic\\ncondition of waveform coding exemplified by PCM. Clearly, if we assign the discrete\\namplitude levels with sufficiently close spacing, then we may make the approximated\\nsignal practically indistinguishable from the original message signal. For a formal\\ndefinition of amplitude quantization, or just quantization for short, we say:\\nQuantization is the process of transforming the sample amplitude m(nTs) of a\\nmessage signal m(t) at time t = nTs into a discrete amplitude v(nTs) taken from a\\nfinite set of possible amplitudes.\\nThis definition assumes that the quantizer (i.e., the device performing the quantization\\nprocess) is memoryless and instantaneous, which means that the transformation at time\\nt = nTs is not affected by earlier or later samples of the message signal m(t). This simple\\nform of scalar quantization, though not optimum, is commonly used in practice.\\nWhen dealing with a memoryless quantizer, we may simplify the notation by dropping\\nthe time index. Henceforth, the symbol mk is used in place of m(kTs), as indicated in the\\nblock diagram of a quantizer shown in Figure 6.8a. Then, as shown in Figure 6.8b, the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 298,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the time index. Henceforth, the symbol mk is used in place of m(kTs), as indicated in the\\nblock diagram of a quantizer shown in Figure 6.8a. Then, as shown in Figure 6.8b, the\\nsignal amplitude m is specified by the index k if it lies inside the partition cell\\n(6.20) where (6.21) and L is the total number of amplitude levels used in the quantizer. The discrete amplitudes\\nmk, k = 1, 2, , L, at the quantizer input are called decision levels or decision thresholds. At\\nthe quantizer output, the index k is transformed into an amplitude vk that represents all ampli-\\ntudes of the cell Jk; the discrete amplitudes vk, k = 1, 2,, L, are called representation levels\\nor reconstruction levels. The spacing between two adjacent representation levels is called a\\nquantum or step-size. Thus, given a quantizer denoted by g(), the quantized output v equals\\nvk if the input sample m belongs to the interval Jk. In effect, the mapping (see Figure 6.8a)\\n(6.22)\\ndefines the quantizer characteristic, described by a staircase function.\\nJk: mk m mk+1     k  1 2 L   = mk m kTs   = v gm   = Figure 6.8 Description of a memoryless quantizer.\\n(a) (b) Quantizer g(.) Continuous sample m Discrete sample v mk + 1 mk + 2 mk - 1 vk mk Jk',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 298,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Quantization and its Statistical Characterization Quantizers can be of a uniform or nonuniform type. In a uniform quantizer, the\\nrepresentation levels are uniformly spaced; otherwise, the quantizer is nonuniform. In this\\nsection, we consider only uniform quantizers; nonuniform quantizers are considered in\\nSection 6.5. The quantizer characteristic can also be of midtread or midrise type. Figure\\n9a shows the input-output characteristic of a uniform quantizer of the midtread type,\\nwhich is so called because the origin lies in the middle of a tread of the staircaselike graph.\\nFigure 6.9b shows the corresponding input-output characteristic of a uniform quantizer of\\nthe midrise type, in which the origin lies in the middle of a rising part of the staircaselike\\ngraph. Despite their different appearances, both the midtread and midrise types of uniform\\nquantizers illustrated in Figure 6.9 are symmetric about the origin.\\nQuantization Noise\\nInevitably, the use of quantization introduces an error defined as the difference between\\nthe continuous input sample m and the quantized output sample v. The error is called\\nquantization noise.1 Figure 6.10 illustrates a typical variation of quantization noise as a\\nfunction of time, assuming the use of a uniform quantizer of the midtread type.\\nLet the quantizer input m be the sample value of a zero-mean random variable M. (If\\nthe input has a nonzero mean, we can always remove it by subtracting the mean from the\\ninput and then adding it back after quantization.) A quantizer, denoted by g(), maps the\\ninput random variable M of continuous amplitude into a discrete random variable V; their\\nrespective sample values m and v are related by the nonlinear function g() in (6.22). Let\\nthe quantization error be denoted by the random variable Q of sample value q. We may\\nthus write (6.23) or, correspondingly, (6.24) Figure 6.9 Two types of quantization: (a) midtread and (b) midrise.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 299,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the quantization error be denoted by the random variable Q of sample value q. We may\\nthus write (6.23) or, correspondingly, (6.24) Figure 6.9 Two types of quantization: (a) midtread and (b) midrise.\\n(a) (b) 4 2 -2 -4 -4 -2 0 Output level Input level 2 4 4 2 -2 -4 -4 -2 0 Output level Input level 2 4 qmv - = QMV - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 299,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nWith the input M having zero mean and the quantizer assumed to be symmetric as in\\nFigure 6.9, it follows that the quantizer output V and, therefore, the quantization error Q\\nwill also have zero mean. Thus, for a partial statistical characterization of the quantizer in\\nterms of output signal-to-(quantization) noise ratio, we need only find the mean-square\\nvalue of the quantization error Q.\\nConsider, then, an input m of continuous amplitude, which, symmetrically, occupies the\\nrange [-mmax, mmax]. Assuming a uniform quantizer of the midrise type illustrated in\\nFigure 6.9b, we find that the step size of the quantizer is given by\\n(6.25)\\nwhere L is the total number of representation levels. For a uniform quantizer, the\\nquantization error Q will have its sample values bounded by -/2  q  /2. If the step size\\n is sufficiently small (i.e., the number of representation levels L is sufficiently large), it is\\nreasonable to assume that the quantization error Q is a uniformly distributed random\\nvariable and the interfering effect of the quantization error on the quantizer input is similar\\nto that of thermal noise, hence the reference to quantization error as quantization noise.\\nWe may thus express the probability density function of the quantization noise as\\n(6.26)\\nFor this to be true, however, we must ensure that the incoming continuous sample does not\\noverload the quantizer. Then, with the mean of the quantization noise being zero, its\\nvariance is the same as the mean-square value; that is,\\nFigure 6.10\\nIllustration of the\\nquantization process.\\n1 Input wave 2 Quantized output Magnitude Difference between curves 1 & 2 Time Error  2mmax L ---------------- = fQ q  1 ---,  2--- q  2---   - 0, otherwise      = Q',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 300,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '4 Quantization and its Statistical Characterization (6.27)\\nSubstituting (6.26) into (6.27), we get\\n(6.28)\\nTypically, the L-ary number k, denoting the kth representation level of the quantizer, is\\ntransmitted to the receiver in binary form. Let R denote the number of bits per sample used\\nin the construction of the binary code. We may then write\\n(6.29) or, equivalently, (6.30) Hence, substituting (6.29) into (6.25), we get the step size\\n(6.31)\\nThus, the use of (6.31) in (6.28) yields\\n(6.32)\\nLet P denote the average power of the original message signal m(t). We may then express\\nthe output signal-to-noise ratio of a uniform quantizer as\\n(6.33)\\nEquation (6.33) shows that the output signal-to-noise ratio of a uniform quantizer (SNR)O\\nincreases exponentially with increasing number of bits per sample R, which is intuitively\\nsatisfying.\\nEXAMPLE\\nSinusoidal Modulating Signal\\nConsider the special case of a full-load sinusoidal modulating signal of amplitude Am,\\nwhich utilizes all the representation levels provided. The average signal power is\\n(assuming a load of 1 )\\nQ 2 \\x02 Q2   = q2fQ q  dq 2  - 2   = Q 2 1 --- q2 dq 2  - 2   = 2 12 ------ = L 2R = R L 2 log =  2mmax 2R ---------------- = Q 2 1 3---mmax 2 2 2R - = SNR  O P Q 2 ------- = 3P mmax 2 ------------      22R = P Am 2 2 ------- =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 301,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nThe total range of the quantizer input is 2Am, because the modulating signal swings\\nbetween -Am and Am. We may, therefore, set mmax = Am, in which case the use of (6.32)\\nyields the average power (variance) of the quantization noise as\\nThus, the output signal-to-noise of a uniform quantizer, for a full-load test tone, is\\n(6.34)\\nExpressing the signal-to-noise (SNR) in decibels, we get\\n(6.35)\\nThe corresponding values of signal-to-noise ratio for various values of L and R, are given in\\nTable 6.1. For sinusoidal modulation, this table provides a basis for making a quick estimate\\nof the number of bits per sample required for a desired output signal-to-noise ratio.\\nConditions of Optimality of Scalar Quantizers\\nIn designing a scalar quantizer, the challenge is how to select the representation levels and\\nsurrounding partition cells so as to minimize the average quantization power for a fixed\\nnumber of representation levels.\\nTo state the problem in mathematical terms: consider a message signal m(t) drawn from\\na stationary process and whose dynamic range, denoted by -A  m  A, is partitioned into\\na set of L cells, as depicted in Figure 6.11. The boundaries of the partition cells are defined\\nby a set of real numbers m1, m2, , mL - 1 that satisfy the following three conditions:\\nTable 6.1 Signal-to-(quantization) noise ratio for varying number of\\nrepresentation levels for sinusoidal modulation\\nNo. of representation levels L\\nNo. of bits per sample R\\nSNR (dB) 128 256 5 6 7 8 31.8 37.8 43.8 49.8 Q 2 1 3---Am 2 2 2 - R = SNR  O Am 2 2  Am 2 2 2 - R 3  --------------------------\\n3 2--- 22R   = = 10 SNR  O 10 log 1.8 6R + = m1 A - = mL 1 - A = mk mk 1 - for k 1 2 L   =  Figure 6.11 Illustrating the partitioning of the dynamic range\\n-Am of a message signal m(t) into a set of L cells.\\n2A m1 = -A mL + 1 = +A mL - 1 mL m2 m3 . . .',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 302,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '4 Quantization and its Statistical Characterization The kth partition cell is defined by (6.20), reproduced here for convenience:\\nJk : mk < m < mk - 1 for k = 1, 2, , L\\n(6.36)\\nLet the representation levels (i.e., quantization values) be denoted by vk, k = 1, 2, , L.\\nThen, assuming that d(m,vk) denotes a distortion measure for using vk to represent all\\nthose values of the input m that lie inside the partition cell Jk, the goal is to find the two\\nsets and that minimize the average distortion\\n(6.37)\\nwhere fM(m) is the probability density function of the random variable M with sample\\nvalue m.\\nA commonly used distortion measure is defined by\\n(6.38)\\nin which case we speak of the mean-square distortion. In any event, the optimization problem\\nstated herein is nonlinear, defying an explicit, closed-form solution. To get around this diffi-\\nculty, we resort to an algorithmic approach for solving the problem in an iterative manner.\\nStructurally speaking, the quantizer consists of two components with interrelated\\ndesign parameters:\\n\\nAn encoder characterized by the set of partition cells\\n; this is located in the\\ntransmitter.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 303,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'An encoder characterized by the set of partition cells\\n; this is located in the\\ntransmitter.\\n\\nA decoder characterized by the set of representation levels\\n; this is located\\nin the receiver.\\nAccordingly, we may identify two critically important conditions that provide the\\nmathematical basis for all algorithmic solutions to the optimum quantization problem.\\nOne condition assumes that we are given a decoder and the problem is to find the optimum\\nencoder in the transmitter. The other condition assumes that we are given an encoder and\\nthe problem is to find the optimum decoder in the receiver. Henceforth, these two\\nconditions are referred to as condition I and II, respectively.\\nCondition I:\\nOptimality of the Encoder for a Given Decoder\\nThe availability of a decoder means that we have a certain codebook in mind. Let the\\ncodebook be defined by\\n(6.39)\\nGiven the codebook , the problem is to find the set of partition cells that\\nminimizes the mean-square distortion D. That is, we wish to find the encoder defined by\\nthe nonlinear mapping (6.40) such that we have (6.41) vk  k 1 = L Jk  k 1 = L D dm,vk  fM m   dm m vk   k 1 = L  = d m,vk   m vk -  2 = Jk  k 1 = L vk  k 1 = L : vk  k 1 = L Jk  k 1 = L gm   vk k 1 2 L   =  = D mgm     fM m   d dM min m vk    d  fM m   dm m Jk   k 1 = L   A - A  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 303,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nFor the lower bound specified in (6.41) to be attained, we require that the nonlinear\\nmapping of (6.40) be satisfied only if the condition (6.42)\\nThe necessary condition described in (6.42) for optimality of the encoder for a specified\\ncodebook  is recognized as the nearest-neighbor condition. In words, the nearest\\nneighbor condition requires that the partition cell Jk should embody all those values of the\\ninput m that are closer to vk than any other element of the codebook . This optimality\\ncondition is indeed intuitively satisfying.\\nCondition II:\\nOptimality of the Decoder for a Given Encoder\\nConsider next the reverse situation to that described under condition I, which may be\\nstated as follows: optimize the codebook for the decoder, given that the\\nset of partition cells\\ncharacterizing the encoder is fixed. The criterion for\\noptimization is the average (mean-square) distortion:\\n(6.43)\\nThe probability density function fM(m) is clearly independent of the codebook . Hence,\\ndifferentiating D with respect to the representation level vk, we readily obtain\\n(6.44)\\nSetting equal to zero and then solving for vk, we obtain the optimum value\\n(6.45)\\nThe denominator in (6.45) is just the probability pk that the random variable M with\\nsample value m lies in the partition cell Jk, as shown by\\n(6.46)\\nAccordingly, we may interpret the optimality condition of (6.45) as choosing the\\nrepresentation level vk to equal the conditional mean of the random variable M, given that\\nM lies in the partition cell Jk. We can thus formally state that the condition for optimality\\nof the decoder for a given encoder as follows:\\n(6.47)\\nwhere \\x02 is the expectation operator. Equation (6.47) is also intuitively satisfying.\\nNote that the nearest neighbor condition (I) for optimality of the encoder for a given\\ndecoder was proved for a generic average distortion. However, the conditional mean',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 304,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Note that the nearest neighbor condition (I) for optimality of the encoder for a given\\ndecoder was proved for a generic average distortion. However, the conditional mean\\nrequirement (condition II) for optimality of the decoder for a given encoder was proved for\\nd m vk    d m vj    holds for all j k    vk   = Lk 1 = Jk  k 1 = LDm vk -  2fM m   dm m Jk   k 1 = L  = D  vk ------- 2 m vk -  fM m   dm m Jk   k 1 = L  - = D/ vk   vk opt  mfM m   dm m Jk   fM m   dm m Jk   ----------------------------------------------\\n= pk \\x03 mk M mk 1 +     = fM m   dm m Jk   = vk opt  \\x02 M mk M mk 1 +     =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 304,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '5 Pulse-Code Modulation the special case of a mean-square distortion. In any event, these two conditions are\\nnecessary for optimality of a scalar quantizer. Basically, the algorithm for designing the\\nquantizer consists of alternately optimizing the encoder in accordance with condition I,\\nthen optimizing the decoder in accordance with condition II, and continuing in this\\nmanner until the average distortion D reaches a minimum. The optimum quantizer\\ndesigned in this manner is called the Lloyd-Max quantizer.2 Pulse-Code Modulation\\nWith the material on sampling, PAM, and quantization presented in the preceding\\nsections, the stage is set for describing PCM, for which we offer the following definition:\\nPCM is a discrete-time, discrete-amplitude waveform-coding process, by means\\nof which an analog signal is directly represented by a sequence of coded pulses.\\nSpecifically, the transmitter consists of two components: a pulse-amplitude modulator followed\\nby an analog-to-digital (A/D) converter. The latter component itself embodies a quantizer\\nfollowed by an encoder. The receiver performs the inverse of these two operations: digital-to-\\nanalog (D/A) conversion followed by pulse-amplitude demodulation. The communication\\nchannel is responsible for transporting the encoded pulses from the transmitter to the receiver.\\nFigure 6.12, a block diagram of the PCM, shows the transmitter, the transmission path\\nfrom the transmitter output to the receiver input, and the receiver.\\nIt is important to realize, however, that once distortion in the form of quantization noise\\nis introduced into the encoded pulses, there is absolutely nothing that can be done at the\\nreceiver to compensate for that distortion. The only design precaution that can be taken is\\nto choose a number of representation levels in the receiver that is large enough to ensure\\nthat the quantization noise is imperceptible for human use at the receiver output.\\nFigure 6.12 Block diagram of PCM system.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 305,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'that the quantization noise is imperceptible for human use at the receiver output.\\nFigure 6.12 Block diagram of PCM system.\\nAnalog source of message signal Pulse-amplitude modulator Pulse-amplitude demodulator Digital-to- analog converter Analog-to- digital converter Digitally encoded message signal\\nacross the transmission path\\nTransmitter Destination . . . . . . . . . . Receiver',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 305,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nSampling in the Transmitter\\nThe incoming message signal is sampled with a train of rectangular pulses short enough to\\nclosely approximate the instantaneous sampling process. To ensure perfect reconstruction of\\nthe message signal at the receiver, the sampling rate must be greater than twice the highest\\nfrequency component W of the message signal in accordance with the sampling theorem. In\\npractice, a low-pass anti-aliasing filter is used at the front end of the pulse-amplitude\\nmodulator to exclude frequencies greater than W before sampling and which are of\\nnegligible practical importance. Thus, the application of sampling permits the reduction of\\nthe continuously varying message signal to a limited number of discrete values per second.\\nQuantization in the Transmitter\\nThe PAM representation of the message signal is then quantized in the analog-to-digital\\nconverter, thereby providing a new representation of the signal that is discrete in both time\\nand amplitude. The quantization process may follow a uniform law as described in Section\\nIn telephonic communication, however, it is preferable to use a variable separation\\nbetween the representation levels for efficient utilization of the communication channel.\\nConsider, for example, the quantization of voice signals. Typically, we find that the range\\nof voltages covered by voice signals, from the peaks of loud talk to the weak passages of\\nweak talk, is on the order of 1000 to 1. By using a nonuniform quantizer with the feature\\nthat the step size increases as the separation from the origin of the input-output amplitude\\ncharacteristic of the quantizer is increased, the large end-steps of the quantizer can take\\ncare of possible excursions of the voice signal into the large amplitude ranges that occur\\nrelatively infrequently. In other words, the weak passages needing more protection are\\nfavored at the expense of the loud passages. In this way, a nearly uniform percentage',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 306,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'relatively infrequently. In other words, the weak passages needing more protection are\\nfavored at the expense of the loud passages. In this way, a nearly uniform percentage\\nprecision is achieved throughout the greater part of the amplitude range of the input signal.\\nThe end result is that fewer steps are needed than would be the case if a uniform quantizer\\nwere used; hence the improvement in channel utilization.\\nAssuming memoryless quantization, the use of a nonuniform quantizer is equivalent to\\npassing the message signal through a compressor and then applying the compressed signal\\nto a uniform quantizer, as illustrated in Figure 6.13a. A particular form of compression law\\nthat is used in practice is the so-called -law,3 which is defined by\\n(6.48)\\nwhere ln, i.e., loge, denotes the natural logarithm, m and v are the input and output\\nvoltages of the compressor, and  is a positive constant. It is assumed that m and,\\nv 1 m +   ln 1  +   ln ------------------------------\\n=\\nFigure 6.13\\n(a) Nonuniform quantization\\nof the message signal in the\\ntransmitter. (b) Uniform\\nquantization of the original\\nmessage signal in the receiver.\\nInput message signal m(t) Compressed output signal Compressed signal Compressor Uniform quantizer Uniformly quantized version of the original\\nmessage signal m(t) Expander (b) (a)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 306,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '5 Pulse-Code Modulation therefore, v are scaled so that they both lie inside the interval [-1, 1]. The -law is plotted\\nfor three different values of in Figure 6.14a. The case of uniform quantization\\ncorresponds to  = 0. For a given value of , the reciprocal slope of the compression curve\\nthat defines the quantum steps is given by the derivative of the absolute value |m| with\\nrespect to the corresponding absolute value |v|; that is,\\n(6.49)\\nFrom (6.49) it is apparent that the -law is neither strictly linear nor strictly logarithmic.\\nRather, it is approximately linear at low input levels corresponding to |m|  1 and\\napproximately logarithmic at high input levels corresponding to |m|  1.\\nAnother compression law that is used in practice is the so-called A-law, defined by\\n(6.50)\\nwhere A is another positive constant. Equation (6.50) is plotted in Figure 6.14b for varying\\nA. The case of uniform quantization corresponds to A = 1. The reciprocal slope of this\\nsecond compression curve is given by the derivative of |m| with respect to |v|, as shown by\\n(6.51) Figure 6.14 Compression laws: (a) -law; (b) A-law. (b) (a) 0 0.2 0.4 0.6 Normalized input, |m|\\nNormalized output, |v|\\n8 1.0 0.2 0.4 0.6 0.8 1.0 = 100 = 5 = 0 0 0.2 0.4 0.6 Normalized input, |m| m +   = v Am 1 A ln + ------------------, 0 m 1 A---   1 A m   ln + 1 A ln + -------------------------------,\\n1 A--- m 1            = dm dv ----------- 1 A ln + A ------------------ , 0 m 1 A---   (1 lnA) m + , 1 A--- m 1            =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 307,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nTo restore the signal samples to their correct relative level, we must, of course, use a device\\nin the receiver with a characteristic complementary to the compressor. Such a device is\\ncalled an expander. Ideally, the compression and expansion laws are exactly the inverse of\\neach other. With this provision in place, we find that, except for the effect of quantization, the\\nexpander output is equal to the compressor input. The cascade combination of a compressor\\nand an expander, depicted in Figure 6.13, is called a compander.\\nFor both the -law and A-law, the dynamic range capability of the compander improves\\nwith increasing  and A, respectively. The SNR for low-level signals increases at the expense\\nof the SNR for high-level signals. To accommodate these two conflicting requirements (i.e.,\\na reasonable SNR for both low- and high-level signals), a compromise is usually made in\\nchoosing the value of parameter  for the -law and parameter A for the A-law. The typical\\nvalues used in practice are  = 255 for the law and A = 87.6 for the A-law.4\\nEncoding in the Transmitter\\nThrough the combined use of sampling and quantization, the specification of an analog\\nmessage signal becomes limited to a discrete set of values, but not in the form best suited\\nto transmission over a telephone line or radio link. To exploit the advantages of sampling\\nand quantizing for the purpose of making the transmitted signal more robust to noise,\\ninterference, and other channel impairments, we require the use of an encoding process to\\ntranslate the discrete set of sample values to a more appropriate form of signal. Any plan\\nfor representing each of this discrete set of values as a particular arrangement of discrete\\nevents constitutes a code. Table 6.2 describes the one-to-one correspondence between\\nrepresentation levels and codewords for a binary number system for R = 4 bits per sample.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 308,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'events constitutes a code. Table 6.2 describes the one-to-one correspondence between\\nrepresentation levels and codewords for a binary number system for R = 4 bits per sample.\\nFollowing the terminology of Chapter 5, the two symbols of a binary code are customarily\\ndenoted as 0 and 1. In practice, the binary code is the preferred choice for encoding for the\\nfollowing reason:\\nThe maximum advantage over the effects of noise encountered in a communication\\nsystem is obtained by using a binary code because a binary symbol withstands a\\nrelatively high level of noise and, furthermore, it is easy to regenerate.\\nThe last signal-processing operation in the transmitter is that of line coding, the purpose of\\nwhich is to represent each binary codeword by a sequence of pulses; for example,\\nsymbol 1 is represented by the presence of a pulse and symbol 0 is represented by absence\\nof the pulse. Line codes are discussed in Section 6.10. Suppose that, in a binary code, each\\ncodeword consists of R bits. Then, using such a code, we may represent a total of 2R\\ndistinct numbers. For example, a sample quantized into one of 256 levels may be\\nrepresented by an 8-bit codeword.\\nInverse Operations in the PCM Receiver\\nThe first operation in the receiver of a PCM system is to regenerate (i.e., reshape and clean\\nup) the received pulses. These clean pulses are then regrouped into codewords and decoded\\n(i.e., mapped back) into a quantized pulse-amplitude modulated signal. The decoding\\nprocess involves generating a pulse the amplitude of which is the linear sum of all the pulses\\nin the codeword. Each pulse is weighted by its place value (20, 21, 22, , 2R - 1) in the code,\\nwhere R is the number of bits per sample. Note, however, that whereas the analog-to-digital',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 308,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '5 Pulse-Code Modulation converter in the transmitter involves both quantization and encoding, the digital-to-analog\\nconverter in the receiver involves decoding only, as illustrated in Figure 6.12.\\nThe final operation in the receiver is that of signal reconstruction. Specifically, an\\nestimate of the original message signal is produced by passing the decoder output through\\na low-pass reconstruction filter whose cutoff frequency is equal to the message\\nbandwidth W. Assuming that the transmission link (connecting the receiver to the\\ntransmitter) is error free, the reconstructed message signal includes no noise with the\\nexception of the initial distortion introduced by the quantization process.\\nPCM Regeneration along the Transmission Path\\nThe most important feature of a PCM systems is its ability to control the effects of\\ndistortion and noise produced by transmitting a PCM signal through the channel,\\nconnecting the receiver to the transmitter. This capability is accomplished by\\nreconstructing the PCM signal through a chain of regenerative repeaters, located at\\nsufficiently close spacing along the transmission path.\\nTable 6.2 Binary number system for T = 4 bits/sample\\nOrdinal number of\\nrepresentation level\\nLevel number expressed\\nas sum of powers of\\nBinary number 0 0000 1 20 0001 2 21 0010 3 21 + 20 0011 4 22 0100 5 22 + 20 0101 6 22 + 21 0110 7 22 + 21 + 20 0111 8 23 1000 9 23 + 20 1001 10 23 + 21 1010 11 23 + 21 + 20 1011 12 23 + 22 1100 13 23 + 22 + 20 1101 14 23 + 22 + 21 1110 15 23 + 22 + 21 + 20',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 309,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nAs illustrated in Figure 6.15, three basic functions are performed in a regenerative\\nrepeater: equalization, timing, and decision making. The equalizer shapes the received\\npulses so as to compensate for the effects of amplitude and phase distortions produced by\\nthe non-ideal transmission characteristics of the channel. The timing circuitry provides a\\nperiodic pulse train, derived from the received pulses, for sampling the equalized pulses at\\nthe instants of time where the SNR ratio is a maximum. Each sample so extracted is com-\\npared with a predetermined threshold in the decision-making device. In each bit interval, a\\ndecision is then made on whether the received symbol is 1 or 0 by observing whether the\\nthreshold is exceeded or not. If the threshold is exceeded, a clean new pulse representing\\nsymbol 1 is transmitted to the next repeater; otherwise, another clean new pulse represent-\\ning symbol 0 is transmitted. In this way, it is possible for the accumulation of distortion and\\nnoise in a repeater span to be almost completely removed, provided that the disturbance is\\nnot too large to cause an error in the decision-making process. Ideally, except for delay, the\\nregenerated signal is exactly the same as the signal originally transmitted. In practice, how-\\never, the regenerated signal departs from the original signal for two main reasons:\\nThe unavoidable presence of channel noise and interference causes the repeater to\\nmake wrong decisions occasionally, thereby introducing bit errors into the\\nregenerated signal.\\nIf the spacing between received pulses deviates from its assigned value, a jitter is\\nintroduced into the regenerated pulse position, thereby causing distortion.\\nThe important point to take from this subsection on PCM is the fact that regeneration\\nalong the transmission path is provided across the spacing between individual regenerative',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 310,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The important point to take from this subsection on PCM is the fact that regeneration\\nalong the transmission path is provided across the spacing between individual regenerative\\nrepeaters (including the last stage of regeneration at the receiver input) provided that the\\nspacing is short enough. If the transmitted SNR ratio is high enough, then the regenerated\\nPCM data stream is the same as the transmitted PCM data stream, except for a practically\\nnegligibly small bit error rate (BER). In other words, under these operating conditions,\\nperformance degradation in the PCM system is essentially confined to quantization noise\\nin the transmitter. Noise Considerations in PCM Systems\\nThe performance of a PCM system is influenced by two major sources of noise:\\nChannel noise, which is introduced anywhere between the transmitter output and the\\nreceiver input; channel noise is always present, once the equipment is switched on.\\nQuantization noise, which is introduced in the transmitter and is carried all the way\\nalong to the receiver output; unlike channel noise, quantization noise is signal\\ndependent, in the sense that it disappears when the message signal is switched off.\\nFigure 6.15\\nBlock diagram of\\nregenerative repeater.\\nTiming circuit Decision- making device Regenerated PCM wave Distorted PCM wave Amplifier- equalizer',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 310,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 Noise Considerations in PCM Systems Naturally, these two sources of noise appear simultaneously once the PCM system is in\\noperation. However, the traditional practice is to consider them separately, so that we may\\ndevelop insight into their individual effects on the system performance.\\nThe main effect of channel noise is to introduce bit errors into the received signal. In\\nthe case of a binary PCM system, the presence of a bit error causes symbol 1 to be\\nmistaken for symbol 0, or vice versa. Clearly, the more frequently bit errors occur, the\\nmore dissimilar the receiver output becomes compared with the original message signal.\\nThe fidelity of information transmission by PCM in the presence of channel noise may be\\nmeasured in terms of the average probability of symbol error, which is defined as the\\nprobability that the reconstructed symbol at the receiver output differs from the\\ntransmitted binary symbol on the average. The average probability of symbol error, also\\nreferred to as the BER, assumes that all the bits in the original binary wave are of equal\\nimportance. When, however, there is more interest in restructuring the analog waveform of\\nthe original message signal, different symbol errors may be weighted differently; for\\nexample, an error in the most significant bit in a codeword (representing a quantized\\nsample of the message signal) is more harmful than an error in the least significant bit.\\nTo optimize system performance in the presence of channel noise, we need to minimize\\nthe average probability of symbol error. For this evaluation, it is customary to model the\\nchannel noise as an ideal additive white Gaussian noise (AWGN) channel. The effect of\\nchannel noise can be made practically negligible by using an adequate signal energy-to-\\nnoise density ratio through the provision of short-enough spacing between the regenerative\\nrepeaters in the PCM system. In such a situation, the performance of the PCM system is',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 311,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'noise density ratio through the provision of short-enough spacing between the regenerative\\nrepeaters in the PCM system. In such a situation, the performance of the PCM system is\\nessentially limited by quantization noise acting alone.\\nFrom the discussion of quantization noise presented in Section 6.4, we recognize that\\nquantization noise is essentially under the designers control. It can be made negligibly\\nsmall through the use of an adequate number of representation levels in the quantizer and\\nthe selection of a companding strategy matched to the characteristics of the type of\\nmessage signal being transmitted. We thus find that the use of PCM offers the possibility\\nof building a communication system that is rugged with respect to channel noise on a scale\\nthat is beyond the capability of any analog communication system; hence its use as a\\nstandard against which other waveform coders (e.g., DPCM and DM) are compared.\\nError Threshold\\nThe underlying theory of BER calculation in a PCM system is deferred to Chapter 8. For\\nthe present, it suffices to say that the average probability of symbol error in a binary\\nencoded PCM receiver due to AWGN depends solely on EbN0, which is defined as the\\nratio of the transmitted signal energy per bit Eb, to the noise spectral density N0. Note that\\nthe ratio EbN0 is dimensionless even though the quantities Eb and N0 have different\\nphysical meaning. In Table 6.3, we present a summary of this dependence for the case of a\\nbinary PCM system, in which symbols 1 and 0 are represented by rectangular pulses of\\nequal but opposite amplitudes. The results presented in the last column of the table assume\\na bit rate of 105 bits/s.\\nFrom Table 6.3 it is clear that there is an error threshold (at about 11 dB). For EbN0\\nbelow the error threshold the receiver performance involves significant numbers of errors,\\nand above it the effect of channel noise is practically negligible. In other words, provided',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 311,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'below the error threshold the receiver performance involves significant numbers of errors,\\nand above it the effect of channel noise is practically negligible. In other words, provided\\nthat the ratio EbN0 exceeds the error threshold, channel noise has virtually no effect on',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 311,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nthe receiver performance, which is precisely the goal of PCM. When, however, EbN0\\ndrops below the error threshold, there is a sharp increase in the rate at which errors occur\\nin the receiver. Because decision errors result in the construction of incorrect codewords,\\nwe find that when the errors are frequent, the reconstructed message at the receiver output\\nbears little resemblance to the original message signal.\\nAn important characteristic of a PCM system is its ruggedness to interference, caused\\nby impulsive noise or cross-channel interference. The combined presence of channel noise\\nand interference causes the error threshold necessary for satisfactory operation of the PCM\\nsystem to increase. If, however, an adequate margin over the error threshold is provided in\\nthe first place, the system can withstand the presence of relatively large amounts of\\ninterference. In other words, a PCM system is robust with respect to channel noise and\\ninterference, providing further confirmation to the point made in the previous section that\\nperformance degradation in PCM is essentially confined to quantization noise in the\\ntransmitter.\\nPCM Noise Performance Viewed in Light of the Information\\nCapacity Law\\nConsider now a PCM system that is known to operate above the error threshold, in which\\ncase we would be justified to ignore the effect of channel noise. In other words, the noise\\nperformance of the PCM system is essentially determined by quantization noise acting\\nalone. Given such a scenario, how does the PCM system fare compared with the\\ninformation capacity law, derived in Chapter 5?\\nTo address this question of practical importance, suppose that the system uses a\\ncodeword consisting of n symbols with each symbol representing one of M possible\\ndiscrete amplitude levels; hence the reference to the system as an M-ary PCM system.\\nFor this system to operate above the error threshold, there must be provision for a large',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 312,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'discrete amplitude levels; hence the reference to the system as an M-ary PCM system.\\nFor this system to operate above the error threshold, there must be provision for a large\\nenough noise margin. For the PCM system to operate above the error threshold as proposed, the requirement\\nfor a noise margin that is sufficiently large to maintain a negligible error rate due to\\nchannel noise. This, in turn, means there must be a certain separation between the M\\ndiscrete amplitude levels. Call this separation c, where c is a constant and\\n= N0B is the\\nTable 6.3 Influence of EbN0 on the probability of error\\nEbN0 (dB) Probability of error Pe For a bit rate of 105 bits/s,\\nthis is about one error every',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 312,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 Noise Considerations in PCM Systems noise variance measured in a channel bandwidth B. The number of amplitude levels M is\\nusually an integer power of 2. The average transmitted power will be least if the amplitude\\nrange is symmetrical about zero. Then, the discrete amplitude levels, normalized with\\nrespect to the separation c, will have the values 12, 32, , (M - 1)2. We assume\\nthat these M different amplitude levels are equally likely. Accordingly, we find that the\\naverage transmitted power is given by (6.52)\\nSuppose that the M-ary PCM system described herein is used to transmit a message signal\\nwith its highest frequency component equal to W hertz. The signal is sampled at the\\nNyquist rate of 2W samples per second. We assume that the system uses a quantizer of the\\nmidrise type, with L equally likely representation levels. Hence, the probability of\\noccurrence of any one of the L representation levels is 1L. Correspondingly, the amount\\nof information carried by a single sample of the signal is log2 L bits. With a maximum\\nsampling rate of 2W samples per second, the maximum rate of information transmission of\\nthe PCM system measured in bits per second is given by bits/s\\n(6.53)\\nSince the PCM system uses a codeword consisting of n code elements with each one\\nhaving M possible discrete amplitude values, we have Mn different possible codewords.\\nFor a unique encoding process, therefore, we require\\n(6.54)\\nClearly, the rate of information transmission in the system is unaffected by the use of an\\nencoding process. We may, therefore, eliminate L between (6.53) and (6.54) to obtain bits/s\\n(6.55)\\nEquation (6.52) defines the average transmitted power required to maintain an M-ary PCM\\nsystem operating above the error threshold. Hence, solving this equation for the number of\\ndiscrete amplitude levels, we may express the number M in terms of the average\\ntransmitted power P and channel noise variance\\n= N0B as follows:\\n(6.56)\\nTherefore, substituting (6.56) into (6.55), we obtain',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 313,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'transmitted power P and channel noise variance\\n= N0B as follows:\\n(6.56)\\nTherefore, substituting (6.56) into (6.55), we obtain\\n(6.57)\\nThe channel bandwidth B required to transmit a rectangular pulse of duration 1/(2nW),\\nrepresenting a symbol in the codeword, is given by\\n(6.58) P 2 M ----- 1 2---  2 3 2---  2 +  M 1 - 2 --------------    2 + + c  2 = c22 M2 1 - 12 ----------------     = Rb 2W L 2 log = L Mn = Rb 2Wn M 2 log = 2 M 1 12P c2N0B ---------------- +      1 2  = Rb Wn 1 12P c2N0B ---------------- +       2 log = B nW =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 313,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nwhere  is a constant with a value lying between 1 and 2. Using the minimum possible\\nvalue  = 1, we find that the channel bandwidth B = nW. We may thus rewrite (6.57) as bits/s\\n(6.59)\\nwhich defines the upper bound on the information capacity realizable by an M-ary PCM\\nsystem.\\nFrom Chapter 5 we recall that, in accordance with Shannons information capacity law,\\nthe ideal transmission system is described by the formula bits/s\\n(6.60)\\nThe most interesting point derived from the comparison of (6.59) with (6.60) is the fact\\nthat (6.59) is of the right mathematical form in an information-theoretic context. To be\\nmore specific, we make the following statement:\\nPower and bandwidth in a PCM system are exchanged on a logarithmic\\nbasis, and the information capacity of the system is proportional to the\\nchannel bandwidth B.\\nAs a corollary, we may go on to state:\\nWhen the SNR ratio is high, the bandwidth-noise trade-off follows an\\nexponential law in PCM.\\nFrom the study of noise in analog modulation systems,5 it is known that the use of\\nfrequency modulation provides the best improvement in SNR ratio. To be specific, when\\nthe carrier-to-noise ratio is high enough, the bandwidth-noise trade-off follows a square\\nlaw in frequency modulation (FM). Accordingly, in comparing the noise performance of\\nFM with that of PCM we make the concluding statement:\\nPCM is more efficient than FM in trading off an increase in bandwidth for\\nimproved noise performance.\\nIndeed, this statement is further testimony for the PCM being viewed as a standard for\\nwaveform coding. Prediction-Error Filtering for Redundancy Reduction\\nWhen a voice or video signal is sampled at a rate slightly higher than the Nyquist rate, as\\nusually done in PCM, the resulting sampled signal is found to exhibit a high degree of\\ncorrelation between adjacent samples. The meaning of this high correlation is that, in an',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 314,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'usually done in PCM, the resulting sampled signal is found to exhibit a high degree of\\ncorrelation between adjacent samples. The meaning of this high correlation is that, in an\\naverage sense, the signal does not change rapidly from one sample to the next. As a result,\\nthe difference between adjacent samples has a variance that is smaller than the variance of\\nthe original signal. When these highly correlated samples are encoded, as in the standard\\nPCM system, the resulting encoded signal contains redundant information. This kind of\\nsignal structure means that symbols that are not absolutely essential to the transmission of\\nRb B 1 12P c2N0B ---------------- +       2 log = C B 1 P N0B ---------- +     2 log =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 314,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '7 Prediction-Error Filtering for Redundancy Reduction information are generated as a result of the conventional encoding process described in\\nSection 6.5. By reducing this redundancy before encoding, we obtain a more efficient coded\\nsignal, which is the basic idea behind DPCM. Discussion of this latter form of waveform\\ncoding is deferred to the next section. In this section we discuss prediction-error filtering,\\nwhich provides a method for reduction and, therefore, improved waveform coding.\\nTheoretical Considerations\\nTo elaborate, consider the block diagram of Figure 6.16a, which includes:\\n\\na direct forward path from the input to the output;\\n\\na predictor in the forward direction as well; and\\n\\na comparator for computing the difference between the input signal and the\\npredictor output.\\nThe difference signal, so computed, is called the prediction error. Correspondingly, a filter\\nthat operates on the message signal to produce the prediction error, illustrated in Figure\\n16a, is called a prediction-error filter.\\nTo simplify the presentation, let\\n(6.61)\\ndenote a sample of the message signal m(t) taken at time t = nTs. Then, with denoting\\nthe corresponding predictor output, the prediction error is defined by\\n(6.62)\\nwhere en is the amount by which the predictor fails to predict the input sample mn exactly.\\nIn any case, the objective is to design the predictor so as to minimize the variance of the\\nprediction error en. In so doing, we effectively end up using a smaller number of bits to\\nrepresent en than the original message sample mn; hence, the need for a smaller\\ntransmission bandwidth.\\nFigure 6.16 Block diagram of (a) prediction-error filter and (b) its inverse.\\nmn m nTs   = m n en mn m n - =  Message signal m(t) m(nTs) = mn mn Sample every Ts Seconds Predictor + - Predictor Prediction error en Sampled message signal mn Prediction error en + -  (a) (b)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 315,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nThe prediction-error filter operates on the message signal on a sample-by-sample basis\\nto produce the prediction error. With such an operation performed in the transmitter, how\\ndo we recover the original message signal from the prediction error at the receiver? To\\naddress this fundamental question in a simple-minded and yet practical way, we invoke the\\nuse of linerarity. Let the operator L denote the action of the predictor, as shown by\\n(6.63)\\nAccordingly, we may rewrite (6.62) in operator form as follows: (6.64)\\nUnder the assumption of linearity, we may invert (6.64) to recover the message sample\\nfrom the prediction error, as shown by\\n(6.65)\\nEquation (6.65) is immediately recognized as the equation of a feedback system, as\\nillustrated in Figure 6.16b. Most importantly, in functional terms, this feedback system\\nmay be viewed as the inverse of prediction-error filtering.\\nDiscrete-Time Structure for Prediction\\nTo simplify the design of the linear predictor in Figure 6.16, we propose to use a discrete-time\\nstructure in the form of a finite-duration impulse response (FIR) filter, which is well known in\\nthe digital signal-processing literature. The FIR filter was briefly discussed in Chapter 2.\\nFigure 6.17 depicts an FIR filter, consisting of two functional components:\\n\\na set of p unit-delay elements, each of which is represented by z-1; and',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 316,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'a set of p unit-delay elements, each of which is represented by z-1; and\\n\\na corresponding set of adders used to sum the scaled versions of the delayed inputs,\\nmn - 1, mn - 2, , mn - p.\\nThe overall linearly predicted output is thus defined by the convolution sum\\n(6.66)\\nwhere p is called the prediction order. Minimization of the prediction-error variance is\\nachieved by a proper choice of the FIR filter-coefficients as described next.\\nFigure 6.17 Block diagram of an FIR filter of order p.\\nm n L mn   = en mn L mn   - = 1 L -  mn   = mn 1 1 L - -------------    en   = m n wkmn k - k 1 = p  = Message sample mn mn - 1 mn - 2 mn - p + 1 mn - p . . . . . . . . . z - 1 Prediction of message sample: w1 z - 1 z - 1 w2 wp - 1 wp mn',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 316,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '7 Prediction-Error Filtering for Redundancy Reduction First, however, we make the following assumption:\\nThe message signal m(t) is drawn from a stationary stochastic processor M(t)\\nwith zero mean.\\nThis assumption may be satisfied by processing the message signal on a block-by-block\\nbasis, with each block being just long enough to satisfy the assumption in a pseudo-\\nstationary manner. For example, a block duration of 40 ms is considered to be adequate\\nfor voice signals.\\nWith the random variable Mn assumed to have zero mean, it follows that the variance of\\nthe prediction error en is the same as its mean-square value. We may thus define\\n(6.67)\\nas the index of performance. Substituting (6.65) and (6.66) into (6.67) and then expanding\\nterms, the index of performance is expressed as follows:\\n(6.68)\\nMoreover, under the above assumption of pseudo-stationarity, we may go on to introduce\\nthe following second-order statistical parameters for mn treated as a sample of the\\nstochastic process M(t) at t = nTs:\\nVariance\\n(6.69)\\nAutocorrelation function\\n(6.70)\\nNote that to simplify the notation in (6.67) to (6.70), we have applied the expectation\\noperator  to samples rather than the corresponding random variables.\\nIn any event, using (6.69) and (6.70), we may reformulate the index of performance of\\n(6.68) in the new form involving statistical parameters:\\n(6.71)\\nDifferentiating this index of performance with respect to the filter coefficients, setting the\\nresulting expression equal to zero, and then rearranging terms, we obtain the following\\nsystem of simultaneous equations:\\n(6.72)\\nwhere wo,j is the optimal value of the jth filter coefficient wj. This optimal set of equations\\nis the discrete-time version of the celebrated Wiener-Hopf equations for linear prediction.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 317,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '(6.72)\\nwhere wo,j is the optimal value of the jth filter coefficient wj. This optimal set of equations\\nis the discrete-time version of the celebrated Wiener-Hopf equations for linear prediction.\\nJ \\x02 e2 n    = J w   \\x02 mn 2   2 wk\\x02 mnmn k -   wjwk\\x02 mn j - mn k -   k 1 = p  j 1 = p  + k 1 = p  - = M 2 \\x02 mn \\x02 mn   -  2   = \\x02 mn 2   = for \\x02 mn   0 = RM k j -  \\x02 mn j - mn k -   = J w   M 2 2 wkRM k  wjwkRM k j -  k 1 = p  j 1 = p  + k 1 = p  - = wo jRM k j -  j 1 = p  RM k  k 1 2 p   =  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 317,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nWith compactness of mathematical exposition in mind, we find it convenient to\\nformulate the Wiener-Hopf equations in matrix form, as shown by\\n(6.73) where (6.74) is the p-by-1 optimum coefficient vector of the FIR predictor, (6.75)\\nis the p-by-1 autocorrelation vector of the original message signal, excluding the mean-\\nsquare value represented by RM, 0, and (6.76)\\nis the p-by-y correlation matrix of the original message signal, including RM, 0.6\\nCareful examination of (6.76) reveals the Toeplitz property of the autocorrelation\\nmatrix RM, which embodies two distinctive characteristics:\\nAll the elements on the main diagonal of the matrix RM are equal to the mean-\\nsquare value or, equivalently under the zero-mean assumption, the variance of the\\nmessage sample mn, as shown by\\nThe matrix is symmetric about the main diagonal.\\nThis Toeplitz property is a direct consequence of the assumption that message signal m(t)\\nis the sample function of a stationary stochastic process. From a practical perspective, the\\nToeplitz property of the autocorrelation matrix RM is important in that all of its elements\\nare uniquely defined by the autocorrelation sequence\\n. Moreover, from the\\ndefining equation (6.75), it is clear that the autocorrelation vector rM is uniquely defined\\nby the autocorrelation sequence\\n. We may therefore make the following\\nstatement:\\nThe p filter coefficients of the optimized linear predictor, configured in the form\\nof an FIR filter, are uniquely defined by the variance and the\\nautocorrelation sequence\\n, which pertain to the message signal\\nm(t) drawn from a weakly stationary process.\\nTypically, we have RMwo rM = wo wo 1  wo 2  wo p     T = rM RM 1  RM 2  RM p      T = RM RM 0  RM 1  RM p 1 -  RM 1  RM 0  RM p 2 -      RM p 1 -  RM p 2 -   RM 0  = RM 0  M 2 = RM k   k 0 = p 1 - RM k   k 1 = p M 2 RM 0  = RM k   k 0 = p 1 - RM k  RM 0  for k 1 2 p   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 318,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '7 Prediction-Error Filtering for Redundancy Reduction Under this condition, we find that the autocorrelation matrix RM is also invertible; that is,\\nthe inverse matrix exists. We may therefore solve (6.73) for the unknown value of the\\noptimal coefficient vector wo using the formula7\\n(6.77)\\nThus, given the variance and autocorrelation sequence\\n, we may uniquely\\ndetermine the optimized coefficient vector of the linear predictor, wo, defining an FIR\\nfilter of order p; and with it our design objective is satisfied.\\nTo complete the linear prediction theory presented herein, we need to find the\\nminimum mean-square value of prediction error, resulting from the use of the optimized\\npredictor. We do this by first reformulating (6.71) in the matrix form:\\n(6.78)\\nwhere the superscript T denotes matrix transposition, is the inner product of the\\np-by-1 vectors wo and rM, and the matrix product is a quadratic form. Then,\\nsubstituting the optimum formula of (6.77) into (6.78), we find that the minimum mean-\\nsquare value of prediction error is given by\\n(6.79)\\nwhere we have used the property that the autocorrelation matrix of a weakly stationary\\nprocess is symmetric; that is,\\n(6.80)\\nBy definition, the quadratic form is always positive. Accordingly, from (6.79)\\nit follows that the minimum value of the mean-square prediction error Jmin is always\\nsmaller than the variance of the zero-mean message sample mn that is being\\npredicted. Through the use of linear prediction as described herein, we have thus satisfied\\nthe objective:\\nTo design a prediction-error filter the output of which has a smaller variance\\nthan the variance of the message sample applied to its input, we need to follow\\nthe optimum formula of (6.77).\\nThis statement provides the rationale for going on to describe how the bandwidth\\nrequirement of the standard PCM can be reduced through redundancy reduction. However,\\nbefore proceeding to do so, it is instructive that we consider an adaptive implementation of\\nthe linear predictor.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 319,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'before proceeding to do so, it is instructive that we consider an adaptive implementation of\\nthe linear predictor.\\nRM 1 - wo RM 1 - rM = M 2 RM k   k 1 = p J wo   M 2 2wo T rM wo T RMwo + - = wo T rM wo T RMwo Jmin M 2 2 RM 1 - rM   TrM - RM 1 - rM   TRM RM 1 - rM   + = M 2 2rM T RM 1 - rM rM T RM 1 - rM + - = M 2 rM T RM 1 - rM - = RM T RM = rM T RM 1 - rM M',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 319,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nLinear Adaptive Prediction\\nThe use of (6.77) for calculating the optimum weight vector of a linear predictor requires\\nknowledge of the autocorrelation function Rm,k of the message signal sequence\\nwhere p is the prediction order. What if knowledge of this sequence is not available? In\\nsituations of this kind, which occur frequently in practice, we may resort to the use of an\\nadaptive predictor.\\nThe predictor is said to be adaptive in the following sense:\\n\\nComputation of the tap weights wk, k = 1, 2, , p, proceeds in an iterative manner,\\nstarting from some arbitrary initial values of the tap weights.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 320,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The algorithm used to adjust the tap weights (from one iteration to the next) is self-\\ndesigned, operating solely on the basis of available data.\\nThe aim of the algorithm is to find the minimum point of the bowl-shaped error surface\\nthat describes the dependence of the cost function J on the tap weights. It is, therefore,\\nintuitively reasonable that successive adjustments to the tap weights of the predictor be\\nmade in the direction of the steepest descent of the error surface; that is, in a direction\\nopposite to the gradient vector whose elements are defined by\\n(6.81)\\nThis is indeed the idea behind the method of deepest descent. Let wk, n denote the value of\\nthe kth tap weight at iteration n. Then, the updated value of this weight at iteration n + 1 is\\ndefined by\\n(6.82)\\nwhere  is a step-size parameter that controls the speed of adaptation and the factor 1/2 is\\nincluded for convenience of presentation. Differentiating the cost function J of (6.68) with\\nrespect to wk, we readily find that\\n(6.83)\\nFrom a practical perspective, the formula for the gradient gk in (6.83) could do with further\\nsimplification that ignores the expectation operator. In effect, instantaneous values are\\nused as estimates of autocorrelation functions. The motivation for this simplification is to\\npermit the adaptive process to proceed forward on a step-by-step basis in a self-organized\\nmanner. Clearly, by ignoring the expectation operator in (6.83), the gradient gk takes on a\\ntime-dependent value, denoted by gk, n. We may thus write\\n(6.84)\\nwhere is an estimate of the filter coefficient wj, n at time n.\\nThe stage is now set for substituting (6.84) into (6.82), where in the latter equation\\nis substituted for wk, n; this change is made to account for dispensing with the expectation\\noperator: mk  p k 0 = gk J wk --------- k 1 2 p   =  = wk n 1 +  wk n  1 2---gk k 1 2 p   =  - = gk 2mnmn k -   - wjmn j - mn k -   j 1 = p  + = gk n  2mnmn k - - 2mn k - w j n mn j k 1 2 p   =  j 1 = p  + = w j n  w k n',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 320,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Differential Pulse-Code Modulation (6.85)\\nwhere en is the new prediction error defined by\\n(6.86)\\nNote that the current value of the message signal, mn, plays a role as the desired response\\nfor predicting the value of mn given the past values of the message signal: mn - 1, mn - 2,\\n, mn - p.\\nIn words, we may express the adaptive filtering algorithm of (6.85) as follows:\\nThe algorithm just described is the popular least-mean-square (LMS) algorithm,\\nformulated for the purpose of linear prediction. The reason for popularity of this adaptive\\nfiltering algorithm is the simplicity of its implementation. In particular, the computational\\ncomplexity of the algorithm, measured in terms of the number of additions and\\nmultiplications, is linear in the prediction order p. Moreover, the algorithm is not only\\ncomputationally efficient but it is also effective in performance.\\nThe LMS algorithm is a stochastic adaptive filtering algorithm, stochastic in the sense\\nthat, starting from the initial condition defined by\\n, it seeks to find the\\nminimum point of the error surface by following a zig-zag path. However, it never finds\\nthis minimum point exactly. Rather, it continues to execute a random motion around the\\nminimum point of the error surface (Haykin, 2013). Differential Pulse-Code Modulation\\nDPCM, the scheme to be considered for channel-bandwidth conservation, exploits the\\nidea of linear prediction theory with a practical difference:\\nIn the transmitter, the linear prediction is performed on a quantized version\\nof the message sample instead of the message sample itself, as illustrated in\\nFigure 6.18. w k n 1 +  w k n  1 2--- - gk n  = w = k n  mnmn k - w j n mn j - mn k - j 1 = p  -         + w = k n  mn k - mn w j n mn j - j 1 = p  -         + w = k n  mn k - en + en mn w j n mn j - j 1 = p  - = Prediction error computed at time n ( ) Message signal mn delayed by k time steps\\n( ) Step-size parameter ( ) Old value of the same\\nfilter coefficient at time n\\n(\\n)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 321,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '( ) Step-size parameter ( ) Old value of the same\\nfilter coefficient at time n\\n(\\n)\\nUpdated value of the kth\\nfilter coefficient at time n +\\n( )= +  wk 0   k 1 = p',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 321,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nThe resulting process is referred to as differential quantization. The motivation behind the\\nuse of differential quantization follows from two practical considerations:\\nWaveform encoding in the transmitter requires the use of quantization.\\nWaveform decoding in the receiver, therefore, has to process a quantized signal.\\nIn order to cater to both requirements in such a way that the same structure is used for\\npredictors in both the transmitter and the receiver, the transmitter has to perform prediction-\\nerror filtering on the quantized version of the message signal rather than the signal itself, as\\nshown in Figure 6.19a. Then, assuming a noise-free channel, the predictors in the transmitter\\nand receiver operate on exactly the same sequence of quantized message samples.\\nTo demonstrate this highly desirable and distinctive characteristic of differential PCM,\\nwe see from Figure 6.19a that\\n(6.87)\\nFigure 6.18 Block diagram of a differential quantizer.\\nFigure 6.19 DPCM system: (a) transmitter; (b) receiver.\\nen Quantizer Linear predictor mn mq, n eq, n mn  + + eq n  en qn + = DPCM encoded signal, mq, n DPCM decoded output Adder Linear predictor Quantizer Encoder Comparator - + Sampled version of message signal, mn Noisy version of DPCM encoded signal + + (a) Decoder + + (b) en mq, n eq, n mn    Linear predictor',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 322,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '8 Differential Pulse-Code Modulation where qn is the quantization noise produced by the quantizer operating on the prediction\\nerror en. Moreover, from Figure 6.19a, we readily see that\\n(6.88)\\nwhere is the predicted value of the original message sample mn; thus, (6.88) is in\\nperfect agreement with Figure 6.18. Hence, the use of (6.87) in (6.88) yields\\n(6.89)\\nWe may now invoke (6.88) of linear prediction theory to rewrite (6.89) in the equivalent\\nform:\\n(6.90)\\nwhich describes a quantized version of the original message sample mn.\\nWith the differential quantization scheme of Figure 6.19a at hand, we may now expand\\non the structures of the transmitter and receiver of DPCM.\\nDPCM Transmitter\\nOperation of the DPCM transmitter proceeds as follows:\\nGiven the predicted message sample\\n, the comparator at the transmitter input\\ncomputes the prediction error en, which is quantized to produce the quantized\\nversion of en in accordance with (6.87).\\nWith and eq, n at hand, the adder in the transmitter produces the quantized\\nversion of the original message sample mn, namely mq, n, in accordance with (6.88).\\nThe required one-step prediction is produced by applying the sequence of\\nquantized samples to a linear FIR predictor of order p.\\nThis multistage operation is clearly cyclic, encompassing three steps that are repeated at\\neach time step n. Moreover, at each time step, the encoder operates on the quantized\\nprediction error eq, n to produce the DPCM-encoded version of the original message\\nsample mn. The DPCM code so produced is a lossy-compressed version of the PCM code;\\nit is lossy because of the prediction error.\\nDPCM Receiver\\nThe structure of the receiver is much simpler than that of the transmitter, as depicted in\\nFigure 6.19b. Specifically, first, the decoder reconstructs the quantized version of the\\nprediction error, namely eq, n. An estimate of the original message sample mn is then\\ncomputed by applying the decoder output to the same predictor used in the transmitter of',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 323,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'prediction error, namely eq, n. An estimate of the original message sample mn is then\\ncomputed by applying the decoder output to the same predictor used in the transmitter of\\nFigure 6.19a. In the absence of channel noise, the encoded signal at the receiver input is\\nidentical to the encoded signal at the transmitter output. Under this ideal condition, we\\nfind that the corresponding receiver output is equal to mq, n, which differs from the original\\nsignal sample mn only by the quantization error qn incurred as a result of quantizing the\\nprediction error en.\\nFrom the foregoing analysis, we thus observe that, in a noise-free environment, the\\nlinear predictors in the transmitter and receiver of DPCM operate on the same sequence of\\nsamples, mq, n. It is with this point in mind that a feedback path is appended to the\\nquantizer in the transmitter of Figure 6.19a.\\nmq n  m n eq n  + = m n mq n  m n en qn + + = mq n  mn qn + = m n m n m n mq k   k 1 = p',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 323,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nProcessing Gain\\nThe output SNR of the DPCM system, shown in Figure 6.19, is, by definition,\\n(6.91)\\nwhere is the variance of the original signal sample mn, assumed to be of zero mean,\\nand is the variance of the quantization error qn, also of zero mean. We may rewrite\\n(6.91) as the product of two factors, as shown by\\n(6.92)\\nwhere, in the first line, is the variance of the prediction error en. The factor (SNR)Q\\nintroduced in the second line is the signal-to-quantization noise ratio, which is itself\\ndefined by\\n(6.93)\\nThe other factor Gp is the processing gain produced by the differential quantization\\nscheme; it is formally defined by\\n(6.94)\\nThe quantity Gp, when it is greater than unity, represents a gain in signal-to-noise ratio,\\nwhich is due to the differential quantization scheme of Figure 6.19. Now, for a given\\nmessage signal, the variance is fixed, so that Gp is maximized by minimizing the\\nvariance of the prediction error en. Accordingly, the objective in implementing the\\nDPCM should be to design the prediction filter so as to minimize the prediction-error\\nvariance,\\n.\\nIn the case of voice signals, it is found that the optimum signal-to-quantization noise\\nadvantage of the DPCM over the standard PCM is in the neighborhood of 4-11dB. Based\\non experimental studies, it appears that the greatest improvement occurs in going from no\\nprediction to first-order prediction, with some additional gain resulting from increasing\\nthe order p of the prediction filter up to 4 or 5, after which little additional gain is obtained.\\nSince 6 dB of quantization noise is equivalent to 1 bit per sample by virtue of the results\\npresented in Table 6.1 for sinusoidal modulation, the advantage of DPCM may also be\\nexpressed in terms of bit rate. For a constant signal-to-quantization noise ratio, and\\nassuming a sampling rate of 8 kHz, the use of DPCM may provide a saving of about 8-',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 324,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'expressed in terms of bit rate. For a constant signal-to-quantization noise ratio, and\\nassuming a sampling rate of 8 kHz, the use of DPCM may provide a saving of about 8-\\n16 kHz (i.e., 1 to 2 bits per sample) compared with the standard PCM.\\nSNR  O M 2 Q 2 -------- = M 2 Q 2 SNR  O M 2 E 2 --------      E 2 Q 2 -------       = Gp SNR  Q = E 2 SNR  Q E 2 Q 2 ------- = Gp M 2 E 2 -------- = M 2 M 2 E',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 324,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '9 Delta Modulation 305 6.9 Delta Modulation In choosing DPCM for waveform coding, we are, in effect, economizing on transmission\\nbandwidth by increasing system complexity, compared with standard PCM. In other\\nwords, DPCM exploits the complexity-bandwidth tradeoff. However, in practice, the need\\nmay arise for reduced system complexity compared with the standard PCM. To achieve\\nthis other objective, transmission bandwidth is traded off for reduced system complexity,\\nwhich is precisely the motivation behind DM. Thus, whereas DPCM exploits the\\ncomplexity-bandwidth tradeoff, DM exploits the bandwidth-complexity tradeoff. We may,\\ntherefore, differentiate between the standard PCM, the DPCM, and the DM along the lines\\ndescribed in Figure 6.20. With the bandwidth-complexity tradeoff being at the heart of\\nDM, the incoming message signal m(t) is oversampled, which requires the use of a\\nsampling rate higher than the Nyquist rate. Accordingly, the correlation between adjacent\\nsamples of the message signal is purposely increased so as to permit the use of a simple\\nquantizing strategy for constructing the encoded signal.\\nDM Transmitter\\nIn the DM transmitter, system complexity is reduced to the minimum possible by using the\\ncombination of two strategies:\\nSingle-bit quantizer, which is the simplest quantizing strategy; as depicted in Figure\\n21, the quantizer acts as a hard limiter with only two decision levels, namely, .\\nSingle unit-delay element, which is the most primitive form of a predictor; in other\\nwords, the only component retained in the FIR predictor of Figure 6.17 is the front-end\\nblock labeled z-1, which acts as an accumulator.\\nThus, replacing the multilevel quantizer and the FIR predictor in the DPCM transmitter of\\nFigure 6.19a in the manner described under points 1 and 2, respectively, we obtain the\\nblock diagram of Figure 6.21a for the DM transmitter.\\nFrom this figure, we may express the equations underlying the operation of the DM',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 325,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'block diagram of Figure 6.21a for the DM transmitter.\\nFrom this figure, we may express the equations underlying the operation of the DM\\ntransmitter by the following set of equations (6.95)-(6.97): (6.95)\\nFigure 6.20 Illustrating the tradeoffs\\nbetween standard PCM, DPCM, and DM.\\nen mn m n - = mn mq n 1 -  - = Increasing System complexity DPCM Standard PCM DM Transmission bandwidth Increasing',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 325,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\n(6.96)\\n(6.97)\\nAccording to (6.95) and (6.96), two possibilities may naturally occur:\\nThe error signal en (i.e., the difference between the message sample mn and its\\napproximation\\n) is positive, in which case the approximation is\\nincreased by the amount ; in this first case, the encoder sends out symbol 1.\\nThe error signal en is negative, in which case the approximation is\\nreduced by the amount ; in this second case, the encoder sends out symbol 0.\\nFrom this description it is apparent that the delta modulator produces a staircase\\napproximation to the message signal, as illustrated in Figure 6.22a. Moreover, the rate of\\ndata transmission in DM is equal to the sampling rate fs = 1Ts, as illustrated in the binary\\nsequence of Figure 6.22b.\\nFigure 6.21 DM system: (a) transmitter; (b) receiver.\\nReconstructed message signal Decoder Low-pass filter Accumulator + + Noisy version of DM encoded signal mq, n z - 1 DM encoded signal mq, n Encoder One-bit quantizer Comparator Accumulator - + Sampled message signal mn + + z - 1 (a) (b) en eq, n mq, n - 1 mq, n    eq n   en   sgn = +if en 0   - if en 0       = mq n  mq n 1 -  eq n  + = m n m n mq n 1 -  = m n mq n 1 -  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 326,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '9 Delta Modulation 307 DM Receiver Following a procedure similar to the way in which we constructed the DM transmitter of\\nFigure 6.21a, we may construct the DM receiver of Figure 6.21b as a special case of the\\nDPCM receiver of Figure 6.19b. Working through the operation of the DM receiver, we\\nfind that reconstruction of the staircase approximation to the original message signal is\\nachieved by passing the sequence of positive and negative pulses (representing symbols\\nand 0, respectively) through the block labeled accumulator.\\nUnder the assumption that the channel is distortionless, the accumulated output is the\\ndesired mq,n given that the decoded channel output is eq,n. The out-of-band quantization\\nnoise in the high-frequency staircase waveform in the accumulator output is suppressed by\\npassing it through a low-pass filter with a cutoff frequency equal to the message\\nbandwidth.\\nQuantization Errors in DM\\nDM is subject to two types of quantization error: slope overload distortion and granular\\nnoise. We will discuss the case of slope overload distortion first.\\nStarting with (6.97), we observe that this equation is the digital equivalent of\\nintegration, in the sense that it represents the accumulation of positive and negative\\nincrements of magnitude . Moreover, denoting the quantization error applied to the\\nmessage sample mn by qn, we may express the quantized message sample as\\n(6.98)\\nWith this expression for mq,n at hand, we find from (6.98) that the quantizer input is\\n(6.99)\\nThus, except for the delayed quantization error qn -1 , the quantizer input is a first\\nbackward difference of the original message sample. This difference may be viewed as a\\nFigure 6.22 Illustration of DM.\\n0 0 Binary sequence at modulator output 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 Ts t \\x02 m(t) Staircase approximation mq, n (a) (b) mq n  mn qn + = en mn mn 1 - qn 1 - +   - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 327,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\ndigital approximation to the quantizer input or, equivalently, as the inverse of the digital\\nintegration process carried out in the DM transmitter. If, then, we consider the maximum\\nslope of the original message signal m(t), it is clear that in order for the sequence of\\nsamples {mq,n} to increase as fast as the sequence of message samples {mn} in a region of\\nmaximum slope of m(t), we require that the condition\\n(6.100)\\nbe satisfied. Otherwise, we find that the step-size  is too small for the staircase\\napproximation mq(t) to follow a steep segment of the message signal m(t), with the result\\nthat mq(t) falls behind m(t), as illustrated in Figure 6.23. This condition is called slope\\noverload, and the resulting quantization error is called slope-overload distortion (noise).\\nNote that since the maximum slope of the staircase approximation mq(t) is fixed by the\\nstep size , increases and decreases in mq(t) tend to occur along straight lines. For this\\nreason, a delta modulator using a fixed step size is often referred to as a linear delta\\nmodulator.\\nIn contrast to slope-overload distortion, granular noise occurs when the step size  is\\ntoo large relative to the local slope characteristics of the message signal m(t), thereby\\ncausing the staircase approximation mq(t) to hunt around a relatively flat segment of m(t);\\nthis phenomenon is also illustrated in the tail end of Figure 6.23. Granular noise is\\nanalogous to quantization noise in a PCM system.\\nAdaptive DM\\nFrom the discussion just presented, it is appropriate that we need to have a large step size\\nto accommodate a wide dynamic range, whereas a small step size is required for the\\naccurate representation of relatively low-level signals. It is clear, therefore, that the choice\\nof the optimum step size that minimizes the mean-square value of the quantization error in\\na linear delta modulator will be the result of a compromise between slope-overload',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 328,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'of the optimum step size that minimizes the mean-square value of the quantization error in\\na linear delta modulator will be the result of a compromise between slope-overload\\ndistortion and granular noise. To satisfy such a requirement, we need to make the delta\\nmodulator adaptive, in the sense that the step size is made to vary in accordance with the\\ninput signal. The step size is thereby made variable, such that it is enlarged during\\nintervals when the slope-overload distortion is dominant and reduced in value when the\\ngranular (quantization) noise is dominant.\\nFigure 6.23 Illustration of the two different forms of quantization error in DM.\\n Ts ----- max dm t dt --------------  Ts  Slope-overload distortion Granular noise Staircase approximation mq(t) m(t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 328,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '10 Line Codes 309 6.10 Line Codes In this chapter, we have described three basic waveform-coding schemes: PCM, DPCM,\\nand DM. Naturally, they differ from each other in several ways: transmission-bandwidth\\nrequirement, transmitter-receiver structural composition and complexity, and quantization\\nnoise. Nevertheless, all three of them have a common need: line codes for electrical\\nrepresentation of the encoded binary streams produced by their individual transmitters, so\\nas to facilitate transmission of the binary streams across the communication channel.\\nFigure 6.24 displays the waveforms of five important line codes for the example data\\nstream 01101001. Figure 6.25 displays their individual power spectra (for positive\\nfrequencies) for randomly generated binary data, assuming that first, symbols 0 and 1 are\\nequiprobable, second, the average power is normalized to unity, and third, the frequency f\\nis normalized with respect to the bit rate 1Tb. In what follows, we describe the five line\\ncodes involved in generating the coded waveforms of Figure 6.24.\\nFigure 6.24 Line codes for the electrical representations of binary data: (a) unipolar\\nnonreturn-to-zero (NRZ) signaling; (b) polar NRZ signaling; (c) unipolar return-to-zero\\n(RZ) signaling; (d) bipolar RZ signaling; (e) split-phase or Manchester code.\\n0 (e) 0 (d) 0 (c) 0 (b) 0 A (a) 0 1 1 0 1 Binary data 0 0 1 Time A -A A -A A -A A Tb',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 329,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nFigure 6.25 Power spectra of line codes: (a) unipolar NRZ signal; (b) polar NRZ signal; (c) unipolar\\nRZ signal; (d) bipolar RZ signal; (e) Manchester-encoded signal. The frequency is normalized with\\nrespect to the bit rate 1Tb, and the average power is normalized to unity.\\n0 1 (c) 2 0 0.25 0.5 0 Delta function of weight 1/2 1 (a) 2 0 0.5 1 0 1 (b) 2 0 0.5 1 0 1 (d) 2 0 0.5 0.9 0 1 Normalized frequency Normalized frequency A2 = 2 Normalized power spectral density\\nNormalized power spectral density\\nNormalized power spectral density\\nNormalized power spectral density\\nNormalized power spectral density\\nNormalized frequency Normalized frequency Normalized frequency (e) 2 0 0.5 A2 = 1 A2 = 4 A2 = 1 A2 = 4 Delta function of weight 1 Delta function of weight 0.1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 330,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '10 Line Codes Unipolar NRZ Signaling\\nIn this line code, symbol 1 is represented by transmitting a pulse of amplitude A for the\\nduration of the symbol, and symbol 0 is represented by switching off the pulse, as in\\nFigure 6.24a. The unipolar NRZ line code is also referred to as on-off signaling.\\nDisadvantages of on-off signaling are the waste of power due to the transmitted DC level\\nand the fact that the power spectrum of the transmitted signal does not approach zero at\\nzero frequency.\\nPolar NRZ Signaling\\nIn this second line code, symbols 1 and 0 are represented by transmitting pulses of\\namplitudes +A and -A, respectively, as illustrated in Figure 6.24b. The polar NRZ line\\ncode is relatively easy to generate, but its disadvantage is that the power spectrum of the\\nsignal is large near zero frequency.\\nUnipolar RZ Signaling\\nIn this third line code, symbol 1 is represented by a rectangular pulse of amplitude A and\\nhalf-symbol width and symbol 0 is represented by transmitting no pulse, as illustrated in\\nFigure 6.24c. An attractive feature of the unipolar RZ line code is the presence of delta\\nfunctions at f = 0, 1Tb in the power spectrum of the transmitted signal; the delta\\nfunctions can be used for bit-timing recovery at the receiver. However, its disadvantage is\\nthat it requires 3 dB more power than polar RZ signaling for the same probability of\\nsymbol error.\\nBipolar RZ Signaling\\nThis line code uses three amplitude levels, as indicated in Figure 6.24(d). Specifically,\\npositive and negative pulses of equal amplitude (i.e., +A and -A) are used alternately for\\nsymbol 1, with each pulse having a half-symbol width; no pulse is always used for symbol\\nA useful property of the bipolar RZ signaling is that the power spectrum of the\\ntransmitted signal has no DC component and relatively insignificant low-frequency\\ncomponents for the case when symbols 1 and 0 occur with equal probability. The bipolar\\nRZ line code is also called alternate mark inversion (AMI) signaling.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 331,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'components for the case when symbols 1 and 0 occur with equal probability. The bipolar\\nRZ line code is also called alternate mark inversion (AMI) signaling.\\nSplit-Phase (Manchester Code)\\nIn this final method of signaling, illustrated in Figure 6.24e, symbol 1 is represented by a\\npositive pulse of amplitude A followed by a negative pulse of amplitude -A, with both\\npulses being half-symbol wide. For symbol 0, the polarities of these two pulses are\\nreversed. A unique property of the Manchester code is that it suppresses the DC\\ncomponent and has relatively insignificant low-frequency components, regardless of the\\nsignal statistics. This property is essential in some applications.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 331,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses Summary and Discussion\\nIn this chapter we introduced two fundamental and complementary processes:\\n\\nSampling, which operates in the time domain; the sampling process is the link\\nbetween an analog waveform and its discrete-time representation.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 332,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Quantization, which operates in the amplitude domain; the quantization process is\\nthe link between an analog waveform and its discrete-amplitude representation.\\nThe sampling process builds on the sampling theorem, which states that a strictly band-\\nlimited signal with no frequency components higher than W Hz is represented uniquely by\\na sequence of samples taken at a uniform rate equal to or greater than the Nyquist rate of\\n2W samples per second. The quantization process exploits the fact that any human sense,\\nas ultimate receiver, can only detect finite intensity differences.\\nThe sampling process is basic to the operation of all pulse modulation systems, which\\nmay be classified into analog pulse modulation and digital pulse modulation. The\\ndistinguishing feature between them is that analog pulse modulation systems maintain a\\ncontinuous amplitude representation of the message signal, whereas digital pulse\\nmodulation systems also employ quantization to provide a representation of the message\\nsignal that is discrete in both time and amplitude.\\nAnalog pulse modulation results from varying some parameter of the transmitted\\npulses, such as amplitude, duration, or position, in which case we speak of PAM, pulse-\\nduration modulation, or pulse-position modulation, respectively. In this chapter we\\nfocused on PAM, as it is used in all forms of digital pulse modulation.\\nDigital pulse modulation systems transmit analog message signals as a sequence of\\ncoded pulses, which is made possible through the combined use of sampling and\\nquantization. PCM is an important form of digital pulse modulation that is endowed with\\nsome unique system advantages, which, in turn, have made it the standard method of\\nmodulation for the transmission of such analog signals as voice and video signals. The\\nadvantages of PCM include robustness to noise and interference, efficient regeneration of\\nthe coded pulses along the transmission path, and a uniform format for different kinds of\\nbaseband signals.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 332,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'advantages of PCM include robustness to noise and interference, efficient regeneration of\\nthe coded pulses along the transmission path, and a uniform format for different kinds of\\nbaseband signals.\\nIndeed, it is because of this list of advantages unique to PCM that it has become the\\nmethod of choice for the construction of public switched telephone networks (PSTNs). In\\nthis context, the reader should carefully note that the telephone channel viewed from the\\nPSTN by an Internet service provider, for example, is nonlinear due to the use of\\ncompanding and, most importantly, it is entirely digital. This observation has a significant\\nimpact on the design of high-speed modems for communications between a computer user\\nand server, which will be discussed in Chapter 8.\\nDM and DPCM are two other useful forms of digital pulse modulation. The principal\\nadvantage of DM is the simplicity of its circuitry, which is achieved at the expense of\\nincreased transmission bandwidth. In contrast, DPCM employs increased circuit\\ncomplexity to reduce channel bandwidth. The improvement is achieved by using the idea\\nof prediction to reduce redundant symbols from an incoming data stream. A further\\nimprovement in the operation of DPCM can be made through the use of adaptivity to\\naccount for statistical variations in the input data. By so doing, bandwidth requirement\\nmay be reduced significantly without serious degradation in system performance.8',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 332,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': 'Problems 313 Problems Sampling Process 6.1 In natural sampling, an analog signal g(t) is multiplied by a periodic train of rectangular pulses c(t),\\neach of unit area. Given that the pulse repetition frequency of this periodic train is fs and the duration\\nof each rectangular pulse is T (with fsT  1), do the following:\\na. Find the spectrum of the signal s(t) that results from the use of natural sampling; you may assume\\nthat time t = 0 corresponds to the midpoint of a rectangular pulse in c(t).\\nb. Show that the original signal g(t) may be recovered exactly from its naturally sampled version,\\nprovided that the conditions embodied in the sampling theorem are satisfied. Specify the Nyquist rate and the Nyquist interval for each of the following signals:\\na. g(t) = sinc(200t).\\nb. g(t) = sinc2(200t).\\nc. g(t) = sinc(200t) + sinc2(200t). Discussion of the sampling theorem presented in Section 6.2 was confined to the time domain.\\nDescribe how the sampling theorem can be applied in the frequency domain.\\nPulse-Amplitude Modulation Figure P6.4 shows the idealized spectrum of a message signal m(t). The signal is sampled at a rate\\nequal to 1 kHz using flat-top pulses, with each pulse being of unit amplitude and duration 0.1ms.\\nDetermine and sketch the spectrum of the resulting PAM signal. In this problem, we evaluate the equalization needed for the aperture effect in a PAM system. The\\noperating frequency f = fs2, which corresponds to the highest frequency component of the message\\nsignal for a sampling rate equal to the Nyquist rate. Plot 1/sinc(0.5TTs) versus TTs, and hence find\\nthe equalization needed when TTs = 0.1. Consider a PAM wave transmitted through a channel with white Gaussian noise and minimum\\nbandwidth BT = 1/2Ts, where Ts is the sampling period. The noise is of zero mean and power\\nspectral density N02. The PAM signal uses a standard pulse g(t) with its Fourier transform defined\\nby\\nBy considering a full-load sinusoidal modulating wave, show that PAM and baseband-signal',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 333,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'spectral density N02. The PAM signal uses a standard pulse g(t) with its Fourier transform defined\\nby\\nBy considering a full-load sinusoidal modulating wave, show that PAM and baseband-signal\\ntransmission have equal SNRs for the same average transmitted power. Twenty-four voice signals are sampled uniformly and then time-division multiplexed (TDM). The\\nsampling operation uses flat-top samples with 1 s duration. The multiplexing operation includes\\nFigure P6.4 |M( f )| 0 400 -400 f (Hz) G f 1 2BT ----------, f BT  0, f BT       =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 333,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nprovision for synchronization by adding an extra pulse of sufficient amplitude and also 1 s duration.\\nThe highest frequency component of each voice signal is 3.4 kHz.\\na. Assuming a sampling rate of 8 kHz, calculate the spacing between successive pulses of the\\nmultiplexed signal.\\nb. Repeat your calculation assuming the use of Nyquist rate sampling. Twelve different message signals, each with a bandwidth of 10 kHz, are to be multiplexed and\\ntransmitted. Determine the minimum bandwidth required if the multiplexing/modulation method\\nused is time-division multiplexing (TDM), which was discussed in Chapter 1.\\nPulse-Code Modulation A speech signal has a total duration of 10 s. It is sampled at the rate of 8 kHz and then encoded. The\\nsignal-to-(quantization) noise ratio is required to be 40 dB. Calculate the minimum storage capacity\\nneeded to accommodate this digitized speech signal. Consider a uniform quantizer characterized by the input-output relation illustrated in Figure 6.9a.\\nAssume that a Gaussian-distributed random variable with zero mean and unit variance is applied to\\nthis quantizer input.\\na. What is the probability that the amplitude of the input lies outside the range -4 to +4?\\nb. Using the result of part a, show that the output SNR of the quantizer is given by\\nwhere R is the number of bits per sample. Specifically, you may assume that the quantizer input\\nextends from -4 to 4. Compare the result of part b with that obtained in Example 2. A PCM system uses a uniform quantizer followed by a 7-bit binary encoder. The bit rate of the\\nsystem is equal to 50  106 bits/s.\\na. What is the maximum message bandwidth for which the system operates satisfactorily?\\nb. Determine the output signal-to-(quantization) noise when a full-load sinusoidal modulating wave\\nof frequency 1 MHz is applied to the input. Show that with a nonuniform quantizer the mean-square value of the quantization error is\\napproximately equal to',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 334,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'of frequency 1 MHz is applied to the input. Show that with a nonuniform quantizer the mean-square value of the quantization error is\\napproximately equal to\\n, where i is the ith step size and pi is the probability that the\\ninput signal amplitude lies within the ith interval. Assume that the step size i is small compared\\nwith the excursion of the input signal. a. A sinusoidal signal with an amplitude of 3.25 V is applied to a uniform quantizer of the midtread\\ntype whose output takes on the values 0, 1, 2, 3 V. Sketch the waveform of the resulting\\nquantizer output for one complete cycle of the input.\\nb. Repeat this evaluation for the case when the quantizer is of the midrise type whose output takes\\non the values 0.5, 1.5, 2.5, 3.5 V. The signal\\nis transmitted using a 40-bit binary PCM system. The quantizer is of the midrise type, with a step\\nsize of 1V. Sketch the resulting PCM wave for one complete cycle of the input. Assume a sampling\\nrate of four samples per second, with samples taken at t(s) = 1/8, 3/8, 5/8,  Figure P6.15 shows a PCM signal in which the amplitude levels of +1V and -1V are used to\\nrepresent binary symbols 1 and 0, respectively. The codeword used consists of three bits. Find the\\nsampled version of an analog signal from which this PCM signal is derived.\\nSNR  O 6R 7.2 dB - = 1 12   ii 2pi m t (volts) 6 2t   sin =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 334,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems 315 6.16 Consider a chain of (n - 1) regenerative repeaters, with a total of n sequential decisions made on a\\nbinary PCM wave, including the final decision made at the receiver. Assume that any binary symbol\\ntransmitted through the system has an independent probability p1 of being inverted by any repeater.\\nLet pn represent the probability that a binary symbol is in error after transmission through the\\ncomplete system.\\na. Show that\\nb. If p1 is very small and n is not too large, what is the corresponding value of pn? Discuss the basic issues involved in the design of a regenerative repeater for PCM.\\nLinear Prediction A one-step linear predictor operates on the sampled version of a sinusoidal signal. The sampling rate\\nis equal to 10f0, where f0 is the frequency of the sinusoid. The predictor has a single coefficient\\ndenoted by w1.\\na. Determine the optimum value of w1 required to minimize the prediction-error variance.\\nb. Determine the minimum value of the prediction error variance. A stationary process X(t) has the following values for its autocorrelation function:\\na. Calculate the coefficients of an optimum linear predictor involving the use of three unit-time\\ndelays.\\nb. Calculate the variance of the resulting prediction error. Repeat the calculations of Problem 6.19, but this time use a linear predictor with two unit-time\\ndelays. Compare the performance of this second optimum linear predictor with that considered in\\nProblem 6.19.\\nDifferential Pulse-Code Modulation A DPCM system uses a linear predictor with a single tap. The normalized autocorrelation function\\nof the input signal for a lag of one sampling interval is 0.75. The predictor is designed to minimize\\nthe prediction-error variance. Determine the processing gain attained by the use of this predictor. Calculate the improvement in processing gain of a DPCM system using the optimized three-tap\\nlinear predictor. For this calculation, use the autocorrelation function values of the input signal',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 335,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'linear predictor. For this calculation, use the autocorrelation function values of the input signal\\nspecified in Problem 6.19. In this problem, we compare the performance of a DPCM system with that of an ordinary PCM\\nsystem using companding.\\nFigure P6.15 Tb t 0 +1 -1 pn 1 2--- 1 1 2p1 -  n -   = RX 0  1 = RX 0  0.8 = RX 0  0.6 = RX 0  0.4 =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 335,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nFor a sufficiently large number of representation levels, the signal-to-(quantization) noise ratio of\\nPCM systems, in general, is defined by\\nwhere 2n is the number of representation levels. For a companded PCM system using the -law, the\\nconstant  is itself defined by\\nFor a DPCM system, on the other hand, the constant  lies in the range -3 <  < 15 dBs. The\\nformulas quoted herein apply to telephone-quality speech signals.\\nCompare the performance of the DPCM system against that of the -companded PCM system with\\n = 255 for each of the following scenarios:\\na. The improvement in (SNR)O realized by DPCM over companded PCM for the same number of\\nbits per sample.\\nb. The reduction in the number of bits per sample required by DPCM, compared with the\\ncompanded PCM for the same (SNR)O. In the DPCM system depicted in Figure P6.24, show that in the absence of channel noise, the\\ntransmitting and receiving prediction filters operate on slightly different input signals. Figure P6.25 depicts the block diagram of adaptive quantization for DPCM. The quantization is of a\\nbackward estimation kind because samples of the quantization output and prediction errors are used\\nto continuously derive backward estimates of the variance of the message signal. This estimate\\ncomputed at time n is denoted by\\n. Given this estimate, the step size is varied so as to match the\\nactual variance of the message sample mn, as shown by\\nwhere is the estimate of the standard deviation and is a constant. An attractive feature of the\\nadaptive scheme in Figure P6.25 is that samples of the quantization output and the prediction error\\nare used to compute the predictors coefficients.\\nModify the block diagram of the DPCM transmitter in Figure 6.19a so as to accommodate adaptive\\nprediction with backward estimation.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 336,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'are used to compute the predictors coefficients.\\nModify the block diagram of the DPCM transmitter in Figure 6.19a so as to accommodate adaptive\\nprediction with backward estimation.\\n10 SNR   10 O log (dB)  6n + = (dB) 4.77 20 1  +   log 10 log -  Figure P6.24 Prediction filter Transmitter Receiver Output Input Prediction filter Quantizer and coder Channel Decoder   mn + + + - mn  2 m n  n m n  = Figure P6.25  2 m n   Input mn Output Decoder Level estimator Encoder Level estimator Receiver Transmitter',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 336,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems 317 Delta Modulation 6.26 Consider a test signal m(t) defined by a hyperbolic tangent function:\\nwhere A and  are constants. Determine the minimum step size  for DM of this signal, which is\\nrequired to avoid slope-overload distortion. Consider a sine wave of frequency fm and amplitude Am, which is applied to a delta modulator of\\nstep size . Show that slope-overload distortion will occur if\\nwhere Ts is the sampling period. What is the maximum power that may be transmitted without\\nslope-overload distortion? A linear delta modulator is designed to operate on speech signals limited to 3.4 kHz. The\\nspecifications of the modulator are as follows:\\n\\nSampling rate = 10fNyquist, where fNyquist is the Nyquist rate of the speech signal.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 337,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Step size  = 100 mV.\\nThe modulator is tested with a 1kHz sinusoidal signal. Determine the maximum amplitude of this\\ntest signal required to avoid slope-overload distortion. In this problem, we derive an empirical formula for the average signal-to-(quantization) noise ratio of\\na DM system with a sinusoidal signal of amplitude A and frequency fm as the test signal. Assume that\\nthe power spectral density of the granular noise generated by the system is governed by the formula\\nwhere fs is the sampling rate and  is the step size. (Note that this formula is basically the same as that\\nfor the power spectral density of quantization noise in a PCM system with /2 for PCM being replaced\\nby  for DM.) The DM system is designed to handle analog message signals limited to bandwidth W.\\na. Show that the average quantization noise power produced by the system is\\nwhere it is assumed that the step size  has been chosen in accordance with the formula used in\\nProblem 6.28 so as to avoid slope-overload distortion.\\nb. Hence, determine the signal-to-(quantization) noise ratio of the DM system for a sinusoidal input. Consider a DM system designed to accommodate analog message signals limited to bandwidth\\nW = 5 kHz. A sinusoidal test signal of amplitude A = 1V and frequency fm = 1 kHz is applied to the\\nsystem. The sampling rate of the system is 50 kHz.\\na. Calculate the step size  required to minimize slope overload distortion.\\nb. Calculate the signal-to-(quantization) noise ratio of the system for the specified sinusoidal test\\nsignal.\\nFor these calculations, use the formula derived in Problem 6.29. Consider a low-pass signal with a bandwidth of 3 kHz. A linear DM system with step size  = 0.1V\\nis used to process this signal at a sampling rate 10 times the Nyquist rate.\\na. Evaluate the maximum amplitude of a test sinusoidal signal of frequency 1kHz, which can be\\nprocessed by the system without slope-overload distortion.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 337,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'a. Evaluate the maximum amplitude of a test sinusoidal signal of frequency 1kHz, which can be\\nprocessed by the system without slope-overload distortion.\\nm t A t   tanh = Am  2fmTs ------------------  SN f 2 6fs ------- = N 42A2fm 2 W 3fs 3 ---------------------------\\n=',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 337,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nb. For the specifications given in part a, evaluate the output SNR under (i) prefiltered and (ii)\\npostfiltered conditions. In the conventional form of DM, the quantizer input may be viewed as an approximate to the\\nderivative of the incoming message signal m(t). This behavior leads to a drawback of DM:\\ntransmission disturbances (e.g., noise) result in an accumulation error in the demodulated signal.\\nThis drawback can be overcome by integrating the message signal m(t) prior to DM, resulting in\\nthree beneficial effects:\\na. Low frequency content of m(t) is pre-emphasized.\\nb. Correlation between adjacent samples of m(t) is increased, tending to improve overall system\\nperformance by reducing the variance of the error signal at the quantizer input.\\nc. Design of the receiver is simplified.\\nSuch a DM scheme is called delta-sigma modulation.\\nConstruct a block diagram of the delta-sigma modulation system in such a way that it provides an\\ninterpretation of the system as a smoothed version of 1-bit PCM in the following composite sense:\\n\\nsmoothness implies that the comparator output is integrated prior to quantization, and',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 338,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'smoothness implies that the comparator output is integrated prior to quantization, and\\n\\n1-bit modulation merely restates that the quantizer consists of a hard limiter with only two\\nrepresentation levels.\\nExplain how the receiver of the delta-sigma modulation system is simplified, compared with\\nconventional DM. Line Codes 6.33 In this problem, we derive the formulas used to compute the power spectra of Figure 6.25 for the five\\nline codes described in Section 6.10. In the case of each line code, the bit duration is Tb and the pulse\\namplitude A is conditioned to normalize the average power of the line code to unity as indicated in Fig-\\nure 6.25. Assume that the data stream is randomly generated and symbols 0 and 1 are equally likely.\\nDerive the power spectral densities of these line codes as summarized here:\\na. Unipolar NRZ signals:\\nb. Polar NRZ signals:\\nc. Unipolar RZ signals:\\nd. Bipolar RZ signals:\\ne. Manchester-encoded signals:\\nHence, confirm the spectral plots displayed in Figure 6.25.\\nS f A2Tb 4 ------------ sinc2 fTb\\n 1 1 Tb -----f +     = S f A2Tb sinc2 fTb   = S f A2Tb 16 ------------ sinc2 fTb\\n2 -------    1 1 Tb -----  n  - =   f n Tb ----- -     + = S f A2Tb 4 ------------ sinc2 fTb\\n2 -------     fTb   2 sin = S f A2Tb 4 ------------ sinc2 fTb\\n2 -------     2 sin fTb 2 -----------     =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 338,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems 319 6.34 A randomly generated data stream consists of equiprobable binary symbols 0 and 1. It is encoded\\ninto a polar NRZ waveform with each binary symbol being defined as follows:\\na. Sketch the waveform so generated, assuming that the data stream is 00101110.\\nb. Derive an expression for the power spectral density of this signal and sketch it.\\nc. Compare the power spectral density of this random waveform with that defined in part b of\\nProblem 6.33. Given the data stream 1110010100, sketch the transmitted sequence of pulses for each of the\\nfollowing line codes:\\na. unipolar NRZ b. polar NRZ c. unipolar RZ d. bipolar RZ e. Manchester code. Computer Experiments **6.36 A sinusoidal signal of frequency is sampled at the rate of 8 kHz and then applied to\\na sample-and-hold circuit to produce a flat-topped PAM signal s(t) with pulse duration T = 500\\n.\\na. Compute the waveform of the PAM signal s(t).\\nb. Compute\\n, denoting the magnitude spectrum of the PAM signal s(t).\\nc. Compute the envelope of\\n. Hence confirm that the frequency at which this envelope goes\\nthrough zero for the first time is equal to (1T) = 20 kHz. **6.37\\nIn this problem, we use computer simulation to compare the performance of a companded PCM\\nsystem using the\\n-law against that of the corresponding system using a uniform quantizer. The\\nsimulation is to be performed for a sinusoidal input signal of varying amplitude.\\nWith a companded PCM system in mind, Table 6.4 describes the 15-segment pseudo-linear\\ncharacteristic that consists of 15 linear segments configured to approximate the logarithmic\\n-law s t t Tb -----    , cos Tb 2----- t Tb 2-----   - 0, otherwise      = f0 104 2Hz  = s Sf S f Table 6.4 The 15-segment companding characteristic ( = 255) Linear segment number Step-size Projections of segment end\\npoints onto the horizontal axis\\n0 2 31 1a, 1b 4 95 2a, 2b 8 223 3a, 3b 16 479 4a, 4b 32 991 5a, 5b 64 2015 6a, 6b 128 4063 7a, 7b 256 8159',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 339,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nof (6.48), with = 255. This approximation is constructed in such a way that the segment endpoints\\nin Table 6.4 lie on the compression curve computed from (6.48).\\na. Using the\\n-law described in Table 6.4, plot the output signal-to-noise ratio as a function of the\\ninput signal-to-noise ratio, both ratios being expressed in decibels.\\nb. Compare the results of your computation in part (a) with a uniform quantizer having\\nrepresentation levels. **6.38\\nIn this experiment we study the linear adaptive prediction of a signal xn governed by the following\\nrecursion:\\nwhere vn is drawn from a discrete-time white noise process of zero mean and unit variance. (A\\nprocess generated in this manner is referred to as an autoregressive process of order two.)\\nSpecifically, the adaptive prediction is performed using the normalized LMS algorithm defined by\\nwhere p is the prediction order and is the normalized step-size parameter. The important point to\\nnote here is that is dimensionless and stability of the algorithm is assured by choosing it in\\naccordance with the formula\\nThe algorithm is initiated by setting\\nThe learning curve of the algorithm is defined as a plot of the mean-square error versus the number\\nof iterations n for specified parameter values, which is obtained by averaging the plot of versus n\\nover a large number of different realizations of the algorithm.\\na. Plot the learning curves for the adaptive prediction of xn for a fixed prediction order p = 5 and\\nthree different values of step-size parameter: = 0.0075, 0.05, and 0.5.\\nb. What observations can you make from the learning curves of part a? **6.39\\nIn this problem, we study adaptive delta modulation, the underlying principle of which is two-fold:\\nIf successive errors are of opposite polarity, then the delta modulator is operating in the granular\\nmode, in which case the step size is reduced.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 340,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'If successive errors are of opposite polarity, then the delta modulator is operating in the granular\\nmode, in which case the step size is reduced.\\nIf, on the other hand, the successive errors are of the same polarity, then the delta modulator is\\noperating in the slope-overload mode, in which case the step size is increased.\\nParts a and b of Figure P6.39 depict the block diagrams of the transmitter and receiver of the\\nadaptive delta modulator, respectively, in which the step size, is increased or decreased by a factor\\nof 50% at each iteration of the adaptive process, as shown by:\\n  xn 0.8xn 1 - 0.1xn 2 - - 0.1vn + = xn wk n xn k - k 1 = p  = en xn xn - = wk n 1 +  wk n   xn k - 2 k 1 = p           xn k - en k + 1 2 p   = =   0  2   wk 0  0 for all k = en 2  n n 1 - mq n  ------------- mq n  0.5mq n  1 - +   if n 1 - min  min if n 1 - min       =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 340,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems where n is the step size at iteration (time step) n of the adaptation algorithm, and mq,n is the 1-bit\\nquantizer output that equals\\n.\\nSpecifications: The input signal applied to the transmitter is sinusoidal as shown by\\nwhere A = 10 and fm = fs 100 where fs is the sampling frequency; the step size\\n;\\n.\\na. Using the above-described adaptation algorithm, use a computer to plot the resulting waveform\\nfor one complete cycle of the sinusoidal modulating signal, and also display the coded modulator\\noutput in the transmitter.\\nb. For the same specifications, repeat the computation using linear modulation.\\nc. Comment on the results obtained in parts a and b of the problem.\\n1  mt A 2 f sin = mt n 1 for all n = min 1 8  = Figure P6.39 Sampled channel output - + + + Sampled message signal n n n   (a) Reconstructed message signal  (b) One-bit quantizer Adaptive algorithm z - 1 Adaptive algorithm z - 1 z - 1 z - 1 mq,n - 1 mn',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 341,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nNotes\\nFor an exhaustive study of quantization noise in signal processing and communications, see\\nWidrow and Kollar (2008).\\nThe two necessary conditions of (3.42) and (3.47) for optimality of a scalar quantizer were\\nreported independently by Lloyd (1957) and Max (1960), hence the name Lloyd-Max quantizer.\\nThe derivation of these two optimality conditions presented in this chapter follows the book by\\nGersho and Gray (1992).\\nThe -law is used in the USA, Canada, and Japan. On the other hand, in Europe, the A-law is\\nused for signal compression.\\nIn actual PCM systems, the companding circuitry does not produce an exact replica of the\\nnonlinear compression curves shown in Figure 6.14. Rather, it provides a piecewise linear\\napproximation to the desired curve. By using a large enough number of linear segments, the\\napproximation can approach the true compression curve very closely; for detailed discussion of this\\nissue, see Bellamy (1991).\\nFor a discussion of noise in analog modulation systems with particular reference to FM, see\\nChapter 4 of Communication Systems (Haykin, 2001).\\nTo simplify notational matters, RM is used to denote the autocorrelation matrix in (6.70) rather\\nthan RMM as in Chapter 4 on Stochastic Processes. To see the rationale for this simplification, the\\nreader is referred to (6.79) for simplicity. For the same reason, henceforth the practice adopted in this\\nchapter will be continued for the rest of the book, dealing with autocorrelation matrices and power\\nspectral density.\\nAn optimum predictor that follows (6.77) is said to be a special case of the Wiener filter.\\nFor a detailed discussion of adaptive DPCM involving the use of adaptive quantization with\\nforward estimation as well as backward estimation, the reader is referred to the classic book (Jayant\\nand Noll, 1984).',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 342,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '323 CHAPTER 7 Signaling over AWGN Channels Chapter 6 on the conversion of analog waveforms into coded pulses represents the\\ntransition from analog communications to digital communications. This transition has\\nbeen empowered by several factors:\\nEver-increasing advancement of digital silicon chips, digital signal processing, and\\ncomputers, which, in turn, has prompted further enhancement in digital silicon\\nchips, thereby repeating the cycle of improvement.\\nImproved reliability, which is afforded by digital communications to a much greater\\nextent than is possible with analog communications.\\nBroadened range of multiplexing of users, which is enabled by the use of digital\\nmodulation techniques.\\nCommunication networks, for which, in one form or another, the use of digital\\ncommunications is the preferred choice.\\nIn light of these compelling factors, we may justifiably say that we live in a digital\\ncommunications world. For an illustrative example, consider the remote connection of\\ntwo digital computers, with one computer acting as the information source by calculating\\ndigital outputs based on observations and inputs fed into it; the other computer acts as the\\nrecipient of the information. The source output consists of a sequence of 1s and 0s, with\\neach binary symbol being emitted every Tb seconds. The transmitting part of the digital\\ncommunication system takes the 1s and 0s emitted by the source computer and encodes\\nthem into distinct signals denoted by s1(t) and s2(t), respectively, which are suitable for\\ntransmission over the analog channel. Both s1(t) and s2(t) are real-valued energy signals,\\nas shown by\\n(7.1)\\nWith the analog channel represented by an AWGN model, depicted in Figure 7.1, the\\nreceived signal is defined by\\n(7.2)\\nwhere w(t) is the channel noise. The receiver has the task of observing the received signal\\nx(t) for a duration of Tb seconds and then making an estimate of the transmitted signal\\nEi si 2 t dt i 1 2  =  0 Tb = x t si t w t, + = 0 t Tb  i 1 2  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 343,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nsi(t), or equivalently the ith symbol, i = 1, 2. However, owing to the presence of channel\\nnoise, the receiver will inevitably make occasional errors. The requirement, therefore, is to\\ndesign the receiver so as to minimize the average probability of symbol error, defined as\\n(7.3)\\nwhere 1 and 2 are the prior probabilities of transmitting symbols 1 and 0, respectively,\\nand is the estimate of the symbol 1 or 0 sent by the source, which is computed by the\\nreceiver. The and are conditional probabilities.\\nIn minimizing the average probability of symbol error between the receiver output and\\nthe symbol emitted by the source, the motivation is to make the digital communication\\nsystem as reliable as possible. To achieve this important design objective in a generic\\nsetting that involves an M-ary alphabet whose symbols are denoted by m1, m2, , mM, we\\nhave to understand two basic issues:\\nHow to optimize the design of the receiver so as to minimize the average probability\\nof symbol error.\\nHow to choose the set of signals s1(t), s2(t), , sM(t) for representing the symbols\\nm1, m2, , mM, respectively, since this choice affects the average probability of\\nsymbol error.\\nThe key question is how to develop this understanding in a principled as well as insightful\\nmanner. The answer to this fundamental question is found in the geometric representation\\nof signals. Geometric Representation of Signals\\nThe essence of geometric representation of signals1 is to represent any set of M energy\\nsignals {si(t)} as linear combinations of N orthonormal basis functions, where N  M.\\nThat is to say, given a set of real-valued energy signals, s1(t), s2(t), , sM(t), each of\\nduration T seconds, we write\\n(7.4)\\nwhere the coefficients of the expansion are defined by\\n(7.5)\\nFigure 7.1 AWGN model of a channel.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 344,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'duration T seconds, we write\\n(7.4)\\nwhere the coefficients of the expansion are defined by\\n(7.5)\\nFigure 7.1 AWGN model of a channel.\\nTransmitted signal si(t) Received signal x(t) + + White Gaussian noise w(t)  Pe 1\\x02(m 0 1 sent) = = + 2\\x02(m 1 0 sent) = m \\x02(m 0 1 sent) = \\x02(m 1 0 sent) = si t sijj t j 1 = N  = 0 t T  i 1 2 M   =    sij si tj t dt i 1 2 M   = j 1 2 N   =    0 T  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 344,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '2 Geometric Representaton of Signals The real-valued basis functions 1(t), 2(t), , N(t) form an orthonormal set, by which\\nwe mean\\n(7.6)\\nwhere ij is the Kronecker delta. The first condition of (7.6) states that each basis function\\nis normalized to have unit energy. The second condition states that the basis functions\\n1(t), 2(t), , N(t) are orthogonal with respect to each other over the interval 0  t  T.\\nFor prescribed i, the set of coefficients may be viewed as an N-dimensional\\nsignal vector, denoted by si. The important point to note here is that the vector si bears a\\none-to-one relationship with the transmitted signal si(t):\\n\\nGiven the N elements of the vector si operating as input, we may use the scheme\\nshown in Figure 7.2a to generate the signal si(t), which follows directly from (7.4).\\nThis figure consists of a bank of N multipliers with each multiplier having its own\\nbasis function followed by a summer. The scheme of Figure 7.2a may be viewed as\\na synthesizer.\\n\\nConversely, given the signals si(t), i = 1, 2, , M, operating as input, we may use\\nthe scheme shown in Figure 7.2b to calculate the coefficients si1, si2, , siN which\\nfollows directly from (7.5). This second scheme consists of a bank of N product-\\nintegrators or correlators with a common input, and with each one of them supplied\\nwith its own basis function. The scheme of Figure 7.2b may be viewed as an\\nanalyzer.\\nFigure 7.2 (a) Synthesizer for generating the signal si(t). (b) Analyzer for reconstructing the signal\\nvector {si}. i tj t dt 0 T  ij 1 if i j = 0 if i j     = = sij  j 1 = N si1 1(t) si2 si(t) (a) (b) dt siN N(t)  si(t) si1 si2 siN dt dt        0 T \\x02 0 T \\x02 0 T \\x02  1(t)  2(t)  2(t)   N(t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 345,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nAccordingly, we may state that each signal in the set {si(t)} is completely determined by\\nthe signal vector\\n(7.7)\\nFurthermore, if we conceptually extend our conventional notion of two- and three-\\ndimensional Euclidean spaces to an N-dimensional Euclidean space, we may visualize the\\nset of signal vectors {si|i = 1, 2, , M} as defining a corresponding set of M points in an\\nN-dimensional Euclidean space, with N mutually perpendicular axes labeled 1, 2, ,\\nN. This N-dimensional Euclidean space is called the signal space.\\nThe idea of visualizing a set of energy signals geometrically, as just described, is of\\nprofound theoretical and practical importance. It provides the mathematical basis for the\\ngeometric representation of energy signals in a conceptually satisfying manner. This form\\nof representation is illustrated in Figure 7.3 for the case of a two-dimensional signal space\\nwith three signals; that is, N = 2 and M = 3.\\nIn an N-dimensional Euclidean space, we may define lengths of vectors and angles\\nbetween vectors. It is customary to denote the length (also called the absolute value or\\nnorm) of a signal vector si by the symbol\\n. The squared length of any signal vector si is\\ndefined to be the inner product or dot product of si with itself, as shown by\\nFigure 7.3 Illustrating the geometric representation of signals for\\nthe case when N = 2 and M = 3.\\n3 2 1 -1 -2 -3 0 -3 -2 -1 1 2 3 s1 s3 s2 2  1  si si1 si2  siN  = i 1 2 M   = si',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 346,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '2 Geometric Representaton of Signals (7.8)\\nwhere sij is the jth element of si and the superscript T denotes matrix transposition.\\nThere is an interesting relationship between the energy content of a signal and its\\nrepresentation as a vector. By definition, the energy of a signal si(t) of duration T seconds is\\n(7.9)\\nTherefore, substituting (7.4) into (7.9), we get\\nInterchanging the order of summation and integration, which we can do because they are\\nboth linear operations, and then rearranging terms we get\\n(7.10)\\nSince, by definition, the j(t) form an orthonormal set in accordance with the two\\nconditions of (7.6), we find that (7.10) reduces simply to (7.11)\\nThus, (7.8) and (7.11) show that the energy of an energy signal si(t) is equal to the squared\\nlength of the corresponding signal vector si(t).\\nIn the case of a pair of signals si(t) and sk(t) represented by the signal vectors si and sk,\\nrespectively, we may also show that\\n(7.12)\\nEquation (7.12) states:\\nThe inner product of the energy signals si(t) and sk(t) over the interval [0,T] is\\nequal to the inner product of their respective vector representations si and sk.\\nNote that the inner product is invariant to the choice of basis functions\\n,\\nin that it only depends on the components of the signals si(t) and sk(t) projected onto each\\nof the basis functions.\\nsi 2 si Tsi = sij 2 j 1 = N  i 1 2 M   =  = Ei si 2 t dt i 1 2 M   =  0 T  = Ei sijj t j 1 = N  sikk t k 1 = N  dt 0 T  = Ei sijsik j tk t dt 0 T  k 1 = N  j 1 = N  = Ei sij 2 j 1 = N  = si 2 = si tsk t dt 0 T  si Tsk = si Tsk j t  j 1 = N',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 347,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nYet another useful relation involving the vector representations of the energy signals\\nsi(t) and sk(t) is described by\\n(7.13)\\nwhere is the Euclidean distance dik between the points represented by the signal\\nvectors si and sk.\\nTo complete the geometric representation of energy signals, we need to have a\\nrepresentation for the angle ik subtended between two signal vectors si and sk. By\\ndefinition, the cosine of the angle ik is equal to the inner product of these two vectors\\ndivided by the product of their individual norms, as shown by\\n(7.14)\\nThe two vectors si and sk are thus orthogonal or perpendicular to each other if their inner\\nproduct is zero, in which case ik = 90; this condition is intuitively satisfying.\\nEXAMPLE\\nThe Schwarz Inequality\\nConsider any pair of energy signals s1(t) and s2(t). The Schwarz inequality states\\n(7.15)\\nThe equality holds if, and only if, s2(t) = cs1(t), where c is any constant.\\nTo prove this important inequality, let s1(t) and s2(t) be expressed in terms of the pair of\\northonormal basis functions 1(t) and 2(t) as follows:\\nwhere 1(t) and 2(t) satisfy the orthonormality conditions over the time interval\\n:\\nOn this basis, we may represent the signals s1(t) and s2(t) by the following respective pair\\nof vectors, as illustrated in Figure 7.4:\\nsi sk - 2 sij skj -  2 j 1 = N  = si t sk t -  2 dt 0 T  = si sk - ik   cos si Tsk si sk ------------------ = si Tsk s1 ts2 t dt  -      2 s1 2 t dt  -       s2 2 t dt  -        s1 t s111 t s122 t + = s2 t s211 t s222 t + = , -   i tj t dt  -   ij 1 for j i = 0 otherwise    = = s1 s11 s12 = s2 s21 s22 =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 348,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '2 Geometric Representaton of Signals From Figure 7.4 we readily see that the cosine of angle  subtended between the vectors s1\\nand s2 is\\n(7.16)\\nwhere we have made use of (7.14) and (7.12). Recognizing that |cos|  1, the Schwarz\\ninequality of (7.15) immediately follows from (7.16). Moreover, from the first line of\\n(7.16) we note that |cos| = 1 if, and only if, s2 = cs1; that is, s2(t) = cs1(t), where c is an\\narbitrary constant.\\nProof of the Schwarz inequality, as presented here, applies to real-valued signals. It may\\nbe readily extended to complex-valued signals, in which case (7.15) is reformulated as\\n(7.17)\\nwhere the asterisk denotes complex conjugation and the equality holds if, and only if,\\ns2(t) = cs1(t), where c is a constant.\\nGram-Schmidt Orthogonalization Procedure\\nHaving demonstrated the elegance of the geometric representation of energy signals with\\nan example, how do we justify it in mathematical terms? The answer to this question lies\\nin the Gram-Schmidt orthogonalization procedure, for which we need a complete\\northonormal set of basis functions. To proceed with the formulation of this procedure,\\nsuppose we have a set of M energy signals denoted by s1(t), s2(t), , sM(t). Starting with\\ns1(t) chosen from this set arbitrarily, the first basis function is defined by\\n(7.18)\\nwhere E1 is the energy of the signal s1(t).\\nFigure 7.4 Vector representations of signals s1(t) and s2(t), providing\\nthe background picture for proving the Schwarz inequality.\\ns12 s22 0 s21 s11 s1 s2 2  1    cos s1 Ts2 s1 s2 ------------------- = s1 ts2 t dt  -   s1 2 t dt  -      1 2  s2 2 t dt  -      1 2  ------------------------------------------------------------------------------------\\n= s1 ts2 * t dt  -   s1 t2 dt  -      1 2  s2 t2 dt  -      1 2   1 t s1 t E1 ----------- =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 349,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nThen, clearly, we have\\nwhere the coefficient and 1(t) has unit energy as required.\\nNext, using the signal s2(t), we define the coefficient s21 as\\nWe may thus introduce a new intermediate function\\n(7.19)\\nwhich is orthogonal to 1(t) over the interval 0  t  T by virtue of the definition of s21 and\\nthe fact that the basis function 1(t) has unit energy. Now, we are ready to define the\\nsecond basis function as\\n(7.20)\\nSubstituting (7.19) into (7.20) and simplifying, we get the desired result\\n(7.21)\\nwhere E2 is the energy of the signal s2(t). From (7.20) we readily see that\\nin which case (7.21) yields\\nThat is to say, 1(t) and 2(t) form an orthonormal pair as required.\\nContinuing the procedure in this fashion, we may, in general, define\\n(7.22)\\nwhere the coefficients sij are themselves defined by\\nFor i = 1, the function gi(t) reduces to si(t).\\nGiven the gi(t), we may now define the set of basis functions\\n(7.23) s1 t E11 t = s11 t1 t = s11 E1 = s21 s2 t1 t dt 0 T  = g2 t s2 t s211 t - = 2 t g2 t g2 2 t dt 0 T  -----------------------------\\n= 2 t s2 t s211 t - E2 s21 2 - --------------------------------------\\n= 2 2 t dt 0 T  1 = 1 t2 t dt 0 T  0 = gi t si t sijj t j 1 = i 1 -  - = sij si tj t dt, j 1 2 i 1 -   = 0 T  = i t gi t gi 2 t dt 0 T  -----------------------------\\nj 1 2 N   =  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 350,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '2 Geometric Representaton of Signals which form an orthonormal set. The dimension N is less than or equal to the number of\\ngiven signals, M, depending on one of two possibilities:\\n\\nThe signals s1(t), s2(t), , sM(t) form a linearly independent set, in which case\\nN = M.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 351,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The signals s1(t), s2(t), , sM(t) are not linearly independent, in which case N  M\\nand the intermediate function gi(t) is zero for i > N.\\nNote that the conventional Fourier series expansion of a periodic signal, discussed in\\nChapter 2, may be viewed as a special case of the Gram-Schmidt orthogonalization\\nprocedure. Moreover, the representation of a band-limited signal in terms of its samples\\ntaken at the Nyquist rate, discussed in Chapter 6, may be viewed as another special case.\\nHowever, in saying what we have here, two important distinctions should be made:\\nThe form of the basis functions 1(t), 2(t), , N(t) has not been specified. That is\\nto say, unlike the Fourier series expansion of a periodic signal or the sampled\\nrepresentation of a band-limited signal, we have not restricted the Gram-Schmidt\\northogonalization procedure to be in terms of sinusoidal functions (as in the Fourier\\nseries) or sinc functions of time (as in the sampling process).\\nThe expansion of the signal si(t) in terms of a finite number of terms is not an\\napproximation wherein only the first N terms are significant; rather, it is an exact\\nexpression, where N and only N terms are significant.\\nEXAMPLE\\n2B1Q Code\\nThe 2B1Q code is the North American line code for a special class of modems called\\ndigital subscriber lines. This code represents a quaternary PAM signal as shown in the\\nGray-encoded alphabet of Table 7.1. The four possible signals s1(t), s2(t), s3(t), and s4(t)\\nare amplitude-scaled versions of a Nyquist pulse. Each signal represents a dibit (i.e., pair\\nof bits). The issue of interest is to find the vector representation of the 2B1Q code.\\nThis example is simple enough for us to solve it by inspection. Let 1(t) denote a pulse\\nnormalized to have unit energy. The 1(t) so defined is the only basis function for the\\nvector representation of the 2B1Q code. Accordingly, the signal-space representation of\\nthis code is as shown in Figure 7.5. It consists of four signal vectors s1, s2, s3, and s4,',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 351,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'vector representation of the 2B1Q code. Accordingly, the signal-space representation of\\nthis code is as shown in Figure 7.5. It consists of four signal vectors s1, s2, s3, and s4,\\nwhich are located on the 1-axis in a symmetric manner about the origin. In this example,\\nwe have M = 4 and N = 1.\\nTable 7.1 Amplitude levels of the 2B1Q code\\nSignal Amplitude Gray code s1(t) -3 00 s2(t) -1 01 s3(t) +1 11 s4(t) +3',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 351,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nWe may generalize the result depicted in Figure 7.5 for the 2B1Q code as follows: the\\nsignal-space diagram of an M-ary PAM signal, in general, is one-dimensional with M\\nsignal points uniformly positioned on the only axis of the diagram. Conversion of the Continuous AWGN Channel into a\\nVector Channel\\nSuppose that the input to the bank of N product integrators or correlators in Figure 7.2b is\\nnot the transmitted signal si(t) but rather the received signal x(t) defined in accordance\\nwith the AWGN channel of Figure 7.1. That is to say,\\n(7.24)\\nwhere w(t) is a sample function of the white Gaussian noise process W(t) of zero mean and\\npower spectral density N02. Correspondingly, we find that the output of correlator j, say,\\nis the sample value of a random variable Xj, whose sample value is defined by\\n(7.25)\\nThe first component, sij, is the deterministic component of xj due to the transmitted signal\\nsi(t), as shown by\\n(7.26)\\nThe second component, wj, is the sample value of a random variable Wj due to the channel\\nnoise w(t), as shown by\\n(7.27)\\nConsider next a new stochastic process whose sample function is related to\\nthe received signal x(t) as follows:\\n(7.28)\\nFigure 7.5 Signal-space representation of the 2B1Q code.\\n2 s1 s2 s3 s4 1 0 2 1 1  x t si t w t 0 t T  i 1 2 M   =     + = xj x tj t dt 0 T  = sij wj j 1 2 N   =  + = sij si tj t dt 0 T  = wj w tj t dt 0 T  = Xt xt xt x t xjj t j 1 = N  - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 352,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '3 Conversion of the Continuous AWGN Channel into a Vector Channel Substituting (7.24) and (7.25) into (7.28), and then using the expansion of (7.4), we get\\n(7.29)\\nThe sample function\\n, therefore, depends solely on the channel noise w(t). On the\\nbasis of (7.28) and (7.29), we may thus express the received signal as (7.30)\\nAccordingly, we may view as a remainder term that must be included on the right-\\nhand side of (7.30) to preserve equality. It is informative to contrast the expansion of the\\nreceived signal x(t) given in (7.30) with the corresponding expansion of the transmitted\\nsignal si(t) given in (7.4): the expansion of (7.4), pertaining to the transmitter, is entirely\\ndeterministic; on the other hand, the expansion of (7.30) is random (stochastic) due to the\\nchannel noise at the receiver input.\\nStatistical Characterization of the Correlator Outputs\\nWe now wish to develop a statistical characterization of the set of N correlator outputs. Let\\nX(t) denote the stochastic process, a sample function of which is represented by the\\nreceived signal x(t). Correspondingly, let Xj denote the random variable whose sample\\nvalue is represented by the correlator output xj, j = 1, 2, , N. According to the AWGN\\nmodel of Figure 7.1, the stochastic process X(t) is a Gaussian process. It follows,\\ntherefore, that Xj is a Gaussian random variable for all j in accordance with Property 1 of a\\nGaussian process (Chapter 4). Hence, Xj is characterized completely by its mean and\\nvariance, which are determined next.\\nLet Wj denote the random variable represented by the sample value wj produced by the\\njth correlator in response to the white Gaussian noise component w(t). The random\\nvariable Wj has zero mean because the channel noise process W(t) represented by w(t) in\\nthe AWGN model of Figure 7.1 has zero mean by definition. Consequently, the mean of Xj\\ndepends only on sij, as shown by',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 353,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the AWGN model of Figure 7.1 has zero mean by definition. Consequently, the mean of Xj\\ndepends only on sij, as shown by\\n(7.31) xt si t w t sij wj +  j t j 1 = N  - + = w t wjj t j 1 = N  - = wt = xt x t xjj t xt + j 1 = N  = xjj t wt + j 1 = N  = wt Xj \\x03 Xj   = \\x03 sij Wj +   = sij \\x03 Wj   + = sij =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 353,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nTo find the variance of Xj, we start with the definition\\n(7.32)\\nwhere the last line follows from (7.25) with xj and wj replaced by Xj and Wj, respectively.\\nAccording to (7.27), the random variable Wj is defined by\\nWe may therefore expand (7.32) as\\n(7.33)\\nInterchanging the order of integration and expectation, which we can do because they are\\nboth linear operations, we obtain\\n(7.34)\\nwhere RW(t,u) is the autocorrelation function of the noise process W(t). Since this noise is\\nstationary, RW(t,u) depends only on the time difference t - u. Furthermore, since W(t) is\\nwhite with a constant power spectral density N02, we may express RW(t,u) as\\n(7.35)\\nTherefore, substituting (7.35) into (7.34) and then using the sifting property of the delta\\nfunction (t), we get\\nSince the j(t) have unit energy, by definition, the expression for noise variance\\nreduces to\\n(7.36)\\nThis important result shows that all the correlator outputs, denoted by Xj with j = 1, 2, ,\\nN, have a variance equal to the power spectral density N02 of the noise process W(t).\\nXj 2 var Xj   = \\x03 Xj sij -  2   = \\x03 Wj 2   = Wj W tj t dt 0 T  = Xj 2 \\x03 W tj t dt W u j u  du 0 T  0 T  = \\x03 j tj u W tW u  dt du 0 T  0 T  = Xj 2 j tj u \\x03 W tW u    dt du 0 T  0 T  = j tj u RW t u    dt du 0 T  0 T  = RW t u    N0 2 ------    t u -   = Xj 2 N0 2 ------ j tj u t u -   dt du 0 T  0 T  = N0 2 ------ j 2 t dt 0 T  = x 2 ,j Xj 2 N0 2 ------ for all j  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 354,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '3 Conversion of the Continuous AWGN Channel into a Vector Channel Moreover, since the basic functions j(t) form an orthonormal set, Xj and Xk are\\nmutually uncorrelated, as shown by\\n(7.37)\\nSince the Xj are Gaussian random variables, (7.37) implies that they are also statistically\\nindependent in accordance with Property 4 of a Gaussian process (Chapter 4).\\nDefine the vector of N random variables\\n(7.38)\\nwhose elements are independent Gaussian random variables with mean values equal to sij\\nand variances equal to N02. Since the elements of the vector X are statistically\\nindependent, we may express the conditional probability density function of the vector X,\\ngiven that the signal si(t) or the corresponding symbol mi was sent, as the product of the\\nconditional probability density functions of its individual elements; that is,\\n(7.39)\\nwhere the vector x and scalar xj are sample values of the random vector X and random\\nvariable Xj, respectively. The vector x is called the observation vector; correspondingly, xj\\nis called an element of the observation vector. A channel that satisfies (7.39) is said to be a\\nmemoryless channel.\\nSince each Xj is a Gaussian random variable with mean sij and variance N02, we have\\n(7.40) cov XjXk   \\x03 Xj Xj -  Xk Xk -     = \\x03 Xj sij -  Xk sik -     = \\x03 WjWk   = \\x03 W tj t dt W u k u  du 0 T  0 T  = j tk u RW t u    dt du 0 T  0 T  = N0 2 ------ j tk u t u -   dt du 0 T  0 T  = N0 2 ------ j tk u  dt 0 T  = 0 j k   = X X1 X2  XN = fX x mi   fXj j 1 = N  xj mi   i 1 2 M   =  = fXj xj mi   1 N0 -------------- 1 N0 ------ xj sij -  2 -  j 1 2 N   = i 1 2 M   =    exp =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 355,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nTherefore, substituting (7.40) into (7.39) yields\\n(7.41)\\nwhich completely characterizes the first term of (7.30).\\nHowever, there remains the noise term in (7.30) to be accounted for. Since the\\nnoise process W(t) represented by w(t) is Gaussian with zero mean, it follows that the noise\\nprocess represented by the sample function is also a zero-mean Gaussian\\nprocess. Finally, we note that any random variable\\n, say, derived from the noise\\nprocess by sampling it at time tk, is in fact statistically independent of the random\\nvariable Xj; that is to say:\\n(7.42)\\nSince any random variable based on the remainder noise process is independent of\\nthe set of random variables {Xj} as well as the set of transmitted signals {si(t)}, (7.42)\\nstates that the random variable is irrelevant to the decision as to which particular\\nsignal was actually transmitted. In other words, the correlator outputs determined by the\\nreceived signal x(t) are the only data that are useful for the decision-making process;\\ntherefore, they represent sufficient statistics for the problem at hand. By definition,\\nsufficient statistics summarize the whole of the relevant information supplied by an\\nobservation vector.\\nWe may now summarize the results presented in this section by formulating the\\ntheorem of irrelevance:\\nInsofar as signal detection in AWGN is concerned, only the projections of the\\nnoise onto the basis functions of the signal set affect the sufficient\\nstatistics of the detection problem; the remainder of the noise is irrelevant.\\nPutting this theorem into a mathematical context, we may say that the AWGN channel\\nmodel of Figure 7.1a is equivalent to an N-dimensional vector channel described by the\\nequation x = si + w, i = 1, 2, , M (7.43) where the dimension N is the number of basis functions involved in formulating the signal\\nvector si for all i. The individual components of the signal vector si and the additive Gaussian',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 356,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'vector si for all i. The individual components of the signal vector si and the additive Gaussian\\nnoise vector w are defined by (7.5) and (7.27), respectively. The theorem of irrelevance and\\nits mathematical description given in (7.43) are indeed basic to the understanding of the\\nsignal-detection problem as described next. Just as importantly, (7.43) may be viewed as the\\nbaseband version of the time-dependent received signal of (7.24).\\nLikelihood Function\\nThe conditional probability density functions fX(x|mi),\\ni = 1, 2, , M, provide the very\\ncharacterization of an AWGN channel. Their derivation leads to a functional dependence\\non the observation vector x given the transmitted message symbol mi. However, at the\\nfX x mi   N0  N 2  - 1 N0 ------ xj sij -  2 j 1 = N  -  i 1 2 M   = exp = wt Wt wt Wtk   Wt \\x03 XjWtk     0 = j 1 2 N   = 0 tk T      Wt Wtk   si t  i 1 = M',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 356,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Optimum Receivers Using Coherent Detection receiver we have the exact opposite situation: we are given the observation vector x and\\nthe requirement is to estimate the message symbol mi that is responsible for generating x.\\nTo emphasize this latter viewpoint, we follow Chapter 3 by introducing the idea of a\\nlikelihood function, denoted by l(mi) and defined by\\n(7.44)\\nHowever, tt is important to recall from Chapter 3 that although l(mi) and fX(x|mi) have\\nexactly the same mathematical form, their individual meanings are quite different.\\nIn practice, we find it more convenient to work with the log-likelihood function,\\ndenoted by L(mi) and defined by\\n(7.45)\\nwhere ln denotes the natural logarithm. The log-likelihood function bears a one-to-one\\nrelationship to the likelihood function for two reasons:\\nBy definition, a probability density function is always nonnegative. It follows,\\ntherefore, that the likelihood function is likewise a nonnegative quantity.\\nThe logarithmic function is a monotonically increasing function of its argument.\\nThe use of (7.41) in (7.45) yields the log-likelihood function for an AWGN channel as (7.46)\\nwhere we have ignored the constant term -(N2)ln(N0) since it bears no relation\\nwhatsoever to the message symbol mi. Recall that the sij, j = 1, 2, , N, are the elements\\nof the signal vector si representing the message symbol mi. With (7.46) at our disposal, we\\nare now ready to address the basic receiver design problem. Optimum Receivers Using Coherent Detection\\nMaximum Likelihood Decoding\\nSuppose that, in each time slot of duration T seconds, one of the M possible signals s1(t),\\ns2(t), , sM(t) is transmitted with equal probability, 1M. For geometric signal representa-\\ntion, the signal si(t), i = 1, 2, , M, is applied to a bank of correlators with a common input\\nand supplied with an appropriate set of N orthonormal basis functions, as depicted in Figure\\n2b. The resulting correlator outputs define the signal vector si. Since knowledge of the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 357,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'and supplied with an appropriate set of N orthonormal basis functions, as depicted in Figure\\n2b. The resulting correlator outputs define the signal vector si. Since knowledge of the\\nsignal vector si is as good as knowing the transmitted signal si(t) itself, and vice versa, we\\nmay represent si(t) by a point in a Euclidean space of dimension N  M. We refer to this\\npoint as the transmitted signal point, or message point for short. The set of message points\\ncorresponding to the set of transmitted signals is called a message constellation.\\nHowever, representation of the received signal x(t) is complicated by the presence of\\nadditive noise w(t). We note that when the received signal x(t) is applied to the bank of N\\ncorrelators, the correlator outputs define the observation vector x. According to (7.43), the\\nvector x differs from the signal vector si by the noise vector w, whose orientation is\\ncompletely random, as it should be.\\nl mi   fX x mi   i  1 2 M   = = L mi   l ln mi   i  1 2 M   = = L mi   1 N0 ------ xj sij -  2 i 1 2 M   =  j 1 = N  - = si t  i 1 = M',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 357,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nThe noise vector w is completely characterized by the channel noise w(t); the converse\\nof this statement, however, is not true, as explained previously. The noise vector w\\nrepresents that portion of the noise w(t) that will interfere with the detection process; the\\nremaining portion of this noise, denoted by\\n, is tuned out by the bank of correlators\\nand, therefore, irrelevant.\\nBased on the observation vector x, we may represent the received signal x(t) by a point\\nin the same Euclidean space used to represent the transmitted signal. We refer to this\\nsecond point as the received signal point. Owing to the presence of noise, the received\\nsignal point wanders about the message point in a completely random fashion, in the sense\\nthat it may lie anywhere inside a Gaussian-distributed cloud centered on the message\\npoint. This is illustrated in Figure 7.6a for the case of a three-dimensional signal space.\\nFor a particular realization of the noise vector w (i.e., a particular point inside the random\\ncloud of Figure 7.6a) the relationship between the observation vector x and the signal\\nvector si is as illustrated in Figure 7.6b.\\nWe are now ready to state the signal-detection problem:\\nGiven the observation vector x, perform a mapping from x to an estimate of\\nthe transmitted symbol, mi, in a way that would minimize the probability of\\nerror in the decision-making process.\\nGiven the observation vector x, suppose that we make the decision\\n. The\\nprobability of error in this decision, which we denote by Pe(mi|x), is simply\\n(7.47)\\nThe requirement is to minimize the average probability of error in mapping each given\\nobservation vector x into a decision. On the basis of (7.47), we may, therefore, state the\\noptimum decision rule:\\nSet if for all and k = 1, 2, , M. (7.48) Figure 7.6 Illustrating the effect of (a) noise perturbation on (b) the location of the received',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 358,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'optimum decision rule:\\nSet if for all and k = 1, 2, , M. (7.48) Figure 7.6 Illustrating the effect of (a) noise perturbation on (b) the location of the received\\nsignal point. wt m m mi = Pe mi x   1 \\x02 mi sent x   - = m mi = \\x02 mi sent x   \\x02 mk sent x    k i  Noise cloud 0 (a) 0 Observation vector x Signal vector si Noise vector w Message point Received signal point (b) 1  1  2  2  3  3',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 358,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Optimum Receivers Using Coherent Detection The decision rule described in (7.48) is referred to as the maximum a posteriori probability\\n(MAP) rule. Correspondingly, the system used to implement this rule is called a maximum\\na posteriori decoder.\\nThe requirement of (7.48) may be expressed more explicitly in terms of the prior\\nprobabilities of the transmitted signals and the likelihood functions, using Bayes rule\\ndiscussed in Chapter 3. For the moment, ignoring possible ties in the decision-making\\nprocess, we may restate the MAP rule as follows:\\nSet if (7.49) where k is the prior probability of transmitting symbol mk, fX(x|mi) is the\\nconditional probability density function of the random observation vector X\\ngiven the transmission of symbol mk, and fX(x) is the unconditional probability\\ndensity function of X.\\nIn (7.49), we now note the following points:\\n\\nthe denominator term fX(x) is independent of the transmitted symbol;\\n\\nthe prior probability k = i when all the source symbols are transmitted with equal\\nprobability; and',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 359,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the denominator term fX(x) is independent of the transmitted symbol;\\n\\nthe prior probability k = i when all the source symbols are transmitted with equal\\nprobability; and\\n\\nthe conditional probability density function fX(x|mk) bears a one-to-one relationship\\nto the log-likelihood function L(mk).\\nAccordingly, we may simply restate the decision rule of (7.49) in terms of L(mk) as\\nfollows: Set if is maximum for k = i.\\n(7.50)\\nThe decision rule of (7.50) is known as the maximum likelihood rule, discussed previously\\nin Chapter 3; the system used for its implementation is correspondingly referred to as the\\nmaximum likelihood decoder. According to this decision rule, a maximum likelihood\\ndecoder computes the log-likelihood functions as metrics for all the M possible message\\nsymbols, compares them, and then decides in favor of the maximum. Thus, the maximum\\nlikelihood decoder is a simplified version of the maximum a posteriori decoder, in that the\\nM message symbols are assumed to be equally likely.\\nIt is useful to have a graphical interpretation of the maximum likelihood decision rule.\\nLet Z denote the N-dimensional space of all possible observation vectors x. We refer to\\nthis space as the observation space. Because we have assumed that the decision rule must\\nsay\\n, where i = 1, 2, , M, the total observation space Z is correspondingly\\npartitioned into M-decision regions, denoted by Z1, Z2, , ZM. Accordingly, we may\\nrestate the decision rule of (7.50) as\\nObservation vector x lies in region Zi if is maximum for k = i.\\n(7.51)\\nAside from the boundaries between the decision regions Z1, Z2, , ZM, it is clear that this\\nset of regions covers the entire observation space. We now adopt the convention that all\\nties are resolved at random; that is, the receiver simply makes a random guess.\\nSpecifically, if the observation vector x falls on the boundary between any two decision\\nm mi = kfX x mi   fX x  ---------------------------\\nis maximum for k i = m mi = L mk   m mi = L mk',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 359,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nregions, Zi and Zk, say, the choice between the two possible decisions and is resolved a priori by the flip of a fair coin. Clearly, the outcome of such an\\nevent does not affect the ultimate value of the probability of error since, on this boundary,\\nthe condition of (7.48) is satisfied with the equality sign.\\nThe maximum likelihood decision rule of (7.50) or its geometric counterpart described\\nin (7.51) assumes that the channel noise w(t) is additive. We next specialize this rule for\\nthe case when w(t) is both white and Gaussian.\\nFrom the log-likelihood function defined in (7.46) for an AWGN channel, we note that\\nL(mk) attains its maximum value when the summation term is minimized by\\nthe choice k = i. Accordingly, we may formulate the maximum likelihood decision rule for\\nan AWGN channel as\\nObservation vector x lies in region Zi if is minimum for k = i.\\n(7.52)\\nNote we have used minimum as the optimizing condition in (7.52) because the minus\\nsign in (7.46) has been ignored. Next, we note from the discussion presented in Section\\n2 that (7.53) where is the Euclidean distance between the observation vector x at the receiver\\ninput and the transmitted signal vector sk. Accordingly, we may restate the decision rule of\\n(7.53) as\\n(7.54)\\nIn words, (7.54) states that the maximum likelihood decision rule is simply to choose the\\nmessage point closest to the received signal point, which is intuitively satisfying.\\nIn practice, the decision rule of (7.54) is simplified by expanding the summation on the\\nleft-hand side of (7.53) as\\n(7.55)\\nThe first summation term of this expansion is independent of the index k pertaining to the\\ntransmitted signal vector sk and, therefore, may be ignored. The second summation term is\\nthe inner product of the observation vector x and the transmitted signal vector sk. The third',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 360,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'transmitted signal vector sk and, therefore, may be ignored. The second summation term is\\nthe inner product of the observation vector x and the transmitted signal vector sk. The third\\nsummation term is the transmitted signal energy (7.56) m mi = m mk = xj skj -  2 j 1 = N  xj skj -  2 j 1 = N  xj skj -  2 j 1 = N  x sk - 2 = x sk - Observation vector x lies in region Zi if Euclidean distance x\\nsk - is minimum for k i = xj skj -  2 j 1 = N  xj 2 2 xjskj skj 2 j 1 = N  + j 1 = N  - j 1 = N  = Ek skj 2 j 1 = N  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 360,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Optimum Receivers Using Coherent Detection Accordingly, we may reformulate the maximum-likelihood decision rule one last time: (7.57)\\nFrom (7.57) we infer that, for an AWGN channel, the M decision regions are bounded by\\nlinear hyperplane boundaries. The example in Figure 7.7 illustrates this statement for\\nM = 4 signals and N = 2 dimensions, assuming that the signals are transmitted with equal\\nenergy E and equal probability.\\nCorrelation Receiver\\nIn light of the material just presented, the optimum receiver for an AWGN channel and for\\nthe case when the transmitted signals s1(t), s2(t), , sM(t) are equally likely is called a\\ncorrelation receiver; it consists of two subsystems, which are detailed in Figure 7.8:\\nDetector (Figure 7.8a), which consists of M correlators supplied with a set of\\northonormal basis functions 1(t), 2(t), , N(t) that are generated locally; this\\nbank of correlators operates on the received signal x(t), 0  t  T, to produce the\\nobservation vector x.\\nMaximum-likelihood decoder (Figure 7.8b), which operates on the observation\\nvector x to produce an estimate of the transmitted symbol mi, i = 1, 2, , M, in\\nsuch a way that the average probability of symbol error is minimized.\\nIn accordance with the maximum likelihood decision rule of (7.57), the decoder multiplies\\nthe N elements of the observation vector x by the corresponding N elements of each of the\\nM signal vectors s1, s2, , sM. Then, the resulting products are successively summed in\\naccumulators to form the corresponding set of inner products {xTsk|k = 1, 2, , M}.\\nFigure 7.7\\nIllustrating the partitioning of the\\nobservation space into decision regions\\nfor the case when N = 2 and M = 4; it is\\nassumed that the M transmitted symbols\\nare equally likely. Message point 3 Region Z3 Region Z4 Region Z2 Region Z1 Decision boundary Decision boundary Message point 1 Message point 2 Message point 4 EEE - E - 2  1  Observation vector x lies in region Zi if',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 361,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'xjskj 1 2---Ek - j 1 = N          is maximim for k i, where Ekis transmitted energy.\\n=\\nm',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 361,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nNext, the inner products are corrected for the fact that the transmitted signal energies may\\nbe unequal. Finally, the largest one in the resulting set of numbers is selected, and an\\nappropriate decision on the transmitted message is thereby made.\\nMatched Filter Receiver\\nThe detector shown in Figure 7.8a involves a set of correlators. Alternatively, we may use\\na different but equivalent structure in place of the correlators. To explore this alternative\\nmethod of implementing the optimum receiver, consider a linear time-invariant filter with\\nimpulse response hj(t). With the received signal x(t) operating as input, the resulting filter\\noutput is defined by the convolution integral\\nFigure 7.8\\n(a) Detector or demodulator. (b) Signal\\ntransmission decoder.\\nx(t) x1 x2 xN s1 x s2 xTsM xTs2 xTs1 sM Accumulator Inner-product calculator\\nAccumulator Accumulator EM Estimate m 1 2 E2 1 2 E1 1 2 Select largest + - + - + - (a) (b) dt dt dt    0 T \\x02 0 T \\x02 0 T \\x02       1(t)  N(t)  2(t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 362,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '4 Optimum Receivers Using Coherent Detection To proceed further, we evaluate this integral over the duration of a transmitted symbol,\\nnamely 0  t  T. With time t restricted in this manner, we may replace the variable  with\\nt and go on to write\\n(7.58)\\nConsider next a detector based on a bank of correlators. The output of the jth correlator is\\ndefined by the first line of (7.25), reproduced here for convenience of representation:\\n(7.59)\\nFor yj(T) to equal xj, we find from (7.58) and (7.59) that this condition is satisfied provided\\nthat we choose\\nEquivalently, we may express the condition imposed on the desired impulse response of\\nthe filter as\\n(7.60)\\nWe may now generalize the condition described in (7.60) by stating:\\nGiven a pulse signal (t) occupying the interval 0  t  T, a linear time-invariant\\nfilter is said to be matched to the signal (t) if its impulse response h(t) satisfies\\nthe condition\\n(7.61)\\nA time-invariant filter defined in this way is called a matched filter. Correspondingly, an\\noptimum receiver using matched filters in place of correlators is called a matched-filter\\nreceiver. Such a receiver is depicted in Figure 7.9, shown below.\\nyj t x  hj t  -   d  -   = yj T  x thj T t -   dt 0 T  = xj x tj t dt 0 T  = hj T t -   j t = for 0 t T  and j 1 2 M   = hj t jT t -   = for 0 t T  and j 1 2 M   = h t T t -   for 0 t T  = Figure 7.9 Detector part of matched\\nfilter receiver; the signal transmission\\ndecoder is as shown in Figure 7.8(b).\\nObservation vector x Received signal x(t) Matched filters Sample at t = T x1 x2 xN (T - t) (T - t) (T - t)   N  2  1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 363,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels Probability of Error\\nTo complete the statistical characterization of the correlation receiver of Figure 7.8a or its\\nequivalent, the matched filter receiver of Figure 7.9, we need to evaluate its performance\\nin the presence of AWGN. To do so, suppose that the observation space Z is partitioned\\ninto a set of regions,\\n, in accordance with the maximum likelihood decision rule.\\nSuppose also that symbol mi (or, equivalently, signal vector si) is transmitted and an\\nobservation vector x is received. Then, an error occurs whenever the received signal point\\nrepresented by x does not fall inside region Zi associated with the message point si.\\nAveraging over all possible transmitted symbols assumed to be equiprobable, we see that\\nthe average probability of symbol error is (7.62)\\nwhere we have used the standard notation to denote the conditional probability of an\\nevent. Since x is the sample value of random vector X, we may rewrite (7.62) in terms of\\nthe likelihood function as follows, given that the message symbol mi is sent:\\n(7.63)\\nFor an N-dimensional observation vector, the integral in (7.63) is likewise N-dimensional.\\nInvariance of the Probability of Error to Rotation\\nThere is a uniqueness to the way in which the observation space Z is partitioned into the\\nset of regions Z1, Z2, , ZM in accordance with the maximum likelihood detection of a\\nsignal in AWGN; that uniqueness is defined by the message constellation under study. In\\nparticular, we may make the statement:\\nChanges in the orientation of the message constellation with respect to both the\\ncoordinate axes and origin of the signal space do not affect the probability of\\nsymbol error Pe defined in (7.63).\\nThis statement embodies the invariance property of the average probability of symbol\\nerror Pe with respect to notation and translation, which is the result of two facts:\\nIn maximum likelihood detection, the probability of symbol error Pe depends solely',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 364,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'error Pe with respect to notation and translation, which is the result of two facts:\\nIn maximum likelihood detection, the probability of symbol error Pe depends solely\\non the relative Euclidean distance between a received signal point and message point\\nin the constellation.\\nThe AWGN is spherically symmetric in all directions in the signal space.\\nZi  j 1 = M Pe i\\x02(x does not lie in Zi mi sent)\\ni 1 = M  = 1 M ----- \\x02 x does not lie in Zi mi sent\\n i 1 M  =  i 1 = M  = 1 1 M ----- \\x02 x lies in Zi mi sent\\n  i 1 = M  - = Pe 1 1 M ----- fX Zi x mi   dx i 1 = M  - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 364,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '5 Probability of Error To elaborate, consider first the invariance of Pe with respect to rotation. The effect of a\\nrotation applied to all the message points in a constellation is equivalent to multiplying the\\nN-dimensional signal vector si by an N-by-N orthonormal matrix denoted by Q for all i.\\nBy definition, the matrix Q satisfies the condition\\n(7.64)\\nwhere the superscript T denotes matrix transposition and I is the identity matrix whose\\ndiagonal elements are all unity and its off-diagonal elements are all zero. According to\\n(7.64), the inverse of the real-valued orthonormal matrix Q is equal to its own transpose.\\nThus, in dealing with rotation, the message vector si is replaced by its rotated version\\n(7.65)\\nCorrespondingly, the N-by-1 noise vector w is replaced by its rotated version\\n(7.66)\\nHowever, the statistical characteristics of the noise vector are unaffected by this rotation\\nfor three reasons:\\nFrom Chapter 4 we recall that a linear combination of Gaussian random variables is\\nalso Gaussian. Since the noise vector w is Gaussian, by assumption, then it follows\\nthat the rotated noise vector wrotate is also Gaussian.\\nSince the noise vector w has zero mean, the rotated noise vector wrotate also has zero\\nmean, as shown by\\n(7.67)\\nThe covariance matrix of the noise vector w is equal to (N02)I, where N02 is the\\npower spectral density of the AWGN w(t) and I is the identity matrix; that is\\n(7.68)\\nHence, the covariance matrix of the rotated noise vector is\\n(7.69)\\nwhere, in the last two lines, we have made use of (7.68) and (7.64).\\nIn light of these three reasons, we may, therefore, express the observation vector in the\\nrotated message constellation as\\n(7.70) QQT I = si rotate  Qsi i 1 2 M   =  = wrotate Qw = \\x03 wrotate   \\x03 Qw   = Q\\x03 w   = 0 = \\x03 wwT   N0 2 ------I = \\x03 wrotatewrotate T   \\x03 Qw Qw  T   = \\x03 QwwTQT   = Q\\x03 wwT  QT = N0 2 ------QQT = N0 2 ------I = xrotate Qsi w i 1 2 M   =  + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 365,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nUsing (7.65) and (7.70), we may now express the Euclidean distance between the rotated\\nvectors xrotate and srotate as\\n(7.71)\\nwhere, in the last line, we made use of (7.43).\\nWe may, therefore, formally state the principle of rotational invariance:\\nIf a message constellation is rotated by the transformation\\nsi,rotate = Qsi,\\ni = 1, 2, , M\\nwhere Q is an orthonormal matrix, then the probability of symbol error Pe\\nincurred in maximum likelihood signal-detection over an AWGN channel is\\ncompletely unchanged.\\nEXAMPLE\\nIllustration of Rotational Invariance\\nTo illustrate the principle of rotational invariance, consider the signal constellation shown\\nin Figure 7.10a. The constellation is the same as that of Figure 7.10b, except for the fact\\nthat it has been rotated through 45. Although these two constellations do indeed look\\ndifferent in a geometric sense, the principle of rotational invariance teaches us\\nimmediately that the Pe is the same for both of them.\\nInvariance of the Probability to Translation\\nConsider next the invariance of Pe to translation. Suppose all the message points in a\\nsignal constellation are translated by a constant vector amount a, as shown by\\n(7.72) xrotate si rotate  - Qsi w Qsi - + = w = x si - i 1 2 M   =  = Figure 7.10 A pair of signal constellations for illustrating the principle\\nof rotational invariance.\\n0 (a) 0 (b) - 2  - 2  2  2  2  1  1  2  - -     si translate  si a i 1 2 M   =  - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 366,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '5 Probability of Error The observation vector is correspondingly translated by the same vector amount, as shown\\nby\\n(7.73)\\nFrom (7.72) and (7.73) we see that the translation a is common to both the translated signal\\nvector si and translated observation vector x. We, therefore, immediately deduce that\\n(7.74)\\nand thus formulate the principle of translational invariance:\\nIf a signal constellation is translated by a constant vector amount, then the\\nprobability of symbol error Pe incurred in maximum likelihood signal detection\\nover an AWGN channel is completely unchanged.\\nEXAMPLE\\nTranslation of Signal Constellation\\nAs an example, consider the two signal constellations shown in Figure 7.11, which pertain\\nto a pair of different four-level PAM signals. The constellation of Figure 7.11b is the same\\nas that of Figure 7.11a, except for a translation 32 to the right along the 1-axis. The\\nprinciple of translational invariance teaches us that the Pe is the same for both of these\\nsignal constellations.\\nUnion Bound on the Probability of Error\\nFor AWGN channels, the formulation of the average probability of symbol error2 Pe is\\nconceptually straightforward, in that we simply substitute (7.41) into (7.63).\\nUnfortunately, however, numerical computation of the integral so obtained is impractical,\\nexcept in a few simple (nevertheless, important) cases. To overcome this computational\\ndifficulty, we may resort to the use of bounds, which are usually adequate to predict the\\nSNR (within a decibel or so) required to maintain a prescribed error rate. The\\napproximation to the integral defining Pe is made by simplifying the integral or\\nsimplifying the region of integration. In the following, we use the latter procedure to\\ndevelop a simple yet useful upper bound, called the union bound, as an approximation to\\nthe average probability of symbol error for a set of M equally likely signals (symbols) in\\nan AWGN channel.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 367,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'develop a simple yet useful upper bound, called the union bound, as an approximation to\\nthe average probability of symbol error for a set of M equally likely signals (symbols) in\\nan AWGN channel.\\nLet Aik, with (i,k) = 1, 2, , M, denote the event that the observation vector x is closer\\nto the signal vector sk than to si, when the symbol mi (message vector si) is sent. The\\nconditional probability of symbol error when symbol mi is sent, Pe(mi), is equal to the\\nxtranslate x a - = xtranslate si translate  - x si - for i 1 2 M   =  = Figure 7.11 A pair of signal constellations for illustrating the principle of translational invariance.\\n(a) (b) 0 -3 /2 3 /2 - /2 /2 0 3 2 1  1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 367,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nprobability of the union of events, defined by the set\\n. Probability theory\\nteaches us that the probability of a finite union of events is overbounded by the sum of the\\nprobabilities of the constituent events. We may, therefore, write\\n(7.75)\\nEXAMPLE\\nConstellation of Four Message Points\\nTo illustrate applicability of the union bound, consider Figure 7.12 for the case of M = 4.\\nFigure 7.12a shows the four message points and associated decision regions, with the\\npoint s1 assumed to represent a transmitted symbol. Figure 7.12b shows the three\\nconstituent signal-space descriptions where, in each case, the transmitted message point s1\\nand one other message point are retained. According to Figure 7.12a the conditional\\nprobability of symbol error, Pe(mi), is equal to the probability that the observation vector x\\nAik  k 1 = k i  M Pe mi   \\x02 Aik   i 1 2 M   =  k 1 = k i  M   Figure 7.12 Illustrating the union bound. (a) Constellation of four message points. (b) Three\\nconstellations with a common message point and one other message point x retained from the\\noriginal constellation.\\n(a) (b) s1 s3 s2 s4 x s1 s3 x s1 s2 x s1 s4 x 2  2  2  1  1  1  2  1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 368,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '5 Probability of Error lies in the shaded region of the two-dimensional signal-space diagram. Clearly, this\\nprobability is less than the sum of the probabilities of the three individual events that x lies\\nin the shaded regions of the three constituent signal spaces depicted in Figure 7.12b.\\nPairwise Error Probability\\nIt is important to note that, in general, the probability \\x02(Aik) is different from the\\nprobability\\n, which is the probability that the observation vector x is closer\\nto the signal vector sk (i.e., symbol mk) than every other when the vector si (i.e., symbol\\nmi) is sent. On the other hand, the probability \\x02(Aik) depends on only two signal vectors,\\nsi and sk. To emphasize this difference, we rewrite (7.75) by adopting pik in place of\\n\\x02(Aik). We thus write\\n(7.76)\\nThe probability pik is called the pairwise error probability, in that if a digital\\ncommunication system uses only a pair of signals, si and sk, then pik is the probability of\\nthe receiver mistaking sk for si.\\nConsider then a simplified digital communication system that involves the use of two\\nequally likely messages represented by the vectors si and sk. Since white Gaussian noise is\\nidentically distributed along any set of orthogonal axes, we may temporarily choose the\\nfirst axis in such a set as one that passes through the points si and sk; for three illustrative\\nexamples, see Figure 7.12b. The corresponding decision boundary is represented by the\\nbisector that is perpendicular to the line joining the points si and sk. Accordingly, when the\\nvector si (i.e., symbol mi) is sent, and if the observation vector x lies on the side of the\\nbisector where sk lies, an error is made. The probability of this event is given by\\n(7.77)\\nwhere dik in the lower limit of the integral is the Euclidean distance between signal vectors\\nsi and sk; that is,\\n(7.78)\\nTo change the integral of (7.77) into a standard form, define a new integration variable\\n(7.79)\\nEquation (7.77) is then rewritten in the desired form',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 369,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'si and sk; that is,\\n(7.78)\\nTo change the integral of (7.77) into a standard form, define a new integration variable\\n(7.79)\\nEquation (7.77) is then rewritten in the desired form\\n(7.80) \\x02 m mk mi =   Pe mi   pik i  1 2 M   = k 1 = k i  M   pik \\x02(x is closer to skthan si when si is sent)\\n = 1 N0 -------------- v2 N0 ------ -       exp dv dik 2    = dik si sk - = z 2 N0 ------v = pik 1 2 ---------- z2 2---- -     exp dz dik 2N0    =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 369,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nThe integral in (7.80) is the Q-function of (3.68) that was introduced in Chapter 3. In terms\\nof the Q-function, we may now express the probability pik in the compact form\\n(7.81)\\nCorrespondingly, substituting (7.81) into (7.76), we write\\n(7.82)\\nThe probability of symbol error, averaged over all the M symbols, is, therefore, over-\\nbounded as follows:\\n(7.83)\\nwhere i is the probability of sending symbol mi.\\nThere are two special forms of (7.83) that are noteworthy:\\nSuppose that the signal constellation is circularly symmetric about the origin. Then,\\nthe conditional probability of error Pe(mi) is the same for all i, in which case (7.83)\\nreduces to\\n(7.84)\\nFigure 7.10 illustrates two examples of circularly symmetric signal constellations.\\nDefine the minimum distance of a signal constellation dmin as the smallest Euclidean\\ndistance between any two transmitted signal points in the constellation, as shown by\\n(7.85)\\nThen, recognizing that the Q-function is a monotonically decreasing function of its\\nargument, we have\\n(7.86)\\nTherefore, in general, we may simplify the bound on the average probability of\\nsymbol error in (7.83) as\\n(7.87) pik Q dik 2N0 --------------       = Pe mi   Q dik 2N0 --------------       i 1 2 M   =  k 1 = k i  M   Pe iPe mi   i 1 = M  = iQ dik 2N0 --------------       k 1 = k i  M  i=1 M   Pe Q dik 2N0 --------------       for all i k 1 = k i  M   dmin min k i  = dik for all i and k Q dik 2N0 --------------       Q dmin 2N0 --------------       for all i and k  Pe M 1 -  Q dmin 2N0 --------------',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 370,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '5 Probability of Error The Q-function in (7.87) is itself upper bounded as3\\n(7.88)\\nAccordingly, we may further simplify the bound on Pe in (7.87) as\\n(7.89)\\nIn words, (7.89) states the following:\\nIn an AWGN channel, the average probability of symbol error Pe\\ndecreases exponentially as the squared minimum distance,\\n.\\nBit Versus Symbol Error Probabilities\\nThus far, the only figure of merit we have used to assess the noise performance of a digital\\ncommunication system in AWGN has been the average probability of symbol (word)\\nerror. This figure of merit is the natural choice when messages of length m = log2M are\\ntransmitted, such as alphanumeric symbols. However, when the requirement is to transmit\\nbinary data such as digital computer data, it is often more meaningful to use another figure\\nof merit called the BER. Although, in general, there are no unique relationships between\\nthese two figures of merit, it is fortunate that such relationships can be derived for two\\ncases of practical interest, as discussed next.\\nCase 1:\\nM-tuples Differing in Only a Single Bit\\nSuppose that it is possible to perform the mapping from binary to M-ary symbols in such a\\nway that the two binary M-tuples corresponding to any pair of adjacent symbols in the M-ary\\nmodulation scheme differ in only one bit position. This mapping constraint is satisfied by\\nusing a Gray code. When the probability of symbol error Pe is acceptably small, we find that\\nthe probability of mistaking one symbol for either one of the two nearest symbols is\\ngreater than any other kind of symbol error. Moreover, given a symbol error, the most\\nprobable number of bit errors is one, subject to the aforementioned mapping constraint.\\nSince there are log2M bits per symbol, it follows that the average probability of symbol error\\nis related to the BER as follows:\\n(7.90)\\nwhere, in the first line, is the symbol for union as used in set theory. We also note that',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 371,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'is related to the BER as follows:\\n(7.90)\\nwhere, in the first line, is the symbol for union as used in set theory. We also note that\\n(7.91) Q dmin 2N0 --------------       1 2 ---------- dmin 2 4N0 ---------- -       exp  Pe M 1 - 2 --------------     dmin 2 4N0 ---------- -       exp  dmin 2 Pe \\x02( ith bit is in error   i 1 = M 2 log  = \\x02 ith bit is in error\\n  i 1 = M 2 log   M 2 BER    log =  Pe \\x02 ith bit is in error\\n  BER =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 371,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nIt follows, therefore, that the BER is bounded as follows:\\n(7.92)\\nCase 2:\\nNumber of Symbols Equal to Integer Power of\\nSuppose next M = 2K, where K is an integer. We assume that all symbol errors are equally\\nlikely and occur with probability\\nwhere Pe is the average probability of symbol error. To find the probability that the ith bit\\nin a symbol is in error, we note that there are 2K - 1 cases of symbol error in which this\\nparticular bit is changed and there are 2K - 1 cases in which it is not. Hence, the BER is\\n(7.93) or, equivalently, (7.94) Note that, for large M, the BER approaches the limiting value of Pe2. Note also that the\\nbit errors are not independent in general. Phase-Shift Keying Techniques Using Coherent Detection\\nWith the background material on the coherent detection of signals in AWGN presented in\\nSections 7.2-7.4 at our disposal, we are now ready to study specific passband data-\\ntransmission systems. In this section, we focus on the family of phase-shift keying (PSK)\\ntechniques, starting with the simplest member of the family discussed next.\\nBinary Phase-Shift Keying\\nIn a binary PSK system, the pair of signals s1(t) and s2(t) used to represent binary symbols\\n1 and 0, respectively, is defined by\\n(7.95)\\n(7.96)\\nwhere Tb is the bit duration and Eb is the transmitted signal energy per bit. We find it con-\\nvenient, although not necessary, to assume that each transmitted bit contains an integral\\nnumber of cycles of the carrier wave; that is, the carrier frequency fc is chosen equal to\\nncTb for some fixed integer nc. A pair of sinusoidal waves that differ only in a relative\\nphase-shift of 180, defined in (7.95) and (7.96), is referred to as an antipodal signal.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 372,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'ncTb for some fixed integer nc. A pair of sinusoidal waves that differ only in a relative\\nphase-shift of 180, defined in (7.95) and (7.96), is referred to as an antipodal signal.\\nPe M 2 log ---------------- BER Pe   Pe M 1 - -------------- Pe 2K 1 - --------------- = BER 2K 1 - 2K 1 - ---------------      Pe = BER M 2  M 1 - --------------    Pe = s1 t 2Eb Tb --------- 2fct   0 t Tb   cos = s2 t 2Eb Tb --------- 2fct  +   cos 2Eb Tb --------- 2fct   0 t Tb   cos - = =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 372,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 Phase-Shift Keying Techniques Using Coherent Detection Signal-Space Diagram of Binary PSK Signals\\nFrom this pair of equations it is clear that, in the case of binary PSK, there is only one\\nbasis function of unit energy:\\n(7.97)\\nThen, we may respectively express the transmitted signals s1(t) and s2(t) in terms of 1(t) as\\n(7.98) (7.99) A binary PSK system is, therefore, characterized by having a signal space that is\\none-dimensional (i.e., N = 1), with a signal constellation consisting of two message points\\n(i.e., M = 2). The respective coordinates of the two message points are\\n(7.100)\\n(7.101)\\nIn words, the message point corresponding to s1(t) is located at and the\\nmessage point corresponding to s2(t) is located at\\n. Figure 7.13a displays the\\n1 t 2 Tb ----- 2fct   0 t Tb   cos = s1 t Eb1 t 0 t Tb   = s2 t Eb - 1 t 0 t Tb   = s11 s1 t1 t dt 0 Tb = + Eb = s21 s2 t1 t dt 0 Tb = Eb - = s11 + Eb = s21 Eb - = Figure 7.13 (a) Signal-space diagram\\nfor coherent binary PSK system. (b) The waveforms depicting the transmitted signals\\ns1(t) and s2(t), assuming\\nnc = 2. 0 s2(t) t Message point 1 Decision boundary Message point 2 0 - Eb - 2Eb/Tb 2Eb/Tb Eb Region Z1 Region Z2 Tb 0 s1(t) t - 2Eb/Tb 2Eb/Tb Tb 1  (a) (b)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 373,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nsignal-space diagram for binary PSK and Figure 7.13b shows example waveforms of\\nantipodal signals representing s1(t) and s2(t). Note that the binary constellation of Figure\\n13 has minimum average energy.\\nGeneration of a binary PSK signal follows readily from (7.97) to (7.99). Specifically, as\\nshown in the block diagram of Figure 7.14a, the generator (transmitter) consists of two\\ncomponents:\\nPolar NRZ-level encoder, which represents symbols 1 and 0 of the incoming binary\\nsequence by amplitude levels and\\n, respectively.\\nProduct modulator, which multiplies the output of the polar NRZ encoder by the\\nbasis function 1(t); in effect, the sinusoidal 1(t) acts as the carrier of the binary\\nPSK signal.\\nAccordingly, binary PSK may be viewed as a special form of DSB-SC modulation that\\nwas studied in Section 2.14.\\nError Probability of Binary PSK Using Coherent Detection\\nTo make an optimum decision on the received signal x(t) in favor of symbol 1 or symbol\\n(i.e., estimate the original binary sequence at the transmitter input), we assume that the\\nreceiver has access to a locally generated replica of the basis function 1(t). In other\\nwords, the receiver is synchronized with the transmitter, as shown in the block diagram of\\nFigure 7.14b. We may identify two basic components in the binary PSK receiver:\\nCorrelator, which correlates the received signal x(t) with the basis function 1(t) on\\na bit-by-bit basis.\\nDecision device, which compares the correlator output against a zero-threshold,\\nassuming that binary symbols 1 and 0 are equiprobable. If the threshold is exceeded,\\na decision is made in favor of symbol 1; if not, the decision is made in favor of\\nsymbol 0. Equality of the correlator with the zero-threshold is decided by the toss of\\na fair coin (i.e., in a random manner).\\nWith coherent detection in place, we may apply the decision rule of (7.54). Specifically,\\nwe partition the signal space of Figure 7.13 into two regions:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 374,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the set of points closest to message point 1 at\\n; and\\n\\nthe set of points closest to message point 2 at\\n.\\nThis is accomplished by constructing the midpoint of the line joining these two message\\npoints and then marking off the appropriate decision regions. In Figure 7.13, these two\\ndecision regions are marked Z1 and Z2, according to the message point around which they\\nare constructed.\\nThe decision rule is now simply to decide that signal s1(t) (i.e., binary symbol 1) was\\ntransmitted if the received signal point falls in region Z1 and to decide that signal s2(t)\\n(i.e., binary symbol 0) was transmitted if the received signal point falls in region Z2. Two\\nkinds of erroneous decisions may, however, be made:\\nError of the first kind. Signal s2(t) is transmitted but the noise is such that the received\\nsignal point falls inside region Z1; so the receiver decides in favor of signal s1(t).\\nError of the second kind. Signal s1(t) is transmitted but the noise is such that the\\nreceived signal point falls inside region Z2; so the receiver decides in favor of signal s2(t).\\n+ Eb Eb - + Eb Eb -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 374,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 Phase-Shift Keying Techniques Using Coherent Detection To calculate the probability of making an error of the first kind, we note from Figure 7.13a\\nthat the decision region associated with symbol 1 or signal s1(t) is described by\\nwhere the observable element x1 is related to the received signal x(t) by\\n(7.102)\\nThe conditional probability density function of random variable X1, given that symbol\\n(i.e., signal s2(t)) was transmitted, is defined by\\n(7.103)\\nUsing (7.101) in this equation yields\\n(7.104)\\nThe conditional probability of the receiver deciding in favor of symbol 1, given that\\nsymbol 0 was transmitted, is therefore\\n(7.105) Putting (7.106) Figure 7.14 Block diagrams for (a) binary PSK transmitter and (b) coherent\\nbinary PSK receiver.\\nBinary PSK signal s(t) Product modulator Polar nonreturn- to-zero level encoder Binary data sequence Decision device Tb x(t) Choose 1 if x1 > 0 Choose 0 if x1 < 0 = 2 Tb cos (2 fct ) 0 dt \\x02 Threshold = 0 Correlator x1 (a) (b) 1(t)  1(t)   Z1:0 x1    x1 x t1 t dt 0 Tb = fX1 x1 0   1 N0 -------------- 1 N0 ------ x1 s21 -  2 - exp = fX1 x1 0   1 N0 -------------- 1 N0 ------ x1 Eb +   2 - exp = p10 1 N0 -------------- exp 1 N0 ------ x1 Eb +   2 - dx1 0   = z 2 N0 ------ x1 Eb +   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 375,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nand changing the variable of integration from x1 to z, we may compactly rewrite (7.105) in\\nterms of the Q-function:\\n(7.107)\\nUsing the formula of (3.68) in Chapter 3 for the Q-function in (7.107) we get\\n(7.108)\\nConsider next an error of the second kind. We note that the signal space of Figure 7.13a is\\nsymmetric with respect to the origin. It follows, therefore, that p01, the conditional\\nprobability of the receiver deciding in favor of symbol 0, given that symbol 1 was\\ntransmitted, also has the same value as in (7.108).\\nThus, averaging the conditional error probabilities p10 and p01, we find that the average\\nprobability of symbol error or, equivalently, the BER for binary PSK using coherent\\ndetection and assuming equiprobable symbols is given by\\n(7.109)\\nAs we increase the transmitted signal energy per bit Eb for a specified noise spectral\\ndensity N02, the message points corresponding to symbols 1 and 0 move further apart and\\nthe average probability of error Pe is correspondingly reduced in accordance with (7.109),\\nwhich is intuitively satisfying.\\nPower Spectra of Binary PSK Signals\\nExamining (7.97) and (7.98), we see that a binary PSK wave is an example of DSB-SC\\nmodulation that was discussed in Section 2.14. More specifically, it consists of an in-phase\\ncomponent only. Let g(t) denote the underlying pulse-shaping function defined by\\n(7.110)\\nDepending on whether the transmitter input is binary symbol 1 or 0, the corresponding\\ntransmitter output is +g(t) or -g(t), respectively. It is assumed that the incoming binary\\nsequence is random, with symbols 1 and 0 being equally likely and the symbols\\ntransmitted during the different time slots being statistically independent.\\nIn Example 6 of Chapter 4, it was shown that the power spectral density of a random\\nbinary wave so described is equal to the energy spectral density of the symbol shaping\\nfunction divided by the symbol duration. The energy spectral density of a Fourier-',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 376,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'binary wave so described is equal to the energy spectral density of the symbol shaping\\nfunction divided by the symbol duration. The energy spectral density of a Fourier-\\ntransformable signal g(t) is defined as the squared magnitude of the signals Fourier\\ntransform. For the binary PSK signal at hand, the baseband power spectral density is,\\ntherefore, defined by\\np10 1 2 ---------- z2 2---- -     exp dz 2Eb N0    = p10 Q 2Eb N0 ---------       = Pe Q 2Eb N0 ---------       = g t 2Eb Tb ---------, 0 t Tb  0, otherwise      =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 376,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 Phase-Shift Keying Techniques Using Coherent Detection (7.111)\\nExamining (7.111), we may make the following observations on binary PSK:\\nThe power spectral density SB(f) is symmetric about the vertical axis, as expected.\\nSB(f) goes through zero at multiples of the bit rate; that is, f = 1Tb, 2Tb, \\nWith sin2(Tbf) limited to a maximum value of unity, SB(f) falls off as the inverse\\nsquare of the frequency, f.\\nThese three observations are all embodied in the plot of SB(f) versus f, presented in Figure 7.15.\\nFigure 7.15 also includes a plot of the baseband power spectral density of a binary\\nfrequency-shift keying (FSK) signal, details of which are presented in Section 7.8.\\nComparison of these two spectra is deferred to that section.\\nQuadriphase-Shift Keying\\nThe provision of reliable performance, exemplified by a very low probability of error, is\\none important goal in the design of a digital communication system. Another important\\ngoal is the efficient utilization of channel bandwidth. In this subsection we study a\\nbandwidth-conserving modulation scheme known as quadriphase-shift keying (QPSK),\\nusing coherent detection.\\nAs with binary PSK, information about the message symbols in QPSK is contained in\\nthe carrier phase. In particular, the phase of the carrier takes on one of four equally spaced\\nFigure 7.15 Power spectra of binary PSK and FSK signals.\\nSB f 2Eb Tb f   2 sin Tbf  2 --------------------------------------\\n= 2Eb c sin Tb f   2 = 0 0.5 Binary PSK Delta function (part of FSK spectrum)\\nBinary FSK 1.0 Normalized frequency, f Tb\\nNormalized power spectral density, SB( f )/2Eb',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 377,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nvalues, such as 4, 34, 54, and 74. For this set of values, we may define the trans-\\nmitted signal as\\n(7.112)\\nwhere E is the transmitted signal energy per symbol and T is the symbol duration. The\\ncarrier frequency fc equals nc/T for some fixed integer nc. Each possible value of the phase\\ncorresponds to a unique dibit (i.e., pair of bits). Thus, for example, we may choose the\\nforegoing set of phase values to represent the Gray-encoded set of dibits, 10, 00, 01, and\\n11, where only a single bit is changed from one dibit to the next.\\nSignal-Space Diagram of QPSK Signals\\nUsing a well-known trigonometric identity, we may expand (7.112) to redefine the\\ntransmitted signal in the canonical form:\\n(7.113)\\nwhere i = 1, 2, 3, 4. Based on this representation, we make two observations:\\nThere are two orthonormal basis functions, defined by a pair of quadrature carriers: (7.114) (7.115) Figure 7.16 Signal-space diagram of\\nQPSK system. si t 2E T ------- 2fct 2i 1 -   4--- + , cos 0 t T  i 1 2 3 4  =    0, elsewhere        = si t 2E T ------- 2i 1 -   4--- 2fct   2E T ------- 2i 1 -   4--- 2fct   sin sin - cos cos = 1 t 2 T--- 2fct   0 t T   cos = 2 t 2 T--- 2fct   0 t T   sin = 0 Region Z3 Message point m3 (01) Message point m2 (00) Message point m1 (10) Message point m4 (11) Decision boundary Decision boundary Region Z4 Region Z2 Region Z1 E/2 E/2 E/2 - E/2 - 2  1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 378,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '6 Phase-Shift Keying Techniques Using Coherent Detection\\nThere are four message points, defined by the two-dimensional signal vector\\n(7.116)\\nElements of the signal vectors, namely si1 and si2, have their values summarized in\\nTable 7.2; the first two columns give the associated dibit and phase of the QPSK signal.\\nAccordingly, a QPSK signal has a two-dimensional signal constellation (i.e., N = 2) and\\nfour message points (i.e., M = 4) whose phase angles increase in a counterclockwise\\ndirection, as illustrated in Figure 7.16. As with binary PSK, the QPSK signal has minimum\\naverage energy. EXAMPLE 6 QPSK Waveforms Figure 7.17 illustrates the sequences and waveforms involved in the generation of a QPSK\\nsignal. The input binary sequence 01101000 is shown in Figure 7.17a. This sequence is\\ndivided into two other sequences, consisting of odd- and even-numbered bits of the input\\nsequence. These two sequences are shown in the top lines of Figure 7.17b and c. The\\nwaveforms representing the two components of the QPSK signal, namely si11(t) and\\nsi22(t) are also shown in Figure 7.17b and c, respectively. These two waveforms may\\nindividually be viewed as examples of a binary PSK signal. Adding them, we get the\\nQPSK waveform shown in Figure 7.17d.\\nTo define the decision rule for the coherent detection of the transmitted data sequence,\\nwe partition the signal space into four regions, in accordance with Table 7.2. The\\nindividual regions are defined by the set of symbols closest to the message point\\nrepresented by message vectors s1, s2, s3, and s4. This is readily accomplished by\\nconstructing the perpendicular bisectors of the square formed by joining the four message\\npoints and then marking off the appropriate regions. We thus find that the decision regions\\nTable 7.2 Signal-space characterization of QPSK',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 379,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'points and then marking off the appropriate regions. We thus find that the decision regions\\nTable 7.2 Signal-space characterization of QPSK\\nGray-encoded input dibit Phase of QPSK signal (radians) Coordinates of message points si1 si2 11 /4 01 3/4 00 5/4 10 7/4 + E 2  + E 2  E 2  - + E 2  E 2  - E 2  - + E 2  E 2  - si E 2i 1 -   4---     cos E - 2i 1 -   4---     sin i  1 2 3 4  = =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 379,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nare quadrants whose vertices coincide with the origin. These regions are marked Z1, Z2,\\nZ3, and Z4 in Figure 7.17, according to the message point around which they are\\nconstructed.\\nGeneration and Coherent Detection of QPSK Signals\\nExpanding on the binary PSK transmitter of Figure 7.14a, we may build on (7.113) to\\n(7.115) to construct the QPSK transmitter shown in Figure 7.18a. A distinguishing feature\\nof the QPSK transmitter is the block labeled demultiplexer. The function of the\\ndemultiplexer is to divide the binary wave produced by the polar NRZ-level encoder into\\ntwo separate binary waves, one of which represents the odd-numbered dibits in the\\nincoming binary sequence and the other represents the even-numbered dibits.\\nAccordingly, we may make the following statement:\\nThe QPSK transmitter may be viewed as two binary PSK generators that work\\nin parallel, each at a bit rate equal to one-half the bit rate of the original binary\\nsequence at the QPSK transmitter input.\\nFigure 7.17 (a) Input binary sequence. (b) Odd-numbered dibits of input sequence and associated\\nbinary PSK signal. (c) Even-numbered dibits of input sequence and associated binary PSK signal.\\n(d) QPSK waveform defined as s(t) = si1\\n1 (t) + si2\\n2 (t).\\nEven-numbered sequence\\nPolarity of coefficient si2\\nOdd-numbered sequence\\nPolarity of coefficient si1\\nInput binary sequence 0 Dibit 01 1 ttt si1 (t) si2 (t) s(t) 1 Dibit 10 0 1 Dibit 10 0 0 Dibit 00 0 0 - 1 + 1 + 0 - 0 - 0 - 1 + 0 - (a) (d) (b) (c) 2  1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 380,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '6 Phase-Shift Keying Techniques Using Coherent Detection Expanding on the binary PSK receiver of Figure 7.14b, we find that the QPSK receiver is\\nstructured in the form of an in-phase path and a quadrature path, working in parallel as\\ndepicted in Figure 7.18b. The functional composition of the QPSK receiver is as follows:\\nPair of correlators, which have a common input x(t). The two correlators are\\nsupplied with a pair of locally generated orthonormal basis functions 1(t) and 2(t),\\nwhich means that the receiver is synchronized with the transmitter. The correlator\\noutputs, produced in response to the received signal x(t), are denoted by x1 and x2,\\nrespectively.\\nPair of decision devices, which act on the correlator outputs x1 and x2 by comparing\\neach one with a zero-threshold; here, it is assumed that the symbols 1 and 0 in the\\nFigure 7.18 Block diagram of (a) QPSK transmitter and (b) coherent QPSK receiver.\\nBinary data sequence Polar nonreturn- to-zero level encoder Demultiplexer 1(t) = a2(t) a1(t) QPSK signal s(t) 2/T cos(2fct) = + + 2/T sin(2fct) dt Decision device dt x2 x1 Decision device Multiplexer Quadrature channel Threshold = 0 Threshold = 0 In-phase channel Estimate of transmitted binary sequence Received signal x(t) T 0\\x02 T 0\\x02  (a) (b) 2(t)  1(t)  2(t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 381,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\noriginal binary stream at the transmitter input are equally likely. If x1 > 0, a decision\\nis made in favor of symbol 1 for the in-phase channel output; on the other hand, if\\nx1 0, then a decision is made in favor of symbol 0. Similar binary decisions are\\nmade for the quadrature channel.\\nMultiplexer, the function of which is to combine the two binary sequences produced\\nby the pair of decision devices. The resulting binary sequence so produced provides\\nan estimate of the original binary stream at the transmitter input.\\nError Probability of QPSK\\nIn a QPSK system operating on an AWGN channel, the received signal x(t) is defined by\\n(7.117)\\nwhere w(t) is the sample function of a white Gaussian noise process of zero mean and\\npower spectral density N02.\\nReferring to Figure 7.18a, we see that the two correlator outputs, x1 and x2, are\\nrespectively defined as follows:\\n(7.118) and (7.119) Thus, the observable elements x1 and x2 are sample values of independent Gaussian\\nrandom variables with mean values equal to and\\n, respectively, and with\\na common variance equal to N02.\\nThe decision rule is now simply to say that s1(t) was transmitted if the received signal\\npoint associated with the observation vector x falls inside region Z1; say that s2(t) was\\ntransmitted if the received signal point falls inside region Z2, and so on for the other two\\nregions Z3 and Z4. An erroneous decision will be made if, for example, signal s4(t) is\\ntransmitted but the noise w(t) is such that the received signal point falls outside region Z4.\\nTo calculate the average probability of symbol error, recall that a QPSK receiver is in\\nfact equivalent to two binary PSK receivers working in parallel and using two carriers that\\nare in phase quadrature. The in-phase channel x1 and the quadrature channel output x2',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 382,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'fact equivalent to two binary PSK receivers working in parallel and using two carriers that\\nare in phase quadrature. The in-phase channel x1 and the quadrature channel output x2\\nx t si t w t 0 t T  i 1 2 3 4  =    + = x1 x t1 t dt 0 T  = E 2i 1 -   4--- w1 + cos = E 2--- w1 +  = x2 x t2 t dt 0 T  = E 2i 1 -   4--- w2 + sin = E 2---  w2 + = E 2   E 2',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 382,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 Phase-Shift Keying Techniques Using Coherent Detection (i.e., the two elements of the observation vector x) may be viewed as the individual\\noutputs of two binary PSK receivers. Thus, according to (7.118) and (7.119), these two\\nbinary PSK receivers are characterized as follows:\\n\\nsignal energy per bit equal to E2, and',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 383,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'noise spectral density equal to N02.\\nHence, using (7.109) for the average probability of bit error of a coherent binary PSK\\nreceiver, we may express the average probability of bit error in the in-phase and\\nquadrature paths of the coherent QPSK receiver as\\n(7.120)\\nwhere E is written in place of 2Eb. Another important point to note is that the bit errors in\\nthe in-phase and quadrature paths of the QPSK receiver are statistically independent. The\\ndecision device in the in-phase path accounts for one of the two bits constituting a symbol\\n(dibit) of the QPSK signal, and the decision device in the quadrature path takes care of the\\nother dibit. Accordingly, the average probability of a correct detection resulting from the\\ncombined action of the two channels (paths) working together is\\n(7.121)\\nThe average probability of symbol error for QPSK is therefore\\n(7.122)\\nIn the region where (EN0) >> 1, we may ignore the quadratic term on the right-hand side of\\n(7.122), so the average probability of symbol error for the QPSK receiver is approximated as\\n(7.123)\\nEquation (7.123) may also be derived in another insightful way, using the signal-space\\ndiagram of Figure 7.16. Since the four message points of this diagram are circularly\\nsymmetric with respect to the origin, we may apply the approximate formula of (7.85)\\nbased on the union bound. Consider, for example, message point m1 (corresponding to\\ndibit 10) chosen as the transmitted message point. The message points m2 and m4\\n(corresponding to dibits 00 and 11) are the closest to m1. From Figure 7.16 we readily find\\nthat m1 is equidistant from m2 and m4 in a Euclidean sense, as shown by\\nAssuming that EN0 is large enough to ignore the contribution of the most distant message\\npoint m3 (corresponding to dibit 01) relative to m1, we find that the use of (7.85) with the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 383,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Assuming that EN0 is large enough to ignore the contribution of the most distant message\\npoint m3 (corresponding to dibit 01) relative to m1, we find that the use of (7.85) with the\\nP Q E N0 ------     = Pc 1 P -  2 = 1 Q E N0 ------     - 2 = 1 2Q E N0 ------     Q2 E N0 ------     + - = Pe 1 Pc - = 2Q E N0 ------     Q2 E N0 ------     - = Pe 2Q E N0 ------      d12 d14 2E = =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 383,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nequality sign yields an approximate expression for Pe that is the same as that of (7.123).\\nNote that in mistaking either m2 or m4 for m1, a single bit error is made; on the other hand,\\nin mistaking m3 for m1, two bit errors are made. For a high enough EN0, the likelihood of\\nboth bits of a symbol being in error is much less than a single bit, which is a further\\njustification for ignoring m3 in calculating Pe when m1 is sent.\\nIn a QPSK system, we note that since there are two bits per symbol, the transmitted\\nsignal energy per symbol is twice the signal energy per bit, as shown by\\n(7.124)\\nThus, expressing the average probability of symbol error in terms of the ratio EbN0, we\\nmay write\\n(7.125)\\nWith Gray encoding used for the incoming symbols, we find from (7.120) and (7.124) that\\nthe BER of QPSK is exactly\\n(7.126)\\nWe may, therefore, state that a QPSK system achieves the same average probability of bit\\nerror as a binary PSK system for the same bit rate and the same EbN0, but uses only half\\nthe channel bandwidth. Stated in another way:\\nFor the same Eb/N0 and, therefore, the same average probability of bit error, a\\nQPSK system transmits information at twice the bit rate of a binary PSK system\\nfor the same channel bandwidth.\\nFor a prescribed performance, QPSK uses channel bandwidth better than binary PSK,\\nwhich explains the preferred use of QPSK over binary PSK in practice.\\nEarlier we stated that the binary PSK may be viewed as a special case of DSB-SC\\nmodulation. In a corresponding way, we may view the QPSK as a special case of the\\nquadrature amplitude modulation (QAM) in analog modulation theory.\\nPower Spectra of QPSK Signals\\nAssume that the binary wave at the modulator input is random with symbols 1 and 0 being\\nequally likely, and with the symbols transmitted during adjacent time slots being\\nstatistically independent. We then make the following observations pertaining to the in-\\nphase and quadrature components of a QPSK signal:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 384,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'statistically independent. We then make the following observations pertaining to the in-\\nphase and quadrature components of a QPSK signal:\\nDepending on the dibit sent during the signaling interval -Tb  t  Tb, the in-phase\\ncomponent equals +g(t) or -g(t), and similarly for the quadrature component. The\\ng(t) denotes the symbol-shaping function defined by\\n(7.127) E 2Eb = Pe 2Q 2Eb N0 ---------        BER Q 2Eb N0 ---------       = g t E T---, 0 t T  0, otherwise      =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 384,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 Phase-Shift Keying Techniques Using Coherent Detection Hence, the in-phase and quadrature components have a common power spectral\\ndensity, namely, E sinc2(Tf).\\nThe in-phase and quadrature components are statistically independent. Accordingly,\\nthe baseband power spectral density of the QPSK signal equals the sum of the\\nindividual power spectral densities of the in-phase and quadrature components, so\\nwe may write\\n(7.128)\\nFigure 7.19 plots SB(f), normalized with respect to 4Eb, versus the normalized frequency\\nTb f. This figure also includes a plot of the baseband power spectral density of a certain\\nform of binary FSK called minimum shift keying, the evaluation of which is presented in\\nSection 7.8. Comparison of these two spectra is deferred to that section.\\nOffset QPSK\\nFor a variation of the QPSK, consider the signal-space diagram of Figure 7.20a that\\nembodies all the possible phase transitions that can arise in the generation of a QPSK\\nsignal. More specifically, examining the QPSK waveform illustrated in Figure 7.17 for\\nExample 6, we may make three observations:\\nThe carrier phase changes by 180 whenever both the in-phase and quadrature\\ncomponents of the QPSK signal change sign. An example of this situation is\\nillustrated in Figure 7.17 when the input binary sequence switches from dibit 01 to\\ndibit 10.\\nFigure 7.19 Power spectra of QPSK and MSK signals.\\n0 0.25 QPSK MSK 0.5 Normalized frequency, f Tb\\nNormalized power spectral density, SB( f )/4Eb\\n75 1.0 0.5 0.1 1.0 SB f 2E sinc2 Tf   = 4Eb sinc2 2Tb f   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 385,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nThe carrier phase changes by 90 whenever the in-phase or quadrature component\\nchanges sign. An example of this second situation is illustrated in Figure 7.17 when\\nthe input binary sequence switches from dibit 10 to dibit 00, during which the in-\\nphase component changes sign, whereas the quadrature component is unchanged.\\nThe carrier phase is unchanged when neither the in-phase component nor the\\nquadrature component changes sign. This last situation is illustrated in Figure 7.17\\nwhen dibit 10 is transmitted in two successive symbol intervals.\\nSituation 1 and, to a much lesser extent, situation 2 can be of a particular concern when the\\nQPSK signal is filtered during the course of transmission, prior to detection. Specifically,\\nthe 180 and 90 shifts in carrier phase can result in changes in the carrier amplitude (i.e.,\\nenvelope of the QPSK signal) during the course of transmission over the channel, thereby\\ncausing additional symbol errors on detection at the receiver.\\nTo mitigate this shortcoming of QPSK, we need to reduce the extent of its amplitude\\nfluctuations. To this end, we may use offset QPSK.4 In this variant of QPSK, the bit stream\\nresponsible for generating the quadrature component is delayed (i.e., offset) by half a\\nsymbol interval with respect to the bit stream responsible for generating the in-phase\\ncomponent. Specifically, the two basis functions of offset QPSK are defined by\\n(7.129) and (7.130) The 1(t) of (7.129) is exactly the same as that of (7.114) for QPSK, but the 2(t) of\\n(7.130) is different from that of (7.115) for QPSK. Accordingly, unlike QPSK, the phase\\ntransitions likely to occur in offset QPSK are confined to 90, as indicated in the signal-\\nspace diagram of Figure 7.20b. However, 90 phase transitions in offset QPSK occur\\ntwice as frequently but with half the intensity encountered in QPSK. Since, in addition to\\n90 phase transitions, 180 phase transitions also occur in QPSK, we find that',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 386,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'twice as frequently but with half the intensity encountered in QPSK. Since, in addition to\\n90 phase transitions, 180 phase transitions also occur in QPSK, we find that\\namplitude fluctuations in offset QPSK due to filtering have a smaller amplitude than in the\\ncase of QPSK.\\nFigure 7.20 Possible paths for switching between the message points\\nin (a) QPSK and (b) offset QPSK.\\n1 t 2 T--- 2fct   0 t T   cos = 2 t 2 T--- 2fct   T 2--- t 3T 2 ------   sin = 0 0 (a) (b) 2  1  1  2',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 386,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 Phase-Shift Keying Techniques Using Coherent Detection Despite the delay T/2 applied to the basis function 2(t) in (7.130) compared with that\\nin (7.115) for QPSK, the offset QPSK has exactly the same probability of symbol error in\\nan AWGN channel as QPSK. The equivalence in noise performance between these PSK\\nschemes assumes the use of coherent detection at the receiver. The reason for the\\nequivalence is that the statistical independence of the in-phase and quadrature components\\napplies to both QPSK and offset QPSK. We may, therefore, say that Equation (7.123) for\\nthe average probability of symbol error applies equally well to the offset QPSK.\\nM-ary PSK\\nQPSK is a special case of the generic form of PSK commonly referred to as M-ary PSK,\\nwhere the phase of the carrier takes on one of M possible values: i = 2(i - 1)M, where\\ni = 1, 2, , M. Accordingly, during each signaling interval of duration T, one of the M\\npossible signals\\n(7.131)\\nis sent, where E is the signal energy per symbol. The carrier frequency fc = ncT for some\\nfixed integer nc.\\nEach si(t) may be expanded in terms of the same two basis functions 1(t) and 2(t); the\\nsignal constellation of M-ary PSK is, therefore, two-dimensional. The M message points\\nare equally spaced on a circle of radius and center at the origin, as illustrated in Figure\\n21a for the case of octaphase-shift-keying (i.e., M = 8).\\nFrom Figure 7.21a we see that the signal-space diagram is circularly symmetric. We\\nmay, therefore, apply (7.85), based on the union bound, to develop an approximate formula\\nfor the average probability of symbol error for M-ary PSK. Suppose that the transmitted\\nsignal corresponds to the message point m1, whose coordinates along the 1- and 2-axes are and 0, respectively. Suppose that the ratio EN0 is large enough to consider the nearest\\ntwo message points, one on either side of m1, as potential candidates for being mistaken for\\nm1 due to channel noise. This is illustrated in Figure 7.21b for the case of M = 8. The',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 387,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'two message points, one on either side of m1, as potential candidates for being mistaken for\\nm1 due to channel noise. This is illustrated in Figure 7.21b for the case of M = 8. The\\nEuclidean distance for each of these two points from m1 is (for M = 8)\\nHence, the use of (7.85) yields the average probability of symbol error for coherent M-ary\\nPSK as\\n(7.132)\\nwhere it is assumed that M  4. The approximation becomes extremely tight for fixed M, as\\nEN0 is increased. For M = 4, (7.132) reduces to the same form given in (7.123) for QPSK.\\nPower Spectra of M-ary PSK Signals\\nThe symbol duration of M-ary PSK is defined by\\n(7.133) si t 2E T ------- 2fct 2 M ------ i 1 -   + i 1 2 M   =  cos = E + E d12 d18 2 E = =  M -----     sin Pe 2Q 2E N0 -------  M -----     sin  T Tb M 2 log =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 387,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nFigure 7.21 (a) Signal-space diagram for octaphase-shift keying (i.e., M = 8). The\\ndecision boundaries are shown as dashed lines. (b) Signal-space diagram illustrating\\nthe application of the union bound for octaphase-shift keying.\\nDecision boundary Decision boundary Message point m1 Message point m1 0 0 /M m8 m7 m7 m6 m5 m5 m4 m3 m3 m2 m8 m6 m4 m2 EEE - E - Decision boundary EEE - E - (a) (b) 2  1  2  1   /M',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 388,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '6 Phase-Shift Keying Techniques Using Coherent Detection where Tb is the bit duration. Proceeding in a manner similar to that described for a QPSK\\nsignal, we may show that the baseband power spectral density of an M-ary PSK signal is\\ngiven by\\n(7.134)\\nFigure 7.22 is a plot of the normalized power spectral density SB(f)2Eb versus the\\nnormalized frequency Tbf for three different values of M, namely M = 2, 4, 8. Equation\\n(7.134) includes (7.111) for M = 2 and (7.128) for M = 4 as two special cases.\\nThe baseband power spectra of M-ary PSK signals plotted in Figure 7.22 possess a\\nmain lobe bounded by well-defined spectral nulls (i.e., frequencies at which the power\\nspectral density is zero). In light of the discussion on the bandwidth of signals presented in\\nChapter 2, we may use the main lobe as a basis for bandwidth assessment. Accordingly,\\ninvoking the notion of null-to-null bandwidth, we may say that the spectral width of the\\nmain lobe provides a simple, yet informative, measure for the bandwidth of M-ary PSK\\nsignals. Most importantly, a large fraction of the average signal power is contained inside\\nthe main lobe. On this basis, we may define the channel bandwidth required to pass M-ary\\nPSK signals through an analog channel as\\n(7.135)\\nwhere T is the symbol duration. But the symbol duration T is related to the bit duration Tb\\nby (7.133). Moreover, the bit rate Rb = 1Tb. Hence, we may redefine the channel\\nbandwidth of (7.135) in terms of the bit rate as\\n(7.136)\\nFigure 7.22 Power spectra of M-ary PSK signals for M = 2, 4, 8.\\nSB f 2E sinc2 Tf   = 2Eb( M) sinc2 Tbf M 2 log     2 log = B 2 T--- = B 2Rb M 2 log ----------------- = 0 0.5 Normalized frequency, Tb f\\n0 1.0 2.0 Normalized power spectral density, SB( f )/2Eb',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 389,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nBased on this formula, the bandwidth efficiency of M-ary PSK signals is given by\\n(7.137)\\nTable 7.3 gives the values of  calculated from (7.137) for varying M. In light of (7.132)\\nand Table 7.3, we now make the statement:\\nAs the number of states in M-ary PSK is increased, the bandwidth efficiency is\\nimproved at the expense of error performance.\\nHowever, note that if we are to ensure that there is no degradation in error performance,\\nwe have to increase EbN0 to compensate for the increase in M. M-ary Quadrature Amplitude Modulation\\nIn an M-ary PSK system, the in-phase and quadrature components of the modulated signal\\nare interrelated in such a way that the envelope is constrained to remain constant. This\\nconstraint manifests itself in a circular constellation for the message points, as illustrated\\nin Figure 7.21a. However, if this constraint is removed so as to permit the in-phase and\\nquadrature components to be independent, we get a new modulation scheme called M-ary\\nQAM. The QAM is a hybrid form of modulation, in that the carrier experiences amplitude\\nas well as phase-modulation.\\nIn M-ary PAM, the signal-space diagram is one-dimensional. M-ary QAM is a two-\\ndimensional generalization of M-ary PAM, in that its formulation involves two orthogonal\\npassband basis functions:\\n(7.138)\\nLet dmin denote the minimum distance between any two message points in the QAM\\nconstellation. Then, the projections of the ith message point on the 1- and 2-axes are\\nrespectively defined by ai dmin2 and bi dmin2, where i = 1, 2, , M. With the separation\\nbetween two message points in the signal-space diagram being proportional to the square\\nroot of energy, we may therefore set\\n(7.139)\\nTable 7.3 Bandwidth efficiency of M-ary PSK signals\\nM 2 4 8 16 32 64  (bit(sHz)) 0.5 1 1.5 2 2.5 3  Rb B ------ = M 2 log 2 ----------------- = 1 t 2 T--- 2fct   0 t T   cos = 2 t 2 T--- 2fct   0 t T   sin = dmin 2 ---------- E0 =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 390,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '7 M-ary Quadrature Amplitude Modulation where E0 is the energy of the message signal with the lowest amplitude. The transmitted\\nM-ary QAM signal for symbol k can now be defined in terms of E0:\\n(7.140)\\nThe signal sk(t) involves two phase-quadrature carriers, each one of which is modulated by\\na set of discrete amplitudes; hence the terminology quadrature amplitude modulation.\\nIn M-ary QAM, the constellation of message points depends on the number of possible\\nsymbols, M. In what follows, we consider the case of square constellations, for which the\\nnumber of bits per symbol is even.\\nQAM Square Constellations\\nWith an even number of bits per symbol, we write\\n(7.141)\\nUnder this condition, an M-ary QAM square constellation can always be viewed as the\\nCartesian product of a one-dimensional L-ary PAM constellation with itself. By definition,\\nthe Cartesian product of two sets of coordinates (representing a pair of one-dimensional\\nconstellations) is made up of the set of all possible ordered pairs of coordinates with the\\nfirst coordinate in each such pair being taken from the first set involved in the product and\\nthe second coordinate taken from the second set in the product.\\nThus, the ordered pairs of coordinates naturally form a square matrix, as shown by\\n(7.142)\\nTo calculate the probability of symbol error for this M-ary QAM, we exploit the following\\nproperty:\\nA QAM square constellation can be factored into the product of the\\ncorresponding L-ary PAM constellation with itself.\\nTo exploit this statement, we may proceed in one of two ways:\\nApproach 1:\\nWe start with a signal constellation of the M-ary PAM for a prescribed M,\\nand then build on it to construct the corresponding signal constellation of the M-ary QAM.\\nApproach 2:\\nWe start with a signal constellation of the M-ary QAM, and then use it to\\nconstruct the corresponding orthogonal M-ary PAMS.\\nIn the example to follow, we present a systematic procedure based on Approach 1.\\nEXAMPLE\\nM-ary QAM for M =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 391,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'construct the corresponding orthogonal M-ary PAMS.\\nIn the example to follow, we present a systematic procedure based on Approach 1.\\nEXAMPLE\\nM-ary QAM for M =\\nIn Figure 7.23, we have constructed two signal constellations for the 4-ary PAM, one\\nvertically oriented along the 1-axis in part a of the figure, and the other horizontally\\nsk t 2E0 T ---------ak 2fct   2E0 T ---------bk 2fct   0 t T  k 0 1 2       =    sin - cos = L M L: positive integer , = ai bi    L - 1 L 1 -  +   L - 3 L 1 -  +    L 1 L 1 -  -   L - 1 L 3 -  +   L - 3 L 3 -  +    L 1 L 3 -  -      L - 1 L - 1 +  +   L - 3 L - 1 +  +   L 1 L - 1 +  -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 391,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\noriented along the 2-axis in part b of the figure. These two parts are spatially orthogonal\\nto each other, accounting for the two-dimensional structure of the M-ary QAM. In\\ndeveloping this structure, the following points should be born in mind:\\n\\nThe same binary sequence is used for both 4-ary PAM constellations.\\n\\nThe Gray encoding rule is applied, which means that as we move from one\\ncodeword to an adjacent one, only a single bit is changed.\\n\\nIn constructing the 4-ary QAM constellation, we move from one quadrant to the\\nnext in a counterclockwise direction.\\nWith four quadrants constituting the 4-ary QAM, we proceed in four stages as follows:\\nStage 1:\\nFirst-quadrant constellation. Referring to Figure 7.23, we use the codewords\\nalong the positive parts of the 2 and 1-axes, respectively, to write\\nStage 2:\\nSecond-quadrant constellation. Following the same procedure as in Stage 1, we\\nwrite 11 10 Top to bottom 10 11 Left to right 1110 1111 1010 1011 First quadrant 11 10 Top to bottom 01 00 Left to right  1101 1100 1001 1000 Second quadrant Figure 7.23 The two orthogonal constellations of the\\n4-ary PAM. (a) Vertically oriented\\nconstellation. (b) Horizontally oriented\\nconstellation. As mentioned in the text,\\nwe move top-down along the 2-axis and\\nfrom left to right along the 1-axis.\\n-d/2 -3d/2 3d/2 d/2 0 d/2 -d/2 -3d/2 3d/2 (a) (b) 2  1  0 11 10 00 01 01 00 10',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 392,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '7 M-ary Quadrature Amplitude Modulation Stage 3:\\nThird-quadrant constellation. Again, following the same procedure as before,\\nwe next write\\nStage 4:\\nFourth-quadrant constellation. Finally, we write\\nThe final step is to piece together these four constituent 4-ary PAM constellations to\\nconstruct the 4-ary QAM constellations as described in Figure 7.24. The important point\\nto note here is that all the codewords in Figure 7.24 obey the Gray encoding rule, not only\\nwithin each quadrant but also as we move from one quadrant to the next.\\nAverage Probability of Error\\nIn light of the equivalence established between the M-ary QAM and M-ary PAM, we may\\nformulate the average probability of error of the M-ary QAM by proceeding as follows:\\nThe probability of correct detection for M-ary QAM is written as\\n(7.143)\\nwhere is the probability of symbol error for the L-ary PAM.\\n00 01 Top to bottom 01 00 Left to right 0001 0000 0101 0100 Third quadrant 00 01 Top to bottom 10 11 Left to right  0010 0011 0110 0111 Fourth quadrant Figure 7.24 (a) Signal-space diagram of M-ary QAM for\\nM = 16; the message points in each quadrant\\nare identified with Gray-encoded quadbits.\\n-d/2 -3d/2 3d/2 d/2 1101 1100 1110 1111 1001 1000 1010 1011 0001 0000 0010 0011 0101 0100 0110 0111 d/2 -d/2 -3d/2 3d/2 2  1  Pc 1 Pe -   = 2 Pe',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 393,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nWith\\n, the probability of symbol error is itself defined by (7.144)\\nThe probability of symbol error for M-ary QAM is given by\\n(7.145)\\nwhere it is assumed that is small enough compared with unity to justify ignoring\\nthe quadratic term.\\nHence, using (7.143) and (7.144) in (7.145), we find that the probability of symbol error\\nfor M-ary QAM is approximately given by\\n(7.146)\\nThe transmitted energy in M-ary QAM is variable, in that its instantaneous value naturally\\ndepends on the particular symbol transmitted. Therefore, it is more logical to express Pe in\\nterms of the average value of the transmitted energy rather than E0. Assuming that the L\\namplitude levels of the in-phase or quadrature component of the M-ary QAM signal are\\nequally likely, we have\\n(7.147)\\nwhere the overall scaling factor 2 accounts for the equal contributions made by the in-phase\\nand quadrature components. The limits of the summation and the scaling factor 2 inside the\\nlarge parentheses account for the symmetric nature of the pertinent amplitude levels around\\nzero. Summing the series in (7.147), we get\\n(7.148) (7.149) Accordingly, we may rewrite (7.146) in terms of Eav as\\n(7.150)\\nwhich is the desired result.\\nThe case of M = 4 is of special interest. The signal constellation for this particular value\\nof M is the same as that for QPSK. Indeed, putting M = 4 in (7.150) and noting that, for\\nthis special case, Eav equals E, where E is the energy per symbol, we find that the resulting\\nL M = Pe Pe 2 1 1 M --------- -    Q 2E0 N0 ---------       = Pe 1 Pc - = 1 1 Pe -  2 - = 2Pe  Pe Pe 4 1 1 M --------- -    Q 2E0 N0 ---------        Eav 2 2E0 L --------- 2i 1 -  2 i 1 = L 2   = Eav 2 L2 1 -  E0 3 -----------------------------\\n= 2 M 1 -  E0 3 ----------------------------\\n= Pe 4 1 1 M --------- -    Q 3Eav M 1 -  N0 -------------------------',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 394,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '8 Frequency-Shift Keying Techniques Using Coherent Detection formula for the probability of symbol error becomes identical to that in (7.123) for QPSK;\\nand so it should. Frequency-Shift Keying Techniques Using Coherent Detection\\nM-ary PSK and M-ary QAM share a common property: both of them are examples of\\nlinear modulation. In this section, we study a nonlinear method of modulation known as\\nFSK using coherent detection. We begin the study by considering the simple case of\\nbinary FSK, for which M = 2.\\nBinary FSK\\nIn binary FSK, symbols 1 and 0 are distinguished from each other by transmitting one of\\ntwo sinusoidal waves that differ in frequency by a fixed amount. A typical pair of\\nsinusoidal waves is described by\\n(7.151)\\nwhere i = 1, 2 and Eb is the transmitted signal energy per bit; the transmitted frequency is\\nset at\\n(7.152)\\nSymbol 1 is represented by s1(t) and symbol 0 by s2(t). The FSK signal described here is\\nknown as Sundes FSK. It is a continuous-phase signal, in the sense that phase continuity\\nis always maintained, including the inter-bit switching times.\\nFrom (7.151) and (7.152), we observe directly that the signals s1(t) and s2(t) are\\northogonal, but not normalized to have unit energy. The most useful form for the set of\\northonormal basis functions is described by\\n(7.153)\\nwhere i = 1, 2. Correspondingly, the coefficient sij for where i = 1, 2 and j = 1, 2 is defined\\nby (7.154) si t 2Eb Tb --------- 2fit  , cos 0 t Tb  0, elsewhere      = fi nc 1 + Tb -------------- for some fixed integer nc and i\\n1 2  = = i t 2 Tb ----- 2fit  , cos 0 t Tb  0, elsewhere      = sij si tj t dt 0 Tb = 2Eb Tb --------- 2fit   2 Tb ----- 2fjt   dt cos cos 0 Tb =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 395,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nCarrying out the integration in (7.154), the formula for sij simplifies to\\n(7.155)\\nThus, unlike binary PSK, binary FSK is characterized by having a signal-space diagram\\nthat is two-dimensional (i.e., N = 2) with two message points (i.e., M = 2), as shown in\\nFigure 7.25. The two message points are defined by the vectors\\n(7.156) sij Eb, i j = 0, i j       = s1 Eb 0 = Figure 7.25 Signal-space diagram for binary FSK system. The diagram also includes example\\nwaveforms of the two modulated signals s1(t) and s2(t).\\n0 s2(t) t - 2Eb/Tb 2Eb/Tb Tb 0 s1(t) t - 2Eb/Tb 2Eb/Tb Tb Decision boundary Message point m2 Message point m1 0 Region Z1 Region Z2 Eb Eb 2Eb 1  2',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 396,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '8 Frequency-Shift Keying Techniques Using Coherent Detection\\n377 and (7.157) The Euclidean distance is equal to\\n. Figure 7.25 also includes a couple of\\nwaveforms representative of signals s1(t) and s2(t).\\nGeneration and Coherent Detection of Binary FSK Signals\\nThe block diagram of Figure 7.26a describes a scheme for generating the binary FSK\\nsignal; it consists of two components:\\nOn-off level encoder, the output of which is a constant amplitude of in\\nresponse to input symbol 1 and zero in response to input symbol 0.\\nPair of oscillators, whose frequencies f1 and f2 differ by an integer multiple of the\\nbit rate 1Tb in accordance with (7.152). The lower oscillator with frequency f2 is\\npreceded by an inverter. When in a signaling interval, the input symbol is 1, the\\nupper oscillator with frequency f1 is switched on and signal s1(t) is transmitted,\\nwhile the lower oscillator is switched off. On the other hand, when the input symbol\\nis 0, the upper oscillator is switched off, while the lower oscillator is switched on\\nFigure 7.26 Block diagram for (a) binary FSK transmitter and (b) coherent binary FSK receiver.\\ns2 0 Eb = s1 s2 - 2Eb Eb Binary data sequence On-off level encoder = m(t) m(t) Binary FSK signal s(t) 2/Tb cos(2f1t) = + + + - 2/Tb cos(2f2t) dt Decision device Threshold = 0 y x1 Choose 1 if y > 0 Choose 0 if y < 0 x(t) Inverter Tb dt x2 Tb 0\\x02 0\\x02   (a) (b) 1(t)  2(t)  1(t)  2(t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 397,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nand signal s2(t) with frequency f2 is transmitted. With phase continuity as a\\nrequirement, the two oscillators are synchronized with each other. Alternatively, we\\nmay use a voltage-controlled oscillator, in which case phase continuity is\\nautomatically satisfied.\\nTo coherently detect the original binary sequence given the noisy received signal x(t), we\\nmay use the receiver shown in Figure 7.26b. It consists of two correlators with a common\\ninput, which are supplied with locally generated coherent reference signals 1(t) and 2(t).\\nThe correlator outputs are then subtracted, one from the other; the resulting difference y is\\nthen compared with a threshold of zero. If y  0, the receiver decides in favor of 1. On the\\nother hand, if y  0, it decides in favor of 0. If y is exactly zero, the receiver makes a\\nrandom guess (i.e., flip of a fair coin) in favor of 1 or 0.\\nError Probability of Binary FSK\\nThe observation vector x has two elements x1 and x2 that are defined by, respectively,\\n(7.158) and (7.159) where x(t) is the received signal, whose form depends on which symbol was transmitted.\\nGiven that symbol 1 was transmitted, x(t) equals s1(t) + w(t), where w(t) is the sample\\nfunction of a white Gaussian noise process of zero mean and power spectral density N02.\\nIf, on the other hand, symbol 0 was transmitted, x(t) equals s2(t) + w(t).\\nNow, applying the decision rule of (7.57) assuming the use of coherent detection at the\\nreceiver, we find that the observation space is partitioned into two decision regions,\\nlabeled Z1 and Z2 in Figure 7.25. The decision boundary, separating region Z1 from region\\nZ2, is the perpendicular bisector of the line joining the two message points. The receiver\\ndecides in favor of symbol 1 if the received signal point represented by the observation\\nvector x falls inside region Z1. This occurs when x1  x2. If, on the other hand, we have',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 398,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'decides in favor of symbol 1 if the received signal point represented by the observation\\nvector x falls inside region Z1. This occurs when x1  x2. If, on the other hand, we have\\nx1 x2, the received signal point falls inside region Z2 and the receiver decides in favor of\\nsymbol 0. On the decision boundary, we have x1 = x2, in which case the receiver makes a\\nrandom guess in favor of symbol 1 or 0.\\nTo proceed further, we define a new Gaussian random variable Y whose sample value y\\nis equal to the difference between x1 and x2; that is,\\n(7.160)\\nThe mean value of the random variable Y depends on which binary symbol was\\ntransmitted. Given that symbol 1 was sent, the Gaussian random variables X1 and X2,\\nwhose sample values are denoted by x1 and x2, have mean values equal to and zero,\\nrespectively. Correspondingly, the conditional mean of the random variable Y given that\\nsymbol 1 was sent is (7.161) x1 x t1 t dt 0 Tb = x2 x t2 t dt 0 Tb = y x1 x2 - = Eb \\x03 Y 1   \\x03 X1 1   \\x03 X2 1   - = + Eb =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 398,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Frequency-Shift Keying Techniques Using Coherent Detection On the other hand, given that symbol 0 was sent, the random variables X1 and X2 have\\nmean values equal to zero and\\n, respectively. Correspondingly, the conditional mean\\nof the random variable Y given that symbol 0 was sent is\\n(7.162)\\nThe variance of the random variable Y is independent of which binary symbol was sent.\\nSince the random variables X1 and X2 are statistically independent, each with a variance\\nequal to N02, it follows that\\n(7.163)\\nSuppose we know that symbol 0 was sent. The conditional probability density function of\\nthe random variable Y is then given by\\n(7.164)\\nSince the condition x1  x2 or, equivalently, y  0 corresponds to the receiver making a\\ndecision in favor of symbol 1, we deduce that the conditional probability of error given\\nthat symbol 0 was sent is\\n(7.165)\\nTo put the integral in (7.165) in a standard form involving the Q-function, we set\\n(7.166)\\nThen, changing the variable of integration from y to z, we may rewrite (7.165) as\\n(7.167)\\nSimilarly, we may show the p01, the conditional probability of error given that symbol\\nwas sent, has the same value as in (7.167). Accordingly, averaging p10 and p01 and\\nassuming equiprobable symbols, we find that the average probability of bit error or,\\nequivalently, the BER for binary FSK using coherent detection is\\n(7.168) Eb \\x03 Y 0   \\x03 X1 0   \\x03 X2 0   - = Eb - = var Y  var X1   var X2   + = N0 = fY y 0   1 2N0 ----------------- y Eb +   2 2N0 ---------------------------\\n- exp = p10 \\x02 y 0 symbol 0 was sent    = fY y 0   dy 0   = 1 2N0 ----------------- y Eb +   2 2N0 ---------------------------\\n- exp dy 0   = y Eb + N0 ------------------- z = p10 1 2 ---------- z2 2---- -     exp dz Eb N0    = Q Eb N0 ------       = Pe Q Eb N0 ------       =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 399,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nComparing (7.108) and (7.168), we see that for a binary FSK receiver to maintain the\\nsame BER as in a binary PSK receiver, the bit energy-to-noise density ratio, EbN0, has to\\nbe doubled. This result is in perfect accord with the signal-space diagrams of Figures 7.13\\nand 7.25, where we see that in a binary PSK system the Euclidean distance between the\\ntwo message points is equal to\\n, whereas in a binary FSK system the corresponding\\ndistance is\\n. For a prescribed Eb, the minimum distance dmin in binary PSK is,\\ntherefore, times that in binary FSK. Recall from (7.89) that the probability of error\\ndecreases exponentially as\\n; hence the difference between (7.108) and (7.168).\\nPower Spectra of Binary FSK Signals\\nConsider the case of Sundes FSK, for which the two transmitted frequencies f1 and f2\\ndiffer by an amount equal to the bit rate 1Tb, and their arithmetic mean equals the nominal\\ncarrier frequency fc; as mentioned previously, phase continuity is always maintained,\\nincluding inter-bit switching times. We may express this special binary FSK signal as a\\nfrequency-modulated signal, defined by\\n(7.169)\\nUsing a well-known trigonometric identity, we may reformulate s(t) in the expanded form\\n(7.170)\\nIn the last line of (7.170), the plus sign corresponds to transmitting symbol 0 and the\\nminus sign corresponds to transmitting symbol 1. As before, we assume that the symbols\\nand 0 in the binary sequence applied to the modulator input are equally likely, and that the\\nsymbols transmitted in adjacent time slots are statistically independent. Then, based on the\\nrepresentation of (7.170), we may make two observations pertaining to the in-phase and\\nquadrature components of a binary FSK signal with continuous phase:\\nThe in-phase component is completely independent of the input binary wave. It\\nequals for all time t. The power spectral density of this\\ncomponent, therefore, consists of two delta functions at and weighted',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 400,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'equals for all time t. The power spectral density of this\\ncomponent, therefore, consists of two delta functions at and weighted\\nby the factor Eb2Tb, and occurring at f = 12Tb.\\nThe quadrature component is directly related to the input binary sequence. During\\nthe signaling interval 0  t  Tb, it equals -g(t) when we have symbol 1 and +g(t)\\nwhen we have symbol 0, with g(t) denoting a symbol-shaping function defined by\\n(7.171) 2 Eb 2Eb 2 dmin 2 s t 2Eb Tb --------- 2fct t Tb -----      0 t Tb   cos = s t 2Eb Tb --------- t Tb -----      2fct   2Eb Tb --------- t Tb -----      2fct   sin sin - cos cos = 2Eb Tb --------- t Tb -----     2fct   2Eb Tb --------- t Tb -----     2fct   sin sin  cos cos = 2Eb Tb  t Tb    cos t 1/2Tb  = g t 2Eb Tb --------- t T-----    , sin 0 t T  0, elsewhere      =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 400,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Frequency-Shift Keying Techniques Using Coherent Detection The energy spectral density of g(t) is defined by\\n(7.172)\\nThe power spectral density of the quadrature component equals\\n. It is also\\napparent that the in-phase and quadrature components of the binary FSK signal are\\nindependent of each other. Accordingly, the baseband power spectral density of Sundes\\nFSK signal equals the sum of the power spectral densities of these two components, as\\nshown by\\n(7.173)\\nFrom Chapter 4, we recall the following relationship between baseband modulated power\\nspectra:\\n(7.174)\\nwhere fc is the carrier frequency. Therefore, substituting (7.173) into (7.174), we find that\\nthe power spectrum of the binary FSK signal contains two discrete frequency components,\\none located at (fc + 12Tb) = f1 and the other located at (fc - 12Tb) = f2, with their average\\npowers adding up to one-half the total power of the binary FSK signal. The presence of\\nthese two discrete frequency components serves a useful purpose: it provides a practical\\nbasis for synchronizing the receiver with the transmitter.\\nExamining (7.173), we may make the following statement:\\nThe baseband power spectral density of a binary FSK signal with continuous\\nphase ultimately falls off as the inverse fourth power of frequency.\\nIn Figure 7.15, we plotted the baseband power spectra of (7.111) and (7.173). (To simplify\\nmatters, we have only plotted the results for positive frequencies.) In both cases, SB(f) is\\nshown normalized with respect to 2Eb, and the frequency is normalized with respect to the\\nbit rate Rb = 1Tb. The difference in the falloff rates of these spectra can be explained on\\nthe basis of the pulse shape g(t). The smoother the pulse, the faster the drop of spectral\\ntails to zero. Thus, since binary FSK with continuous phase has a smoother pulse shape, it\\nhas lower sidelobes than binary PSK does.\\nSuppose, next, the FSK signal exhibits phase discontinuity at the inter-bit switching',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 401,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'has lower sidelobes than binary PSK does.\\nSuppose, next, the FSK signal exhibits phase discontinuity at the inter-bit switching\\ninstants, which arises when the two oscillators supplying the basis functions with\\nfrequencies f1 and f2 operate independently of each other. In this discontinuous scenario,\\nwe find that power spectral density ultimately falls off as the inverse square of frequency.\\nAccordingly, we may state:\\nA binary FSK signal with continuous phase does not produce as much\\ninterference outside the signal band of interest as a corresponding FSK signal\\nwith discontinuous phase does.\\ng f 8EbTb Tbf   2 cos 2 4Tb 2f 2 1 -   2 --------------------------------------------\\n= g fTb  SB f Eb 2Tb --------- f 1 2Tb --------- -     f 1 2Tb --------- +     + 8EbTb Tb f   2 cos 2 4Tb 2f 2 1 -   2 --------------------------------------------\\n+ = SS f 1 4--- SB f fc -   SB f fc +   +   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 401,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nThe important point to take from this statement is summed up as follows: when\\ninterference is an issue of practical concern, continuous FSK is preferred over its\\ndiscontinuous counterpart. However, this advantage of continuous FSK is gained at the\\nexpense of increased system complexity.\\nMinimum Shift Keying\\nIn the coherent detection of binary FSK signal, the phase information contained in the\\nreceived signal is not fully exploited, other than to provide for synchronization of the\\nreceiver to the transmitter. We now show that by proper use of the continuous-phase\\nproperty when performing detection it is possible to improve the noise performance of the\\nreceiver significantly. Here again, this improvement is achieved at the expense of\\nincreased system complexity.\\nConsider a continuous-phase frequency-shift keying (CPFSK) signal, which is defined\\nfor the signaling interval 0  t  Tb as follows:\\n(7.175)\\nwhere Eb is the transmitted signal energy per bit and Tb is the bit duration. The defining\\nequation (7.175) distinguishes itself from that of (7.151) in using the phase (0). This new\\nterm, denoting the value of the phase at time t = 0, sums up the past history of the FM\\nprocess up to time t = 0. The frequencies f1 and f2 are sent in response to binary symbols\\nand 0, respectively, applied to the modulator input.\\nAnother useful way of representing the CPFSK signal s(t) is to express it as a\\nconventional angle-modulated signal:\\n(7.176)\\nwhere (t) is the phase of s(t) at time t. When the phase (t) is a continuous function of\\ntime, we find that the modulated signal s(t) is itself also continuous at all times, including\\nthe inter-bit switching times. The phase (t) of a CPFSK signal increases or decreases\\nlinearly with time during each bit duration of Tb seconds, as shown by\\n(7.177)\\nwhere the plus sign corresponds to sending symbol 1 and the minus sign corresponds to',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 402,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'linearly with time during each bit duration of Tb seconds, as shown by\\n(7.177)\\nwhere the plus sign corresponds to sending symbol 1 and the minus sign corresponds to\\nsending symbol 0; the dimensionless parameter h is to be defined. Substituting (7.177)\\ninto (7.176), and then comparing the angle of the cosine function with that of (7.175), we\\ndeduce the following pair of relations:\\n(7.178) s t 2Eb Tb --------- 2f1t 0  +   cos for symbol 1 2Eb Tb --------- 2f2t 0  +   cos for symbol 0          = s t 2Eb Tb --------- 2fct t +   cos = t 0  h Tb ------    t 0 t Tb    = fc h 2Tb --------- + f1 =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 402,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Frequency-Shift Keying Techniques Using Coherent Detection (7.179)\\nSolving this pair of equations for fc and h, we get\\n(7.180) and (7.181) The nominal carrier frequency fc is, therefore, the arithmetic mean of the transmitted\\nfrequencies f1 and f2. The difference between the frequencies f1 and f2, normalized with\\nrespect to the bit rate 1Tb, defines the dimensionless parameter h, which is referred to as\\nthe deviation ratio.\\nPhase Trellis\\nFrom (7.177) we find that, at time t = Tb,\\n(7.182)\\nThat is to say, sending symbol 1 increases the phase of a CPFSK signal s(t) by h radians,\\nwhereas sending symbol 0 reduces it by an equal amount.\\nThe variation of phase (t) with time t follows a path consisting of a sequence of\\nstraight lines, the slopes of which represent frequency changes. Figure 7.27 depicts\\npossible paths starting from t = 0. A plot like that shown in this figure is called a phase\\ntree. The tree makes clear the transitions of phase across successive signaling intervals.\\nMoreover, it is evident from the figure that the phase of a CPFSK signal is an odd or even\\nmultiple of h radians at odd or even multiples of the bit duration Tb, respectively.\\nFigure 7.27 Phase tree.\\nfc h 2Tb --------- - f2 = fc 1 2--- f1 f2 +   = h Tb f1 f2 -   = Tb   0  - h for symbol 1 h - for symbol 0    = 0 - h -2 h -3 h -4 h  h 4 h 3 h 2 h 2Tb 4Tb 6Tb 8Tb t (t) - (0), radians',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 403,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nThe phase tree described in Figure 7.27 is a manifestation of phase continuity, which is\\nan inherent characteristic of a CPFSK signal. To appreciate the notion of phase continuity,\\nlet us go back for a moment to Sundes FSK, which is also a CPFSK signal as previously\\ndescribed. In this case, the deviation ratio h is exactly unity. Hence, according to Figure\\n27, the phase change over one bit interval is  radians. But, a change of + radians is\\nexactly the same as a change of - radians, modulo 2. It follows, therefore, that in the\\ncase of Sundes FSK there is no memory; that is, knowing which particular change\\noccurred in the previous signaling interval provides no help in the current signaling\\ninterval.\\nIn contrast, we have a completely different situation when the deviation ratio h is\\nassigned the special value of 12. We now find that the phase can take on only the two\\nvalues 2 at odd multiples of Tb, and only the two values 0 and  at even multiples of Tb,\\nas in Figure 7.28. This second graph is called a phase trellis, since a trellis is a treelike\\nstructure with re-emerging branches. Each path from left to right through the trellis of\\nFigure 7.28 corresponds to a specific binary sequence at the transmitter input. For\\nexample, the path shown in boldface in Figure 7.28 corresponds to the binary sequence\\n1101000 with (0) = 0. Henceforth, we focus on h = 12.\\nWith h = 12, we find from (7.181) that the frequency deviation (i.e., the difference\\nbetween the two signaling frequencies f1 and f2) equals half the bit rate; hence the\\nfollowing statement:\\nThe frequency deviation h = 1/2 is the minimum frequency spacing that allows\\nthe two FSK signals representing symbols 1 and 0 to be coherently orthogonal.\\nIn other words, symbols 1 and 0 do not interfere with one another in the process of\\ndetection. It is for this reason that a CPFSK signal with a deviation ratio of one-half is\\ncommonly referred to as minimum shift-keying (MSK).5',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 404,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'detection. It is for this reason that a CPFSK signal with a deviation ratio of one-half is\\ncommonly referred to as minimum shift-keying (MSK).5\\nSignal-Space Diagram of MSK\\nUsing a well-known trigonometric identity in (7.176), we may expand the CPFSK signal\\ns(t) in terms of its in-phase and quadrature components as\\n(7.183)\\nFigure 7.28 Phase trellis; boldfaced path represents the sequence 1101000.\\ns t 2Eb Tb --------- t 2fct   2Eb Tb --------- t 2fct   sin sin - cos cos = 0 - /2 -/2  2Tb 4Tb 6Tb 8Tb t (t) - (0), radians',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 404,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Frequency-Shift Keying Techniques Using Coherent Detection Consider, first, the in-phase component\\n. With the deviation ratio h = 12,\\nwe have from (7.177) that\\n(7.184)\\nwhere the plus sign corresponds to symbol 1 and the minus sign corresponds to symbol 0.\\nA similar result holds for (t) in the interval -Tb  t  0, except that the algebraic sign is\\nnot necessarily the same in both intervals. Since the phase (0) is 0 or  depending on the\\npast history of the modulation process, we find that in the interval -Tb  t  Tb, the polarity\\nof cos(t) depends only on (0), regardless of the sequence of 1s and 0s transmitted\\nbefore or after t = 0. Thus, for this time interval, the in-phase component consists of the\\nhalf-cycle cosine pulse:\\n(7.185)\\nwhere the plus sign corresponds to (0) = 0 and the minus sign corresponds to (0) = . In\\na similar way, we may show that, in the interval 0  t  2Tb, the quadrature component of\\ns(t) consists of the half-cycle sine pulse:\\n(7.186)\\nwhere the plus sign corresponds to (Tb) = 2 and the minus sign corresponds to\\n(Tb) = -2. From the discussion just presented, we see that the in-phase and quadrature\\ncomponents of the MSK signal differ from each other in two important respects:\\n\\nthey are in phase quadrature with respect to each other and\\n\\nthe polarity of the in-phase component sI(t) depends on (0), whereas the polarity of\\nthe quadrature component sQ(t) depends on (Tb).\\nMoreover, since the phase states (0) and (Tb) can each assume only one of two possible\\nvalues, any one of the following four possibilities can arise:\\n(0) = 0 and (Tb) = /2, which occur when sending symbol 1.\\n(0) =  and (Tb) = /2, which occur when sending symbol 0.\\n2Eb Tb  t cos t 0   2Tb --------- 0 t Tb    = sI t 2Eb Tb --------- t cos = 2Eb Tb --------- 0   2Tb ---------t     cos cos = 2Eb Tb ---------  2Tb ---------t     Tb - t Tb   cos  = sQ t 2Eb Tb --------- t sin = 2Eb Tb --------- Tb    2Tb ---------t     sin sin = 2Eb Tb ---------  2Tb ---------t     0 t 2Tb   sin  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 405,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\n(0) =  and (Tb) = -/2 (or, equivalently, 3/2 modulo 2), which occur when\\nsending symbol 1.\\n(0) = 0 and (Tb) = -/2, which occur when sending symbol 0.\\nThis fourfold scenario, in turn, means that the MSK signal itself can assume one of four\\npossible forms, depending on the values of the phase-state pair: (0) and (Tb).\\nSignal-Space Diagram\\nExamining the expansion of (7.183), we see that there are two orthonormal basis functions\\n1(t) and 2(t) characterizing the generation of MSK; they are defined by the following\\npair of sinusoidally modulated quadrature carriers:\\n(7.187)\\n(7.188)\\nWith the formulation of a signal-space diagram in mind, we rewrite (7.183) in the compact\\nform\\n(7.189)\\nwhere the coefficients s1 and s2 are related to the phase states (0) and (Tb), respectively.\\nTo evaluate s1, we integrate the product s(t)1(t) with respect to time t between the limits -Tb\\nand Tb, obtaining\\n(7.190)\\nSimilarly, to evaluate s2 we integrate the product s(t)2(t) with respect to time t between\\nthe limits 0 and 2Tb, obtaining\\n(7.191)\\nExamining (7.190) and (7.191), we now make three observations:\\nBoth integrals are evaluated for a time interval equal to twice the bit duration.\\nThe lower and upper limits of the integral in (7.190) used to evaluate s1 are shifted\\nby the bit duration Tb with respect to those used to evaluate s2.\\nThe time interval 0  t  Tb, for which the phase states (0) and (Tb) are defined, is\\ncommon to both integrals.\\nIt follows, therefore, that the signal constellation for an MSK signal is two-dimensional\\n(i.e., N = 2), with four possible message points (i.e., M = 4), as illustrated in the signal-\\nspace diagram of Figure 7.29. Moving in a counterclockwise direction, the coordinates of\\nthe message points are as follows:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 406,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'space diagram of Figure 7.29. Moving in a counterclockwise direction, the coordinates of\\nthe message points are as follows:\\n1 t 2 Tb -----  2Tb ---------t     2fct   0 t Tb   cos cos = 2 t 2 Tb -----  2Tb ---------t     2fct   0 t Tb   sin sin = s t s11 t s22 t 0 t Tb   + = s1 s t1 t dt Tb - Tb = Eb 0    Tb - t Tb   cos = s2 s t2 t dt 0 2Tb  = Eb Tb     0 t Tb   sin =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 406,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Frequency-Shift Keying Techniques Using Coherent Detection\\n387 , , and . The possible values of (0) and (Tb), corresponding to these four message points, are\\nalso included in Figure 7.29. The signal-space diagram of MSK is thus similar to that of\\nQPSK in that both of them have four message points in a two-dimensional space.\\nHowever, they differ in a subtle way that should be carefully noted:\\n\\nQPSK, moving from one message point to an adjacent one, is produced by sending a\\ntwo-bit symbol (i.e., dibit).\\n\\nMSK, on the other hand, moving from one message point to an adjacent one, is\\nproduced by sending a binary symbol, 0 or 1. However, each symbol shows up in\\ntwo opposite quadrants, depending on the value of the phase-pair: (0) and (Tb).\\nTable 7.4 presents a summary of the values of (0) and (Tb), as well as the corresponding\\nvalues of s1 and s2 that are calculated for the time intervals -Tb  t  Tb and 0  t  2Tb,\\nrespectively. The first column of this table indicates whether symbol 1 or symbol 0 was\\nsent in the interval 0  t  Tb. Note that the coordinates of the message points, s1 and s2,\\nhave opposite signs when symbol 1 is sent in this interval, but the same sign when symbol\\n0 is sent. Accordingly, for a given input data sequence, we may use the entries of Table 7.4\\nto derive on a bit-by-bit basis the two sequences of coefficients required to scale 1(t) and\\n2(t), and thereby determine the MSK signal s(t).\\nFigure 7.29 Signal-space diagram for MSK system.\\n+ Eb,+ Eb   Eb - ,+ Eb   - Eb,- Eb    + Eb,- Eb   [ (0) = , (Tb) = /2]\\nMessage point m2: Symbol\\n[ (0) = , (Tb) = - /2]\\nMessage point m1: Symbol\\n[ (0) = 0, (Tb) = - /2]\\nMessage point m3: Symbol\\nMessage point m4: Symbol\\n[ (0) = 0, (Tb) = /2]\\nRegion Z3 Region Z4 Region Z2 Region Z1 Decision boundary Eb - Decision boundary Eb Eb Eb - 2  1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 407,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nEXAMPLE\\nMSK Waveforms\\nFigure 7.30 shows the sequences and waveforms involved in the generation of an MSK\\nsignal for the binary sequence 1101000. The input binary sequence is shown in Figure 7.30a.\\nThe two modulation frequencies are f1 = 5/4Tb and f2 = 3/4Tb. Assuming that at time t =\\nTable 7.4 Signal-space characterization of MSK\\nTransmitted binary symbol, 0  t  Tb\\nPhase states (rad)\\nCoordinates of message points\\n(0) (Tb) s1 s2 0 0 -/2 1 -/2 0 +/2 1 0 +/2 + Eb + Eb  Eb - + Eb  Eb - Eb - + Eb Eb - Figure 7.30 (a) Input binary sequence.\\n(b) Waveform of scaled time function\\ns11(t). (c) Waveform of scaled time\\nfunction s22(t). (d) Waveform of the\\nMSK signal s(t) obtained by adding\\ns11(t) and s22(t) on a bit-by-bit basis.\\n0 2Tb 4Tb 6Tb 1 1 0 1 0 0 0 Input binary sequence\\nTime scale (kTb) Polarity of s1 s1 1(t)  0 + - - 0 + s(t)  (kTb) Polarity of s2 s2 2(t)  /2 - - - +  /2 /2 /2 - ttt (a) (b) (c) (d)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 408,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '8 Frequency-Shift Keying Techniques Using Coherent Detection the phase (0) is zero, the sequence of phase states is as shown in Figure 7.30, modulo 2.\\nThe polarities of the two sequences of factors used to scale the time functions 1(t) and 2(t)\\nare shown in the top lines of Figure 7.30b and c. These two sequences are offset relative to\\neach other by an interval equal to the bit duration Tb. The waveforms of the resulting two\\ncomponents of s(t), namely, s11(t) and s22(t), are shown in Figure 7.30b and c. Adding\\nthese two modulated waveforms, we get the desired MSK signal s(t) shown in Figure 7.30d.\\nGeneration and Coherent Detection of MSK Signals\\nWith h = 1/2, we may use the block diagram of Figure 7.31a to generate the MSK signal.\\nThe advantage of this method of generating MSK signals is that the signal coherence and\\ndeviation ratio are largely unaffected by variations in the input data rate. Two input sinu-\\nsoidal waves, one of frequency fc = nc4Tb for some fixed integer nc and the other of\\nfrequency 14Tb, are first applied to a product modulator. This modulator produces two\\nphase-coherent sinusoidal waves at frequencies f1 and f2, which are related to the carrier\\nFigure 7.31 Block diagrams for (a) MSK transmitter and (b) coherent MSK receiver.\\nsscos (2fct) cos BPF (f1) + + + + + MSK signal s(t) (a) (b) a2(t) a1(t) - BPF (f2) Narrowband filters t x1 Threshold = 0 In-phase channel Phase estimate (0) Decision device dt Input x(t) -Tb Tb x2 Threshold = 0 Quadrature channel Phase estimate (Tb) Decision device dt 0 2Tb Logic circuit for interleaving phase decisions Output binary sequence \\x02 \\x02    2Tb 1(t)  2(t)  1(t)  2(t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 409,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nfrequency fc and the bit rate 1/Tb in accordance with (7.178) and (7.179) for deviation ratio\\nh = 12. These two sinusoidal waves are separated from each other by two narrowband fil-\\nters, one centered at f1 and the other at f2. The resulting filter outputs are next linearly\\ncombined to produce the pair of quadrature carriers or orthonormal basis functions 1(t)\\nand 2(t). Finally, 1(t) and 2(t) are multiplied with two binary waves a1(t) and a2(t), both\\nof which have a bit rate equal to 1/(2Tb). These two binary waves are extracted from the\\nincoming binary sequence in the manner described in Example 7.\\nFigure 7.31b shows the block diagram of the coherent MSK receiver. The received\\nsignal x(t) is correlated with 1(t) and 2(t). In both cases, the integration interval is 2Tb\\nseconds, and the integration in the quadrature channel is delayed by Tb seconds with respect\\nto that in the in-phase channel. The resulting in-phase and quadrature channel correlator\\noutputs, x1 and x2, are each compared with a threshold of zero; estimates of the phase (0)\\nand (Tb) are then derived in the manner described previously. Finally, these phase\\ndecisions are interleaved so as to estimate the original binary sequence at the transmitter\\ninput with the minimum average probability of symbol error in an AWGN channel.\\nError Probability of MSK\\nIn the case of an AWGN channel, the received signal is given by\\nwhere s(t) is the transmitted MSK signal and w(t) is the sample function of a white\\nGaussian noise process of zero mean and power spectral density N0/2. To decide whether\\nsymbol 1 or symbol 0 was sent in the interval 0  t  Tb, say, we have to establish a\\nprocedure for the use of x(t) to detect the phase states (0) and (Tb).\\nFor the optimum detection of (0), we project the received signal x(t) onto the\\nreference signal over the interval -Tb  t  Tb, obtaining\\n(7.192)\\nwhere s1 is as defined by (7.190) and w1 is the sample value of a Gaussian random',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 410,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'reference signal over the interval -Tb  t  Tb, obtaining\\n(7.192)\\nwhere s1 is as defined by (7.190) and w1 is the sample value of a Gaussian random\\nvariable of zero mean and variance N0/2. From the signal-space diagram of Figure 7.29,\\nwe see that if x1 > 0, the receiver chooses the estimate\\n. On the other hand, if\\nx1 0, it chooses the estimate\\n.\\nSimilarly, for the optimum detection of (Tb), we project the received signal x(t) onto\\nthe second reference signal 2(t) over the interval 0  t  2Tb, obtaining\\n(7.193)\\nwhere s2 is as defined by (7.191) and w2 is the sample value of another independent\\nGaussian random variable of zero mean and variance N02. Referring again to the signal-\\nspace diagram of Figure 7.29, we see that if x2  0, the receiver chooses the estimate\\n. If, however,\\n, the receiver chooses the estimate\\n. x t s t w t + = 1 t x1 x t1 t dt Tb - Tb = s1 w1 + =  0  0 =  0   = x2 x t2 t dt 0 2Tb  = s2 w2 0 t 2Tb   + = Tb   2  - = x2 0   Tb   2  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 410,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Frequency-Shift Keying Techniques Using Coherent Detection To reconstruct the original binary sequence, we interleave the above two sets of phase\\nestimates in accordance with Table 7.4, by proceeding as follows:\\n If estimates and , or alternatively if\\nand\\n, then the receiver decides in favor of symbol 0.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 411,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'If, on the other hand, the estimates and , or alternatively if and , then the receiver decides in favor of symbol 1.\\nMost importantly, examining the signal-space diagram of Figure 7.29, we see that the\\ncoordinates of the four message points characterizing the MSK signal are identical to those\\nof the QPSK signal in Figure 7.16. Moreover, the zero-mean noise variables in (7.192) and\\n(7.193) have exactly the same variance as those for the QPSK signal in (7.118) and (7.119).\\nIt follows, therefore, that the BER for the coherent detection of MSK signals is given by\\n(7.194)\\nwhich is the same as that of QPSK in (7.126). In both MSK and QPSK, this good\\nperformance is the result of coherent detection being performed in the receiver on the\\nbasis of observations over 2Tb seconds.\\nPower Spectra of MSK Signals\\nAs with the binary FSK signal, we assume that the input binary wave is random, with\\nsymbols 1 and 0 being equally likely and the symbols sent during adjacent time slots being\\nstatistically independent. Under these assumptions, we make three observations:\\nDepending on the value of phase state (0), the in-phase component equals +g(t) or\\n-g(t), where the pulse-shaping function\\n(7.195)\\nThe energy spectral density of g(t) is\\n(7.196)\\nThe power spectral density of the in-phase component equals\\n.\\nDepending on the value of the phase state (Tb), the quadrature component equals\\n+g(t) or -g(t), where we now have\\n(7.197)\\nDespite the difference in which the time interval over two adjacent time slots is\\ndefined in (7.195) and (7.197), we get the same energy spectral density as in (7.196).\\n 0  0 =  0  Tb   2  - = =  0   =  Tb   2  - =  0   =  Tb   2  - =  0  0 =  Tb   2  = Pe Q 2Eb N0 ---------       = g t 2Eb Tb --------- t 2Tb ---------    , cos Tb t Tb  - 0, otherwise      = g f 32EbTb 2 ------------------- 2Tbf   cos 16Tb 2 f2 1 - ----------------------------\\n2 = g f2Tb  g t 2Eb Tb --------- t 2Tb ---------    , sin 0 t 2Tb  - 0, otherwise      =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 411,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nHence, the in-phase and quadrature components have the same power spectral\\ndensity.\\nThe in-phase and quadrature components of the MSK signal are statistically\\nindependent; it follows that the baseband power spectral density of s(t) is given by\\n(7.198)\\nA plot of the baseband power spectrum of (7.198) is included in Figure 7.19, where the\\npower spectrum is normalized with respect to 4Eb and the frequency f is normalized with\\nrespect to the bit rate 1Tb. Figure 7.19 also includes the corresponding plot of (7.128) for\\nthe QPSK signal. As stated previously, for f 1/Tb the baseband power spectral density of\\nthe MSK signal falls off as the inverse fourth power of frequency, whereas in the case of\\nthe QPSK signal it falls off as the inverse square of frequency. Accordingly, MSK does not\\nproduce as much interference outside the signal band of interest as QPSK does. This is a\\ndesirable characteristic of MSK, especially when the digital communication system\\noperates with a bandwidth limitation in an interfering environment.\\nGaussian-Filtered MSK\\nFrom the detailed study of MSK just presented, we may summarize its desirable\\nproperties:\\n\\nmodulated signal with constant envelope;\\n\\nrelatively narrow-bandwidth occupancy;',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 412,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'modulated signal with constant envelope;\\n\\nrelatively narrow-bandwidth occupancy;\\n\\ncoherent detection performance equivalent to that of QPSK.\\nHowever, the out-of-band spectral characteristics of MSK signals, as good as they are, still\\ndo not satisfy the stringent requirements of certain applications such as wireless communi-\\ncations. To illustrate this limitation, we find from (7.198) that, at Tb f = 0.5, the baseband\\npower spectral density of the MSK signal drops by only 10 log109 = 9.54 dB below its mid-\\nband value. Hence, when the MSK signal is assigned a transmission bandwidth of 1Tb, the\\nadjacent channel interference of a wireless-communication system using MSK is not low\\nenough to satisfy the practical requirements of a multiuser-communications environment.\\nRecognizing that the MSK signal can be generated by direct FM of a voltage-controlled\\noscillator, we may overcome this practical limitation of MSK by modifying its power\\nspectrum into a more compact form while maintaining the constant-envelope property of\\nthe MSK signal. This modification can be achieved through the use of a premodulation\\nlow-pass filter, hereafter referred to as a baseband pulse-shaping filter. Desirably, the\\npulse-shaping filter should satisfy the following three conditions:\\n\\nfrequency response with narrow bandwidth and sharp cutoff characteristics;\\n\\nimpulse response with relatively low overshoot; and\\n\\nevolution of a phase trellis with the carrier phase of the modulated signal assuming\\nthe two values /2 at odd multiples of the bit duration Tb and the two values 0 and\\n at even multiples of Tb as in MSK.\\nSB f 2 g f 2Tb --------------     = 32Eb 2 ------------- 2Tbf   cos 16Tb 2 f 2 1 - ---------------------------- =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 412,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Frequency-Shift Keying Techniques Using Coherent Detection The frequency-response condition is needed to suppress the high-frequency components\\nof the modified frequency-modulated signal. The impulse-response condition avoids\\nexcessive deviations in the instantaneous frequency of the modified frequency-modulated\\nsignal. Finally, the condition imposed on phase-trellis evolution ensures that the modified\\nfrequency-modulated signal can be coherently detected in the same way as the MSK\\nsignal, or it can be noncoherently detected as a simple binary FSK signal if so desired.\\nThese three conditions can be satisfied by passing an NRZ-level-encoded binary data\\nstream through a baseband pulse-shaping filter whose impulse response (and, likewise, its\\nfrequency response) is defined by a Gaussian function. The resulting method of binary\\nFM is naturally referred to as Gaussian-filtered minimum-shift keying (GMSK).6\\nLet W denote the 3 dB baseband bandwidth of the pulse-shaping filter. We may then\\ndefine the transfer function H(f) and impulse response h(t) of the pulse-shaping filter as:\\n(7.199) and (7.200) where ln denotes the natural algorithm. The response of this Gaussian filter to a\\nrectangular pulse of unit amplitude and duration Tb, centered on the origin, is given by\\n(7.201)\\nThe pulse response g(t) in (7.201) provides the basis for building the GMSK modulator, with\\nthe dimensionless time-bandwidth product WTb playing the role of a design parameter.\\nH f 2 ln 2 -------- f W -----    2 - exp = h t 2 2 ln --------W 22 2 ln ---------W2t2 -     exp = g t h t  -   d Tb 2  - Tb 2   = 2 2 ln --------W 22 2 ln ---------W2 t  -  2 - d exp Tb 2  - Tb 2   = Figure 7.32 Frequency-shaping pulse g(t) of (7.201)\\nshifted in time by 2.5Tb and truncated\\nat 2.5Tb for varying time-bandwidth\\nproduct WTb.\\nNormalized time, t/Tb\\n0 Amplitude -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 WTb = 0.2 WTb = 0.25 WTb = 0.3',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 413,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nUnfortunately, the pulse response g(t) is noncausal and, therefore, not physically\\nrealizable for real-time operation. Specifically, g(t) is nonzero for t  -Tb2, where t = -Tb2\\nis the time at which the input rectangular pulse (symmetrically positioned around the origin)\\nis applied to the Gaussian filter. For a causal response, g(t) must be truncated and shifted in\\ntime. Figure 7.32 presents plots of g(t), which has been truncated at t = 2.5Tb and then\\nshifted in time by 2.5Tb. The plots shown here are for three different settings: WTb = 0.2,\\n25, and 0.3. Note that as WTb is reduced, the time spread of the frequency-shaping pulse is\\ncorrespondingly increased.\\nFigure 7.33 shows the machine-computed power spectra of MSK signals (expressed in\\ndecibels) versus the normalized frequency difference (f - fc)Tb, where fc is the mid-band\\nfrequency and Tb is the bit duration.7 The results plotted in Figure 7.33 are for varying\\nvalues of the time-bandwidth product WTb. From this figure we may make the following\\nobservations:\\n\\nThe curve for the limiting condition WTb corresponds to the case of ordinary\\nMSK.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 414,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The curve for the limiting condition WTb corresponds to the case of ordinary\\nMSK.\\n\\nWhen WTb is less than unity, increasingly more of the transmit power is\\nconcentrated inside the passband of the GMSK signal.\\nAn undesirable feature of GMSK is that the processing of NRZ binary data by a Gaussian\\nfilter generates a modulating signal that is no longer confined to a single bit interval as in\\nordinary MSK, which is readily apparent from Figure 7.33. Stated in another way, the tails\\nof the Gaussian impulse response of the pulse-shaping filter cause the modulating signal to\\nspread out to adjust symbol intervals. The net result is the generation of intersymbol\\ninterference, the extent of which increases with decreasing WTb. In light of this discussion\\nand the various plots presented in Figure 7.33, we find that the value assigned to the time-\\nbandwidth product WTb offers a tradeoff between spectral compactness and system-\\nperformance loss.\\nFigure 7.33 Power spectra of MSK and GMSK signals for varying\\ntime-bandwidth product.\\n = 0.0 0.5 1.0 1.5 Normalized frequency, |f - fc|Tb Power spectral density (dB)\\n-60 -40 -50 -20 -30 MSK WTb = 0.2 WTb = 0.25 WTb = 0.3 0 -10',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 414,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Frequency-Shift Keying Techniques Using Coherent Detection To explore the issue of performance degradation resulting from the use of GMSK\\ncompared with MSK, consider the coherent detection in the presence of AWGN.\\nRecognizing that GMSK is a special kind of binary FM, we may express its average\\nprobability of symbol error Pe by the empirical formula\\n(7.202)\\nwhere, as before, Eb is the signal energy per bit and N0/2 is the noise spectral density. The\\nfactor  is a constant whose value depends on the time-bandwidth product WTb. Comparing\\n(7.202) for GMSK with (7.194) for ordinary MSK, we may view 10 log10(2), expressed in\\ndecibels, as a measure of performance degradation of GMSK compared with ordinary MSK.\\nFigure 7.34 shows the machine-computed value of 10 log10(2) versus WTb. For ordinary\\nMSK we have\\n, in which case (7.202) with  = 2 assumes exactly the same form\\nas (7.194) and there is no degradation in performance, which is confirmed by Figure 7.34.\\nFor GMSK with WTb = 0.3 we find from Figure 7.34 that there is a degradation in\\nperformance of about 0.46dB, which corresponds to 2 = 0.9. This degradation in\\nperformance is a small price to pay for the highly desirable spectral compactness of the\\nGMSK signal.\\nM-ary FSK\\nConsider next the M-ary version of FSK, for which the transmitted signals are defined by\\n(7.203)\\nwhere i = 1, 2, , M, and the carrier frequency fc = nc/(2T) for some fixed integer nc. The\\ntransmitted symbols are of equal duration T and have equal energy E. Since the individual\\nFigure 7.34 Theoretical EbN0 degradation of GMSK for varying\\ntime-bandwidth product.\\n0 0.2 Degradation (dB) 0.4 Time-bandwidth product, WTb\\n6 0.8 0 1 2 3 Pe Q Eb N0 ----------       = WTb  = si t 2E T -------  T--- nc i +  t 0 t T   cos =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 415,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nsignal frequencies are separated by 1/(2T) Hz, the M-ary FSK signals in (7.203) constitute\\nan orthogonal set; that is,\\n(7.204)\\nHence, we may use the transmitted signals si(t) themselves, except for energy\\nnormalization, as a complete orthonormal set of basis functions, as shown by\\n(7.205)\\nAccordingly, the M-ary FSK is described by an M-dimensional signal-space diagram.\\nFor the coherent detection of M-ary FSK signals, the optimum receiver consists of a\\nbank of M correlators or matched filters, with i(t) of (7.205) providing the basis\\nfunctions. At the sampling times t = kT, the receiver makes decisions based on the largest\\nmatched filter output in accordance with the maximum likelihood decoding rule. An exact\\nformula for the probability of symbol error is, however, difficult to derive for a coherent\\nM-ary FSK system. Nevertheless, we may use the union bound of (7.88) to place an upper\\nbound on the average probability of symbol error for M-ary FSK. Specifically, since the\\nminimum distance dmin in M-ary FSK is\\n, using (7.87) we get (assuming\\nequiprobable symbols)\\n(7.206)\\nFor fixed M, this bound becomes increasingly tight as the ratio EN0 is increased. Indeed,\\nit becomes a good approximation to Pe for values of Pe  10-3. Moreover, for M = 2 (i.e.,\\nbinary FSK), the bound of (7.202) becomes an equality; see (7.168).\\nPower Spectra of M-ary FSK Signals\\nThe spectral analysis of M-ary FSK signals8 is much more complicated than that of M-ary\\nPSK signals. A case of particular interest occurs when the frequencies assigned to the\\nmultilevels make the frequency spacing uniform and the frequency deviation h = 12. That\\nis, the M signal frequencies are separated by 1/2T, where T is the symbol duration. For\\nh = 12, the baseband power spectral density of M-ary FSK signals is plotted in Figure\\n35 for M = 2, 4, 8.\\nBandwidth Efficiency of M-ary FSK Signals\\nWhen the orthogonal signals of an M-ary FSK signal are detected coherently, the adjacent',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 416,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '35 for M = 2, 4, 8.\\nBandwidth Efficiency of M-ary FSK Signals\\nWhen the orthogonal signals of an M-ary FSK signal are detected coherently, the adjacent\\nsignals need only be separated from each other by a frequency difference 12T so as to\\nmaintain orthogonality. Hence, we may define the channel bandwidth required to transmit\\nM-ary FSK signals as\\n(7.207)\\nFor multilevels with frequency assignments that make the frequency spacing uniform and\\nequal to 1/2T, the bandwidth B of (7.207) contains a large fraction of the signal power.\\nsi tsj t dt 0 T  0 i j   = i t 1 E -------si t for 0 t T  and i 1 2 M   = = 2E Pe M 1 -  Q E N0 ------      B M 2T ------ =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 416,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Frequency-Shift Keying Techniques Using Coherent Detection This is readily confirmed by looking at the baseband power spectral plots shown in Figure\\nFrom (7.133) we recall that the symbol period T is equal to\\n. Hence, using\\n, we may redefine the channel bandwidth B for M-ary FSK signals as (7.208)\\nThe bandwidth efficiency of M-ary signals is therefore (7.209)\\nTable 7.5 gives the values of calculated from (7.207) for varying M.\\nComparing Tables 7.3 and 7.5, we see that increasing the number of levels M tends to\\nincrease the bandwidth efficiency of M-ary PSK signals, but it also tends to decrease the\\nbandwidth efficiency of M-ary FSK signals. In other words, M-ary PSK signals are\\nspectrally efficient, whereas M-ary FSK signals are spectrally inefficient.\\nFigure 7.35 Power spectra of M-ary PSK signals for M = 2, 4, 8.\\nTable 7.5 Bandwidth efficiency of M-ary FSK signals\\nM 2 4 8 16 32 64  (bits/(sHz)) 1 1 0.75 0.5 0.3125 0.1875 0 0.5 M = 2 M = 4 M = 8 1.0 Normalized frequency, f Tb\\nNormalized power spectral density, SB( f )/4Eb\\n5 3.0 2.5 2.0 0.5 1.0 Tb 2 M log Rb 1 Tb  = B RbM 2 log2 M ----------------------\\n=  Rb B ------ = 2 log2 M M -------------------- =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 417,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels Comparison of M-ary PSK and M-ary FSK from an\\nInformation-Theoretic Viewpoint\\nBandwidth efficiency, as just discussed, provides one way of contrasting the capabilities of\\nM-ary PSK and M-ary FSK. Another way of contrasting the capabilities of these two\\ngeneralized digital modulation schemes is to look at the bandwidth-power tradeoff viewed\\nin light of Shannons information capacity law, which was discussed previously in Chapter 5.\\nConsider, first, an M-ary PSK system that employs a nonorthogonal set of M phase-\\nshifted signals for the transmission of binary data over an AWGN channel. Referring back\\nto Section 7.6, recall that (7.137) defines the bandwidth efficiency of the M-ary PSK\\nsystem, using the null-to-null bandwidth. Based on this equation, Figure 7.36 plots the\\noperating points for different phase-level numbers M = 2, 4, 8, 16, 32, 64. Each point on\\nthe operating curve corresponds to an average probability of symbol error Pe = 10-5; this\\nvalue of Pe is small enough to assume error-free transmission. Given this fixed value of\\nPe, (7.132) for the coherent detection of M-ary PSK is used to calculate the symbol\\nenergy-to-noise density ratio EN0 and, therefore, EbN0 for a prescribed M; Figure 7.36\\nalso includes the capacity boundary for the ideal transmission system, computed in\\naccordance with (5.99). Figure 7.36 teaches us the following:\\nIn M-ary PSK using coherent detection, increasing M improves the bandwidth\\nefficiency, but the Eb/N0 required for the idealized condition of error-free\\ntransmission moves away from the Shannon limit as M is increased.\\nConsider next an M-ary FSK system that uses an orthogonal set of M frequency-shifted\\nsignals for the transmission of binary data over an AWGN channel. As discussed in\\nSection 7.8, the separation between adjacent signal frequencies in the set is 12T, where T\\nis the symbol period. The bandwidth efficiency of M-ary FSK is defined in (7.209), the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 418,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Section 7.8, the separation between adjacent signal frequencies in the set is 12T, where T\\nis the symbol period. The bandwidth efficiency of M-ary FSK is defined in (7.209), the\\nformulation of which also invokes the null-to-null bandwidth. Using this equation, Figure\\n37 plots the operating points for different frequency-level numbers M = 2, 4, 8, 16, 32,\\n64 for the same average probability of symbol error, namely Pe = 10-5. Given this fixed\\nvalue of Pe, (7.206) is used to calculate the E/N0 and, therefore, Eb/N0 required for a\\nprescribed value of M. As in Figure 7.36 for M-ary PSK, Figure 7.37 for M-ary FSK also\\nincludes the capacity boundary for the ideal condition of error-free transmission. Figure\\n37 shows that increasing M in M-ary FSK has the opposite effect to that in M-ary PSK.\\nIn more specific terms, we may state the following:\\nIn M-ary FSK, as the number of frequency-shift levels M is increased-which is\\nequivalent to increased channel-bandwidth requirement-the operating point\\nmoves closer to the Shannon limit.\\nIn other words, in an information-theoretic context, M-ary FSK behaves better than M-ary\\nPSK.\\nIn the final analysis, the choice of M-ary PSK or M-ary FSK for binary data\\ntransmission over an AWGN channel is determined by the design criterion of interest:\\nbandwidth efficiency or the Eb/N0 needed for reliable data transmission.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 418,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '9 Comparison of M-ary PSK and M-ary FSK from an Information-Theoretic Viewpoint Figure 7.36 Comparison of M-ary PSK with the ideal system for Pe = 10-5.\\nFigure 7.37 Comparison of M-ary FSK with the ideal system for Pe = 10-5.\\n1 -6 0 6 12 18 24 30 36 2 0.5 0.4 0.3 0.2 0.1 -1.6 3 4 5 10 20 30 Capacity boundary M = 4 M = 2 Shannon limit Rb Bandwidth efficiency,\\nB M = 8 M = 16 M = 32 M = 64 N0 Eb , dB 1 -6 0 6 12 18 24 30 36 2 0.5 0.4 0.3 0.2 0.1 -1.6 3 4 5 10 20 30 Capacity boundary M = 4 M = 2 Shannon limit Rb Bandwidth efficiency,\\nB M = 8 M = 16 M = 32 M = 64 N0 Eb , dB',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 419,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels Detection of Signals with Unknown Phase\\nUp to this point in the chapter we have assumed that the receiver is perfectly synchronized\\nto the transmitter and the only channel impairment is AWGN. In practice, however, it is\\noften found that, in addition to the uncertainty due to channel noise, there is also\\nuncertainty due to the randomness of certain signal parameters. The usual cause of this\\nuncertainty is distortion in the transmission medium. Perhaps the most common random\\nsignal parameter is the carrier phase, which is especially true for narrowband signals. For\\nexample, the transmission may take place over a multiplicity of paths of different and\\nvariable length, or there may be rapidly varying delays in the propagating medium from\\ntransmitter to receiver. These sources of uncertainty may cause the phase of the received\\nsignal to change in a way that the receiver cannot follow. Synchronization with the phase\\nof the transmitted carrier is then too costly and the designer may simply choose to\\ndisregard the phase information in the received signal at the expense of some degradation\\nin noise performance. A digital communication receiver with no provision made for\\ncarrier phase recovery is said to be noncoherent.\\nOptimum Quadratic Receiver\\nConsider a binary communication system, in which the transmitted signal is defined by\\n(7.210)\\nwhere E is the signal energy, T is the duration of the signaling interval, and the carrier\\nfrequency fi for symbol i is an integer multiple of 1/(2T). For reasons just mentioned, the\\nreceiver operates noncoherently with respect to the transmitter, in which case the received\\nsignal for an AWGN channel is written as\\n(7.211)\\nwhere  is the unknown carrier phase and, as before, w(t) is the sample function of a white\\nGaussian noise process of zero mean and power spectral density N02. Assuming complete\\nlack of prior information about , we may treat it as the sample value of a random variable',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 420,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Gaussian noise process of zero mean and power spectral density N02. Assuming complete\\nlack of prior information about , we may treat it as the sample value of a random variable\\nwith uniform distribution:\\n(7.212)\\nSuch a distribution represents the worst-case scenario that could be encountered in\\npractice. The binary detection problem to be solved may now be stated as follows:\\nGiven the received signal x(t) and confronted with the unknown carrier phase ,\\ndesign an optimum receiver for detecting symbol si represented by the signal\\ncomponent that is contained in x(t).\\nProceeding in a manner similar to that described in Section 7.4, we may formulate the\\nlikelihood function of symbol si given the carrier phase  as\\nsi t 2E T ------- 2fit   0 t T  i 1 2  =    cos = x t 2E T ------- 2fit  +   w t +  for 0 t T  and i 1 2  = cos = f  1 2 ------,      - 0, otherwise      = E 2T    2fit  +   cos',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 420,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '10 Detection of Signals with Unknown Phase (7.213)\\nTo proceed further, we have to remove dependence of l(si()) on phase , which is\\nachieved by integrating it over all possible values of , as shown by\\n(7.214)\\nUsing a well-known trigonometric formula, we may expand the cosine term in (7.214) as\\nCorrespondingly, we may rewrite the integral in the exponent of (7.214) as\\n(7.215)\\nDefine two new terms:\\n(7.216)\\n(7.217)\\nThen, we may go one step further and simplify the inner integral in (7.214) to\\n(7.218)\\nAccordingly, using (7.218) in (7.214), we obtain\\n(7.219)\\nwhere, in the last line, we have used the fact that the definite integral is unaffected by the\\nphase i. l si     E N0T ---------- x t 2fit  +   d cos t 0 T  exp = l si  l si    f  d  -   = 1 2 ------ E N0T ---------- x t 2fit  +   cos 0 T  d exp  -   = 2fit  +   cos 2fit    2fit    sin sin - cos cos = x t 2fit  +   d cos t 0 T   x t 2fit   cos dt 0 T  cos =  x t 2fit   sin dt 0 T  sin - i xt 2fit   cos dt 0 T  2 x t 2fit   sin dt 0 T  2 +      1 2  = i xt 2fit   sin dt 0 T  x t 2fit   cos dt 0 T  -------------------------------------------------\\n1 - tan = x t 2fit  +   cos dt 0 T  i  i  sin i sin - cos cos   = i  i +   cos = l si  1 2 ------ E N0T ----------i  i +   cos exp d  -   = 1 2 ------ E N0T ----------i  cos     exp d +i - +i  = 1 2 ------ E N0T ----------i  cos     exp d  -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 421,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nFrom Appendix C on Bessel functions, we recognize the integral of (7.219) as the\\nmodified Bessel function of zero order, written in the compact form\\n(7.220)\\nUsing this formula, we may correspondingly express the likelihood function for the\\nsignal-detection problem described herein in the compact form\\n(7.221)\\nWith binary transmission as the issue of interest, there are two hypotheses to be\\nconsidered: hypothesis H1, that signal s1(t) was sent, and hypothesis H2, that signal s2 was\\nsent. In light of (7.221), the binary-hypothesis test may now be formulated as follows:\\nThe modified Bessel function I() is a monotonically increasing function of its argument.\\nHence, we may simplify the hypothesis test by focusing on i for given E/N0T. For\\nconvenience of implementation, however, the simplified hypothesis test is carried out in\\nterms of rather than i; that is to say:\\n(7.222)\\nFor obvious reasons, a receiver based on (7.222) is known as the quadratic receiver. In\\nlight of the definition of i given in (7.216), the receiver structure for computing i is as\\nshown in Figure 7.38a. Since the test described in (7.222) is independent of the symbol\\nenergy E, this hypothesis test is said to be uniformly most powerful with respect to E.\\nTwo Equivalent Forms of the Quadratic Receiver\\nWe next derive two equivalent forms of the quadrature receiver shown in Figure 7.38a.\\nThe first form is obtained by replacing each correlator in this receiver with a\\ncorresponding equivalent matched filter. We thus obtain the alternative form of quadrature\\nreceiver shown in Figure 7.38b. In one branch of this receiver, we have a filter matched to\\nthe signal cos(2fit) and in the other branch we have a filter matched to sin(2fit), both of\\nwhich are defined for the signaling interval 0  t  T. At time t = T, the filter outputs are\\nsampled, squared, and then added together.\\nTo obtain the second equivalent form of the quadrature receiver, suppose we have a fil-',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 422,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'sampled, squared, and then added together.\\nTo obtain the second equivalent form of the quadrature receiver, suppose we have a fil-\\nter that is matched to s(t) = cos(2fit + ) for 0  t  T. The envelope of the matched filter\\noutput is obviously unaffected by the value of phase . Therefore, we may simply choose a\\nmatched filter with impulse response cos[2fi(T - t)], corresponding to  = 0. The output\\nof such a filter in response to the received signal x(t) is given by\\n(7.223) I0 E N0T ----------i     1 2 ------ E N0T ----------i  cos     exp d  -   = l si  I0 E N0T ----------i     = I0 E N0T ----------1    H1 >< H2 I0 E N0T ----------2     i 2 1 2 H1 >< H2 2 2 y t x   2fi T t -  +     cos d 0 T  = 2fi T t -     x  2fi   cos d 2fi T t -     sin x  2fi   sin d 0 T  - 0 T  cos =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 422,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '10 Detection of Signals with Unknown Phase The envelope of the matched filter output is proportional to the square root of the sum of\\nthe squares of the two definite integrals in (7.223). This envelope, evaluated at time t = T,\\nis, therefore, given by the following square root:\\nBut this is just a repeat of the output of the quadrature receiver defined earlier. Therefore,\\nthe output (at time T) of a filter matched to the signal cos(2fit + ) of arbitrary phase ,\\nfollowed by an envelope detector, is the same as the quadrature receivers output li. This\\nform of receiver is shown in Figure 7.38c. The combination of matched filter and envelope\\ndetector shown in Figure 7.38c is called a noncoherent matched filter.\\nFigure 7.38 Noncoherent receivers: (a) quadrature receiver using correlators;\\n(b) quadrature receiver using matched fiters; (c) noncoherent matched filter.\\ndt + + yI yI yQ Sample at t = T Sample at t = T Sample at t = T x(t) Filter matched to cos(2fit) cos (2fit) sin (2fit) 0  t  T Filter matched to sin(2fit) 0  t  T dt 0 T Output li 0 T yI yI yQ x(t) 2 yQ 2 Squarer Squarer Square- rooter + + x(t) Output li Output li yQ 2 Squarer Squarer Filter matched to cos(2fit) Envelope detector Square- rooter 0  t  T \\x02 \\x02   (a) (b) (c) x  2fi   cos d 0 T  2 x   2fi   sin d 0 T  2 +      1 2',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 423,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nThe need for an envelope detector following the matched filter in Figure 7.38c may also\\nbe justified intuitively as follows. The output of a filter matched to a rectangular RF wave\\nreaches a positive peak at the sampling instant t = T. If, however, the phase of the filter is\\nnot matched to that of the signal, the peak may occur at a time different from the sampling\\ninstant. In actual fact, if the phases differ by 180, we get a negative peak at the sampling\\ninstant. Figure 7.39 illustrates the matched filter output for the two limiting conditions:\\n= 0 and  = 180 for which the respective waveforms of the matched filter output are\\ndisplayed in parts a and b of the figure. To avoid poor sampling that arises in the absence\\nof prior information about the phase, it is reasonable to retain only the envelope of the\\nmatched filter output, since it is completely independent of the phase mismatch . Noncoherent Orthogonal Modulation Techniques\\nWith the noncoherent receiver structures of Figure 7.38 at our disposal, we may now\\nproceed to study the noise performance of noncoherent orthogonal modulation that\\nincludes two noncoherent receivers as special cases: noncoherent binary FSK; and\\ndifferential PSK (called DPSK), which may be viewed as the noncoherent version of\\nbinary PSK.\\nConsider a binary signaling scheme that involves the use of two orthogonal signals s1(t)\\nand s2(t), which have equal energy. During the signaling interval 0  t  T, where T may be\\nFigure 7.39 Output of matched filter for a rectangular RF wave: (a)\\n; (b) . (a) (b) t 2T T 0 1.0 0 1.0 t 2T T 0(t)  0(t)   0 =  180 =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 424,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '11 Noncoherent Orthogonal Modulation Techniques different from the bit duration Tb, one of these two signals is sent over an imperfect\\nchannel that shifts the carrier phase by an unknown amount. Let g1(t) and g2(t) denote the\\nphase-shifted versions of s1(t) and s2(t) that result from this transmission, respectively. It is\\nassumed that the signals g1(t) and g2(t) remain orthogonal and have the same energy E,\\nregardless of the unknown carrier phase. We refer to such a signaling scheme as\\nnoncoherent orthogonal modulation, hence the title of the section.\\nIn addition to carrier-phase uncertainty, the channel also introduces AWGN w(t) of zero\\nmean and power spectral density N02, resulting in the received signal\\n(7.224)\\nTo tackle the signal detection problem given x(t), we employ the generalized receiver\\nshown in Figure 7.39a, which consists of a pair of filters matched to the transmitted\\nsignals s1(t) and s2(t). Because the carrier phase is unknown, the receiver relies on\\namplitude as the only possible discriminant. Accordingly, the matched-filter outputs are\\nenvelope-detected, sampled, and then compared with each other. If the upper path in\\nFigure 7.38a has an output amplitude l1 greater than the output amplitude l2 of the lower\\npath, the receiver decides in favor of s1(t); the l1 and l2 used here should not be confused\\nwith the symbol l denoting the likelihood function in the preceding section. If the converse\\nis true, the receiver decides in favor of s2(t). When they are equal, the decision may be\\nmade by flipping a fair coin (i.e., randomly). In any event, a decision error occurs when\\nthe matched filter that rejects the signal component of the received signal x(t) has a larger\\noutput amplitude (due to noise alone) than the matched filter that passes it.\\nFrom the discussion presented in Section 7.10 we note that a noncoherent matched\\nfilter (constituting the upper or lower path in the receiver of Figure 7.40a), may be viewed',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 425,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'From the discussion presented in Section 7.10 we note that a noncoherent matched\\nfilter (constituting the upper or lower path in the receiver of Figure 7.40a), may be viewed\\nas being equivalent to a quadrature receiver. The quadrature receiver itself has two\\nchannels. One version of the quadrature receiver is shown in Figure 7.40b. In the upper\\npath, called the in-phase path, the received signal x(t) is correlated with the function\\n, which represents a scaled version of the transmitted signal s1(t) or s2(t) with zero\\ncarrier phase. In the lower path, called the quadrature path, on the other hand, x(t) is\\ncorrelated with another function\\n, which represents the version of that results\\nfrom shifting the carrier phase by -90. The signals and are orthogonal to each other. In actual fact, the signal is the Hilbert transform of\\n; the Hilbert transform\\nwas discussed in Chapter 2. To illustrate the nature of this relationship, let\\n(7.225)\\nwhere m(t) is a band-limited message signal. Typically, the carrier frequency fi is greater than\\nthe highest frequency component of m(t). Then the Hilbert transform is defined by\\n(7.226)\\nfor which reference should be made in Table 2.3 of Chapter 2. Since\\nx t g1 t w t + s1 tsent for 0 t T  g2 t w t + s2 tsent for 0 t T       = i t  i t i t i t  i t  i t i t i t m t 2fit   cos = i t  i t m t 2fit   sin = 2fit  2--- -     cos 2fit   sin =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 425,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nwe see that is indeed obtained from by shifting the carrier cos(2fit) by -90.\\nAn important property of Hilbert transformation is that a signal and its Hilbert transform are\\northogonal to each other. Thus, and are indeed orthogonal to each other, as\\nalready stated.\\nThe average probability of error for the noncoherent receiver of Figure 7.40a is given\\nby the simple formula\\n(7.227)\\nwhere E is the signal energy per symbol and N0/2 is the noise spectral density.\\nDerivation of Equation (7.227)\\nTo derive Equation (7.227)9 we make use of the equivalence depicted in Figure 7.40. In\\nparticular, we observe that, since the carrier phase is unknown, noise at the output of each\\nFigure 7.40 (a) Generalized binary receiver for noncoherent orthogonal modulation. (b) Quadrature\\nreceiver equivalent to either one of the two matched filters in (a); the index i = 1, 2.\\n i t i t i t  i t Pe 1 2--- E 2N0 --------- -     exp = (a) (b) + + xIi xQi xIi x(t) dt 0 T li dt 0 T 2 li 2 xQi 2 Square-law device Square-law device Quadrature channel In-phase channel Square rooter x(t) l1 l2 Envelope detector Filter matched to s1(t) Envelope detector Filter matched to s2(t) Sample at t = T Sample at t = T Comparison device If l1 > l2, choose s1(t). If l1 < l2, choose s2(t). \\x02 \\x02  i(t) i(t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 426,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '11 Noncoherent Orthogonal Modulation Techniques matched filter in Figure 7.40a has two degrees of freedom: in-phase and quadrature.\\nAccordingly, the noncoherent receiver of Figure 7.40a has a total of four noisy parameters\\nthat are conditionally independent given the phase , and also identically distributed. These\\nfour noisy parameters have sample values denoted by xI1, xQ1, and xI2, and xQ2; the first\\ntwo account for degrees of freedom associated with the upper path of Figure 7.40a, and the\\nlatter two account for degrees of freedom associated with the lower path of the figure.\\nThe receiver of Figure 7.40a has a symmetric structure, meaning that the probability of\\nchoosing s2(t) given that s1(t) was transmitted is the same as the probability of choosing\\ns1(t) given that s2(t) was transmitted. In other words, the average probability of error may\\nbe obtained by transmitting s1(t) and calculating the probability of choosing s2(t), or vice\\nversa; it is assumed that the original binary symbols and therefore s1(t) and s2(t) are\\nequiprobable.\\nSuppose that signal s1(t) is transmitted for the interval 0  t  T. An error occurs if the\\nchannel noise w(t) is such that the output l2 of the lower path in Figure 7.40a is greater than\\nthe output l1 of the upper path. Then, the receiver decides in favor of s2(t) rather than s1(t).\\nTo calculate the probability of error so made, we must have the probability density function\\nof the random variable L2 (represented by sample value l2). Since the filter in the lower\\npath is matched to s2(t) and s2(t) is orthogonal to the transmitted signal s1(t), it follows that\\nthe output of this matched filter is due to noise alone. Let xI2 and xQ2 denote the in-phase\\nand quadrature components of the matched filter output in the lower path of Figure 7.40a.\\nThen, from the equivalent structure depicted in this figure, we see that (for i = 2)\\n(7.228)\\nFigure 7.41a shows a geometric interpretation of this relation. The channel noise w(t) is',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 427,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Then, from the equivalent structure depicted in this figure, we see that (for i = 2)\\n(7.228)\\nFigure 7.41a shows a geometric interpretation of this relation. The channel noise w(t) is\\nboth white (with power spectral density N0/2) and Gaussian (with zero mean). Corre-\\nspondingly, we find that the random variables XI2 and XQ2 (represented by sample values\\nxI2 and xQ2) are both Gaussian distributed with zero mean and variance N0/2, given the\\nphase . Hence, we may write\\n(7.229)\\nFigure 7.41 Geometric interpretations of the two path outputs l1 and l2\\nin the generalized non-coherent receiver.\\nl2 xI2 2 xQ2 2 + = fXI2 xI2   1 N0 -------------- xI2 2 N0 ------- -       exp = (a) (b) l2 l1 xI2 (noise) xQ2 (noise) xQ1 (noise) xI1 (signal plus noise)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 427,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nand\\n(7.230)\\nNext, we use the well-known property presented in Chapter 4 on stochastic processes: the\\nenvelope of a Gaussian process represented in polar form is Rayleigh distributed and\\nindependent of the phase  For the situation at hand, therefore, we may state that the\\nrandom variable L2 whose sample value l2 is related to xI2 and xQ2 by (7.228) has the\\nfollowing probability density function:\\n(7.231)\\nFigure 7.42 shows a plot of this probability density function, where the shaded area\\ndefines the conditional probability that l2 > l1. Hence, we have\\n(7.232)\\nSubstituting (7.231) into (7.232) and integrating, we get\\n(7.233)\\nConsider next the output amplitude l1, pertaining to the upper path in Figure 7.40a. Since\\nthe filter in this path is matched to s1(t) and it is assumed that s1(t) is transmitted, it follows\\nthat l1 is due to signal plus noise. Let xI1 and xQ1 denote the components at the output of\\nthe matched filter in the upper path of Figure 7.39a that are in phase and in quadrature\\nwith respect to the received signal, respectively. Then, from the equivalent structure\\ndepicted in Figure 7.40b, we see that, for i = 1,\\n(7.234) fXQ2 xQ2   1 N0 -------------- xQ2 2 N0 --------- -       exp = fL2 l2   2l2 N0 ------- l2 2 N0 ------ -      , exp l2 0  0, elsewhere        = \\x02 l2 l1 l1    fL2 l2   dl2 l1   = \\x02 l2 l1 l1    l1 2 N0 ------ -       exp = l1 xI1 2 xQ1 2 + = Figure 7.42 Calculation of the conditional probability\\nthat l2 > l1, given l1.\\n0 l1 l2 Conditional probability of error fL2(l2)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 428,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '11 Noncoherent Orthogonal Modulation Techniques A geometric interpretation of li is presented in Figure 7.41b. Since a Fourier-transformable\\nsignal and its Hilbert transform form an orthogonal pair, it follows that xI1 is due to signal\\nplus noise, whereas xQ1 is due to noise alone. This statement has two implications:\\n\\nThe random variable XI1 represented by the sample value xI1 is Gaussian distributed\\nwith mean and variance N02, where E is the signal energy per symbol.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 429,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The random variable XQ1 represented by the sample value xQ1 is Gaussian distrib-\\nuted with zero mean and variance N02.\\nHence, we may express the probability density functions of these two independent random\\nvariables as (7.235) and (7.236) respectively. Since the two random variables XI1 and XQ1 are statistically independent,\\ntheir joint probability density function is simply the product of the probability density\\nfunctions given in (7.235) and (7.236).\\nTo find the average probability of error, we have to average the conditional probability\\nof error given in (7.233) over all possible values of l1. Naturally, this calculation requires\\nknowledge of the probability density function of random variables L1 represented by\\nsample value l1. The standard method is now to combine (7.235) and (7.236) to find the\\nprobability density function of L1 due to signal plus noise. However, this leads to rather\\ncomplicated calculations involving the use of Bessel functions. This analytic difficulty\\nmay be circumvented by the following approach. Given xI1 and xQ1, an error occurs when,\\nin Figure 7.40a, the lower paths output amplitude l2 due to noise alone exceeds l1 due to\\nsignal plus noise; squaring both sides of (7.234), we write\\n(7.237)\\nThe probability of the occurrence just described is obtained by substituting (7.237) into\\n(7.233):\\n(7.238)\\nwhich is a probability of error conditioned on the output of the matched filter in the upper path\\nof Figure 7.40a taking on the sample values xI1 and xQ1. This conditional probability multi-\\nplied by the joint probability density function of the random variables XI1 and XQ1 is the\\nerror-density given xI1 and xQ1. Since XI1 and XQ1 are statistically independent, their joint\\nprobability density function equals the product of their individual probability density func-\\ntions. The resulting error-density is a complicated expression in xI1 and xQ1. However, the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 429,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'probability density function equals the product of their individual probability density func-\\ntions. The resulting error-density is a complicated expression in xI1 and xQ1. However, the\\naverage probability of error, which is the issue of interest, may be obtained in a relatively sim-\\nple manner. We first use (7.234), (7.235), and (7.236) to evaluate the desired error-density as\\n(7.239) E fXI1 xI1   1 N0 -------------- xI1 E -   2 N0 ----------------------------\\n- exp = fXQ1 xQ1   1 N0 -------------- xQ1 2 N0 --------- -       exp = l1 2 xI1 2 xQ1 2 + = \\x02 error|xI1 xQ1    xI1 2 xQ1 2 + N0 ----------------------\\n-       exp = \\x02 error|xI1 xQ1   fXI1 xI1  fXQ1 xQ1   1 N0 ---------- 1 N0 ------ xI1 2 xQ1 2 xI1 E -   2 xQ1 2 + + +   -       exp =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 429,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nCompleting the square in the exponent of (7.239) without the scaling factor -1/N0, we\\nmay rewrite it as follows:\\n(7.240)\\nNext, we substitute (7.240) into (7.239) and integrate the error-density over all possible\\nvalues of xI1 and xQ1, thereby obtaining the average probability of error:\\n(7.241)\\nWe now use the following two identities:\\n(7.242) and (7.243) The identity of (7.242) is obtained by considering a Gaussian-distributed variable with\\nmean and variance N04 and recognizing the fact that the total area under the curve\\nof a random variables probability density function is unity. The identity of (7.243) follows\\nas a special case of (7.242). Thus, in light of these two identities, (7.241) reduces to\\nwhich is the desired result presented previously as (7.227). With this formula at our\\ndisposal, we are ready to consider noncoherent binary FSK and DPSK as special cases,\\nwhich we do next in that order.10 Binary Frequency-Shift Keying Using Noncoherent Detection\\nIn binary FSK, the transmitted signal is defined in (7.151) and repeated here for\\nconvenience of presentation:\\n(7.244)\\nwhere Tb is the bit duration and the carrier frequency fi equals one of two possible values\\nf1 and f2; to ensure that the signals representing these two frequencies are orthogonal, we\\nchoose fi = ni/Tb, where ni is an integer. The transmission of frequency f1 represents\\nsymbol 1 and the transmission of frequency f2 represents symbol 0. For the noncoherent',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 430,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'choose fi = ni/Tb, where ni is an integer. The transmission of frequency f1 represents\\nsymbol 1 and the transmission of frequency f2 represents symbol 0. For the noncoherent\\nxI1 2 xQ1 2 xI1 E -   2 xQ1 2 + + + 2 xI1 E 2 ------- -    2 2xQ1 2 E 2--- + + = Pe \\x02 error|xI1 xQ1   fXI1 xI1  fXQ1 xQ1   d  -    -   = xI1 dxQ1 1 N0 ---------- E 2N0 --------- -     2 N0 ------ - xI1 E 2 ------- -    2 dxI1 2xQ1 2 N0 ------------ -       dxQ1 exp  -   exp  -   exp = 2 N0 ------ xI1 E 2 ------- -    2 - dxI1 exp  -   N0 2 ---------- = 2xQ1 2 N0 ------------       - dxQ1 exp  -   N0 2 ---------- = E 2  Pe 1 2--- E 2N0 --------- -     exp = si t 2Eb Tb --------- 2fit  , cos 0 t Tb  0, elsewhere      =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 430,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '13 Differential Phase-Shift Keying detection of this frequency-modulated signal, the receiver consists of a pair of matched\\nfilters followed by envelope detectors, as in Figure 7.43. The filter in the upper path of the\\nreceiver is matched to cos(2f1t) and the filter in the lower path is matched to cos(2f2t)\\nfor the signaling interval 0  t  Tb. The resulting envelope detector outputs are sampled at\\nt = Tb and their values are compared. The envelope samples of the upper and lower paths\\nin Figure 7.43 are shown as l1 and l2. The receiver decides in favor of symbol 1 if l1 > l2\\nand in favor of symbol 0 if l1  l2. If l1 = l2, the receiver simply guesses randomly in favor\\nof symbol 1 or 0.\\nThe noncoherent binary FSK described herein is a special case of noncoherent\\northogonal modulation with T = Tb and E = Eb, where Eb is the signal energy per bit.\\nHence, the BER for noncoherent binary FSK is\\n(7.245)\\nwhich follows directly from (7.227) as a special case of noncoherent orthogonal\\nmodulation. Differential Phase-Shift Keying\\nAs remarked at the beginning of Section 7.9, we may view DPSK as the noncoherent\\nversion of binary PSK. The distinguishing feature of DPSK is that it eliminates the need\\nfor synchronizing the receiver to the transmitter by combining two basic operations at the\\ntransmitter:\\n\\ndifferential encoding of the input binary sequence and\\n\\nPSK of the encoded sequence,\\nfrom which the name of this new binary signaling scheme follows.\\nFigure 7.43 Noncoherent receiver for the detection of binary FSK signals.\\nFilter matched to cos(2f1t) 0  t  Tb Filter matched to cos(2f2t) 0  t  Tb x(t) If l1 > l2, choose 1. If l1 < l2, choose 0. Envelope detector Envelope detector Comparison device l1 l2 Sample at time t = Tb Sample at time t = Tb Pe 1 2--- Eb 2N0 --------- -     exp =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 431,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nDifferential encoding starts with an arbitrary first bit, serving as the reference bit; to\\nthis end, symbol 1 is used as the reference bit. Generation of the differentially encoded\\nsequence then proceeds in accordance with a two-part encoding rule as follows:\\nIf the new bit at the transmitter input is 1, leave the differentially encoded symbol\\nunchanged with respect to the current bit.\\nIf, on the other hand, the input bit is 0, change the differentially encoded symbol\\nwith respect to the current bit.\\nThe differentially encoded sequence, denoted by {dk}, is used to shift the sinusoidal\\ncarrier phase by zero and 180o, representing symbols 1 and 0, respectively. Thus, in terms\\nof phase-shifts, the resulting DPSK signal follows the two-part rule:\\nTo send symbol 1, the phase of the DPSK signal remains unchanged.\\nTo send symbol 0, the phase of the DPSK signal is shifted by 180.\\nEXAMPLE\\nIllustration of DPSK\\nConsider the input binary sequence, denoted\\n, to be 10010011, which is used to\\nderive the generation of a DPSK signal. The differentially encoded process starts with the\\nreference bit 1. Let denote the differentially encoded sequence starting in this\\nmanner and denote its delayed version by one bit. The complement of the\\nmodulo-2 sum of and defines the desired\\n, as illustrated in the top three\\nlines of Table 7.6. In the last line of this table, binary symbols 1 and 0 are represented by\\nphase-shifts of 1 and  radians.\\nError Probability of DPSK\\nBasically, the DPSK is also an example of noncoherent orthogonal modulation when its\\nbehavior is considered over successive two-bit intervals; that is, 0  t  2Tb. To\\nelaborate, let the transmitted DPSK signal be for the first-bit\\ninterval 0  t Tb, which corresponds to symbol 1. Suppose, then, the input symbol for\\nthe second-bit interval Tb  t  2Tb is also symbol 1. According to part 1 of the DPSK\\nencoding rule, the carrier phase remains unchanged, thereby yielding the DPSK signal',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 432,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the second-bit interval Tb  t  2Tb is also symbol 1. According to part 1 of the DPSK\\nencoding rule, the carrier phase remains unchanged, thereby yielding the DPSK signal\\nTable 7.6 Illustrating the generation of DPSK signal\\n{bk} 1 0 0 1 0 0 1 1 {dk - 1} 1 1 0 1 1 0 1 1 reference Differentially encoded sequence {dk}\\n1 1 0 1 1 0 1 1 1 Transmitted phase (radians)\\n0 0 0 0 0 0 0 bk   dk   dk 1 -   bk   dk 1 -   dk     2Eb Tb  2fct   cos',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 432,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '13 Differential Phase-Shift Keying (7.246)\\nSuppose, next, the signaling over the two-bit interval changes such that the symbol at the\\ntransmitter input for the second-bit interval Tb  t  2Tb is 0. Then, according to part 2 of\\nthe DPSK encoding rule, the carrier phase is shifted by  radians (i.e., 180), thereby\\nyielding the new DPSK signal\\n(7.247)\\nWe now readily see from (7.246) and (7.247) that s1(t) and s2(t) are indeed orthogonal\\nover the two-bit interval 0  t  2Tb, which confirms that DPSK is indeed a special form of\\nnoncoherent orthogonal modulation with one difference compared with the case of binary\\nFSK: for DPSK, we have T = 2Tb and E = 2Eb. Hence, using (7.227), we find that the BER\\nfor DPSK is given by\\n(7.248)\\nAccording to this formula, DPSK provides a gain of 3 dB over binary FSK using\\nnoncoherent detection for the same Eb/N0.\\nGeneration of DPSK Signal\\nFigure 7.44 shows the block diagram of the DPSK transmitter. To be specific, the\\ntransmitter consists of two functional blocks:\\n\\nLogic network and one-bit delay (storage) element, which are interconnected so as\\nto convert the raw input binary sequence {bk} into the differentially encoded\\nsequence {dk}.\\n\\nBinary PSK modulator, the output of which is the desired DPSK signal.\\nOptimum Receiver for the Detection of DPSK\\nIn the use of DPSK, the carrier phase is unknown, which complicates the received signal\\nx(t). To deal with the unknown phase  in the differentially coherent detection of the\\nDPSK signal in x(t), we equip the receiver with an in-phase and a quadrature path. We thus\\nhave a signal-space diagram where the received signal points over the two-bit interval\\ns1 t 2Eb Tb --------- 2fct  , cos symbol 1 for 0 t Tb  2Eb Tb --------- 2fct  , cos symbol 0 for Tb t 2Tb           = s2 t 2Eb Tb --------- 2fct  , cos symbol 1 for 0 t Tb  2Eb Tb --------- 2fct  +  , cos symbol 1 for Tb t 2Tb           = Pe 1 2--- Eb N0 ------ -     exp =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 433,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\n0 t 2Tb are defined by (Acos, Asin) and (-Acos, -Asin), where A denotes the\\ncarrier amplitude.\\nThis geometry of possible signals is illustrated in Figure 7.45. For the two-bit interval\\n0  t  2Tb, the receiver measures the coordinates\\n, first, at time t = Tb and then\\nmeasures at time t = 2Tb. The issue to be resolved is whether these two points map\\nto the same signal point or different ones. Recognizing that the vectors x0 and x1, with end\\npoints and\\n, respectively, are points roughly in the same direction if their\\ninner product is positive, we may formulate the binary-hypothesis test with a question:\\nIs the inner product positive or negative?\\nExpressing this statement in analytic terms, we may write\\n(7.249)\\nwhere the threshold is zero for equiprobable symbols.\\nWe now note the following identity:\\nFigure 7.44 Block diagram of a DPSK transmitter.\\nFigure 7.45 Signal-space diagram of received DPSK signal.\\nLogic network Amplitude- level shifter Input binary sequence {bk } {dk -1} {dk} Product modulator DPSK signal 2/Tb cos (2fct) Delay Tb Binary PSK modulator xI0 xQ0  xI1 xQ1  xI0 xQ0  xI1 xQ1  x0 Tx1 xI0xI1 xQ0xQ1 + say 1 >< say 0 0 xI0xI1 xQ0xQ1 + 1 4--- xI0 xI1 +  2 xI0 xI1 -  2 xQ0 xQ1 +  2 xQ0 xQ1 -  2 - + -   = A sin A cos  -A sin -A cos   xI xQ',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 434,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '14 BER Comparison of Signaling Schemes over AWGN Channels Hence, substituting this identity into (7.249), we get the equivalent test:\\n(7.250)\\nwhere the scaling factor 14 is ignored. In light of this equation, the question on the binary\\nhypothesis test for the detection of DPSK may now be restated as follows:\\nGiven the current signal point received in the time interval\\n0 < t < 2Tb, is this point closer to the signal point or its image received in the next time interval Tb < t < 2Tb?\\nThus, the optimum receiver11 for the detection of binary DPSK is as shown in Figure 7.46,\\nthe formulation of which follows directly from the binary hypothesis test of (7.250). This\\nimplementation is simple, in that it merely requires that sample values be stored.\\nThe receiver of Figure 7.46 is said to be optimum for two reasons:\\nIn structural terms, the receiver avoids the use of fancy delay lines that could be\\nneeded otherwise.\\nIn operational terms, the receiver makes the decoding analysis straightforward to\\nhandle, in that the two signals to be considered are orthogonal over the interval\\n[0,2Tb] in accordance with the formula of (7.227). BER Comparison of Signaling Schemes over AWGN Channels\\nMuch of the material covered in this chapter has been devoted to digital modulation\\nschemes operating over AWGN channels. In this section, we present a summary of the\\nFigure 7.46 Block diagram of a DPSK receiver.\\ncos (2 fct) sin (2 fct) Decision device Tb x(t) Say 1 if y > 0 0 dt \\x02 Tb dt \\x02 -90 phase shifter Delay Tb Delay Tb 0 Say 0 if y < 0 Threshold = 0 y + + Quadrature channel    xI0 xI1 +  2 xQ0 xQ1 +  2 xI0 xI1 -  2 - xQ0 xQ1 -  2 - + say 1 >< say 0 0 xI0,xQ0   xI1,xQ1   x - I1, x - Q1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 435,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nBERs of some popular digital modulation schemes, classified into two categories,\\ndepending on the method of detection used in the receiver:\\nClass I: Coherent detection\\n\\nbinary PSK: two symbols, single carrier\\n\\nbinary FSK: two symbols, two carriers one for each symbol\\n\\nQPSK: four symbols, single carrier-the QPSK also includes the QAM, employing\\nfour symbols as a special case\\n\\nMSK: four symbols, two carriers.\\nClass II: Noncoherent detection\\n\\nDPSK: two symbols, single carrier\\n\\nbinary FSK: two symbols, two carriers.\\nTable 7.7 presents a summary of the formulas of the BERs of these schemes separated\\nunder Classes I and II. All the formulas are defined in terms of the ratio of energy per bit\\nto the noise spectral density, EbN0, as summarized herein:\\nUnder Class I, the formulas are expressed in terms of the Q-function. This function\\nis defined as the area under the tail end of the standard Gaussian distribution with\\nzero mean and unit variance; the lower limit in the integral defining the Q-function\\nis dependent solely on EbN0, scaled by the factor 2 for binary PSK, QPSK, and\\nMSK. Naturally, as this SNR ratio is increased, the area under the Q-function is\\nreduced and with it the BER is correspondingly reduced.\\nUnder Class II, the formulas are expressed in terms of an exponential function,\\nwhere the negative exponent depends on the EbN0 ratio for DPSK and its scaled\\nversion by the factor 1/2 for binary FSK. Here again, as the Eb/N0 is increased, the\\nBER is correspondingly reduced.\\nThe performance curves of the digital modulation schemes listed in Table 7.7 are shown in\\nFigure 7.47 where the BER is plotted versus EbN0. As expected, the BERs for all the\\nTable 7.7 Formulas for the BER of digital modulation schemes\\nemploying two or four symbols\\nSignaling Scheme\\nBER\\nI. Coherent detection\\nBinary PSK QPSK MSK Binary FSK II. Noncoherent detection\\nDPSK Binary FSK Q 2Eb N0  Q Eb N0  1 2---exp Eb N0  -   1 2---exp Eb 2N0  -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 436,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '14 BER Comparison of Signaling Schemes over AWGN Channels schemes decrease monotonically with increasing EbN0, with all the graphs having a\\nsimilar shape in the form of a waterfall. Moreover, we can make the following\\nobservations from Figure 7.47:\\nFor any value of Eb/N0, the schemes using coherent detection produce a smaller\\nBER than those using noncoherent detection, which is intuitively satisfying.\\nPSK schemes employing two symbols, namely binary PSK with coherent detection\\nand DPSK with noncoherent detection, require an EbN0 that is 3 dB less than their\\nFSK counterpart to realize the same BER.\\nAt high values of EbN0, DPSK and binary FSK using noncoherent detection\\nperform almost as well, to within about 1 dB of their respective counterparts using\\ncoherent detection for the same BER.\\nFigure 7.47 Comparison of the noise performance of different PSK and FSK schemes.\\n-5 -2.5 0 2.5 5.0 7.5 10 12.5 , dB Eb N0 DPSK 10-5 10-4 10-3 10-2 10-1 0.5 Bit error rate (BER) (a) Coherent binary PSK\\n(b) Coherent QPSK\\n(c) Coherent MSK\\nNoncoherent binary FSK\\nCoherent binary FSK',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 437,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nAlthough under Class I the BER for binary PSK, QPSK, and MSK is governed by\\nthe same formula, there are important differences between them:\\n\\nFor the same channel bandwidth and BER, the QPSK accommodates the\\ntransmission of binary data at twice the rate attainable with binary PSK; in other\\nwords, QPSK is bandwidth conserving.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 438,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'When sensitivity to interfering signals is an issue of practical concern, as in\\nwireless communications, MSK is preferred over QPSK. Synchronization\\nThe coherent reception of a digitally modulated signal, discussed in previous sections of\\nthis chapter, requires that the receiver be synchronous with the transmitter. In this context,\\nwe define the process of synchronization as follows:\\nTwo sequences of related events performed separately, one in the transmitter\\nand the other in the receiver, are said to be synchronous relative to each other\\nwhen the events in one sequence and the corresponding events in the other\\noccur simultaneously, except for some finite delay.\\nThere are two basic modes of synchronization:\\nCarrier synchronization. When coherent detection is used in signaling over AWGN\\nchannels via the modulation of a sinusoidal carrier, knowledge of both the frequency\\nand phase of the carrier is necessary. The process of estimating the carrier phase and\\nfrequency is called carrier recovery or carrier synchronization; in what follows,\\nboth terminologies are used interchangeably.\\nTo perform demodulation, the receiver has to know the instants of time at which the\\nmodulation in the transmitter changes its state. That is, the receiver has to know the\\nstarting and finishing times of the individual symbols, so that it may determine when\\nto sample and when to quench the product-integrators. The estimation of these times is\\ncalled clock recovery or symbol synchronization; here again, both terminologies are\\nused interchangeably.\\nWe may classify synchronization schemes as follows, depending on whether some form of\\naiding is used or not:\\nData-aided synchronization. In data-aided synchronization schemes, a preamble is\\ntransmitted along with the data-bearing signal in a time-multiplexed manner on a\\nperiodic basis. The preamble contains information about the symbol timing, which\\nis extracted by appropriate processing of the channel output at the receiver. Such an',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 438,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'periodic basis. The preamble contains information about the symbol timing, which\\nis extracted by appropriate processing of the channel output at the receiver. Such an\\napproach is commonly used in digital satellite and wireless communications, where\\nthe motivation is to minimize the time required to synchronize the receiver to the\\ntransmitter. Limitations of data-aided synchronization are twofold:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 438,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': 'reduced data-throughput efficiency, which is incurred by assigning a certain\\nportion of each transmitted frame to the preamble, and\\n\\nreduced power efficiency, which results from the allocation of a certain fraction\\nof the transmitted power to the transmission of the preamble.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 438,\n",
       "   'chunk_idx': 3}},\n",
       " {'chunk': '16 Recursive Maximum Likelihood Estimation for Synchronization\\nNondata-aided synchronization. In this second approach, the use of a preamble is\\navoided and the receiver has the task of establishing synchronization by extracting\\nthe necessary information from the noisy distorted modulated signal at the channel\\noutput. Both throughput and power efficiency are thereby improved, but at the\\nexpense of an increase in the time taken to establish synchronization.\\nIn this section, the discussion is focused on nondata-aided forms of carrier and clock\\nrecovery schemes. To be more specific, we adopt an algorithmic approach,12 which is so-\\ncalled on account of the fact that implementation of the sychronizer enables the receiver to\\nestimate the carrier phase and symbol timing in a recursive manner from one time instant\\nto another. The processing is performed on the baseband version of the received signal,\\nusing discrete-time (digital) signal-processing algorithms.\\nAlgorithmic Approach to Synchronization\\nMaximum likelihood decoding played a key role in much of the material on signaling\\ntechniques in AWGN channels presented in Sections 7.4 through 7.13. Maximum\\nlikelihood parameter estimation plays a key role of its own in the algorithmic approach to\\nsynchronization. Both of these methods were discussed previously in Chapter 3 on\\nprobability theory and Bayesian inference. In this context, it may therefore be said that a\\nsense of continuity is being maintained throughout this chapter.\\nGiven the received signal, the maximum likelihood method is used to estimate two\\nparameters: carrier phase and symbol timing, both of which are, of course, unknown.\\nHere, we are assuming that knowledge of the carrier frequency is available at the receiver.\\nMoreover, in the algorithmic approach, the symbol-timing recovery is performed\\nbefore phase recovery. The rationale for proceeding in this way is that once we know the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 439,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Moreover, in the algorithmic approach, the symbol-timing recovery is performed\\nbefore phase recovery. The rationale for proceeding in this way is that once we know the\\nenvelope delay incurred by signal transmission through a dispersive channel, then one\\nsample per symbol at the matched filter output may be sufficient for estimating the\\nunknown carrier phase. Moreover, computational complexity of the receiver is minimized\\nby using synchronization algorithms that operate at the symbol rate 1/T.\\nIn light of the remarks just made, we will develop the algorithmic approach to\\nsynchronization by proceeding as follows:\\nThrough processing the received signal corrupted by channel noise and channel\\ndispersion, the likelihood function is formulated.\\nThe likelihood function is maximized to recover the clock.\\nWith clock recovery achieved, the next step is to maximize the likelihood function to\\nrecover the carrier.\\nThe derivations presented in this chapter focus on the QPSK signal. The resulting\\nformulas may be readily extended to binary PSK symbols as a special case and\\ngeneralized for M-ary PSK signals. Recursive Maximum Likelihood Estimation for Synchronization\\nIn the previous section, we remarked that, in algorithmic synchronization, estimation of\\nthe two unknown parameters, namely carrier phase and symbol timing, is performed in a\\nrecursive manner from one time instant to another.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 439,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nIn other words:\\nDiscrete time is an essential dimension of recursive parameter estimation.\\nMoreover, the estimation is performed at time t = nT, where n is an integer and T is the\\nsymbol duration. Equivalently, we may say that n = t/T denotes the normalized\\n(dimensionless) discrete time.\\nOne other important point to note: recursive estimation of the unknown parameter, be\\nthat the carrier phase or symbol time, plays a key role in the synchronization process.\\nSpecifically, it proceeds across discrete time in accordance with the following rule:\\n(7.251)\\nIn other words, the recursive parameter estimation takes on the structure of an adaptive\\nfiltering algorithm, in which the product of the step-size parameter and error signal\\nassumes the role of an algorithmic adjustment.\\nIn what follows, we derive adaptive filtering algorithms for estimating the unknown\\nsynchronization parameters with the error signal being derived from the likelihood function.\\nLikelihood Functions\\nThe idea of maximum likelihood parameter estimation based on continuous-time\\nwaveforms was discussed in Chapter 3. To briefly review the material described therein,\\nconsider a baseband signal defined by\\nwhere is an unknown parameter and w(t) denotes an AWGN. Given a sample of the\\nsignal x(t), the requirement is to estimate the parameter ; so, we say:\\nThe most likely value of the estimate is the particular for which the\\nlikelihood function l( ) is a maximum.\\nNote that we say a maximum rather than the maximum because it is possible for the\\ngraph of l(\\n) plotted versus to have multiple maxima. In any event, the likelihood\\nfunction given x, namely l( ), is defined as the probability density function with\\nthe roles of x and interchanged, as shown by\\nwhere, for convenience of presentation, we have omitted the conditional dependence of\\non x in l( ).\\nIn the algorithmic synchronization procedures derived in this section, we will be',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 440,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'where, for convenience of presentation, we have omitted the conditional dependence of\\non x in l( ).\\nIn the algorithmic synchronization procedures derived in this section, we will be\\nconcerned only with cases in which the parameter is a scalar. Such cases are referred to\\nas independent estimation. However, when we are confronted with the synchronization of a\\ndigital communication receiver to its transmitter operating over a dispersive channel, we\\nhave two unknown channel-related parameters to deal with: the phase (carrier) delay , and\\nthe group (envelope) delay\\n, both of which were discussed in Chapter 2. In the context of\\nthese two parameters, when we speak of independent estimation for synchronization, we\\nmean that the two parameters and are considered individually rather than jointly.\\nIntuitively speaking, independent estimation is much easier to tackle and visualize than\\njoint estimation, and it may yield more robust estimates in general.\\nUpdated estimate of the parameter     Old estimate of the parameter     Step-size parameter     Error signal      + = x t s t     w t + =         f x     l   f x    =    c g c g',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 440,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '16 Recursive Maximum Likelihood Estimation for Synchronization Let the transmitted signal for symbol i in the QPSK signal be defined by\\n(7.252)\\nwhere E is the signal energy per symbol, T is the symbol period, and is the carrier\\nphase used for transmitting symbol i. For example, for the QPSK we have\\nEquivalently, we may write\\n(7.253)\\nwhere g(t) is the shaping pulse, namely a rectangular pulse of unit amplitude and duration\\nT. By definition, c affects the carrier and g affects the envelope. Accordingly, the\\nreceived signal at the channel output is given by (7.254)\\nwhere w(t) is the channel noise. The new term introduced in (7.254) is an additive\\ncarrier phase attributed to the phase delay produced by the dispersive channel; it is\\ndefined by\\n(7.255)\\nThe minus sign is included in the right-hand side of (7.255) to be consistent with previous\\nnotation used in dealing with signal detection.\\nBoth the carrier phase  and group delay g are unknown. However, it is assumed that\\nthey remain essentially constant over the observation interval 0  t  T0 or through the\\ntransmission of a sequence made up of L0 = T0T symbols.\\nWith used to account for the carrier delay\\n, we may simplify matters by using\\nin place of for the group delay; that is, (7.254) is rewritten as\\n(7.256)\\nAt the receiver, the orthogonal pair of basis functions for QPSK signals is defined by\\n(7.257)\\n(7.258)\\nHere, it is assumed that the receiver has perfect knowledge of the carrier frequency fc,\\nwhich is a reasonable assumption; otherwise, a carrier-frequency offset has to be included\\nthat will complicate the analysis.\\nsi t 2E T ------- 2fct 2 +   0 t T   cos = i i  4--- 2i 1 -   i  1 2 3 4  = = si t 2E T ------- 2fct i +  g t cos = x t 2E T ------- 2 fc t c -   ig t g -   w t + + cos = 2E T ------- 2fct  i + +  g t g -   w t + cos =  c  2fc - c =  c  g xt 2E T ------- 2fct  i + +  g t  -   w t  t T +   + cos = i 1 2 3 4  = 1 t 2 T--- 2fct    t T  +   cos = 2 t 2 T--- 2fct    t T  +   sin =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 441,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nAccordingly, we may represent the received signal x(t) by the baseband vector\\n(7.259) where (7.260) In a corresponding fashion, we may express the signal component of x( ) by the vector\\n(7.261) where (7.262) Assuming that fc is an integer multiple of the symbol rate 1/T, evaluation of the integral in\\n(7.262) shows that dependence of s1 and s2 on the group delay is eliminated, as shown\\nby (7.263) (7.264) We may thus expand on (7.259) to write\\n(7.265) where (7.266) The two elements of the noise vector w are themselves defined by\\n(7.267)\\nThe wk in (7.267) is the sample value of a Gaussian random variable W of zero mean and\\nvariance N02, where N02 is the power spectral density of the channel noise w(t).\\nDependence of the baseband signal vector x on delay is inherited from (7.265).\\nThe conditional probability density function of the random vector X, represented by the\\nsample x at the receiver input given transmission of the ith symbol, and occurrence of the\\ncarrier phase  and group delay resulting from the dispersive channel, is defined by\\n(7.268) x   x1   x2   = xk  x tk t dt k 1 2  =   T  +  =  s i     s1 i     s2 i     = sk i     2E T ------- 2fct  i + +  k t dt k 1 2  =  cos  T  +  = i 1 2 3 4  =  s1 i     E  i +   cos = s2 i     E -  i +   sin = x   s i     w  i 1 2 3 4  =  + = w  w1  w2  = wk w tk t dt k   T  +  1 2  = =  fX x i     1 N0 ---------- 1 N0 ------ x  s i     - 2 -     exp =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 442,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '16 Recursive Maximum Likelihood Estimation for Synchronization Setting equal to zero, (7.268) reduces to\\n(7.269)\\nEquation (7.268) defines the probability density function of the random vector X in the\\ncombined presence of signal and channel noise, whereas (7.269) defines the probability\\ndensity function of x in the presence of channel noise acting alone. Accordingly, we may\\ndefine the likelihood function for QPSK as the ratio of these two probability density\\nfunctions, as shown by\\n(7.270)\\nIn QPSK, we have\\nbecause all four message points lie on a circle of radius\\n. Hence, ignoring the second\\nterm in the exponent in (7.270), we may reduce the likelihood function to\\n(7.271)\\nComplex Terminology for Algorithmic Synchronization\\nBefore proceeding with the derivations of adaptive filtering algorithms for recovery of the\\nclock and carrier, we find it instructive to reformulate the likelihood function of (7.271)\\nusing complex terminology. Such a step is apropos given the fact that the received signal\\nvector as well as its contituent signal and noise vectors in (7.265) are all in their respective\\nbaseband forms.\\nSpecifically, the two-dimensional vector is represented by the complex envelope\\nof the received signal\\n(7.272) where . Correspondingly, the signal vector\\n, comprising the pair of signal components and\\n, is represented by the complex envelope of the transmitter signal\\ncorrupted by carrier phase\\n:\\n(7.273)\\nThe new complex parameter in (7.273) is a symbol indicator in the message\\nconstellation of the QPSK; it is defined by\\n(7.274) s i     fX x s 0 =   1 N0 ---------- 1 N0 ------ x 2 -     exp = l i     fX x i     fX x s 0 =   ---------------------------------\\n= 2 N0 ------xT  s i     1 N0 ------ s i    2 -     exp = s i     constant = E l i     2 N0 ------xT s i         exp = x  x  x1 jx2 + = j 1 - = s i     s1 i     s2 i      s i     s1 i     js2 i     + = E i     j i     sin + cos   = E iej i 1 2 3 4  =  =  i  i eji = i j i sin + cos =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 443,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nCorrespondingly, the complex experimental factor embodying the carrier phase is defined by (7.275) Both (7.274) and (7.275) follow from Eulers formula.\\nWith the complex representations of (7.272) to (7.275) at hand, we may now reformulate\\nthe exponent of the likelihood function in (7.271) in the equivalent complex form:\\n(7.276)\\nwhere Re[.] denotes the real part of the complex expression inside the square brackets.\\nHence, we may make the following statement:\\nThe inner product of the two complex vectors and in (7.276) is replaced by times the real part of the inner product of two complex\\nvariables: and . Two points are noteworthy here:\\nThe complex envelope of the received signal is dependent on the group delay ,\\nhence\\n. The product is made up of the complex symbol indicator\\nattributed to the QPSK signal generated in the transmitter and the exponential term attributed to phase distortion in the channel.\\nIn complex variable theory, given a pair of complex terms and\\n, their\\ninner product could be defined as\\n, as shown in (7.276).\\nThe complex representation on the right-hand side of (7.276), expressed in Cartesian\\nform, is well suited for estimating the unknown phase\\n. On the other hand, for estimating\\nthe unknown group delay , we find it more convenient to use a polar representation for\\nthe inner product of the two vectors and , as shown by (7.277) Indeed, it is a straightforward matter to show that the two complex representations on the\\nright-hand side of (7.276) and (7.277) are indeed equivalent. The reasons for why these\\ntwo representations befit the estimation of carrier phase and group delay\\n, respec-\\ntively, will become apparent in the next two subsections.\\nMoreover, in light of what was said previously, estimation of the group delay should\\nprecede that of the carrier phase. Accordingly, the next subsection is devoted to group-\\ndelay estimation, followed by the sub-section devoted to carrier-phase estimation.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 444,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'precede that of the carrier phase. Accordingly, the next subsection is devoted to group-\\ndelay estimation, followed by the sub-section devoted to carrier-phase estimation.\\nRecursive Estimation of the Group Delay\\nTo begin the task of estimating the unknown group delay, first of all we have to remove\\ndependence of the likelihood function on the unknown carrier phase in  ej  j  sin + cos = 2 N0 ------xTs i     2 E N0 ----------- Re xi s* i \\n     = 2 E N0 ----------- Re x  i*e\\nj -   = x  s i     E x   iej  x   iej  i ej x   ie j x  iej  * x  i*e j - =   x  s i     2 N0 ------xT s i     2 E N0 -----------  ix \\narg x    arg  i   -  -   cos =   l i',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 444,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '16 Recursive Maximum Likelihood Estimation for Synchronization (7.271). To do this, we will average the likelihood function over all possible values of\\ninside the range\\n. To this end, is assumed to be uniformly distributed inside this\\nrange, as shown by (7.278) which is the worst possible situation that can arise in practice. Under this assumption, we\\nmay thus express the average likelihood function as\\n(7.279)\\nwhere, in the last line, we used (7.271).\\nExamining the two alternative complex representations of the likelihood functions\\nexponent given in (7.276) and (7.277), it is the latter that best suits solving the integration\\nin (7.279). Specifically, we may write\\n(7.280)\\nwhere, in the last line, we have made the substitution\\nWe now invoke the definition of the modified Bessel function of zero order, as shown by\\n(see Appendix C)\\n(7.281)\\nUsing this formula, we may, therefore, express the average likelihood function\\nin (7.280) as follows:\\n(7.282)\\nwhere is the complex envelope of the matched filter output in the receiver. By\\ndefinition, for QPSK we have\\n 0 2     f  1 2 ------ 0  2    0 otherwise       = lav  i     l i    f  d 0 2  = 1 2 ------ l i     d 0 2  = 1 2 ------ exp 2 N0 ------xT s i         d 0 2  = lav  i     1 2 ------ 2 E N0 -----------  ix \\n( x       - arg - arg cos exp d 0 2  = 1 2 ------ 2 E N0 ----------- ix   d cos     exp x     l arg + arg - 2 x     i   arg + arg -  =  x    i    - arg - arg = I0 x  1 2 ------ ex  cos d 0 2  = lav  i     lav  i     I0 2 E N0 -----------  ixi \\n    = xi   i 1 for all i  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 445,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nIt follows, therefore, that (7.282) reduces to\\n(7.283)\\nHere, it is important to note that, as a result of averaging the likelihood function over the\\ncarrier phase , we havealso removed dependence on the transmitted symbol for\\nQPSK; this result is intuitively satisfying.\\nIn any event, taking the natural logarithm of lav() in (7.283) to obtain the log-\\nlikelihood function of , we write\\n(7.284)\\nwhere ln denotes the natural logarithm. To proceed further, we need to find a good\\napproximation for Lav(). To this end, we first note that the modified Bessel function I0(x)\\nmay itself be expanded in a power series (see Appendix C):\\nwhere x stands for the product term\\n. For small values of x, we may thus\\napproximate I0(x) as shown by\\nWe may further simplify matters by using the approximation\\n(7.285)\\nFor the problem at hand, small x corresponds to small SNR. Under this condition, we may\\nnow approximate the log-likelihood function of (7.284) as follows:\\n(7.286)\\nWith maximization of Lav() as the objective, we differentiate it with respect to the\\nenvelope delay , obtaining\\n(7.287)\\nwhere is the complex conjugate of and is its derivative with respect to .\\nlav  I0 2 E N0 ----------- x      =  i Lav  lav  ln = I ln 0 2 E N0 ----------- x      = I0 x  1 2---x    2m m!  2 ------------------ m 0 =   = 2  E N0x     I0 x  1 x2 4----- +  I0 ln x  1 x2 4----- +     ln  x2 4-----  for small x Lav  E N0 2 ------ x 2  Lav   ------------------- E N0 2 ------   ----- xi  2 = 2E N0 2 ------- Re x* x\\n  = xi *  x  x',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 446,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '16 Recursive Maximum Likelihood Estimation for Synchronization The formula in (7.287) is the result of operating on the received signal at the channel\\noutput,\\n, defined in (7.254) for a particular symbol of the QPSK signal defined in the\\ninterval [ , T +\\n]. In the course of finding the baseband vector representation of the\\nreceived signal, namely\\n, dependence on time t disappeared in (7.287).\\nNotwithstanding this point, the fact of the matter is the log-likelihood ratio in\\n(7.287) pertains to some point in discrete time n = tT, and it changes with n. To go forward\\nwith recursive estimation of the group delay , we must therefore bring discrete time n into\\nthe procedure. To this end, n is assigned as a subscript to both and in (7.287).\\nThus, with the recursive estimation of following the format described in words in (7.251),\\nwe may define the error signal needed for the recursive estimation of (i.e., symbol-timing\\nrecovery) as follows:\\n(7.288)\\nLet denote the estimate of the unknown group delay  at discrete time n.\\nCorrespondingly, we may introduce two definitions\\n(7.289) and (7.290) Accordingly, we may reformulate the error signal en in (7.288) as follows:\\n(7.291)\\nComputation of the error signal en, therefore, requires the use of two filters:\\nComplex matched filter, which is used for generating\\n.\\nComplex derivative matched filter, which is used for generating\\n.\\nBy design, the receiver is already equipped with the first filter. The second one is new. In\\npractice, the additional computational complexity due to the derivative matched filter is\\nfound to be an undesireable requirement. To dispense with the need for it, we propose to\\napproximate the derivative using a finite difference, as shown by (7.292)\\nNote, however, that in using the finite-difference approximation of (7.292) we have\\nsimplified computation of the derivative matched filter by doubling the symbol rate. It is\\ndesirable to make one further modification to account for the fact that timing estimates are',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 447,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'simplified computation of the derivative matched filter by doubling the symbol rate. It is\\ndesirable to make one further modification to account for the fact that timing estimates are\\nupdated at multiples of the symbol period T and the only available quantities are\\n.\\nConsequently, we replace by the current (updated estimate) and replace\\nby the old estimate\\n. We may thus rewrite (7.292) as follows:\\n(7.293)\\nSo, we finally redefine the error signal as follows:\\n(7.294)\\nwhere the scaling factor 1T is accounted for in what follows.\\nx t   x  Lav   x*  x   en Re xn * xn   = n xn  x nT n +   = xn  xnT n +   = en Re x* nT n +  xnT n +     = xn  xn xnT n +   1 T--- x nT T 2--- n+1/2 + +     x nT T 2--- - n 1/2 - +     -  n n+1/2 n n 1/2 - n 1 - xnT n +   1 T--- x nT T 2--- n + +     x nT T 2--- - n 1 - +     -  en Re x* nT n +  x nT T 2--- n + +     x nT T 2--- - n 1 - +     -       =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 447,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nFinally, building on the format of the recursive estimation procedure described in\\n(7.251), we may formulate the adaptive filtering algorithm for symbol timing recovery:\\n(7.295)\\nwhere we have the following:\\n\\nThe  in (7.295) is the step-size parameter, in which the two scaling factors\\nand 1/T are absorbed; the factor was ignored in moving from (7.287) to\\n(7.288) and the factor 1/T was ignored from (7.293) to (7.294).\\n\\nThe error signal en is defined by (7.294).\\n\\nThe cn is a real number employed as control for the frequency of an oscillator,\\nreferred to as a number-controlled oscillator (NCO).\\nThe closed-loop feedback system for implementing the timing-recovery algorithm of\\n(7.295) is shown in Figure 7.48. From a historical perspective, the scheme shown in this\\nfigure is analogous to the continuous-time version of the traditional early-late gate\\nsynchronizer widely used for timing recovery. In light of this analogy, the scheme of\\nFigure 7.48 is referred to as a recursive early-late delay (NDA-ELD) synchronizer. At\\nevery recursion (i.e., time step), the synchronizer works on three successive samples of the\\nmatched filter output, namely: and The first sample is early and the last one is late, both defined with respect to the middle one.\\nRecursive Estimation of the Carrier Phase\\nWith estimation of the symbol time taken care of, the next step is to estimate the carrier\\nphase\\n. This estimation is also based on the likelihood function defined in (7.270), but\\nFigure 7.48 Nondata-aided early-late delay\\nsynchronizer for estimating the group delay.\\ncn 1 + cn en n  + 0 1 2 3 ...  = = 2E N0 2  2E N0 2  x nT T 2--- n + +    x nT n +    x nT T 2--- n 1 - - +       en x(nT + ) Error detector Loop filter Sample at t = nT +  NCO z-1 ~ ~ Complex envelope of matched filter output\\nat time t: x(t)  n  n',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 448,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '16 Recursive Maximum Likelihood Estimation for Synchronization with a difference: this time we use the complex representation on the right-hand side of\\n(7.276) for the likelihood functions exponent. Thus, the likelihood function of is now\\nexpressed as follows:\\n(7.296)\\nTaking the natural logorithm of both sides of (7.296), the log-likelihood function of is, therefore, given by (7.297) Here again, maximizing the estimate of the carrier phase as the issue of interest, we\\ndifferentiate with respect to , obtaining The real-part operator Re[] is linear; therefore, we may interchange this operation with\\nthe differentiation. Moreover, we have\\nAs a result of the differentiation, the argument in (7.297) is multiplied by -j,\\nwhich, in turn, has the effect of replacing the real-part operator Re[.] by the corresponding\\nimaginary-part operator Im[.] Accordingly, we may express derivative of the log-likelihood\\nfunction in (7.297) with respect to as follows:\\n(7.298)\\nWith this equation at hand, we are now ready to formulate the adaptive filtering algorithm\\nfor estimating the unknown carrier phase\\n. To this end, we incorporate discrete-time n\\ninto the recursive estimation procedure for clock recovery in a manner similar to what we\\ndid for the group delay; specifically:\\nWith the argument of the imaginary-part operator in (7.298) playing the role of error\\nsignal, we write:\\n(7.299)\\nwhere n denotes the normalized discrete-time.\\nThe scaling factor is absorbed in the new step-size parameter\\n.\\nWith denoting the old estimate of the carrier phase and denoting its\\nupdated value, the update rule for the estimation is defined as follows:\\n(7.300)\\nEquations (7.299) and (7.300) not only define the adaptive filtering algorithm for carrier-\\nphase estimation, but also they provide the basis for implementing the algorithm, as shown\\nin Figure 7.49. This figure may be viewed as a generalization of the well-known Costas loop',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 449,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'phase estimation, but also they provide the basis for implementing the algorithm, as shown\\nin Figure 7.49. This figure may be viewed as a generalization of the well-known Costas loop\\nfor the analog synchronization of linear quadrature-amplitude modulation schemes that\\n l   exp 2 E N0 -----------Re x  ie j\\n-       =  L   2 E N0 -----------Re x  ie j\\n-   =  L    L    -------------- 2 E N0 -----------   ------ Re x ie j\\n-   =   ------ e j - jej - - = x ie j -  L    -------------- 2 E N0 -----------Im x  ie j\\n-   =  en Im xn   ne jn -   = 2 E N0   n  n 1 + n 1 + n en n  + 0 1 2 3   = =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 449,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\ninvolve the combined use of in-phase and quadrature components, of which the QPSK is a\\nspecial example. As such, we may refer to the closed-loop synchronization scheme of Figure\\n49 as the recursive Costas loop for phase synchronization.\\nThe following points should be noted in Figure 7.49:\\n\\nThe detector supplies an estimate of the symbol indicator and, therefore, the\\ntransmitted symbol, given the matched filter output.\\n\\nFor the input\\n, the look-up table in the figure supplies the value of the exponential\\n\\nThe output of the error generator is the error signal en, defined in (7.299).\\n\\nThe block labeled z-1 represents a unit-time delay.\\nThe recursive Costas loop of Figure 7.49 uses a first-order digital filter. To improve the\\ntracking performance of this synchronization system, we may use a second-order digital\\nfilter. Figure 7.50 shows an example of a second-order recursive filter made up of a\\ncascade of two first-order sections, with  as an adjustable loop parameter. An important\\nproperty of a second-order recursive filter used in the Costas loop for phase recovery is\\nthat it will eventually lock onto the incoming carrier with no static error, provided that the\\nfrequency error between the receiver and transmitter is initially small.\\nConvergence Considerations\\nThe adaptive behavior of the filtering schemes in Figures 7.48 and 7.49 for group-delay\\nand carrier-phase estimation, respectively, is governed by how the step-size parameters\\nFigure 7.49 The recursive Costas loop for estimating the carrier phase.\\nFigure 7.50 Second-order recursive filter.\\nen n  an Detector + + ~ exp( ) Complex envelope of matched filter output\\nat time t = nT: xn exp(-j ) Look-up table Error generator Unit-delay First-order recursion filter n + 1 z-1     n  n  n j n -   exp  n j  n sin - cos = Input sequence + + + + Output sequence z-1 z-1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 450,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '17 Summary and Discussion are selected. The smaller we make and, likewise,\\n, the more refined will be\\nthe trajectories resulting from application of the algorithms. However, this benefit is\\nattained at the cost of the number of recursions required for convergence of the algorithms.\\nOn the other hand, if the step-size parameter and is assigned a large value, then the\\ntrajectories may follow a zig-zag sort of path. Indeed, if and exceeds a certain critical\\nvalue of its own, it is quite possible for the algorithm to diverge, which means that the\\nsynchronization schemes of Figures 7.48 and 7.49 may become unstable. So, from a\\ndesign perspective, the compromise choice between accuracy of estimation and speed of\\nconvergence may require a detailed attention, both theoretical and experimental. Summary and Discussion\\nThe primary goal of the material presented in this chapter is the formulation of a\\nsystematic procedure for the analysis and design of a digital communication receiver in\\nthe presence of AWGN. The procedure, known as maximum likelihood detection, decides\\nwhich particular transmitted symbol is the most likely cause of the noisy signal observed\\nat the channel output. The approach that led to the formulation of the maximum likelihood\\ndetector (receiver) is called signal-space analysis. The basic idea of the approach is to\\nrepresent each member of a set of transmitted signals by an N-dimensional vector, where\\nN is the number of orthonormal basis functions needed for a unique geometric\\nrepresentation of the transmitted signals. The set of signal vectors so formed defines a\\nsignal constellation in an N-dimensional signal space.\\nFor a given signal constellation, the (average) probability of symbol error, Pe, incurred\\nin maximum likelihood signal detection over an AWGN channel is invariant to rotation of\\nthe signal constellation as well as its translation. However, except for a few simple (but',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 451,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'in maximum likelihood signal detection over an AWGN channel is invariant to rotation of\\nthe signal constellation as well as its translation. However, except for a few simple (but\\nimportant) cases, the numerical calculation of Pe is an impractical proposition. To\\novercome this difficulty, the customary practice is to resort to the use of bounds that lend\\nthemselves to computation in a straightforward manner. In this context, we described the\\nunion bound that follows directly from the signal-space diagram. The union bound is\\nbased on an intuitively satisfying idea:\\nThe probability of symbol error Pe is dominated by the nearest neighbors to the\\ntransmitted signal in the signal-space diagram.\\nThe results obtained using the union bound are usually fairly accurate, particularly when\\nthe SNR is high.\\nWith the basic background theory on optimum receivers covered in the early part of\\nChapter 7 at our disposal, formulas were derived for, or bounds on, the BER for some\\nimportant digital modulation techniques in an AWGN channel:\\nPSK, using coherent detection; it is represented by\\n binary PSK;  QPSK and its variants, namely, such as the offset QPSK;',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 451,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'coherent M-ary PSK, which includes binary PSK and QPSK as special cases with\\nM = 2 and M = 4, respectively.\\nThe DPSK may be viewed as the pseudo-noncoherent form of PSK.\\n and',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 451,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nM-ary QAM, using coherent detection; this modulation scheme is a hybrid form of\\nmodulation that combines amplitude and phase-shift keying. For M = 4, it includes\\nQPSK as a special case.\\nFSK, using coherent detection; it is represented by\\n binary FSK;  MSK and its Gaussian variant known as GMSK;',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 452,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'M-ary FSK.\\nNoncoherent detection schemes, involving the use of binary FSK and DPSK.\\nIrrespective of the digital modulation system of interest, synchronization of the receiver to\\nthe transmitter is essential to the operation of the system. Symbol timing recovery is\\nrequired whether the receiver is coherent or not. If the receiver is coherent, we also require\\nprovision for carrier recovery. In the latter part of the chapter we discussed nondata-aided\\nsynchronizers to cater to these two requirements with emphasis on M-ary PSK,\\nexemplified by QPSK signals, in which the carrier is suppressed. The presentation focused\\non recursive synchronization techniques that are naturally suited for the use of discrete-\\ntime signal processing algorithms.\\nWe conclude the discussion with some additional notes on the two adaptive filtering\\nalgorithms described in Section 7.16 on estimating the unknown parameters: carrier phase\\nand group delay. In a computational context, these two algorithms are in the same class as\\nthe celebrated least-mean-square (LMS) algorithm described by Widrow and Hoff over\\n50 years ago. The LMS algorithm is known for its computational efficiency, effectiveness\\nin performance, and robustness with respect to the nonstationary character of the\\nenvironment in which it is embedded. The two algorithmic phase and delay synchronizers\\nshare the first two properties of the LMS algorithm; for a conjecture, it may well be they\\nare also robust when operating in a nonstationary communication environment.\\nProblems\\nRepresentation of Signals In Chapter 6 we described line codes for pulse-code modulation. Referring to the material presented\\ntherein, formulate the signal constellations for the following line codes:\\na. unipolar nonreturn-to-zero code\\nb. polar nonreturn-to-zero code\\nc. unipolar return-to-zero code\\nd. manchester code. An 8-level PAM signal is defined by\\nwhere Ai = 1, 3, 5, 7. Formulate the signal constellation of',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 452,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'b. polar nonreturn-to-zero code\\nc. unipolar return-to-zero code\\nd. manchester code. An 8-level PAM signal is defined by\\nwhere Ai = 1, 3, 5, 7. Formulate the signal constellation of\\n. Figure P7.3 displays the waveforms of four signals s1(t), s2(t), s3(t), and s4(t).\\na. Using the Gram-Schmidt orthogonalization procedure, find an orthonormal basis for this set of\\nsignals.\\nb. Construct the corresponding signal-space diagram.\\nsi t Ai rect t T--- 1 2--- -     = si t  i 1 =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 452,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': 'Problems 433 7.4 a. Using the Gram-Schmidt orthogonalization procedure, find a set of orthonormal basis functions\\nto represent the three signals s1(t), s2(t), and s3(t) shown in Figure P7.4.\\nb. Express each of these signals in terms of the set of basis functions found in part a. An orthogonal set of signals is characterized by the property that the inner product of any pair of\\nsignals in the set is zero. Figure P7.5 shows a pair of signals s1(t) and s2(t) that satisfy this definition.\\nConstruct the signal constellation for this pair of signals. A source of information emits a set of symbols denoted by\\n. Two candidate modulation\\nschemes, namely pulse-duration modulation (PDM) and pulse-position modulation (PPM), are\\nconsidered for the electrical representation of this set of symbols. In PDM, the ith symbol is\\nrepresented by a pulse of unit amplitude and duration (i/M)T. On the other hand, in PPM, the ith\\nsymbol is represented by a short pulse of unit amplitude and fixed duration, which is transmitted at\\ntime t = (i/M)T. Show that PPM is the only one of the two that can produce an orthogonal set of\\nsignals over the interval 0  t  T. A set of 2M biorthogonal signals is obtained from a set of M ordinary orthogonal signals by\\naugmenting it with the negative of each signal in the set.\\na. The extension of orthogonal to biorthogonal signals leaves the dimensionality of the signal space\\nunchanged. Explain how.\\nb. Construct the signal constellation for the biorthogonal signals corresponding to the pair of\\northogonal signals shown in Figure P7.5.\\nFigure P7.3 0 T t 3 T T 3 1 s1(t) 0 2T t 3 1 s2(t) 0 t 1 s3(t) T 0 t 1 s4(t) Figure P7.4 4 s1(t) 3 2 1 0 -1 -2 -3 -4 1 2 3 t 4 s2(t) 3 2 1 0 -1 -2 -3 -4 -5 1 2 3 t 4 s3(t) 3 2 1 0 -1 -2 -3 -4 1 2 3 Figure P7.5 1 -1 s1(t) 0 T/2 T t 1 s2(t) 0 T t mi  i 1 = M',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 453,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels a. A pair of signals si(t) and sk(t) have a common duration T. Show that the inner product of this\\npair of signals is given by\\nwhere si and sk are the vector representations of si(t) and sk(t), respectively.\\nb. As a follow-up to part a of the problem, show that Consider a pair of complex-valued signals si(t) and sk(t) that are respectively represented by\\nwhere the basis functions 1(t) and 2(t) are both real valued, but the coefficients a11, a12, a21, and\\na22 are complex valued. Prove the complex form of the Schwarz inequality:\\nwhere the asterisk denotes complex conjugation. When is this relation satisfied with the equality sign?\\nStochastic Processes Consider a stochastic process X(t) expanded in the form\\nwhere is a remainder noise term. The form an orthonormal set over the interval\\n0  t  T, and the random variable Xi is defined by Let denote a random variable obtained by observing at time t = tk. Show that Consider the optimum detection of the sinusoidal signal in AWGN:\\na. Determine the correlator output assuming a noiseless input.\\nb. Determine the corresponding matched filter output, assuming that the filter includes a delay T to\\nmake it causal.\\nc. Hence, show that these two outputs are exactly the same only at the time instant t = T.\\nProbability of Error Figure P7.12 shows a pair of signals s1(t) and s2(t) that are orthogonal to each other over the\\nobservation interval 0  t  3T. The received signal is defined by\\nsi tsk t dt 0 T  si Tsk = si t sk t -  2 dt 0 T  si sk - 2 = s1 t= a111 t a122 t +  t   - s2 t= a211 t a222 t +  t   - s1 ts2* t dt x - x  2 s1 t2 dt s2 t2 dt x - x  x - x   X t Xii t Wt 0 t T   + i 1 = N  = Wt i t  i 1 = N Xi X ti t dt 0 T  = Wtk   Wt \\x03 XjWtk     0 = j 1, 2, = N  0 tk T      s t 8t T --------     0 t T   sin =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 454,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Problems where w(t) is white Gaussian noise of zero mean and power spectral density N0/2.\\na. Design a receiver that decides in favor of signals s1(t) or s2(t), assuming that these two signals are\\nequiprobable.\\nb. Calculate the average probability of symbol error incurred by this receiver for E/N0 = 4, where E\\nis the signal energy. In the Manchester code discussed in Chapter 6, binary symbol 1 is represented by the doublet pulse\\ns(t) shown in Figure P7.13, and binary symbol 0 is represented by the negative of this pulse. Derive\\nthe formula for the probability of error incurred by the maximum likelihood detection procedure\\napplied to this form of signaling over an AWGN channel. In the Bayes test, applied to a binary hypothesis-testing problem where we have to choose one of\\ntwo possible hypotheses H0 and H1, we minimize the risk defined by\\nThe parameters C00, C10, C11, and C01 denote the costs assigned to the four possible outcomes of the\\nexperiment: the first subscript indicates the hypothesis chosen and the second the hypothesis that is\\ntrue. Assume that C10 > C00 and C01 > C11. The p0 and p1 denote the a priori probabilities of\\nhypotheses H0 and H1, respectively.\\na. Given the observation vector x, show that the partitioning of the observation space so as to\\nminimize the risk leads to the likelihood ratio test:\\nsay H0 if say H1 if where is the likelihood ratio defined by\\nx t sk t w t + = 0 t 3T  k 1 2  =    Figure P7.12 Figure P7.13 1 -1 s1(t) 0 2T 3T T t 1 -1 s2(t) 0 5T 3T 3T T 2 2 t 2 1 -1 s(t) 0 T/2 T t   C00p0 say H0 H0 is true\\n\\n\\nC10p0 say H1 H0 is true\\n\\n\\nC11p1 say H1 H1 is true\\n\\n\\nC01p1 say H0 H1 is true\\n  + + + =  x    x    x  x  fX x H1   fX x H0   ----------------------\\n=',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 455,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nand  is the threshold of the test defined by\\nb. What are the cost values for which the Bayes criterion reduces to the minimum probability of\\nerror criterion?\\nPrinciples of Rotational and Translational Invariance Continuing with the four line codes considered in Problem 7.1, identify the line codes that have\\nminimum average energy and those that do not. Compare your answers with the observations made\\non these line codes in Chapter 6. Consider the two constellations shown in Figure 7.10. Determine the orthonormal matrix Q that\\ntransforms the constellation shown in Figure 7.10a into the one shown in Figure 7.10b. a. The two signal constellations shown in Figure P7.17 exhibit the same average probability of\\nsymbol error. Justify the validity of this statement.\\nb. Which of these two constellations has minimum average energy? Justify your answer.\\nYou may assume that the symbols pertaining to the message points displayed in Figure P7.17 are\\nequally likely. Simplex (transorthogonal) signals are equally likely highly-correlated signals with the most negative\\ncorrelation that can be achieved with a set of M orthogonal signals. That is, the correlation\\ncoefficient between any pair of signals in the set is defined by\\nOne method of constructing simplex signals is to start with a set of M orthogonal signals each with\\nenergy E and then apply the minimum energy translate.\\nConsider a set of three equally likely symbols whose signal constellation consists of the vertices of\\nan equilateral triangle. Show that these three symbols constitute a simplex code.\\nAmplitude-Shift Keying In the on-off keying version of an ASK system, symbol1 is represented by transmitting a sinusoidal\\ncarrier of amplitude\\n, where Eb is the signal energy per bit and Tb is the bit duration.\\nSymbol 0 is represented by switching off the carrier. Assume that symbols 1 and 0 occur with equal\\nprobability.  p0 C10 C00 -   p1 C01 C11 -   ----------------------------------',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 456,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Symbol 0 is represented by switching off the carrier. Assume that symbols 1 and 0 occur with equal\\nprobability.  p0 C10 C00 -   p1 C01 C11 -   ----------------------------------\\n= Figure P7.17 0 0 (a) 2 (b) 2 - 1 2    1  2  2  2  - -     ij 1 for i j = 1 M 1 -    - for i j     = 2Eb Tb',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 456,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems For an AWGN channel, determine the average probability of error for this ASK system under the\\nfollowing scenarios:\\na. Coherent detection.\\nb. Noncoherent detection, operating with a large value of bit energy-to-noise spectral density ratio\\nEbN0.\\nNote: when x is large, the modified Bessel function of the first kind of zero order may be\\napproximated as follows (see AppendixC):\\nPhase-Shift Keying The PSK signal is applied to a correlator supplied with a phase reference that lies within radians\\nof the exact carrier phase. Determine the effect of the phase error on the average probability of\\nerror of the system. The signal component of a PSK system scheme using coherent detection is defined by\\nwhere 0  t  Tb, the plus sign corresponds to symbol 1, and the minus sign corresponds to symbol\\n0; the parameter k lies in the range 0  k  1. The first term of s(t) represents a carrier component\\nincluded for the purpose of synchronizing the receiver to the transmitter.\\na. Draw a signal-space diagram for the scheme described here. What observations can you make\\nabout this diagram?\\nb. Show that, in the presence of AWGN of zero mean and power spectral density N0/2, the average\\nprobability of error is\\nwhere\\nc. Suppose that 10% of the transmitted signal power is allocated to the carrier component.\\nDetermine the EbN0 required to realize Pe = 10-4.\\nd. Compare this value of EbN0 with that required for a binary PSK scheme using coherent\\ndetection, with the same probability of error. a. Given the input binary sequence 1100100010, sketch the waveforms of the in-phase and\\nquadrature components of a modulated wave obtained using the QPSK based on the signal set of\\nFigure 7.16.\\nb. Sketch the QPSK waveform itself for the input binary sequence specified in part a. Let PeI and PeQ denote the probabilities of symbol error for the in-phase and quadrature channels,\\nrespectively, of a narrowband digital communication system. Show that the average probability of',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 457,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'respectively, of a narrowband digital communication system. Show that the average probability of\\nsymbol error for the overall system is given by\\nPe = PeI + PeQ - PeIPeQ Equation (7.132) is an approximate formula for the average probability of symbol error for M-ary\\nPSK using coherent detection. This formula was derived using the union bound in light of the signal-\\nspace diagram of Figure 7.22b. Given that message point m1 was transmitted, show that the\\napproximation of (7.132) may be derived directly from Figure 7.22b.\\nI0 x  x  exp 2x -----------------    s t Ack 2fct   Ac 1 k2 - 2fct   cos  sin = Pe Q 2Eb N0 --------- 1 k2 -   = Eb 1 2---Ac 2Tb =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 457,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels Find the power spectral density of an offset QPSK signal produced by a random binary sequence in\\nwhich symbols 1 and 0 (represented by 1) are equally likely and the symbols in different time slots\\nare statistically independent and identically distributed. Vestigial sideband modulation (VSB), discussed in Chapter 2, offers another possible modulation\\nmethod for signaling over an AWGN channel.\\na. In particular, a digital VSB transmission system may be viewed as a time-varying one-\\ndimensional system operating at a rate of 2/T dimensions per second, where T is the symbol\\nperiod. Justify the validity of this statement.\\nb. Show that digital VSB is indeed equivalent in performance to the offset QPSK.\\nQuadrature Amplitude Modulation Referring back to Example 7, develop a systematic procedure for constructing M-ary QAM\\nconstellations given the M-ary QAM constellation of Figure 7.24 for M = 16. In effect, this problem\\naddresses the opposite approach to that described in Example 7. Figure P7.28 describes the block diagram of a generalized M-ary QAM modulator. Basically, the\\nmodulator includes a mapper that produces a complex amplitude am input for m = 0, 1, \\nThe real and imaginary parts of am input the basis functions and\\n, respectively. The\\nmodulator is generalized in that it embodies M-ary PSK and M-ary PAM as special cases.\\na. Formulate the underlying mathematics of the modulator described in Figure P7.28.\\nb. Hence, show that M-ary PSK and M-ary PAM are indeed special cases of the M-ary QPSK\\ngenerated by the block diagram of Figure P7.28.\\nFrequency-Shift Keying The signal vectors s1 and s2 are used to represent binary symbols 1 and 0, respectively, in a binary\\nFSK system using coherent detection. The receiver decides in favor of symbol1 when\\nwhere xTsi is the inner product of the observation vector x and the signal vector si, i = 1, 2. Show that',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 458,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'FSK system using coherent detection. The receiver decides in favor of symbol1 when\\nwhere xTsi is the inner product of the observation vector x and the signal vector si, i = 1, 2. Show that\\nthis decision rule is equivalent to the condition x1 > x2, where x1 and x2 are the two elements of the\\nobservation vector x. Assume that the signal vectors s1 and s2 have equal energy. An FSK system transmits binary data at the rate of bits/s. During the course of\\ntransmission, white Gaussian noise of zero mean and power spectral density 10-20 W/Hz is added to\\n1 t 2 t Figure P7.28 Input m = 0, 1, , M - 1 Output sm(t)  Mapper Re[am] Im[am] 1(t)  2(t)  am xTs1 xTs2  2.5 106',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 458,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems the signal. In the absence of noise, the amplitude of the received sinusoidal wave for digit 1 or 0 is\\n1mV. Determine the average probability of symbol error for the following system configurations:\\na. binary FSK using coherent detection;\\nb. MSK using coherent detection;\\nc. binary FSK using noncoherent detection. In an FSK system using coherent detection, the signals s1(t) and s2(t) representing binary symbols\\nand 0, respectively, are defined by Assuming that fc > f, show that the correlation coefficient of the signals s1(t) and s2(t) is\\napproximately given by\\na. What is the minimum value of frequency shift f for which the signals s1(t) and s2(t) are\\northogonal?\\nb. What is the value of f that minimizes the average probability of symbol error?\\nc. For the value of f obtained in part c, determine the increase in EbN0 required so that this FSK\\nscheme has the same noise performance as a binary PSK scheme system, also using coherent\\ndetection. A binary FSK signal with discontinuous phase is defined by\\nwhere Eb is the signal energy per bit, Tb is the bit duration, and 1 and 2 are sample values of\\nuniformly distributed random variables over the interval 0 to 2. In effect, the two oscillators\\nsupplying the transmitted frequencies fc  f /2 operate independently of each other. Assume that\\nfc >>f.\\na. Evaluate the power spectral density of the FSK signal.\\nb. Show that, for frequencies far removed from the carrier frequency fc, the power spectral density\\nfalls off as the inverse square of frequency. How does this result compare with a binary FSK\\nsignal with continuous phase? Set up a block diagram for the generation of Sundes FSK signal s(t) with continuous phase by using\\nthe representation given in (7.170), which is reproduced here Discuss the similarities between MSK and offset QPSK, and the features that distinguish them. There are two ways of detecting an MSK signal. One way is to use a coherent receiver to take full',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 459,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'advantage of the phase information content of the MSK signal. Another way is to use a noncoherent\\ns1 ts2 t  Ac 2fc f 2-----     t 0 t Tb   cos =  s1 ts2 t dt 0 Tb s1 2 t dt 0 Tb --------------------------------------\\nsinc 2f.Tb    = s t 2Eb Tb --------- 2fc f 2----- +    t 1 + for symbol 1 cos 2Eb Tb --------- 2fc f 2----- -    t 2 + for symbol 0 cos          = s t 2Eb Tb --------- t Tb -----     2fct   2Eb Tb --------- t Tb -----     2fct   sin sin  cos cos =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 459,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nreceiver and disregard the phase information. The second method offers the advantage of simplicity\\nof implementation at the expense of a degraded noise performance. By how many decibels do we\\nhave to increase the bit energy-to-noise density ratio EbN0 in the second method so as to realize the\\nsame average probability of symbol error equal to 10-5? a. Sketch the waveforms of the in-phase and quadrature components of the MSK signal in response\\nto the input binary sequence 1100100010.\\nb. Sketch the MSK waveform itself for the binary sequence specified in part a. An NRZ data stream of amplitude levels 1 is passed through a low-pass filter whose impulse\\nresponse is defined by the Gaussian function\\nwhere  is a design parameter defined in terms of the filters 3dB bandwidth by\\na. Show that the transfer function of the filter is defined by\\nHence, demonstrate that the 3dB bandwidth of the filter is indeed equal to W. You may use the\\nlist of Fourier-transform pairs in Table 2.1.\\nb. Determine the response of the filter to a rectangular pulse of unit amplitude and duration T\\ncentered on the origin. Summarize the similarities and differences between the standard MSK and Gaussian filtered MSK\\nsignals. Summarize the basic similarities and differences between the standard MSK and QPSK.\\nNoncoherent Receivers In Section 7.12 we derived the formula for the BER of binary FSK using noncoherent detection as a\\nspecial case of noncoherent orthogonal modulation. In this problem we revisit this issue. As before,\\nwe assume that symbol 1 is represented by signal s1(t) and symbol 0 is represented by signal s2(t).\\nAccording to the material presented in Section 7.12, we note the following:\\n\\nThe random variable L2 represented by the sample value l2 is Rayleigh distributed.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 460,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The random variable L2 represented by the sample value l2 is Rayleigh distributed.\\n\\nThe random variable L1 represented by the sample value l1 is Rician distributed.\\nThe Rayleigh and Rician distributions were discussed in Chapter 4. Using the probability\\ndistributions defined in that chapter, derive (7.245) for the BER of binary FSK, using noncoherent\\ndetection. Figure P7.41a shows a noncoherent receiver using a matched filter for the detection of a sinusoidal\\nsignal of known frequency but random phase and under the assumption of AWGN. An alternative\\nimplementation of this receiver is its mechanization in the frequency domain as a spectrum analyzer\\nreceiver, as in Figure P7.41b, where the correlator computes the finite-time autocorrelation function\\ndefined by\\nShow that the square-law envelope detector output sampled at time t = T in Figure P7.41a is twice\\nthe spectral output of the Fourier transform sampled at frequency f = fc in Figure P7.41b.\\nh t   ------- 2t2 2 ---------- -       exp =  2 ln 2 -------- 1 W ----- = H f 2f2 -   exp = Rx   x tx t  +   0  T    0 T  -  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 460,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems 441 7.42 The binary sequence 1100100010 is applied to the DPSK transmitter of Figure 7.44.\\na. Sketch the resulting waveform at the transmitter output.\\nb. Applying this waveform to the DPSK receiver of Figure 7.46, show that in the absence of noise\\nthe original binary sequence is reconstructed at the receiver output.\\nComparison of Digital Modulation Schemes Using a Single Carrier Binary data are transmitted over a microwave link at the rate of 106 bits/s and the power spectral\\ndensity of the noise at the receiver input is 10-10 W/Hz. Find the average carrier power required to\\nmaintain an average probability of error Pe  10-4 for the following schemes:\\na. Binary PSK using coherent detection;\\nb. DPSK. The values of EbN0 required to realize an average probability of symbol error Pe = 10-4 for binary\\nPSK and binary FSK schemes are equal to 7.2 and 13.5, respectively. Using the approximation\\ndetermine the separation in the values of EbN0 for Pe = 10-4, using:\\na. binary PSK using coherent detection and DPSK;\\nb. binary PSK and QPSK, both using coherent detection;\\nc. binary FSK using (i) coherent detection and (ii) noncoherent detection;\\nd. binary FSK and MSK, both using coherent detection. In Section 7.14 we compared the noise performances of various digital modulation schemes under\\nthe two classes of coherent and noncoherent detection; therein, we used the BER as the basis of\\ncomparison. In this problem we take a different viewpoint and use the average probability of symbol\\nerror Pe, to do the comparison. Plot Pe versus EbN0 for each of these schemes and comment on\\nyour results. Synchronization 7.46 Demonstrate the equivalence of the two complex representations given in (7.276) and (7.277), which\\npertain to the likelihood function. a. In the recursive algorithm of (7.295) for symbol timing recovery, the control signals cn and cn +\\nare both dimensionless. Discuss the units in which the error signal en and step-size parameter\\nare measured.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 461,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'are both dimensionless. Discuss the units in which the error signal en and step-size parameter\\nare measured.\\nb. In the recursive algorithm of (7.300) for phase recovery, the old estimate and the updated\\nestimate of the carrier phase  are both measured in radians. Discuss the units in which the\\nerror signal en and step-size parameter are measured. The binary PSK is a special case of QPSK. Using the adaptive filtering algorithms derived in Section\\n16 for estimating the group delay and carrier phase\\n, find the corresponding adaptive filtering\\nalgorithms for binary PSK. Repeat Problem 7.48, but this time find the adaptive filtering algorithms for M-ary PSK.\\nFigure P7.41 x(t) Filter matched to cos (2fct); 0  t  T Square-law envelope detector x(t) Correlator Fourier transformer Output sampled at f = fc Sample at t = T (a) (b) Output Q u  1 2u -------------- 2 - u2   exp    n  n 1 +',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 461,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels Suppose we transmit a sequence of L0 statistically independent symbols of a QPSK signal, as shown\\nby\\nwhere L0 is not to be confused with the symbol for average log-likelihood Lav. The channel output is\\ncorrupted by AWGN of zero mean and power spectral density N02, carrier phase\\n, and unknown\\ngroup delay .\\na. Determine the likelihood function with respect to the group delay , assuming that is uni-\\nformly distributed.\\nb. Hence, formulate the maximum likelihood estimate of the group delay .\\nc. Compare this feedforward scheme of group-delay estimation with that provided by the NDA-\\nELD synchronizer of Figure 7.48. Repeat Problem 7.50, but this time do the following:\\na. Determine the likelihood function with respect to the carrier phase\\n, assuming that the group\\ndelay is known.\\nb. Hence, formulate the maximum likelihood estimate of the carrier phase\\n.\\nc. Compare this feedforward scheme of a carrier-phase estimation with the recursive Costas loop of\\nFigure 7.49. In Section 7.16 we studied a nondata-aided scheme for carrier phase recovery, based on the log-\\nlikelihood function of (7.296). In this problem we explore the use of this equation for data-aided\\ncarrier phase recovery.\\na. Consider a receiver designed for a linear modulation system. Given that the receiver has\\nknowledge of a preamble of length L0, show that the maximum likelihood estimate of the carrier\\nphase is defined by\\nwhere the preamble is a known sequence of complex symbols and is the\\ncomplex envelope of the corresponding received signal.\\nb. Using the result derived in part a, construct a block diagram for the maximum likelihood phase\\nestimator. Figure P7.53 shows the block diagram of a phase-synchronization system. Determine the phase\\nestimate of the unknown carrier phase in the received signal',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 462,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'estimator. Figure P7.53 shows the block diagram of a phase-synchronization system. Determine the phase\\nestimate of the unknown carrier phase in the received signal\\n. s si i 0 = L0 1 - =          a n*xn n 0 = L0 1 -            arg = a n  n 0 = L0 1 - xn  n 0 = L0 1 - Figure P7.53  x t x(t) cos(2fct) sin(2fct) ( )dt 0 T \\x02 Received signal Phase estimate Multipliers ( )dt arctan 0 T \\x02 Yc Ys Ys Yc ( )  Integrators',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 462,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems 443 Computer Experiments **7.54 In this computer-oriented problem, we study the operation of the NDA-ELD synchronizer for\\nsymbol timing recovery by considering a coherent QPSK system with the following specifications:\\n\\nThe channel response is described by a raised cosine pulse with rolloff factor = 0.5.\\n\\nThe recursive filter is a first-order digital filter with transfer function\\nwhere z-1 denotes unit delay, is the step-size parameter, and A is a parameter, to be defined.\\n\\nThe loop bandwidth BL is 2 of the symbol rate 1T, that is, BLT = 0.02.\\nWith symbol timing recovery as the objective, a logical way to proceed is to plot the S-curve for the\\nNDA-ELD under the following conditions:\\na. EbN0 = 10 dB\\nb. EbN0 = (i.e., noiseless channel).\\nFor NDA-ELD, the scheme shown in Figure P7.54 is responsible for generating the S-curve that\\nplots the timing offset versus the discrete time n = tT.\\nUsing this scheme, plot the S-curves, and comment on the results obtained for parts a and b. In this follow-up to the computer-oriented Problem 7.54, we study the recursive Costas loop for\\nphase recovery using the same system specifications described in Problem 7.54. This time, however,\\nwe use the scheme of Figure P7.54 for measuring the S-curve to plot the phase error versus discrete-\\ntime n = tT. H z z 1 - 1 1  - Az 1 - - -------------------------------------\\n=  Figure P7.54 Figure P7.55 Error detector S( ) Average x(nT + ) t = nT + ~ ~ Complex envelope of matched filter output at time t: x(t) n  n   an exp(-j ) Detector Error generator en S( ) Average xn   Matched filter output',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 463,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over AWGN Channels\\nThe plot is to be carried out under the following conditions:\\na. EbN0 = 5 dB\\nb. EbN0 = 10 dB\\nc. EbN0 = 30 dB (i.e., practically noiseless channel)\\nComment on the results obtained for these three conditions.\\nNotes\\nThe geometric representation of signals was first developed by Kotelnikov (1947) which is a\\ntranslation of the original doctoral dissertation presented in January 1947 before the Academic\\nCouncil of the Molotov Energy Institute in Moscow. In particular, see Part II of the book. This method\\nwas subsequently brought to fuller fruition in the classic book by Wozencraft and Jacobs (1965).\\nThe classic reference for the union bound is Wozencraft and Jacobs (1965).\\nAppendix C addresses the derivation of simple bounds on the Q-function. In (7.88), we have used\\nthe following bound:\\nwhich becomes increasingly tight for large positive values of x.\\nFor an early paper on the offset QPSK, see Gitlin and Ho (1975).\\nThe MSK signal was first described in Doelz and Heald (1961). For a tutorial review of MSK and\\ncomparison with QPSK, see Pasupathy (1979). Since the frequency spacing is only half as much as\\nthe conventional spacing of 1/Tb that is used in the coherent detection of binary FSK signals, this\\nsignaling scheme is also referred to as fast FSK; see deBuda (1972), who was not aware of the\\nDoelz-Heald patent.\\nFor early discussions of GMSK, see Murota and Hirade (1981) and Ishizuke and Hirade (1980).\\nThe analytical specification of the power spectral density of digital FM is difficult to handle,\\nexcept for the case of a rectangular shaped modulating pulse. The paper by Garrison (1975) presents\\na procedure based on the selection of an appropriate duration-limited/level-quantized approximation\\nfor the modulating pulse. The equations developed therein are particularly suitable for machine\\ncomputation of the power spectra of digital FM signals; see the book by Stber (1996).',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 464,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'for the modulating pulse. The equations developed therein are particularly suitable for machine\\ncomputation of the power spectra of digital FM signals; see the book by Stber (1996).\\nA detailed analysis of the spectra of M-ary FSK for an arbitrary value of frequency deviation is\\npresented in the paper by Anderson and Salz (1965).\\nReaders who are not interested in the formal derivation of (7.227) may at this point wish to move\\non to the treatment of noncoherent binary FSK (in Section 7.12) and DPSK (in Section 7.13), two\\nspecial cases of noncoherent orthogonal modulation, without loss of continuity.\\nThe standard method of deriving the BER for noncoherent binary FSK, presented in\\nMcDonough and Whalen (1995) and that for DPSK presented in Arthurs and Dym (1962), involves\\nthe use of the Rician distribution. This distribution arises when the envelope of a sine wave plus\\nadditive Gaussian noise is of interest; see Chapter 4 for a discussion of the Rician distribution. The\\nderivations presented herein avoid the complications encountered in the standard method.\\nThe optimum receiver for differential phase-shift keying is discussed in Simon and Divsalar\\n(1992).\\nFor detailed treatment of the algorithmic approach for solving the synchronization problem in\\nsignaling over AWGN channels, the reader is referred to the books by Mengali and DAndrea (1997)\\nand Meyer et al. (1998). For books on the traditional approach to synchronization, the reader is\\nreferred to Lindsey and Simon (1973).\\nQ x  1 2 ---------- x2 2----- -     exp',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 464,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '445 CHAPTER 8 Signaling over Band-Limited\\nChannels 8.1 Introduction In Chapter 7 we focused attention on signaling over a channel that is assumed to be\\ndistortionless except for the AWGN at the channel output. In other words, there was no\\nlimitation imposed on the channel bandwidth, with the energy per bit to noise spectral\\ndensity ratio Eb/N0 being the only factor to affect the performance of the receiver. In\\nreality, however, every physical channel is not only noisy, but also limited to some finite\\nbandwidth. Hence the title of this chapter: signaling over band-limited channels.\\nThe important point to note here is that if, for example, a rectangular pulse, represent-\\ning one bit of information, is applied to the channel input, the shape of the pulse will be\\ndistorted at the channel output. Typically, the distorted pulse may consist of a main lobe\\nrepresenting the original bit of information surrounded by a long sequence of sidelobes on\\neach side of the main lobe. The sidelobes represent a new source of channel distortion,\\nreferred to as intersymbol interference, so called because of its degrading influence on the\\nadjacent bits of information.\\nThere is a fundamental difference between intersymbol interference and channel noise\\nthat could be summarized as follows:\\n\\nChannel noise is independent of the transmitted signal; its effect on data\\ntransmission over the band-limited channel shows up at the receiver input, once the\\ndata transmission system is switched on.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 465,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Intersymbol interference, on the other hand, is signal dependent; it disappears only\\nwhen the transmitted signal is switched off.\\nIn Chapter 7, channel noise was considered all by itself so as to develop a basic\\nunderstanding of how its presence affects receiver performance. It is logical, therefore,\\nthat in the sequel to that chapter, we initially focus on intersymbol interference acting\\nalone. In practical terms, we may justify a noise-free condition by assuming that the SNR\\nis high enough to ignore the effect of channel noise. The study of signaling over a band-\\nlimited channel, under the condition that the channel is effectively noiseless, occupies\\nthe first part of the chapter. The objective here is that of signal design, whereby the effect\\nof symbol interference is reduced to zero.\\nThe second part of the chapter focuses on a noisy wideband channel. In this case, data\\ntransmission over the channel is tackled by dividing it into a number of subchannels, with',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 465,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\neach subchannel being narrowband enough to permit the application of Shannons\\ninformation capacity law that was considered in Chapter 5. The objective here is that of\\nsystem design, whereby the rate of data transmission through the system is maximized to\\nthe highest level physically possible. Error Rate Due to Channel Noise in a Matched-Filter Receiver\\nWe begin the study of signaling over band-limited channels by determining the operating\\nconditions that would permit us to view the channel to be effectively noiseless. To this\\nend, consider the block diagram of Figure 8.1, which depicts the following data-\\ntransmission scenario: a binary data stream is applied to a noisy channel where the\\nadditive channel noise w(t) is modeled as white and Gaussian with zero mean and power\\nspectral density N02. The data stream is based on polar NRZ signaling, in which symbols\\n1 and 0 are represented by positive and negative rectangular pulses of amplitude A and\\nduration Tb. In the signaling interval 0 < t < Tb, the received signal is defined by\\n(8.1)\\nThe receiver operates synchronously with the transmitter, which means that the matched\\nfilter at the front end of the receiver has knowledge of the starting and ending times of\\neach transmitted pulse. The matched filter is followed by a sampler, and then finally a\\ndecision device. To simplify matters, it is assumed that the symbols 1 and 0 are equally\\nlikely; the threshold in the decision device, namely\\n, may then be set equal to zero. If\\nthis threshold is exceeded, the receiver decides in favor of symbol 1; if not, it decides in\\nfavor of symbol 0. A random choice is made in the case of a tie.\\nFollowing the geometric signal-space theory presented in Section 7.6 on binary PSK,\\nthe transmitted signal constellation consists of a pair of message points located at\\nand\\n. The energy per bit is defined by\\nThe only basis function of the signal-space diagram is a rectangular pulse defined as follows:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 466,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'and\\n. The energy per bit is defined by\\nThe only basis function of the signal-space diagram is a rectangular pulse defined as follows:\\n(8.2)\\nFigure 8.1 Receiver for baseband transmission of binary-encoded data stream using polar\\nNRZ signaling. x t +A + w t symbol 1 was sent  A - + w t symbol 0 was sent     =  + Eb Eb - Eb A2Tb = t Eb Tb   for 0 t Tb  0, otherwise      = Matched filter Polar NRZ signal of amplitude A Say 1 if y > White Gaussian noise w(t) Threshold Sample at time t = Tb   Say 0 if y < y + +  Decision device',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 466,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '3 Intersymbol Interference In mathematical terms, the form of signaling embodied in Figure 8.1 is equivalent to that\\nof binary PSK. Following (7.109), the average probability of symbol error incurred by the\\nmatched-filter receiver in Figure 8.1 is therefore defined by the Q-function\\n(8.3)\\nAlthough this result for NRZ-signaling over an AWGN channel may seem to be special,\\n(8.3) holds for a binary data transmission system where symbol 1 is represented by a\\ngeneric pulse g(t) and symbol 0 is represented by -g(t) under the assumption that the\\nenergy contained in g(t) is equal to Eb. This statement follows from matched-filter theory\\npresented in Chapter 7.\\nFigure 8.2 plots Pe versus the dimensionless SNR, EbN0. The important message to\\ntake from this figure is summed up as follows:\\nThe matched-filter receiver of Figure 8.1 exhibits an exponential improvement\\nin the average probability of symbol error Pe with the increase in Eb/N0.\\nFor example, expressing EbN0 in decibels we see from Figure 8.2 that Pe is on the order\\nof 10-6 when EbN0 = 10 dB. Such a value of Pe is small enough to say that the effect of\\nthe channel noise is ignorable.\\nHenceforth, in the first part of the chapter dealing with signaling over band-limited\\nchannels, we assume that the SNR, EbN0, is large enough to leave intersymbol\\ninterference as the only source of interference. Intersymbol Interference\\nTo proceed with a mathematical study of intersymbol interference, consider a baseband\\nbinary PAM system, a generic form of which is depicted in Figure 8.3. The term\\nbaseband refers to an information-bearing signal whose spectrum extends from (or near)\\nFigure 8.2\\nProbability of error in the\\nsignaling scheme of Figure 8.1. Probability of error, Pe\\n10 Eb/N0, dB 15 10-12 10-10 10-8 10-6 10-4 10-2 Pe Q 2Eb N0 ---------       =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 467,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nzero up to some finite value for positive frequencies. Thus, with the input data stream\\nbeing a baseband signal, the data-transmission system of Figure 8.3 is said to be a\\nbaseband system. Consequently, unlike the subject matter studied in Chapter 7, there is no\\ncarrier modulation in the transmitter and, therefore, no carrier demodulation in the\\nreceiver to be considered.\\nNext, addressing the choice of discrete PAM, we say that this form of pulse modulation\\nis one of the most efficient schemes for data transmission over a baseband channel when\\nthe utilization of both transmit power and channel bandwidth is of particular concern. In\\nthis section, we consider the simple case of binary PAM.\\nReferring back to Figure 8.3, the pulse-amplitude modulator changes the input binary\\ndata stream {bk} into a new sequence of short pulses, short enough to approximate\\nimpulses. More specifically, the pulse amplitude ak is represented in the polar form:\\n(8.4)\\nThe sequence of short pulses so produced is applied to a transmit filter whose impulse\\nresponse is denoted by g(t). The transmitted signal is thus defined by the sequence\\n(8.5)\\nEquation (8.5) is a form of linear modulation, which may be stated in words as follows:\\nA binary data stream represented by the sequence {ak}, where ak = +1 for\\nsymbol 1 and ak = -1 for symbol 0, modulates the basis pulse g(t) and\\nsuperposes linearly to form the transmitted signal s(t).\\nThe signal s(t) is naturally modified as a result of transmission through the channel whose\\nimpulse response is denoted by h(t). The noisy received signal x(t) is passed through a\\nreceive filter of impulse response c(t). The resulting filter output y(t) is sampled\\nsynchronously with the transmitter, with the sampling instants being determined by a clock\\nor timing signal that is usually extracted from the receive-filter output. Finally, the\\nsequence of samples thus obtained is used to reconstruct the original data sequence by',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 468,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'or timing signal that is usually extracted from the receive-filter output. Finally, the\\nsequence of samples thus obtained is used to reconstruct the original data sequence by\\nmeans of a decision device. Specifically, the amplitude of each sample is compared with a\\nzero threshold, assuming that the symbols 1 and 0 are equiprobable. If the zero threshold is\\nexceeded, a decision is made in favor of symbol 1; otherwise a decision is made in favor of\\nFigure 8.3 Baseband binary data transmission system.\\nInput binary data {bk} Say 1 if y(ti) > Say 0 if y(ti) < Sample at time ti = iTb y(ti) y(t) s(t) Clock pulses White Gaussian noise w(t) Threshold {ak} x(t) Pulse- amplitude modulator Transmit filter g(t) Channel, h(t) Receive filter c(t) Decision device Transmitter Channel Receiver     ak +1 if bk is symbol 1 1 - if bk is symbol 0      = s t akg t kTb -   k =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 468,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '3 Intersymbol Interference symbol 0. If the sample amplitude equals the zero threshold exactly, the receiver simply\\nmakes a random guess.\\nExcept for a trivial scaling factor, we may now express the receive filter output as\\n(8.6)\\nwhere the pulse p(t) is to be defined. To be precise, an arbitrary time delay t0 should be\\nincluded in the argument of the pulse p(t - kTb) in (8.6) to represent the effect of\\ntransmission delay through the system. To simplify the exposition, we have put this delay\\nequal to zero in (8.6) without loss of generality; moreover, the channel noise is ignored.\\nThe scaled pulse p(t) is obtained by a double convolution involving the impulse\\nresponse g(t) of the transmit filter, the impulse response h(t) of the channel, and the\\nimpulse response c(t) of the receive filter, as shown by\\n(8.7)\\nwhere, as usual, the star denotes convolution. We assume that the pulse p(t) is normalized\\nby setting\\n(8.8)\\nwhich justifies the use of a scaling factor to account for amplitude changes incurred in the\\ncourse of signal transmission through the system.\\nSince convolution in the time domain is transformed into multiplication in the\\nfrequency domain, we may use the Fourier transform to change (8.7) into the equivalent\\nform\\n(8.9)\\nwhere P(f), G(f), H(f ), and C( f) are the Fourier transforms of p(t), g(t), h(t), and c(t),\\nrespectively.\\nThe receive filter output y(t) is sampled at time ti = iTb, where i takes on integer values;\\nhence, we may use (8.6) to write\\n(8.10)\\nIn (8.10), the first term ai represents the contribution of the ith transmitted bit. The second\\nterm represents the residual effect of all other transmitted bits on the decoding of the ith\\nbit. This residual effect due to the occurrence of pulses before and after the sampling\\ninstant ti is called intersymbol interference (ISI).\\nIn the absence of ISI-and, of course, channel noise-we observe from (8.10) that the\\nsummation term is zero, thereby reducing the equation to',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 469,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'instant ti is called intersymbol interference (ISI).\\nIn the absence of ISI-and, of course, channel noise-we observe from (8.10) that the\\nsummation term is zero, thereby reducing the equation to\\nwhich shows that, under these ideal conditions, the ith transmitted bit is decoded correctly.\\ny t akp t kTb -   k = p t g th tc t = p 0  1 = P f G fH fC f = y ti  akp i k -  Tb   k  - =   = ai akp i kTb -   k  - = k i    + = y ti  ai =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 469,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels Signal Design for Zero ISI\\nThe primary objective of this chapter is to formulate an overall pulse shape p(t) so as to\\nmitigate the ISI problem, given the impulse response of the channel h(t). With this\\nobjective in mind, we may now state the problem at hand:\\nConstruct the overall pulse shape p(t) produced by the entire binary data-\\ntransmission system of Figure 8.3, such that the receiver is enabled to\\nreconstruct the original data stream applied to the transmitter input exactly.\\nIn effect, signaling over the band-limited channel becomes distortionless; hence, we may\\nrefer to the pulse-shaping requirement as a signal-design problem.\\nIn the next section we describe a signal-design procedure, whereby overlapping pulses\\nin the binary data-transmission system of Figure 8.3 are configured in such a way that at\\nthe receiver output they do not interfere with each other at the sampling times ti = iTb. So\\nlong as the reconstruction of the original binary data stream is accomplished, the behavior\\nof the overlapping pulses outside these sampling times is clearly of no practical\\nconsequence. Such a design procedure is rooted in the criterion for distortionless\\ntransmission, which was formulated by Nyquist (1928b) on telegraph transmission theory,\\na theory that is as valid then as it is today.\\nReferring to (8.10), we see that the weighted pulse contribution, ak p(iTb - kTb), must\\nbe zero for all k except for k = 1 for binary data transmission across the band-limited\\nchannel to be ISI free. In other words, the overall pulse-shape p(t) must be designed to\\nsatisfy the requirement\\n(8.11)\\nwhere p(0) is set equal to unity in accordance with the normalization condition of (8.8). A\\npulse p(t) that satisfies the two-part condition of (8.11) is called a Nyquist pulse, and the\\ncondition itself is referred to as Nyquists criterion for distortionless binary baseband data',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 470,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'pulse p(t) that satisfies the two-part condition of (8.11) is called a Nyquist pulse, and the\\ncondition itself is referred to as Nyquists criterion for distortionless binary baseband data\\ntransmission. However, there is no unique Nyquist pulse; rather, there are many pulse\\nshapes that satisfy the Nyquist criterion of (8.11). In the next section we describe two\\nkinds of Nyquist pulses, each with its own attributes. Ideal Nyquist Pulse for Distortionless Baseband\\nData Transmission\\nFrom a design point of view, it is informative to transform the two-part condition of (8.11)\\ninto the frequency domain. Consider then the sequence of samples {p(nTb)}, where n = 0,\\n1, 2,  From the discussion presented in Chapter 6 on the sampling process, we recall\\nthat sampling in the time domain produces periodicity in the frequency domain. In\\nparticular, we may write\\n(8.12) p iTb kTb -   1 for i k = 0 for i k     = Pf Rb P f nRb -   n  - =   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 470,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '5 Ideal Nyquist Pulse for Distortionless Baseband Data Transmission where Rb = 1Tb is the bit rate in bits per second; P(f ) on the left-hand side of (8.12) is the\\nFourier transform of an infinite periodic sequence of delta functions of period Tb whose\\nindividual areas are weighted by the respective sample values of p(t). That is, P( f) is\\ngiven by\\n(8.13)\\nLet the integer m = i - k. Then, i = k corresponds to m = 0 and, likewise,\\ncorresponds to\\n. Accordingly, imposing the conditions of (8.11) on the sample\\nvalues of p(t) in the integral in (8.13), we get\\n(8.14)\\nwhere we have made use of the sifting property of the delta function. Since from (8.8) we\\nhave p(0) = 1, it follows from (8.12) and (8.14) that the frequency-domain condition for\\nzero ISI is satisfied, provided that\\n(8.15)\\nwhere Tb = 1Rb. We may now make the following statement on the Nyquist criterion1 for\\ndistortionless baseband transmission in the frequency domain:\\nThe frequency function P( f) eliminates intersymbol interference for samples\\ntaken at intervals Tb provided that it satisfies (8.15).\\nNote that P(f ) refers to the overall system, incorporating the transmit filter, the channel,\\nand the receive filter in accordance with (8.9).\\nIdeal Nyquist Pulse\\nThe simplest way of satisfying (8.15) is to specify the frequency function P(f ) to be in the\\nform of a rectangular function, as shown by\\n(8.16)\\nwhere rect(f ) stands for a rectangular function of unit amplitude and unit support centered\\non f = 0 and the overall baseband system bandwidth W is defined by\\n(8.17) Pf p mTb  t mTb -     j2ft -   exp m  - =   dt  -   = i k  m 0  Pf p 0  t j2ft -   exp dt  -   = p 0  = P f nRb -   n  - =   Tb = P f 1 2W --------, WfW  - 0, f W       = 1 2W --------rect f 2W --------     = W Rb 2------ 1 2Tb --------- = =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 471,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nAccording to the solution in (8.16), no frequencies of absolute value exceeding half the bit\\nrate are needed. Hence, from Fourier-transform pair 1 of Table 2.2 in Chapter 2, we find\\nthat a signal waveform that produces zero ISI is defined by the sinc function:\\n(8.18)\\nThe special value of the bit rate Rb = 2W is called the Nyquist rate and W is itself called the\\nNyquist bandwidth. Correspondingly, the baseband pulse p(t) for distortionless\\ntransmission described in (8.18) is called the ideal Nyquist pulse, ideal in the sense that the\\nbandwidth requirement is one half the bit rate.\\nFigure 8.4 shows plots of P(f ) and p(t). In part a of the figure, the normalized form of\\nthe frequency function P(f ) is plotted for positive and negative frequencies. In part b of\\nthe figure, we have also included the signaling intervals and the corresponding centered\\nsampling instants. The function p(t) can be regarded as the impulse response of an ideal\\nlow-pass filter with passband magnitude response 12W and bandwidth W. The function\\np(t) has its peak value at the origin and goes through zero at integer multiples of the bit\\nduration Tb. It is apparent, therefore, that if the received waveform y(t) is sampled at the\\ninstants of time t = 0, Tb, 2Tb, , then the pulses defined by ai p(t - iTb) with amplitude\\nai and index i = 0, 1, 2,  will not interfere with each other. This condition is illustrated\\nin Figure 8.5 for the binary sequence 1011010.\\np t 2Wt   sin 2Wt --------------------------\\n= sinc 2Wt   = Figure 8.4 (a) Ideal magnitude response. (b) Ideal basic pulse shape.\\n5 0 Sampling instants -1 1 -2 2 -3 3 1.0 p(t) t Tb 1.0 0 W = = -1 1 2WP( f ) f W 1 2Tb Rb 2 Signaling intervals Tb (a) (b)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 472,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '5 Ideal Nyquist Pulse for Distortionless Baseband Data Transmission Although the use of the ideal Nyquist pulse does indeed achieve economy in\\nbandwidth, in that it solves the problem of zero ISI with the minimum bandwidth possible,\\nthere are two practical difficulties that make it an undesirable objective for signal design:\\nIt requires that the magnitude characteristic of P(f ) be flat from -W to +W, and zero\\nelsewhere. This is physically unrealizable because of the abrupt transitions at the\\nband edges W, in that the Paley-Wiener criterion discussed in Chapter 2 is violated.\\nThe pulse function p(t) decreases as 1| t| for large | t|, resulting in a slow rate of\\ndecay. This is also caused by the discontinuity of P(f ) at W. Accordingly, there is\\npractically no margin of error in sampling times in the receiver.\\nTo evaluate the effect of the timing error alluded to under point 2, consider the sample of\\ny(t) at t = t, where t is the timing error. To simplify the exposition, we may put the\\ncorrect sampling time ti equal to zero. In the absence of noise, we thus have from the first\\nline of (8.10):\\n(8.19)\\nSince 2WTb = 1, by definition, we may reduce (8.19) to\\n(8.20)\\nFigure 8.5 A series of sinc pulses corresponding to the sequence 1011010.\\n-4 -2 0 -1.0 -0.5 0.0 Amplitude 0.5 1.0 Binary sequence 1 0 1 1 0 1 0 2 Time 6 8 4 10 12 y t   akp t kTb -    k  - =   = ak 2W t kTb -     sin 2W t kTb -   --------------------------------------------------\\n      k  - =   = y t   a0 sinc 2Wt   2Wt   sin  ------------------------------\\n1 -  kak 2Wt k - -----------------------\\nk  - = k 0    + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 473,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nThe first term on the right-hand side of (8.20) defines the desired symbol, whereas the\\nremaining series represents the ISI caused by the timing error t in sampling the receiver\\noutput y(t). Unfortunately, it is possible for this series to diverge, thereby causing the\\nreceiver to make erroneous decisions that are undesirable. Raised-Cosine Spectrum\\nWe may overcome the practical difficulties encountered with the ideal Nyquist pulse by\\nextending the bandwidth from the minimum value W = Rb2 to an adjustable value\\nbetween W and 2W. In effect, we are trading off increased channel bandwidth for a more\\nrobust signal design that is tolerant of timing errors. Specifically, the overall frequency\\nresponse P(f ) is designed to satisfy a condition more stringent than that for the ideal\\nNyquist pulse, in that we retain three terms of the summation on the left-hand side of\\n(8.15) and restrict the frequency band of interest to [-W, W], as shown by\\n(8.21)\\nwhere, on the right-hand side, we have set Rb = 12W in accordance with (8.17). We may\\nnow devise several band-limited functions that satisfy (8.21). A particular form of P(f )\\nthat embodies many desirable features is provided by a raised-cosine (RC) spectrum. This\\nfrequency response consists of a flat portion and a roll-off portion that has a sinusoidal\\nform, as shown by:\\n(8.22)\\nIn (8.22), we have introduced a new frequency f1 and a dimensionless parameter , which\\nare related by\\n(8.23)\\nThe parameter  is commonly called the roll-off factor; it indicates the excess bandwidth\\nover the ideal solution, W. Specifically, the new transmission bandwidth is defined by\\n(8.24)\\nThe frequency response P( f ), normalized by multiplying it by the factor 2W, is plotted in\\nFigure 8.6a for  = 0, 0.5, and 1. We see that for  = 0.5 or 1, the frequency response P(f )\\nrolls off gradually compared with the ideal Nyquist pulse (i.e.,  = 0) and it is therefore',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 474,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Figure 8.6a for  = 0, 0.5, and 1. We see that for  = 0.5 or 1, the frequency response P(f )\\nrolls off gradually compared with the ideal Nyquist pulse (i.e.,  = 0) and it is therefore\\neasier to implement in practice. This roll-off is cosine-like in shape, hence the terminology\\nRC spectrum. Just as importantly, the P( f) exhibits odd symmetry with respect to the\\nNyquist bandwidth W, which makes it possible to satisfy the frequency-domain condition\\nof (8.15). P f P f 2W -   P f 2W +   + + 1 2W -------- WfW  -  = P f 1 2W --------, 0 f f1   1 4W -------- 1  2W ------------ f f1 -   cos +      , f1 f 2W f1 -   0, f 2W f1 -           =  1 f1 W ----- - = BT 2W f1 - = W 1  +   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 474,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 Raised-Cosine Spectrum The time response p(t) is naturally the inverse Fourier transform of the frequency\\nresponse P(f ). Hence, transforming the P( f) defined in (8.22) into the time domain, we\\nobtain\\n(8.25)\\nwhich is plotted in Figure 8.6b for  = 0, 0.5, and 1.\\nFigure 8.6 Responses for different roll-off factors: (a) frequency response; (b) time response.\\nt Tb 1 2 1 2 3 2 3 2 0.5 p(t) 1.0 0 1 -1 -3 -2 2 3 0.2 0.4 0.6 0.8 1.0 2WP( f ) 0 - - 1 -1 2 -2 1 2 f W = 0 (a) (b)  = 1  =  1 2 = 0  = 1  =  p t sinc 2Wt   2Wt   cos 1 162W2t2 - ----------------------------------\\n=',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 475,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nThe time response p(t) consists of the product of two factors: the factor sinc(2Wt)\\ncharacterizing the ideal Nyquist pulse and a second factor that decreases as 1| t|2 for large\\n| t |. The first factor ensures zero crossings of p(t) at the desired sampling instants of time\\nt = iTb, with i equal to an integer (positive and negative). The second factor reduces the\\ntails of the pulse considerably below those obtained from the ideal Nyquist pulse, so that\\nthe transmission of binary data using such pulses is relatively insensitive to sampling time\\nerrors. In fact, for  = 1 we have the most gradual roll-off, in that the amplitudes of the\\noscillatory tails of p(t) are smallest. Thus, the amount of ISI resulting from timing error\\ndecreases as the roll-off factor  is increased from zero to unity.\\nThe special case with  = 1 (i.e., f1 = 0) is known as the full-cosine roll-off\\ncharacteristic, for which the frequency response of (8.22) simplifies to\\n(8.26)\\nCorrespondingly, the time response p(t) simplifies to\\n(8.27)\\nThe time response of (8.27) exhibits two interesting properties:\\nAt t = +Tb2 = 14W, we have p(t) = 0.5; that is, the pulse width measured at half\\namplitude is exactly equal to the bit duration Tb.\\nThere are zero crossings at t = 3Tb2, 5Tb2, in addition to the usual zero\\ncrossings at the sampling times t = Tb, 2Tb, \\nThese two properties are extremely useful in extracting timing information from the\\nreceived signal for the purpose of synchronization. However, the price paid for this\\ndesirable property is the use of a channel bandwidth double that required for the ideal\\nNyquist channel for which  = 0: simply put, there is no free lunch.\\nEXAMPLE\\nFIR Modeling of the Raised-Cosine Pulse\\nIn this example, we use the finite-duration impulse response (FIR) filter, also referred to as\\nthe tapped-delay-line (TDL) filter, to model the raised-cosine (RC) filter; both terms are',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 476,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'In this example, we use the finite-duration impulse response (FIR) filter, also referred to as\\nthe tapped-delay-line (TDL) filter, to model the raised-cosine (RC) filter; both terms are\\nused interchangeably. With the FIR filter operating in the discrete-time domain, there are\\ntwo time-scales to be considered:\\nDiscretization of the input signal a(t) applied to the FIR model, for which we write\\n(8.28)\\nwhere T is the sampling period in the FIR model shown in Figure 8.7. The tap inputs\\nin this model are denoted by\\n, which, for\\nsome integer l, occupies the duration 2lT. Note that the FIR model in Figure 8.7 is\\nsymmetric about the midpoint,\\n, which satisfies the symmetric structure of the\\nRC pulse. P f 1 4W -------- 1 f 2W --------     cos +  0 f 2W   0 f 2W       = p t sinc 4Wt   1 16W2t2 - ---------------------------\\n= ntT--- = an an 1 - an l - , an 2l - 1 + an 2l -      an l -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 476,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 Raised-Cosine Spectrum\\nDiscretization of the RC pulse p(t) for which we have\\n(8.29)\\nwhere Tb is the bit duration.\\nTo model the RC pulse properly, the sampling rate of the model, 1T, must be higher than\\nthe bit rate, 1Tb. It follows therefore that the integer m defined in (8.29) must be larger than\\none. In assigning a suitable value to m, we must keep in mind the tradeoff between\\nmodeling accuracy (requiring large m) and computational complexity (preferring small m).\\nIn any event, using (8.17), (8.28), and (8.29), obtaining the product\\n(8.30)\\nand then substituting this result into (8.25), we get the discretized version of RC pulse as\\nshown by\\n(8.31)\\nThere are two computational difficulties encountered in the way in which the discretized\\nRC pulse, pn, is defined in (8.31):\\nThe pulse pn goes on indefinitely with increasing n.\\nThe pulse is also noncausal in that the output signal yn in Figure 8.7 is produced\\nbefore the input an is applied to the FIR model.\\nTo overcome difficulty 1, we truncate the sequence pn such that it occupies a finite dura-\\ntion 2lT for some prescribed integer l, which is indeed what has been done in Figure 8.8.\\nTo mitigate the non-causality problem 2, with T > Tb, the ratio nm must be replaced by\\n(nm) - l. In so doing, the truncated causal RC pulse assumes the following modified form:\\n(8.32)\\nFigure 8.7 TDL model of linear time-invariant system.\\n Input signal h-l h-l + 1 h0 hl - 1 hl TTTT     an an - 1 an - l an - 2l + 1 an - 2l Output signal yn m Tb T----- = Wt n 2m ------- = pn sinc n m    n m    cos 1 42 n m   2 - -------------------------------------\\nn 0 1 2       =  = pn sinc n m---- l -     n m---- l -     cos 1 42 n m---- l -    2 - ---------------------------------------\\n                     lnl   -  = 0 otherwise',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 477,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nwhere the value assigned to the integer l is determined by how long the truncated sequence is desired to be.\\nWith the desired formula of (8.32) for the FIR model of the RC pulse p(t) at hand,\\nFigure 8.8 plots this formula for the following specifications:2\\nSampling of the RC pulse, T = 10\\nBit duration of the RC pulse, Tb = 1\\nNumber of the FIR samples per bit, m = 10\\nRoll-off factor of the RC pulse,\\nTwo noteworthy points that follow from Figure 8.8:\\nThe truncated causal RC pulse pn of length 2l - 10 is symmetric about the\\nmidpoint, n = 5.\\nThe pn is exactly zero at integer multiples of the bit duration Tb.\\nBoth points reaffirm exactly what we know and therefore expect about the RC pulse p(t)\\nplotted in Figure 8.6b. Square-Root Raised-Cosine Spectrum\\nA more sophisticated form of pulse shaping uses the square-root raised-cosine (SRRC)\\nspectrum3 rather than the conventional RC spectrum of (8.22). Specifically, the spectrum\\nof the basic pulse is now defined by the square root of the right-hand side of this equation.\\nThus, using the trigonometric identity\\nwhere, for the problem at hand, the angle\\nFigure 8.8 Discretized RC pulse, computed using the TDL.\\npn  n 1 - = l  0.32 = 1.2 1.0 0.8 0.6 0.4 0.2 0 -0.2 2 0 4 6 8 10 Amplitude Time, n  2 cos 1 2--- 1 2 cos +   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 478,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '7 Square-Root Raised-Cosine Spectrum To avoid confusion, we use G(f ) as the symbol for the SRRC spectrum, and so we may write\\n(8.33)\\nwhere, as before, the roll-off factor  is defined in terms of the frequency parameter f1 and\\nthe bandwidth W as in (8.23).\\nIf, now, the transmitter includes a pre-modulation filter with the transfer function\\ndefined in (8.33) and the receiver includes an identical post-modulation filter, then under\\nideal conditions the overall pulse waveform will experience the squared spectrum G2( f),\\nwhich is the regular RC spectrum. In effect, by adopting the SRRC spectrum G( f) of\\n(8.33) for pulse shaping, we would be working with G2(f ) = P( f) in an overall\\ntransmitter-receiver sense. On this basis, we find that in wireless communications, for\\nexample, if the channel is affected by both fading and AWGN and the pulse-shape filtering\\nis partitioned equally between the transmitter and the receiver in the manner described\\nherein, then effectively the receiver would maximize the output SNR at the sampling\\ninstants.\\nThe inverse Fourier transform of (8.33) defines the SRRC shaping pulse:\\n(8.34)\\nThe important point to note here is the fact that the SRRC shaping pulse g(t) of (8.34) is\\nradically different from the conventional RC shaping pulse of (8.25). In particular, the\\nnew shaping pulse has the distinct property of satisfying the orthogonality constraint\\nunder T-shifts, described by\\n(8.35)\\nwhere T is the symbol duration. Yet, the new pulse g(t) has exactly the same excess\\nbandwidth as the conventional RC pulse.\\nIt is also important to note, however, that despite the added property of orthogonality,\\nthe SRRC shaping pulse of (8.34) lacks the zero-crossing property of the conventional RC\\nshaping pulse defined in (8.25).\\nFigure 8.9a plots the SRRC spectrum G( f) for the roll-off factor  = 0, 0.5, 1; the\\ncorresponding time-domain plots are shown in Figure 8.9b. These plots are naturally',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 479,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Figure 8.9a plots the SRRC spectrum G( f) for the roll-off factor  = 0, 0.5, 1; the\\ncorresponding time-domain plots are shown in Figure 8.9b. These plots are naturally\\n  2W ------------ f f1 -   =  2W ------------ f W 1  -   -   = G f 1 2W ------------ 0 f f1   1 2W ------------  4W ------------ f W 1  -   -         cos f1 f 2W f1 -   0 f 2W f1 -             = , , g t 2W 1 8Wt  2 - -------------------------------\\n2W 1  -  t   sin 2Wt --------------------------------------------\\n   4  ------- 2W 1  +  t   cos +    = g tg t nT -   dt  -   0 for n 1 2      = =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 479,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\ndifferent from those of Figure 8.6 for nonzero . The following example contrasts the\\nwaveform of a specific binary sequence using the SRRC shaping pulse with the\\ncorresponding waveform using the regular RC shaping pulse.\\nFigure 8.9 (a) G(f) for SRRC spectrum. (b) g(t) for SRRC pulse.\\n0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 -2 -3 -1 0 1 2 3 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0 -0.2 -0.4 -2 -3 -1 0 1 2 3 Normalized pulse, g Normalized time, t/T Normalized frequency, f/W\\n2W (t)/ = 1 = 0.5 = 0 (a) (b)    = 1 = 0.5 = 0    G 2W (f)/ Normalized spectrum,',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 480,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '7 Square-Root Raised-Cosine Spectrum EXAMPLE\\nPulse Shaping Comparison Between SRRC and RC\\nUsing the SRRC shaping pulse g(t) of (8.34) with roll-off factor  = 0.5, the requirement\\nis to plot the waveform for the binary sequence 01100 and compare it with the\\ncorresponding waveform obtained by using the conventional RC shaping pulse p(t) of\\n(8.25) with the same roll-off factor.\\nUsing the SRRC pulse g(t) of (8.34) with a multiplying plus sign for binary symbol 1 and\\nmultiplying minus sign for binary symbol 0, we get the dashed pulse train shown in Figure\\n10 for the sequence 01100. The solid pulse train shown in the figure corresponds to the use\\nof the conventional RC pulse p(t) of (8.25). The figure clearly shows that the SRRC\\nwaveform occupies a larger dynamic range than the conventional RC waveform: a feature\\nthat distinguishes one from the other.\\nEXAMPLE\\nFIR Modeling of the Square-Root-Raised-Cosine Pulse\\nIn this example, we study FIR modeling of the SRRC pulse described in (8.34). To be\\nspecific, we follow a procedure similar to that used for the RC pulse g(t) in Example 1,\\ntaking care of the issues of truncation and noncausality. This is done by discretizing the\\nFigure 8.10 Two pulse trains for the sequence 01100, one using regular RC pulse (solid\\nline), and the other using an SRRC pulse (dashed line).\\nAmplitude Normalized time, 1.0 1.5 0.5 0 -0.5 -1.0 -1.5 -2 -3 -1 0 1 2 3 4 Sqrt-raised cosine Raised consine t / Tb',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 481,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nSRRC pulse, g(t), and substituting the dimensionless parameter, (nm) - l, for Wt in\\n(8.34). In so doing we obtain the following sequence\\n(8.36)\\nSince, by definition, the Fourier transform of the SRRC pulse, g(t), is equal to the square\\nroot of the Fourier transform of the RC pulse p(t), we may make the following statement:\\nThe cascade connection of two identical FIR filters, each one defined by (8.36),\\nis essentially equivalent to a TDL filter that exhibits zero intersymbol\\ninterference in accordance with (8.25).\\nWe say essentially here on account of the truncation applied to both (8.32) and (8.36). In\\npractice, when using the SRRC pulse for ISI-free baseband data transmission across a\\nband-limited channel, one FIR filter would be placed in the transmitter and the other\\nwould be in the receiver.\\nTo conclude this example, Figure 8.11a plots the SRRC sequence gn of (8.36) for the\\nsame set of values used for the RC sequence pn in Figure 8.8. Figure 8.11b displays the\\nresult of convolving the sequence in part a with gn, which is, itself.\\nFigure 8.11 (a) Discretized SRRC pulse, computed using FIR modeling.\\n(b) Discretized pulse resulting from the convolution of the pulse in part a with itself.\\ngn 4 Tb ------------- sin 1  -  n m---- l -     4n m---- l -     ----------------------------------------------------\\ncos 1  +  n m---- l -     + 1 162 n m---- l -    2 - ---------------------------------------------------------------------------------------------------------------------\\n0 otherwise             = lnn   -  1.5 1 0.5 0 -0.5 3 2 4 5 6 7 Amplitude Time 8 10 5 0 -5 7 6 8 9 10 11 Amplitude Normalized time 12 (a) (b)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 482,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '8 Post-Processing Techniques: The Eye Pattern Two points are noteworthy from Figure 8.11:\\nThe zero-crossings of the SRRC sequence gn do not occur at integer multiples of the\\nbit duration Tb, which is to be expected.\\nThe sequence plotted in Figure 8.11b is essentially equivalent to the RC sequence\\npn, the zero-crossings of which do occur at integer multiples of the bit duration, and\\nso they should. Post-Processing Techniques: The Eye Pattern\\nThe study of signaling over band-limited channels would be incomplete without discussing\\nthe idea of post-processing, the essence of which is to manipulate a given set of data so as\\nto provide a visual interpretation of the data rather than just numerical listing of the data.\\nFor an illustrative example, consider the formulas for the BER of digital modulation\\nschemes operating over an AWGN channel, which were summarized in Table 7.7 of\\nChapter 7. The graphical plots of the schemes, shown in Figure 7.47, provide an immediate\\ncomparison on how these different modulation schemes compete with each other in terms\\nof performance measured on the basis of their respective BERs for varying EbN0. In other\\nwords, there is much to be gained from graphical plots that are most conveniently made\\npossible by computation.\\nWhat we have in mind in this section, however, is the description of a commonly used\\npost-processor, namely eye patterns, which are particularly suited for the experimental\\nstudy of digital communication systems.\\nThe eye pattern, also referred to as the eye diagram, is produced by the synchronized\\nsuperposition of (as many as possible) successive symbol intervals of the distorted\\nwaveform appearing at the output of the receive filter prior to thresholding. As an\\nillustrative example, consider the distorted, but noise-free, waveform shown in part a of\\nFigure 8.12. Part b of the figure displays the corresponding synchronized superposition of\\nthe waveforms eight binary symbol intervals. The resulting display is called an eye',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 483,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Figure 8.12. Part b of the figure displays the corresponding synchronized superposition of\\nthe waveforms eight binary symbol intervals. The resulting display is called an eye\\npattern because of its resemblance to a human eye. By the same token, the interior of the\\neye pattern is called the eye opening.\\nFigure 8.12 (a) Binary data sequence and its waveform. (b) Corresponding eye pattern.\\n(a) (b) Binary Data 0 1 1 Tb 0 0 0 0 1 1 1 t Tb t',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 483,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nAs long as the additive channel noise is not large, then the eye pattern is well defined\\nand may, therefore, be studied experimentally on an oscilloscope. The waveform under\\nstudy is applied to the deflection plates of the oscilloscope with its time-base circuit\\noperating in a synchronized condition. From an experimental perspective, the eye pattern\\noffers two compelling virtues:\\n\\nThe simplicity of eye-pattern generation.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 484,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The provision of a great deal of insightful information about the characteristics of\\nthe data transmission system. Hence, the wide use of eye patterns as a visual\\nindicator of how well or poorly a data transmission system performs the task of\\ntransporting a data sequence across a physical channel.\\nTiming Features\\nFigure 8.13 shows a generic eye pattern for distorted but noise-free binary data. The\\nhorizontal axis, representing time, spans the symbol interval from to\\n, where\\nTb is the bit duration. From this diagram, we may infer three timing features pertaining to\\na binary data transmission system, exemplified by a PAM system:\\nOptimum sampling time. The width of the eye opening defines the time interval over\\nwhich the distorted binary waveform appearing at the output of the receive filter in\\nthe PAM system can be uniformly sampled without decision errors. Clearly, the\\noptimum sampling time is the time at which the eye opening is at its widest.\\nZero-crossing jitter. In practice, the timing signal (for synchronizing the receiver to\\nthe transmitter) is extracted from the zero-crossings of the waveform that appears at\\nthe receive-filter output. In such a form of synchronization, there will always be\\nirregularities in the zero-crossings, which, in turn, give rise to jitter and, therefore,\\nnonoptimum sampling times.\\nTiming sensitivity. Another timing-related feature is the sensitivity of the PAM\\nsystem to timing errors. This sensitivity is determined by the rate at which the eye\\npattern is closed as the sampling time is varied.\\nFigure 8.13 indicates how these three timing features of the system (and other insightful\\nattributes) can be measured from the eye pattern.\\nFigure 8.13 Interpretation of the eye pattern for a baseband binary data transmission system.\\nTb 2  - Tb 2  Best sampling time Time interval over which the wave is best sampled Distortion at sampling time\\nMargin over noise Zero-crossing jitter Slope = sensitivity to\\ntiming error',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 484,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Post-Processing Techniques: The Eye Pattern The Peak Distortion for Intersymbol Interference\\nHereafter, we assume that the ideal signal amplitude is scaled to occupy the range from -1\\nto 1. We then find that, in the absence of channel noise, the eye opening assumes two\\nextreme values:\\nAn eye opening of unity,4 which corresponds to zero ISI.\\nAn eye opening of zero, which corresponds to a completely closed eye pattern; this\\nsecond extreme case occurs when the effect of intersymbol interference is severe\\nenough for some upper traces in the eye pattern to cross with its lower traces.\\nIt is indeed possible for the receiver to make decision errors even when the channel is\\nnoise free. Typically, an eye opening of 0.5 or better is considered to yield reliable data\\ntransmission.\\nIn a noisy environment, the extent of eye opening at the optimum sampling time\\nprovides a measure of the operating margin over additive channel noise. This measure, as\\nillustrated in Figure 8.13, is referred to as the noise margin.\\nFrom this discussion, it is apparent that the eye opening plays an important role in\\nassessing system performance; hence the need for a formal definition of the eye opening.\\nTo this end, we offer the following definition:\\nEye opening =\\n(8.37)\\nwhere Dpeak denotes a new criterion called the peak distortion. The point to note here is\\nthat peak distortion is a worst-case criterion for assessing the effect of ISI on the\\nperformance (i.e., error rate) of a data transmission system. The relationship between the\\neye opening and peak distortion is illustrated in Figure 8.14. With the eye opening being\\ndimensionless, the peak distortion is dimensionless too. To emphasize this statement, the\\ntwo extreme values of the eye opening translate as follows:\\nZero peak distortion, which occurs when the eye opening is unity.\\nUnity peak distortion, which occurs when the eye pattern is completely closed.\\nFigure 8.14 Illustrating the relationship between peak distortion and eye opening.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 485,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Unity peak distortion, which occurs when the eye pattern is completely closed.\\nFigure 8.14 Illustrating the relationship between peak distortion and eye opening.\\nNote: the ideal signal level is scaled to lie inside the range -1 to +1.\\n1 Dpeak - Amplitude Time 2 Peak distortion Eye opening Ideal signal level Symbol period T 0',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 485,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nWith this background, we offer the following definition:\\nThe peak distortion is the maximum value assumed by the intersymbol\\ninterference over all possible transmitted sequences, with this maximum value\\ndivided by a normalization factor equal to the absolute value of the\\ncorresponding signal level idealized for zero intersymbol interference.\\nReferring to (8.10), the two components embodied in this definition are themselves\\ndefined as follows:\\nThe idealized signal component of the receive filter output is defined by the first\\nterm in (8.10), namely ai, where ai is the ith encoded symbol and unit transmitted\\nsignal energy per bit.\\nThe intersymbol interference is defined by the second term, namely\\n(8.38)\\nwhere pi - k stands for the term\\n. The maximum value of this summation\\noccurs when each encoded symbol ak has the same algebraic sign as pi - k. Therefore,\\n(8.39)\\nHence, invoking the definition of peak distortion, we get the desired formula:\\n(8.40)\\nwhere p0 = 1 for all i = k. Note that, by involving the assumption of a signal amplitude\\nfrom -1 to +1, we have scaled the transmitted signal energy for a binary symbol to be\\nunity.\\nBy its very nature, the peak distortion is a worst-case criterion for data transmission\\nover a noisy channel. The eye opening specifies the smallest possible noise margin.\\nEye Patterns for M-ary Transmission\\nBy definition, an M-ary data transmission system uses M encoded symbols in the\\ntransmitter and thresholds in the receiver. Correspondingly, the eye pattern for an\\nM-ary data transmission system contains eye openings stacked vertically one on\\ntop of the other. The thresholds are defined by the amplitude-transition levels as we move\\nup from one eye opening to the adjacent eye opening. When the encoded symbols are all\\nequiprobable, the thresholds will be equidistant from each other.\\nIn a strictly linear data transmission system with truly transmitted random data',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 486,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'equiprobable, the thresholds will be equidistant from each other.\\nIn a strictly linear data transmission system with truly transmitted random data\\nsequences, all the eye openings would be identical. In practice, however, it is often\\npossible to find asymmetries in the eye pattern of an M-ary data transmission system,\\nwhich are caused by nonlinearities in the communication channel or other distortion-\\nsensitive parts of the system.\\nakpi k - k  - = k i    pik -  Tb   Maximum ISI pi k - k  - = k i    = Dpeak pi k - k  - = k i    = M 1 - M 1 - M 1 -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 486,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Post-Processing Techniques: The Eye Pattern EXAMPLE\\nEye Patterns for Binary and Quaternary Systems\\nFigure 8.15a and b depict the eye patterns for a baseband PAM transmission system using and\\n, respectively. The channel has no bandwidth limitation and the\\nsource symbols used are obtained from a random number generator. An RC pulse is used\\nin both cases. The system parameters used for the generation of these eye patterns are a bit\\nrate of 1Hz and roll-off factor\\n. For the binary case of in Figure 8.15a,\\nFigure 8.15 Eye diagrams of received signal with no bandwidth limitation: (a) M = 2; (b) M = 4.\\nM 2 = M 4 =  0.5 = M 2 = 0 0.5 1 1.5 2 Response -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 Sampling instant (a) Time s Time s -1 0 1 2 3 Response -3.0 -2.0 -1.0 0.0 1.0 2.0 3.0 Sampling instant (b)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 487,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nthe symbol duration T and the bit duration Tb are the same, with Tb = 1s. For the case of\\nM = 4 in Figure 8.15b we have T = Tblog2 M = 2Tb. In both cases we see that the eyes are\\nopen, indicating perfectly reliable operation of the system, perfect in the sense that the ISI\\nis zero.\\nFigure 8.16a and b show the eye patterns for these two baseband-pulse transmission\\nsystems using the same system parameters as before, but this time under a bandwidth-\\nFigure 8.16 Eye diagrams of received signal, using a bandwidth-limited channel: (a) M = 2; (b) M = 4.\\n0 0.5 1 1.5 2 Response -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 Sampling instant (a) Response Sampling instant (b) Time s Time s -1 0 1 2 3 -3.0 -2.0 -1.0 0.0 1.0 2.0 3.0',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 488,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '9 Adaptive Equalization limited condition. Specifically, the channel is now modeled by a low-pass Butterworth\\nfilter, whose frequency response is defined by\\nwhere N is the order of the filter, and f0 is the 3-dB cutoff frequency of the filter. For the\\nresults displayed in Figure 8.16, the following filter parameter values were used:\\nWith the roll-off factor and Nyquist bandwidth\\n, for binary PAM,\\nthe use of (8.24) defines the transmission bandwidth of the PAM transmission system to be\\nAlthough the channel bandwidth cutoff frequency is greater than absolutely necessary, its\\neffect on the passband is observed in a decrease in the size of the eye opening. Instead of\\nthe distinct values at time t = 1s, shown in Figure 8.15a and b, now there is a blurred\\nregion. If the channel bandwidth were to be reduced further, the eye would close even\\nmore until finally no distinct eye opening would be recognizable. Adaptive Equalization\\nIn this section we develop a simple and yet effective algorithm for the adaptive equaliza-\\ntion of a linear channel of unknown characteristics. Figure 8.17 shows the structure of an\\nadaptive synchronous equalizer, which incorporates the matched filtering action. The\\nalgorithm used to adjust the equalizer coefficients assumes the availability of a desired\\nresponse. Ones first reaction to the availability of a replica of the transmitted signal is: If\\nsuch a signal is available at the receiver, why do we need adaptive equalization? To answer\\nthis question, we first note that a typical telephone channel changes little during an aver-\\nage data call. Accordingly, prior to data transmission, the equalizer is adjusted under the\\nH f 1 1 f f0   2N + ------------------------------\\n= N 3 and f0  0.6 Hz for binary PAM\\nN 3 and f0  0.3 Hz for 4-PAM = = = =  0.5 = W 0.5 Hz = BT 0.5 1 0.5 +   0.75 Hz = = Figure 8.17 Block diagram of adaptive equalizer using an adjustable TDL filter.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 489,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '= N 3 and f0  0.6 Hz for binary PAM\\nN 3 and f0  0.3 Hz for 4-PAM = = = =  0.5 = W 0.5 Hz = BT 0.5 1 0.5 +   0.75 Hz = = Figure 8.17 Block diagram of adaptive equalizer using an adjustable TDL filter.\\nVariable weights Error signal en Output yn xn - N w1 wN - 1 wN w0 xn - 1 xn - N + 1 - + Desired response an Input xn T T    T',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 489,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nguidance of a training sequence transmitted through the channel. A synchronized version\\nof this training sequence is generated at the receiver, where (after a time shift equal to the\\ntransmission delay through the channel) it is applied to the equalizer as the desired\\nresponse. A training sequence commonly used in practice is the pseudonoise (PN)\\nsequence, which consists of a deterministic periodic sequence with noise-like characteris-\\ntics. Two identical PN sequence generators are used, one at the transmitter and the other at\\nthe receiver. When the training process is completed, the PN sequence generator is\\nswitched off and the adaptive equalizer is ready for normal data transmission. A detailed\\ndescription of PN sequence generators is presented in Appendix J.\\nLeast-Mean-Square Algorithm (Revisited)\\nTo simplify notational matters, we let\\nThen, the output yn of the tapped-delay-line (TDL) equalizer in response to the input\\nsequence {xn} is defined by the discrete convolution sum (see Figure 8.17)\\n(8.41)\\nwhere wk is the weight at the kth tap and N + 1 is the total number of taps. The tap weights\\nconstitute the adaptive equalizer coefficients. We assume that the input sequence xn has\\nfinite energy. We have used a notation for the equalizer weights in Figure 8.17 that is\\ndifferent from the corresponding notation in Figure 6.17 to emphasize the fact that the\\nequalizer in Figure 8.17 also incorporates matched filtering.\\nThe adaptation may be achieved by observing the error between the desired pulse shape\\nand the actual pulse shape at the equalizer output, measured at the sampling instants, and\\nthen using this error to estimate the direction in which the tap weights of the equalizer\\nshould be changed so as to approach an optimum set of values. For the adaptation, we may\\nuse a criterion based on minimizing the peak distortion, defined as the worst-case',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 490,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'should be changed so as to approach an optimum set of values. For the adaptation, we may\\nuse a criterion based on minimizing the peak distortion, defined as the worst-case\\nintersymbol interference at the output of the equalizer. However, the equalizer so designed\\nis optimum only when the peak distortion at its input is less than 100% (i.e., the\\nintersymbol interference is not too severe). A better approach is to use a mean-square error\\ncriterion, which is more general in application; also, an adaptive equalizer based on the\\nmean-square error (MSE) criterion appears to be less sensitive to timing perturbations\\nthan one based on the peak-distortion criterion. Accordingly, in what follows we use the\\nMSE criterion to derive the adaptive equalization algorithm.\\nLet an denote the desired response defined as the polar representation of the nth\\ntransmitted binary symbol. Let en denote the error signal defined as the difference\\nbetween the desired response an and the actual response yn of the equalizer, as shown by\\n(8.42)\\nIn the least-mean-square (LMS) algorithm for adaptive equalization, the error signal en\\nactuates the adjustments applied to the individual tap weights of the equalizer as the\\nalgorithm proceeds from one iteration to the next. A derivation of the LMS algorithm for\\nxn x nT   = yn y nT   = yn wkxn k - k 0 = N  = en a = n yn -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 490,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '9 Adaptive Equalization adaptive prediction was presented in Section 6.7 of Chapter 6. Recasting (6.85) into its\\nmost general form, we may restate the formula for the LMS algorithm in words as follows:\\n(8.43)\\nLet denote the step-size parameter. From Figure 8.17 we see that the input signal applied\\nto the kth tap weight at time step n is xn - k. Hence, using\\nas the old value of the kth\\ntap weight at time step n, the updated value of this tap weight at time step n + 1 is, in light\\nof (8.43), defined by\\n(8.44) where (8.45) These two equations constitute the LMS algorithm for adaptive equalization.\\nWe may simplify the formulation of the LMS algorithm using matrix notation. Let the\\n(N + 1)-by-1 vector xn denote the tap inputs of the equalizer:\\n(8.46)\\nwhere the superscript T denotes matrix transposition. Correspondingly, let the (N + 1)-by-1\\nvector denote the tap weights of the equalizer:\\n(8.47)\\nWe may then use matrix notation to recast the discrete convolution sum of (8.41) in the\\ncompact form (8.48) where is referred to as the inner product of the vectors xn and\\n. We may now\\nsummarize the LMS algorithm for adaptive equalization as follows:\\nInitialize the algorithm by setting (i.e., set all the tap weights of the\\nequalizer to zero at n = 1, which corresponds to time t = T.\\nFor n = 1, 2, , compute\\nwhere  is the step-size parameter.\\nContinue the iterative computation until the equalizer reaches a steady state, by\\nwhich we mean that the actual mean-square error of the equalizer essentially reaches\\na constant value.\\nThe LMS algorithm is an example of a feedback system, as illustrated in the block\\ndiagram of Figure 8.18, which pertains to the kth filter coefficient. It is therefore possible\\nfor the algorithm to diverge (i.e., for the adaptive equalizer to become unstable).\\nUpdated value of kth tap weight     Old value of kth tap weight     Step-size parameter    Input signal applied',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 491,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'for the algorithm to diverge (i.e., for the adaptive equalizer to become unstable).\\nUpdated value of kth tap weight     Old value of kth tap weight     Step-size parameter    Input signal applied\\nto kth tap weight     Error signal     + = wk n  w k n 1 +  w k n  = xn k - en k 0 1 N   =  + en a = n w k n xn k - k 0 = N  - xn xn  = xn N - 1 + xn N -     T w n w n w 0 n  w 1 n  w N n       = T yn xn T = wn xn Tw n w n w 1 0 = yn xn T = wn  en an = yn - wn 1 +  wn   + = enxn',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 491,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nUnfortunately, the convergence behavior of the LMS algorithm is difficult to analyze.\\nNevertheless, provided that the step-size parameter  is assigned a small value, we find\\nthat after a large number of iterations the behavior of the LMS algorithm is roughly similar\\nto that of the steepest-descent algorithm (discussed in Chapter 6), which uses the actual\\ngradient rather than a noisy estimate for the computation of the tap weights.\\nOperation of the Equalizer\\nThere are two modes of operation for an adaptive equalizer, namely the training mode and\\ndecision-directed mode, as shown in Figure 8.19. During the training mode, a known PN\\nsequence is transmitted and a synchronized version of it is generated in the receiver, where\\n(after a time shift equal to the transmission delay) it is applied to the adaptive equalizer as\\nthe desired response; the tap weights of the equalizer are thereby adjusted in accordance\\nwith the LMS algorithm.\\nWhen the training process is completed, the adaptive equalizer is switched to its second\\nmode of operation: the decision-directed mode. In this mode of operation, the error signal\\nis defined by\\n(8.49)\\nFigure 8.18 Signal-flow graph representation of\\nthe LMS algorithm involving the kth tap weight.\\nFigure 8.19 Illustrating the two operating modes of an adaptive equalizer: for the training mode, the\\nswitch is in position 1; for the tracking mode, it is moved to position 2.\\nOld value wk , n Updated value wk , n + 1 Correction xn - k en + + Unit delay T   en a n yn - = {wk} Training sequence generator + Adaptive equalizer xn yn en - Decision device 2 1 an an',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 492,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '9 Adaptive Equalization where yn is the equalizer output at time t = nT and is the final (not necessarily) correct\\nestimate of the transmitted symbol\\n. Now, in normal operation the decisions made by\\nthe receiver are correct with high probability. This means that the error estimates are\\ncorrect most of the time, thereby permitting the adaptive equalizer to operate satisfactorily.\\nFurthermore, an adaptive equalizer operating in a decision-directed mode is able to track\\nrelatively slow variations in channel characteristics.\\nIt turns out that the larger the step-size parameter  is, the faster the tracking capability\\nof the adaptive equalizer. However, a large step-size parameter  may result in an\\nunacceptably high excess mean-square error, defined as that part of the mean-square value\\nof the error signal in excess of the minimum attainable value, which results when the tap\\nweights are at their optimum settings. We therefore find that, in practice, the choice of a\\nsuitable value for the step-size parameter involves making a compromise between fast\\ntracking and reducing the excess mean-square error.\\nDecision-Feedback Equalization5\\nTo develop further insight into adaptive equalization, consider a baseband channel with\\nimpulse response denoted in its sampled form by the sequence {hn}, where hn = h(nT).\\nThe response of this channel to an input sequence {xn}, in the absence of noise, is given by\\nthe discrete convolution sum\\n(8.50)\\nThe first term of (8.50) represents the desired data symbol. The second term is due to the\\nprecursors of the channel impulse response that occur before the main sample h0\\nassociated with the desired data symbol. The third term is due to the postcursors of the\\nchannel impulse response that occur after the main sample h0. The precursors and\\npostcursors of a channel impulse response are illustrated in Figure 8.20. The idea of\\ndecision-feedback equalization is to use data decisions made on the basis of precursors of',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 493,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'postcursors of a channel impulse response are illustrated in Figure 8.20. The idea of\\ndecision-feedback equalization is to use data decisions made on the basis of precursors of\\nFigure 8.20 Impulse response of a discrete-time channel, depicting\\nthe precursors and postcursors.\\nan an yn hkxn k - k = h0 xn hk xn k - hk xn k - k 0  + k 0  + = t h0 0 Postcursors Precursors',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 493,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nthe channel impulse response to take care of the postcursors; for the idea to work,\\nhowever, the decisions would obviously have to be correct for the DFE to function\\nproperly most of the time.\\nA DFE consists of a feedforward section, a feedback section, and a decision device\\nconnected together as shown in Figure 8.21. The feedforward section consists of a TDL\\nfilter whose taps are spaced at the reciprocal of the signaling rate. The data sequence to be\\nequalized is applied to this section. The feedback section consists of another TDL filter\\nwhose taps are also spaced at the reciprocal of the signaling rate. The input applied to the\\nfeedback section consists of the decisions made on previously detected symbols of the\\ninput sequence. The function of the feedback section is to subtract out that portion of the\\nintersymbol interference produced by previously detected symbols from the estimates of\\nfuture samples.\\nNote that the inclusion of the decision device in the feedback loop makes the equalizer\\nintrinsically nonlinear and, therefore, more difficult to analyze than an ordinary LMS\\nequalizer. Nevertheless, the mean-square error criterion can be used to obtain a\\nmathematically tractable optimization of a DFE. Indeed, the LMS algorithm can be used\\nto jointly adapt both the feedforward tap weights and the feedback tap weights based on a\\ncommon error signal. Broadband Backbone Data Network: Signaling over Multiple\\nBaseband Channels\\nUp to this point in the chapter, the discussion has focused on signaling over a single band-\\nlimited channel and related issues such as adaptive equalization. In order to set the stage for\\nthe rest of the chapter devoted to signaling over a linear broadband channel purposely parti-\\ntioned into a set of subchannels, this section on the broadband backbone data network\\n(PSTN) is intended to provide a transition from the first part of the chapter to the second part.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 494,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'tioned into a set of subchannels, this section on the broadband backbone data network\\n(PSTN) is intended to provide a transition from the first part of the chapter to the second part.\\nThe PSTN was originally built to provide a ubiquitous structure for the digital\\ntransmission of voice signals using PCM, which was discussed previously in Chapter 6.\\nAs such, traditionally, the PSTN has been viewed as an analog network. In reality,\\nhowever, the PSTN has evolved into an almost entirely digital network. We say almost\\nentirely because the analog refers to the local network, which stands for short-\\nconnections from a home to the central office.\\nFor many decades past, data transmission over the PSTN relied on the use of modems;\\nthe term modem is a contraction of modulator-demodulator. Despite the enormous\\nFigure 8.21 Block diagram of decision-feedback equalizer.\\nFeedforward section Input xn Decision device Estimate of transmitted\\nsymbol, an - + Feedback section',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 494,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '11 Digital Subscriber Lines effort that was put into the design of modems, they could not cope with the ever-increasing\\nrate of data transmission. This situation prevailed until the advent of digital subscriber line\\n(DSL) technology in the 1990s. The inquisitive reader may well ask the question: How was\\nit that the modem theorists and designers got it wrong while the DSL theorists and\\ndesigners got it right? Unfortunately, in the development of modems, the telephone\\nchannel was treated as one whole entity. On the other hand, the development of DSL\\nabandoned the traditional approach by viewing the telephone channel as a conglomeration\\nof subchannels extending over a wide frequency band and operating in parallel, and with\\neach subchannel treated as a narrowband channel, thereby exploiting Shannons\\ninformation capacity law in a much more effective manner.\\nIt is therefore not surprising that the DSL technology has converted an ordinary\\ntelephone line into a broadband communication link, so much so that we may now view\\nthe PSTN effectively as a broadband backbone data network, which is being widely used\\nall over the world. The data consist of digital signals generated by computers or Internet\\nservice providers (ISPs). Most importantly, the deployment of DSL technology has\\nliterally made it possible to increase the rate of data transmission across a telephone\\nchannel by orders of magnitude compared with the old modems. This transition from\\nmodem to DSL technology is indeed an impressive engineering accomplishment, which\\nresulted from thinking outside the box.\\nWith this brief historical account, it is apropos that we devote the rest of the chapter to\\nthe underlying theory of the widely used DSL technology. Digital Subscriber Lines\\nThe term DSL is commonly used to refer to a family of different technologies that operate\\nover a local loop less than 1.5 km to provide for digital signal transmission between a user',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 495,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The term DSL is commonly used to refer to a family of different technologies that operate\\nover a local loop less than 1.5 km to provide for digital signal transmission between a user\\nterminal (e.g., computer) and the central office (CO) of a telephone company. Through the\\nCO, the user is connected directly to the so-called broadband backbone data network,\\nwhereby transmission is maintained in the digital domain. In the course of transmission,\\nthe digital signal is switched and routed at regular intervals. Figure 8.22 is a schematic\\ndiagram illustrating that typically the data rate upstream (i.e., in the direction of the ISP) is\\nlower than the data rate downstream (i.e., in the direction of the user). It is for this reason\\nthat the DSL is said to be asymmetric;6 hence the acronym ADSL.\\nThe twisted wire-pair used in the local loop, the only analog part of the data\\ntransmission system as remarked earlier, is inductively loaded. Specifically, extra\\ninductance is purposely supplied by local coils, which are inserted at regular intervals\\nacross the wire-pair. This addition is made in order to produce a fairly flat frequency\\nFigure 8.22 Block diagram depicting the operational environment of DSL.\\nBroadband backbone network Internet service provider Digital subscriber line Computer User Optical fibers Digital subscriber line Central office Downstream Upstream',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 495,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nresponse across the effective voice band. However, the improvement so gained for the\\ntransmission of voice signals is attained at the expense of continually increasing\\nattenuation at frequencies higher than 3.4 kHz. Figure 8.23a illustrates the two different\\nfrequency bands allocated to a frequency-division multiplexive (FDM)-based ADSL; the\\nway in which two filters, one high-pass and the other low-pass, are used to connect the\\nDSL to the local loop is shown in Figure 8.23b.\\nWith access to the wide band represented by frequencies higher than 3.6 kHz, the DSL\\nuses discrete multicarrier transmission (DMT) techniques to convert the twisted wire-pair\\nin the local loop into a broadband communication link; the two terms multichannel and\\nmulticarrier are used interchangeably. The net result is that data rates of 1.5 to 9.0 Mbps\\ndownstream in a bandwidth of up to 1 MHz and over a distance of 2.7 to 5.5 km. Very\\nhigh-bit-rate digital subscriber lines7 (VDSLs) do even better, supporting data rates of 13 to\\n52 Mbps downstream in a bandwidth of up to 30 MHz and over a distance of 0.3 to 1.5 km.\\nThese numbers indicate that the data rates attainable by DSL technology depend on both\\nbandwidth and distance, and the technology continues to improve.\\nThe basic idea behind DMT is rooted in a commonly used engineering paradigm:\\nDivide and conquer.\\nAccording to this paradigm, a difficult problem is solved by dividing it into a number of\\nsimpler problems and then combining the solutions to those simple problems. In the\\ncontext of our present discussion, the difficult problem is that of data transmission over a\\nwideband channel with severe intersymbol interference, and the simpler problems are\\nexemplified by data transmission over relatively straightforward AWGN channels. We\\nmay thus describe the essence of DMT theory, as follows:\\nData transmission over a difficult channel is transformed through the use of',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 496,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'may thus describe the essence of DMT theory, as follows:\\nData transmission over a difficult channel is transformed through the use of\\nadvanced signal processing techniques into the parallel transmission of the\\ngiven data stream over a large number of subchannels, such that each\\nsubchannel may be viewed effectively as an AWGN channel.\\nFigure 8.23 (a) Illustrating the different band allocations for an FDM-based ADSL system.\\n(b) Block diagram of splitter performing the function of a multiplexer or demultiplexer.\\nNote: both filters in the splitter are bidirectional filters.\\nGuard band Low bit-rate upstream band High bit-rate downstream band Frequency POTS band Transmit power Digital subscriber line Local loop High-pass filter Low-pass filter Telephone (a) (b)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 496,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '12 Capacity of AWGN Channel Revisited Naturally, the overall data rate is the sum of the individual data rates over the subchannels\\ndesigned to operate in parallel: this new way of thinking on signaling over wideband\\nchannels is entirely different from the approach described in the first part of the chapter, in\\nthat it builds on ideas described in Chapter 5 on Shannons information theory and in\\nChapter 7 on signaling over AWGN channels. Capacity of AWGN Channel Revisited\\nAt the heart of discrete multichannel data transmission theory is Shannons information\\ncapacity law, discussed in Chapter 5 on information theory. According to this law, the\\ncapacity of an AWGN channel (free from ISI) is defined by\\n(8.51)\\nwhere B is the channel bandwidth in hertz and SNR is measured at the channel output.\\nEquation (8.51) teaches us that, for a given SNR, we can transmit data over an AWGN\\nchannel of bandwidth B at the maximum rate of B bits with arbitrarily small probability of\\nerror, provided that we employ an encoding system of sufficiently high complexity.\\nEquivalently, we may express the capacity C in bits per transmission of channel use as\\n(8.52)\\nIn practice, we usually find that a physically realizable encoding system must transmit data\\nat a rate R less than the maximum possible rate C for it to be reliable. For an implementable\\nsystem operating at low enough probability of symbol error, we thus need to introduce an\\nSNR gap or just gap, denoted by . The gap is a function of the permissible probability of\\nsymbol error Pe and the encoding system of interest. It provides a measure of the\\nefficiency of an encoding system with respect to the ideal transmission system of (8.52).\\nWith C denoting the capacity of the ideal encoding system and R denoting the capacity of\\nthe corresponding implementable encoding system, the gap is defined by\\n(8.53)\\nRearranging (8.53) with R as the focus of interest, we may write\\n(8.54)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 497,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the corresponding implementable encoding system, the gap is defined by\\n(8.53)\\nRearranging (8.53) with R as the focus of interest, we may write\\n(8.54)\\nFor an encoded PAM or QAM operating at Pe = 10-6, for example, the gap is constant at\\n8 dB. Through the use of codes (e.g., trellis codes to be discussed in Chapter 10), the gap may be reduced to as low as 1 dB.\\nLet P denote the transmitted signal power and denote the channel noise variance\\nmeasured over the bandwidth B. The SNR is therefore\\nC B 1 SNR +  bits/s 2 log = C 1 2--- 1 SNR +   bits per transmission\\n2 log =   22C 1 - 22R 1 - ------------------ = SNR 22R 1 - ----------------- = R 1 2---log2 1 SNR  ----------- +     bits per transmission\\n=   2 SNR P 2 ------ =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 497,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nwhere\\nWe may thus finally define the attainable data rate as\\n(8.55)\\nWith this modified version of Shannons information capacity law at hand, we are ready to\\ndescribe discrete multichannel modulation in quantitative terms. Partitioning Continuous-Time Channel into a Set\\nof Subchannels\\nTo be specific in practical terms, consider a linear wideband channel (e.g., twisted wire-\\npair) with an arbitrary frequency response H(f). Let the magnitude response of the\\nchannel, denoted by |H( f )|, be approximated by a staircase function as illustrated in Figure\\n24, with f denoting the width of each frequency step (i.e., subchannel). In the limit, as\\nthe frequency increment f approaches zero, the staircase approximation of the channel\\napproaches the actual H( f ). Along each step of the approximation, the channel may be\\nassumed to operate as an AWGN channel free from intersymbol interference. The problem\\nof transmitting a single wideband signal is thereby transformed into the transmission of a\\nset of narrowband orthogonal signals. Each orthogonal narrowband signal, with its own\\ncarrier, is generated using a spectrally efficient modulation technique such as M-ary QAM,\\nwith AWGN being essentially the only primary source of transmission impairment. This\\nscenario, in turn, means that data transmission over each subchannel of bandwidth f can\\nbe optimized by invoking a modified form of Shannons information capacity law, with the\\noptimization of each subchannel being performed independently of all the others. Thus, in\\npractical signal-processing terms, we may make the following statement:\\nThe need for complicated equalization of a wideband channel is replaced by the\\nneed for multiplexing and demultiplexing the transmission of an incoming data\\nstream over a large number of narrowband subchannels that are continuous and\\ndisjoint.\\nAlthough the resulting complexity of a DMT system so described is indeed high for a',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 498,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'stream over a large number of narrowband subchannels that are continuous and\\ndisjoint.\\nAlthough the resulting complexity of a DMT system so described is indeed high for a\\nlarge number of subchannels, implementation of the entire system can be accomplished in\\na cost-effective manner through the combined use of efficient digital signal-processing\\nalgorithms and very-large-scale integration technology.\\nFigure 8.25 shows a block diagram of the DMT system in its most basic form. The\\nsystem configured here uses QAM, whose choice is justified by virtue of its spectral\\nefficiency. The incoming binary data stream is first applied to a demultiplexer (not shown in\\nthe figure), thereby producing a set of N substreams. Each substream represents a sequence\\nof two-element subsymbols, which, for the symbol interval 0  t  T, is denoted by\\nwhere an and bn are element values along the two coordinates of subchannel n.\\n2 N0B = R 1 2---log2 1 P 2 ---------- +     bits per transmission\\n= an,bn   n 1 2 N   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 498,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '13 Partitioning Continuous-Time Channel into a Set of Subchannels Correspondingly, the passband basis functions of the quadrature-amplitude modulators\\nare defined by the following function pairs:\\n(8.56)\\nThe carrier frequency fn of the nth modulator described in (8.56) is an integer multiple of\\nthe symbol rate 1T, as shown by and the low-pass function (t), common to all the subchannels, is the sinc function\\n(8.57)\\nThe passband basis functions defined here have the following desirable properties, whose\\nproofs are presented as an end-of-chapter problem.\\nPROPERTY\\nFor each n, the two quadrature-modulated sinc functions form an orthogonal pair, as\\nshown by\\n(8.58)\\nThis orthogonal relationship provides the basis for formulating the signal constellation for\\neach of the N modulators in the form of a squared lattice.\\nPROPERTY\\nRecognizing that\\nwe may completely redefine the passband basis functions in the complex form\\n(8.59)\\nwhere the factor has been introduced to ensure that the scaled function has\\nunit energy. Hence, these passband basis functions form an orthonormal set, as shown by\\nFigure 8.24 Staircase approximation of an arbitrary magnitude response of\\na channel, |H( f )|; only positive-frequency portion of the response is shown.\\n0 Actual response Staircase approximation f |H(f)| f t 2fnt  t 2fnt   sin  cos   n 1 2 N   =  fn n T--- n 1 2 N   =  = t 2 T--- sinc t T---    t   -  = t 2fnt   cos    t 2 fnt d sin t  -   0 for all n = j2fnt   exp 2fnt   j 2fnt   sin + cos = 1 2 -------t j2fnt   exp       n 1 2 N   =  1 2  t 2',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 499,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\n(8.60)\\nThe asterisk assigned to the second factor on the left-hand side denotes complex\\nconjugation.\\nEquation (8.60) provides the mathematical basis for ensuring that the N modulator-\\ndemodulator pairs operate independently of each other.\\nPROPERTY\\nThe set of channel-output functions {h(t)\\n(t)} remains orthogonal for a linear channel\\nwith arbitrary impulse response h(t), where denotes convolution.\\nThus, in light of these three properties, the original wideband channel is partitioned into an\\nideal setting of independent subchannels operating in continuous time.\\n1 2 -------t j2fnt   exp 1 2 -------t j2fkt   exp * dt  -   1, k n = 0, k n     =   Figure 8.25 Block diagram of DMT system.\\nModulators cos (2f1t) sin (2f1t) Symbols a1 b1 A1 Modulator 1 h(t) w(t) cos (2f2t) sin (2f2t) a2 b2 A2 Modulator 2 cos (2fNt) sin (2fNt) aN bN AN Modulator N Maximum likelihood detectors cos (2f1t) sin (2f1t) a1 b1 Detector 1 cos (2f2t) Linear wideband noisy channel Transmitter Receiver sin (2f2t) a2 b2 aN bN Detector 2 cos (2fNt) sin (2fNt) Detector N   (t)  (t)  (t)     (t)  (t)  (t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 500,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '13 Partitioning Continuous-Time Channel into a Set of Subchannels Figure 8.25 also includes the corresponding structure of the receiver. It consists of a\\nbank of N coherent detectors, with the channel output being simultaneously applied to the\\ndetector inputs, operating in parallel. Each detector is supplied with a locally generated\\npair of quadrature-modulated sinc functions operating in synchrony with the pair of\\npassband basis function applied to the corresponding modulator in the transmitter.\\nIt is possible for each subchannel to have some residual ISI. However, as the number of\\nsubchannels N approaches infinity, the ISI disappears for all practical purposes. From a\\ntheoretical perspective, we find that, for a sufficiently large N, the bank of coherent\\ndetectors in Figure 8.25 operates as maximum likelihood detectors, operating\\nindependently of each other and on a subsymbol-by-subsymbol basis. (Maximum\\nlikelihood detection was discussed in Chapter 7.)\\nTo define the detector outputs in response to the input subsymbols, we find it\\nconvenient to use complex notation. Let An denote the subsymbol applied to the nth\\nmodulator during the symbol interval 0  t T, as shown by\\n(8.61)\\nThe corresponding detector output is expressed as follows:\\n(8.62)\\nwhere Hn is the complex-valued frequency response of the channel evaluated at the\\nsubchannel carrier frequency f = fn, that is,\\n(8.63)\\nThe Wn in (8.62) is a complex-valued random variable produced by the channel noise\\nw(t); the real and imaginary parts of Wn have zero mean and variance N02. With\\nknowledge of the measured frequency response H(f ) available, we may therefore use\\n(8.62) to compute a maximum likelihood estimate of the transmitted subsymbol An. The\\nestimates so obtained are finally multiplexed to produce the overall\\nestimate of the original binary data transmitted during the interval 0  t  T.\\nTo summarize, for a sufficiently large N, we may implement the receiver as an optimum',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 501,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'estimate of the original binary data transmitted during the interval 0  t  T.\\nTo summarize, for a sufficiently large N, we may implement the receiver as an optimum\\nmaximum likelihood detector that operates as N subsymbol-by-subsymbol detectors. The\\nrationale for building a maximum likelihood receiver in such a simple way is motivated by\\nthe following property:\\nPROPERTY\\nThe passband basis functions constitute an orthonormal set and their orthogonality is\\nmaintained for any channel impulse response h(t).\\nGeometric SNR\\nIn the DMT system of Figure 8.25, each subchannel is characterized by an SNR of its\\nown. It would be highly desirable, therefore, to derive a single measure for the\\nperformance of the entire system in Figure 8.25.\\nTo simplify the derivation of such a measure, we assume that all of the subchannels in\\nFigure 8.25 are represented by one-dimensional constellations. Then, using the modified\\nShannon information capacity law of (8.55), the channel capacity of the entire system is\\nsuccessively expressed as follows:\\nAn an jbn n 1 2 N   =  + = Yn HnAn Wn n 1 2 N   =  + = Hn H fn   n 1 2 N   =  = A 1 A 2 A N',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 501,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\n(8.64)\\nLet (SNR)overall denote the overall SNR of the entire DMT system. Then, in light of\\n(8.54), we may express the rate R as\\n(8.65)\\nAccordingly, comparing (8.65) with (8.64) and rearranging terms, we may write\\n(8.66)\\nAssuming that the SNR, namely\\n, is large enough to ignore the two unity terms\\non the right-hand side of (8.66), we may approximate the overall SNR simply as follows:\\n(8.67)\\nwhich is independent of the gap\\n. We may thus characterize the overall system by an\\nSNR that is the geometric mean of the SNRs of the individual subchannels.\\nThe geometric form of the SNR of (8.67) can be improved considerably by distributing\\nthe available transmit power among the N subchannels on a nonuniform basis. This\\nobjective is attained through the use of loading, which is discussed next.\\nLoading of the DMT System\\nEquation (8.64) for the bit rate of the entire DMT system ignores the effect of the channel\\non system performance. To account for this effect, define (8.68)\\nThen, assuming that the number of subchannels N is large enough, we may treat gn as a\\nconstant over the entire bandwidth f assigned to subchannel n for all n. In such a case, we\\nmay modify the second line of (8.64) for the overall SNR of the system into\\nR 1 N---- Rn n 1 = N  = 1 2N ------- 1 Pn n 2 ---------- +       2 log n 1 = N  = 1 2N ------- 1 Pn n 2 ---------- +       n 1 = N  2 log = 1 2--- 1 Pn n 2 ---------- +       n 1 = N  1 N  2 bits per transmission\\nlog = R 1 2--- 1 SNR  overall  ------------------------------\\n+     2 bits per transmission\\nlog = SNR  overall  1 Pn n 2 ---------- +      1 N  1 - n 1 = N  = Pn n 2    SNR  overall  Pn n 2 ------      1 N  n 1 = N   gn H fn   n 1 2 N   =  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 502,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '13 Partitioning Continuous-Time Channel into a Set of Subchannels\\n483 (8.69) where the and are usually fixed. The noise variance is f N0 for all n, where f\\nis the bandwidth of each subchannel and N02 is the noise power spectral density of the\\nsubchannel. We may therefore optimize the overall bit rate R through a proper allocation\\nof the total transmit power among the various subchannels. However, for this optimization\\nto be of practical value, we must maintain the total transmit power at some constant value\\ndenoted by P, as shown by\\n(8.70)\\nThe optimization we therefore have to deal with is a constrained optimization problem,\\nstated as follows:\\nMaximize the bit rate R for the DMT system through an optimal sharing of the\\ntotal transmit power P between the N subchannels, subject to the constraint that\\nthe total transmit power P is maintained constant.\\nTo solve this optimization problem, we first use the method of Lagrange multipliers8 to set\\nup an objective function (i.e., the Lagrangian) that incorporates (8.69) and the constraint\\nof (8.70) as shown by\\n(8.71)\\nwhere  is the Lagrange multiplier; in the second line of (8.71) the logarithm to base 2 has\\nbeen changed to the natural logarithm written as log2e. Hence, differentiating the\\nLagrangian J with respect to Pn, then setting the result equal to zero and finally\\nrearranging terms, we get\\n(8.72)\\nThe result of (8.72) indicates that the solution to our constrained optimization problem is\\nto have (8.73) R 1 2N ------- 1 gn 2Pn n 2 ------------ +       2 log n 1 = N  = gn 2  n 2 Pn n 1 = N  P = J 1 2N ------- log2 1 gn 2Pn n 2 ------------ +       P Pn n 1 = N  -         + n 1 = N  = 1 2N ------- e 2 loge 1 gn 2Pn n 2 ------------ +       P Pn n 1 = N  -         + n 1 = N  log = 1 2N ------- e 2 log Pn n 2 gn 2 ---------- + -----------------------\\n = Pn n 2 gn 2 ---------- + K for n 1 2 N   = =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 503,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nwhere K is a prescribed constant under the designers control. That is, the sum of the\\ntransmit power and the noise variance (power) scaled by the ratio must be\\nmaintained constant for each subchannel. The process of allocating the transmit power P\\nto the individual subchannels so as to maximize the bit rate of the entire multichannel\\ntransmission system is called loading; this term is not to be confused with loading coils\\nused in twisted wire-pairs. Water-Filling Interpretation of the Constrained\\nOptimization Problem\\nIn solving the constrained optimization problem just described, the two conditions of\\n(8.70) and (8.73) must both be satisfied. The optimum solution so defined has an\\ninteresting interpretation, as illustrated in Figure 8.26 for N = 6, assuming that the gap\\nis maintained constant over all the subchannels. To simplify the illustration in Figure 8.26,\\nwe have set\\n; that is, the average noise power is unity for all N\\nsubchannels. Referring to this figure, we may now make three observations:\\nWith\\n, the sum of power Pn allocated to subchannel n and the scaled noise\\npower satisfies the constraint of (8.73) for four of the subchannels for a\\nprescribed value of the constant K.\\nThe sum of power allocations to these four subchannels consumes all the available\\ntransmit power, maintained at the constant value P.\\nThe remaining two subchannels have been eliminated from consideration because\\nthey would each require negative power to satisfy (8.73) for the prescribed value of\\nthe constant K; from a physical perspective, this condition is clearly unacceptable.\\nThe interpretation illustrated in Figure 8.26 prompts us to refer to the optimum solution of\\n(8.73), subject to the constraint of (8.70), as the water-filling solution; the principle of water-\\nfilling was discussed under Shannons information theory in Chapter 5. This terminology\\ngn 2  Figure 8.26 Water-filling interpretation of the loading problem.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 504,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'filling was discussed under Shannons information theory in Chapter 5. This terminology\\ngn 2  Figure 8.26 Water-filling interpretation of the loading problem.\\n n 2 N0f 1 = = n 2 1 = gn 2  1  g1 2 3 Index of subchannel, n\\n4 5 6 Energy 2  g2 2  g3 2  g4 2  g5 2  g6',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 504,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '14 Water-Filling Interpretation of the Constrained Optimization Problem follows from analogy of our optimization problem with a fixed amount of water-standing\\nfor transmit power-being poured into a container with a number of connected regions, each\\nhaving a different depth-standing for noise power. In such a scenario, the water distributes\\nitself in such a way that a constant water level is attained across the whole container, hence\\nthe term water filling.\\nReturning to the task of how to allocate the fixed transmit power P among the various\\nsubchannels of a multichannel data transmission system so as to optimize the bit rate of\\nthe entire system, we may proceed along the following pair of steps:\\nLet the total transmit power be fixed at the constant value P as in (8.70).\\nLet K denote the constant value prescribed for the sum,\\n, for all n as in\\n(8.73).\\nOn the basis of these two steps, we may then set up the following system of simultaneous\\nequations:\\n(8.74)\\nwhere we have a total of (N + 1) unknowns and (N + 1) equations to solve for them. Using\\nmatrix notation, we may rewrite this system of N + K simultaneous equations in the\\ncompact form\\n(8.75)\\nPremultiplying both sides of (8.75) by the inverse of the (N + 1)-by-(N + 1) matrix on the\\nleft-hand side of the equation, we obtain solutions for the unknowns P1, P2, , PN, and K.\\nWe should always find that K is positive, but it is possible for some of the P values to be\\nnegative. In such a situation, the negative P values are discarded as power cannot be\\nnegative for physical reasons.\\nEXAMPLE\\nLinear Channel with Squared Magnitude Response\\nConsider a linear channel whose squared magnitude response |H( f)|2 has the piecewise-\\nlinear form shown in Figure 8.27. To simplify the example, we have set the gap and the noise variance = 1. Pn n 2 gn 2  + P1 P2 PN + + = P P1 K - = 2 g1 2  - P2 K - = 2 g2 2  -   PN K - = 2 gN 2  - 1 1 1 0 1 0 0 1 - 0 1 0 1 -   0 0 1 1 - P1 P2 P3  K P 2 g1 2  - 2 g2 2  -  2 gN 2  - =  1 = 2',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 505,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nUnder this set of values, the application of (8.74) yields\\nwhere the new parameter 0 < l < 1 has been introduced to distinguish the third equation\\nfrom the second one. Solving these three simultaneous equations for P1, P2, and K, we get\\nSince 0 < l < 1, it follows that P1 > 0, but it is possible for P2 to be negative. This latter\\ncondition can arise if\\nBut then P1 exceeds the prescribed value of transmit power P. Therefore, it follows that, in\\nthis example, the only acceptable solution is to have l(P + 1) < l < 1. Suppose then we\\nhave P = 10 and l = 0.1; under these two conditions the desired solution is\\nThe corresponding water-filling picture for the problem at hand is portrayed in Figure 8.28.\\nFigure 8.27 Squared magnitude response for Example 5.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 506,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '15 DMT System using Discrete Fourier Transform\\n487 n 8.15 DMT System using Discrete Fourier Transform\\nThe material presented in Sections 8.13 and 8.14 provides an insightful introduction to the\\nnotion of multicarrier modulation in a DMT system. In particular, the continuous-time\\nchannel partitioning induced by the passband (modulated) basis functions of (8.56), or equiv-\\nalently (8.59) in complex terms, exhibits a highly desirable property described as follows:\\nOrthogonality of the basis functions, and therefore the channel partitioning,\\nis preserved despite their individual convolutions with the impulse response\\nof the channel.\\nHowever, the DSL system so described has two practical shortcomings:\\nThe passband basis functions use a sinc function that is nonzero for an infinite time\\ninterval, whereas practical considerations favor a finite observation interval.\\nFor a finite number of subchannels N the system is suboptimal; optimality of the\\nsystem is assured only when N approaches infinity.\\nWe may overcome both shortcomings by using DMT, the basic idea of which is to\\ntransform a noisy wideband channel into a set of N subchannels operating in parallel.\\nWhat makes DMT distinctive is the fact that the transformation is performed in discrete\\ntime as well as discrete frequency, paving the way for exploiting digital signal processing.\\nSpecifically, the transmitters input-output behavior of the entire communication system\\nadmits a linear matrix representation, which lends itself to implementation using the DFT.\\nIn the following we know from Chapter 2 on Fourier analysis of signals and systems that\\nthe DFT is the result of discretizing the Fourier transform both in time and frequency.\\nTo exploit this new approach, we first recognize that in a realistic situation the channel\\nhas its nonzero impulse response h(t) essentially confined to a finite interval [0, Tb]. So,\\nFigure 8.28\\nWater-filling profile for\\nExample 5. 1 1 10 2 Index of subchannel n\\nEnergy 0 2 4 6 8 10 12 0.5 9.5',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 507,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nlet the sequence h0, h1, , h denote the baseband equivalent impulse response of the\\nchannel sampled at the rate 1Ts, with\\n(8.76)\\nwhere the role of  is to be clarified. The sampling rate 1Ts is chosen to be greater than\\ntwice the highest frequency component of interest in accordance with the sampling\\ntheorem. To continue with the discrete-time description of the system, let sn = s(nTs)\\ndenote a sample of the transmitted symbol s(t), wn = w(nTs) denote a sample of the\\nchannel noise w(t), and xn = x(nTs) denote the corresponding sample of the channel output\\n(i.e., received signal). The channel performs linear convolution on the incoming symbol\\nsequence {sn} of length N to produce a channel output sequence {xn} of length N + .\\nExtension of the channel output sequence by  samples compared with the channel input\\nsequence is due to the intersymbol interference produced by the channel.\\nTo overcome the effect of ISI, we create a cyclically extended guard interval, whereby\\neach symbol sequence is preceded by a periodic extension of the sequence itself.\\nSpecifically, the last  samples of the symbol sequence are repeated at the beginning of the\\nsequence being transmitted, as shown by\\n(8.77)\\nThe condition described in (8.77) is called a cyclic prefix. The excess bandwidth factor due\\nto the inclusion of the cyclic prefix is therefore N, where N is the number of transmitted\\nsamples after the guard interval.\\nWith the cyclic prefix in place, the matrix description of the channel now takes the new\\nform\\n(8.78)\\nIn a compact way, we may describe the discrete-time representation of the channel in the\\nmatrix form\\n(8.79)\\nwhere the transmitted symbol vector s, the channel noise vector w, and the received signal\\nvector x are all N-by-1 vectors that are respectively defined as follows:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 508,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'matrix form\\n(8.79)\\nwhere the transmitted symbol vector s, the channel noise vector w, and the received signal\\nvector x are all N-by-1 vectors that are respectively defined as follows:\\n(8.80) (8.81) (8.82) Tb 1  +  Ts = sk sN K - for K 1 2    = = xN 1 - xN 2 -  xN  1 - - xN  2 - -  x0 h0 h1 h2 h 1 - h 0  0 0 h0 h1 h 2 - h 1 - h 0      0 0 0  0 h0 h1  h h0 0  0 0 h0 h 1 -      h1 h2 h3  h 0 0  h0 sN 1 - sN 2 -  sN  1 - - sN  2 - -  s0 wN 1 - wN 2 -  wN  1 - - wN  2 - -  w0 + = x Hs w + = s sN 1 - sN 2 - s0     T = w wN 1 - wN 2 - w0     T = x xN 1 - xN 2 - x0     T =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 508,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '15 DMT System using Discrete Fourier Transform We may thus simply depict the discrete-time representation of the channel as in Figure\\nThe N-by-N channel matrix H is itself defined by\\n(8.83)\\nFrom the definition in (8.83), we readily see that the matrix H has the following structural\\ncomposition:\\nEvery row of the matrix is obtained by cyclically applying a right-shift to the\\nprevious row by one position, with the added proviso that the rightmost element\\nof the previous row spills over in the shifting process to be circulated back to\\nthe leftmost element of the next row.\\nAccordingly, the matrix H is referred to as a circulant matrix.\\nBefore proceeding further, it is befitting that we briefly review the DFT and its role in\\nthe spectral decomposition of the circulant matrix H.\\nDiscrete Fourier Transform\\nConsider the N-by-1 vector x of (8.79). Let the DFT of the vector x be denoted by the\\nN-by-1 vector\\n(8.84)\\nwhose kth element is defined by\\n(8.85)\\nThe exponential term exp(-j2knN) is the kernel of the DFT. Correspondingly, the IDFT\\n(i.e., inverse DFT) of the N-by-1 vector X is defined by\\n(8.86)\\nFigure 8.29\\nDiscrete-time representation of\\nmultichannel data transmission system.\\nsHxw  H h0 h1 h2 h 1 - h 0  0 0 h0 h1 h 2 - h 1 - h 0      0 0 0  0 h0 h1  h h0 0  0 0 h0 h 1 -      h1 h2 h3  h 0 0  h0 = X XN 1 - XN 2 -  , X0   T = Xk 1 N -------- xn j2 N ------kn -     k 0 1 N 1 -   =  exp n 0 = N 1 -  = xn 1 N -------- Xk j2 N ------kn     n 0 1 N 1 -   =  exp n 0 = N 1 -  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 509,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nEquations (8.85) and (8.86) follow from discretizing the continuous-time Fourier\\ntransform both in time and frequency, as discussed in Chapter 2 with one difference: the\\nDFT in (8.65) and its inverse in (8.66) have the same scaling factor, for the purpose\\nof symmetry.\\nAlthough the DFT and IDFT appear to be similar in their mathematical formulations,\\ntheir interpretations are different, as discussed previously in Chapter 2. As a reminder, we\\nmay interpret the DFT process described in (8.85) as a system of N complex heterodyning\\nand averaging operations, as shown in Figure 2.32a. In the picture depicted in this part of\\nthe figure, heterodyning refers to the multiplication of the data sequence xn by one of N\\ncomplex exponentials, exp(-j2knN). As such, (8.85) may be viewed as the analysis\\nequation. For the interpretation of (8.86), we may view it as the synthesis equation:\\nspecifically, the complex Fourier coefficient Xk is weighted by one of N complex\\nexponentials exp(-j2knN). At time n, the output xn is formed by summing the weighted\\ncomplex Fourier coefficients, as shown in Figure 2.32b.\\nAn important property of a circulant matrix, exemplified by the channel matrix H of\\n(8.83), is that it permits the spectral decomposition defined by\\n(8.87)\\nwhere the superscript  denotes Hermitian transposition (i.e., the combination of complex\\nconjugation and ordinary matrix transposition). Descriptions of the matrices Q and are\\npresented in the following in that order. The matrix Q is a square matrix defined in terms\\nof the kernel of the N-point DFT as shown by\\n(8.88)\\nFrom this definition, we readily see that the klth element of the N-by-N matrix, Q, starting\\nfrom the bottom right at k = 0 and l = 0 and counting up step-by-step, is\\n(8.89)\\nThe matrix Q is an orthonormal matrix or unitary matrix, in the sense that it satisfies the\\ncondition\\n(8.90)\\nwhere I is the identity matrix. That is, the inverse matrix of Q is equal to the Hermitian',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 510,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The matrix Q is an orthonormal matrix or unitary matrix, in the sense that it satisfies the\\ncondition\\n(8.90)\\nwhere I is the identity matrix. That is, the inverse matrix of Q is equal to the Hermitian\\ntranspose of Q. 1 2  H QQ =  Q 1 N -------- j2 N ------ N 1 -  N 1 -   - exp  j2 N ------2 N 1 -   - exp j2 N ------ N 1 -   - exp 1 j2 N ------ N 1 -  N 2 -   - exp  j2 N ------2 N 2 -   - exp j2 N ------ N 2 -   - exp 1    j2 N ------ N 1 -   - exp  j2 N ------2 -     exp j2 N ------ -     exp 1 1  1 1 1 = qkl 1 N -------- j2 N ------kl -     k l 0 1 N 1 -   =   exp = QQ I =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 510,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '15 DMT System using Discrete Fourier Transform The matrix in (8.87) is a diagonal matrix that contains the N DFT values of the\\nsequence h0, h1, , h that characterize the channel. Denoting these transform values by\\nN - 1, , 1, 0, respectively, we may express as\\n(8.91)\\nNote that  used here are not to be confused with the Lagrange multipliers in Section 8.13.\\nFrom a system design objective, the DFT has established itself as one of the principal\\ntools of digital signal processing by virtue of its efficient computation using the FFT\\nalgorithm, which was also described in Chapter 2. Computationally speaking, the FFT\\nalgorithm requires on the order of Nlog2N operations rather than the N2 operations for\\ndirect computation of the DFT. For efficient implementation of the FFT algorithm, we\\nshould choose the block length N to be an integer power of 2. The computational savings\\nobtained by using the FFT algorithm are made possible by exploiting the special structure\\nof the DFT defined in (8.85). Moreover, these savings become more substantial as we\\nincrease the data length N.\\nFrequency-Domain Description of the Channel\\nWith this brief review of the DFT and its FFT implementations at hand, we are ready to\\nresume our discussion of the DMT system. First, we define\\n(8.92)\\nwhere S is the frequency-domain vector representation of the transmitter output. Each\\nelement of the N-by-1 vector S may be viewed as a complex-valued point in a two-\\ndimensional QAM signal constellation. Given the channel output vector x, we define its\\ncorresponding frequency-domain representation as\\n(8.93)\\nUsing (8.87), (8.92), and (8.93), we may rewrite (8.79) in the equivalent form\\n(8.94)\\nHence, using the equality of (8.90) in (8.94) we may reduce the vector to the simple form (8.95) where (8.96) In expanded (scalar) form, the matrix equation (8.95) reads as follows:\\n(8.97)    N 1 - 0 0 0 N 2 - 0    0 0 0 = s QS = X Qx = X QQQQS W +   = X X S W + = W Qw = Xk kSk Wk k 0 1 N 1 -   =  + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 511,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nwhere the set of frequency-domain values is known for a prescribed channel.\\nNote that Xk is a random variable and wk is a random variable sampled from a white\\nGaussian noise process.\\nFor a channel with additive white noise, (8.97) leads us to make the following\\nimportant statement:\\nThe receiver of a DMT-based DSL is composed of a set of independent\\nprocessors operating in parallel.\\nWith the k being all known, we may thus use the block of frequency-domain values to compute estimates of the corresponding transmitted block of frequency-\\ndomain values . DFT-Based DMT System Equations (8.95), (8.85), (8.86), and (8.97) provide the mathematical basis for the\\nimplementation of DMT using the DFT. Figure 8.30 illustrates the block diagram of the\\nsystem derived from these equations, setting the stage for their practical roles:\\nFigure 8.30 Block diagram of the DFT-based DMT system.\\nk  k 0 = N 1 - Xk  k 0 = N 1 - Sk  k 0 = N 1 - Binary data input Transmitter Receiver Estimate of the original binary data input Demultiplexer Constellation encoder Inverse discrete Fourier transformer Digital- to-analog converter Channel Analog-to- digital converter Parallel-to- serial converter and guard-interval provider Multiplexer Decoder Discrete Fourier transformer Serial-to parallel converter and guard-interval remover',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 512,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '15 DMT System using Discrete Fourier Transform\\nThe transmitter consists of the following functional blocks:\\n\\nDemultiplexer, which converts the incoming serial data stream into parallel form.\\n\\nConstellation encoder, which maps the parallel data into N2 multibit\\nsubchannels with each subchannel being represented by a QAM signal\\nconstellation. Bit allocation among the subchannels is also performed here in\\naccordance with a loading algorithm.\\n\\nIDFT, which transforms the frequency-domain parallel data at the constellation\\nencoder output into parallel time-domain data. For efficient implementation of\\nthe IDFT using the FFT algorithm, we need to choose N = 2k, where k is a\\npositive integer.\\n\\nParallel-to-serial converter, which converts the parallel time-domain data into\\nserial form. Guard intervals stuffed with cyclic prefixes are inserted into the serial\\ndata on a periodic basis before conversion into analog form.\\n\\nDigital-to-analog converter (DAC), which converts the digital data into analog\\nform ready for transmission over the channel.\\nTypically, the DAC includes a transmit filter. Accordingly, the time function h(t) in\\nFigure 8.25 should be redefined as the combined impulse response of the cascade\\nconnection of the transmit filter and the channel.\\nThe receiver performs the inverse operations of the transmitter, as described here:\\n\\nAnalog-to-digital converter (ADC), which converts the analog channel output\\ninto digital form.\\n\\nSerial-to-parallel converter, which converts the resulting bit stream into parallel\\nform. Before this conversion takes place, the guard intervals (cyclic prefixes) are\\nremoved.\\n\\nDFT, which transforms the time-domain parallel data into frequency-domain\\nparallel data; as with the IDFT, the FFT algorithm is used to implement the DFT.\\n\\nDecoder, which uses the DFT output to compute estimates of the original\\nmultibit subchannel data supplied to the transmitter.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 513,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Decoder, which uses the DFT output to compute estimates of the original\\nmultibit subchannel data supplied to the transmitter.\\n\\nMultiplexer, which combines the estimates so computed to produce a\\nreconstruction of the transmitted serial data stream.\\nTo sum up:\\nThanks to the computationally efficient FFT algorithm, the DMT has\\nestablished itself as the standard core technology for the design of asymmetric\\nand very high bit-rate versions of the DSL by virtue of two important\\noperational attributes: effective performance and efficient implementation.\\nPractical Applications of DMT-based DSL\\nAn important application of DMT is in the transmission of data over two-way channels.\\nIndeed, DMT has been standardized for use on ADSLs using twisted wire-pairs. In ADSL,\\nfor example, the DMT provides for the transmission of data downstream (i.e., from an ISP\\nto a subscriber) at the rate of 1.544 Mbits/s and the simultaneous transmission of data\\nupstream (i.e., from the subscriber to the ISP) at 160 kbits/s. This kind of data\\ntransmission capability is well suited for handling data-intensive applications such as\\nvideo-on-demand.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 513,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels\\nDMT is also a core technology in implementing the asymmetric VDSLs, which differs\\nfrom all other DSL transmission techniques because of its ability to deliver extremely high\\ndata rates. For example, a VDSL can provide data rates of 13 to 26 Mbits/s downstream\\nand 2 to 3 MB/s upstream over twisted wire-pairs that emanate from an optical network\\nunit and connect to the subscriber over distances of less than about 1 km. These high data\\nrates allow the delivery of digital TV, super-fast Web surfing and file transfer, and virtual\\noffices at home.\\nFrom a practical perspective, the use of DMT for implementing ADSL and VDSL\\nprovides a number of advantages:\\n\\nThe ability of DMT to maximize the transmitted bit rate, which is provided by\\ntailoring the distribution of information-bearing signals across the channel\\naccording to channel attenuation and noise conditions.\\n\\nAdaptivity to changing line conditions, which is realized by virtue of the fact that the\\nchannel is partitioned into a number of subchannels.\\n\\nReduced sensitivity to impulse noise, which is achieved by spreading its energy over\\nthe many subchannels of the receiver. As the name implies, impulse noise is\\ncharacterized by long, quiet intervals followed by narrow pulses of randomly\\nvarying amplitude. In an ADSL or VDSL environment, impulse noise arises due to\\nswitching transients coupled to twisted wire-pairs in the central office and to various\\nelectrical devices on the users premises.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 514,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Effectively, employment of the DMT system eliminates the need for adaptive\\nchannel equalization. Summary and Discussion\\nIn this chapter devoted to data transmission over band-limited channels, two important\\naspects of this practical problem were discussed.\\nIn the first part of the chapter, we assumed that the SNR at the channel input is large\\nenough for the effect of channel noise to be ignored. Under this assumption, the issue of\\ndealing with intersymbol interference was viewed as a signal design problem. That is, the\\noverall pulse shape p(t) is configured in such a way that p(t) is zero at the sampling times\\nnTb, where Tb is the reciprocal of the bit rate Rb. In so doing, the intersymbol interference\\nis reduced to zero. Finding the pulse shape that satisfies this requirement is best handled in\\nthe frequency domain. The ideal solution is a brick-wall spectrum that is constant over\\nthe interval -W  f  W where W = 12Tb. Tb is the bit duration and W is called the Nyquist\\nbandwidth. Unfortunately, this ideal pulse shape is impractical on two accounts: noncausal\\nbehavior and sensitivity to timing errors. To overcome these two practical difficulties, we\\nproposed the use of an RC spectrum that rolls off gradually from a constant value over a\\nprescribed band toward zero in a half-cosine-like manner on either side of the band. We\\nfinished this first part of the chapter by introducing the SRRC spectrum, where the overall\\npulse shaping is split equally between the transmitter and receiver; this latter form of\\nsignal design finds application in wireless communication.\\nTurning next to the second part of the chapter, we discussed another way of tackling\\ndata transmission over a wideband channel by applying the engineering principle of\\ndivide and conquer. Specifically, a telephone channel, using a twisted wire-pair, is',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 514,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems partitioned into a large number of narrowband subchannels, such that each noisy\\nsubchannel can be handled by applying Shannons information-capacity law. Then,\\nthrough a series of clever mathematical steps, the treatment of a difficult discrete\\nmulticarrier transmission system is modified into a new DMT system. Most\\nimportantly, by exploiting the computational efficiency of the FFT algorithm, practical\\nimplementation of the DMT assumes a well-structured transceiver (i.e., pair of transmitter\\nand receiver) that is effective in performance and efficient in computational terms. Indeed,\\nthe DMT has established itself as the standard core technology for designing the\\nasymmetric and very high bit-rate members of the digital subscriber line family.\\nMoreover, the world-wide deployment of DSL technology has converted an ordinary\\ntelephone line into a broadband communication link, so much so that we may now view\\nthe PSTN as a broadband backbone data network. Most importantly, this analog-to-digital\\nnetwork conversion has made it possible to transmit data at rates in the megabits per\\nsecond region, which is a truly a remarkable engineering achievement.\\nProblems Nyquists Criterion 8.1 The NRZ pulse of Figure P8.1 may be viewed as a very crude form of a Nyquist pulse. Justify this\\nstatement by comparing the spectral characteristics of these two pulses. A binary PAM signal is to be transmitted over a baseband channel with an absolute maximum\\nbandwidth of 75 kHz. The bit duration is 10\\n. Find an RC spectrum that satisfies these\\nrequirements. An analog signal is sampled, quantized, and encoded into a binary PCM. Specifications of the PCM\\nsignal include the following:\\n Sampling rate, 8 kHz  Number of representation levels, 64.\\nThe PCM signal is transmitted over a baseband channel using discrete PAM. Determine the\\nminimum bandwidth required for transmitting the PCM signal if each pulse is allowed to take on the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 515,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The PCM signal is transmitted over a baseband channel using discrete PAM. Determine the\\nminimum bandwidth required for transmitting the PCM signal if each pulse is allowed to take on the\\nfollowing number of amplitude levels: 2, 4, or 8. Consider a baseband binary PAM system that is designed to have an RC spectrum P( f). The\\nresulting pulse p(t) is defined in (8.25). How would this pulse be modified if the system is designed\\nto have a linear phase response? Determine the Nyquist pulse whose inverse Fourier transform is defined by the frequency function\\nP( f) defined in (8.26).\\nFigure P8.1 T 2 - T 2 0 1.0 t p(t) s',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 515,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels Continuing with the defining condition in Problem 8.5, namely\\ndemonstrate that the Nyquist pulse p(t) with the narrowest bandwidth is described by the sinc\\nfunction: A pulse p(t) is said to be orthogonal under T-shifts if it satisfies the condition\\nwhere Tb is the bit duration. In other words, the pulse p(t) is uncorrelated with itself when it is\\nshifted by any integer multiple of Tb. Show that this condition is satisfied by a Nyquist pulse. Let P(f) be an integrable function, the inverse Fourier transform of which is given by\\nand let Tb be given. The pulse p(t) so defined is a Nyquist pulse of bit duration Tb if, and only if, the\\nFourier transform P( f) satisfies the condition\\nUsing the Poisson sum formula described in Chapter 2, demonstrate the validity of this statement. Let g(t) denote a function, the Fourier transform of which is denoted by G(f). The pulse g(t) is\\northogonal under T-shifts in that its Fourier transform G(f) satisfies the condition\\nShow that this condition is satisfied by the SRRC shaping pulse.\\nPartial Response Signaling The sinc pulse is the optimum Nyquist pulse, optimum in the sense it produces zero intersymbol\\ninterference occupying the minimum bandwidth possible W = 1/2Tb, where Tb is the bit duration.\\nHowever, as discussed in Section 8.5, the sinc pulse is prone to timing errors; hence the preference\\nfor the RC spectrum that requires twice the minimum bandwidth, 2W.\\nIn this problem, we explore a new pulse that achieves the minimum possible bandwidth W = 12Tb as\\nthe sinc pulse, but at the expense of a deterministic (i.e., controlled) intersymbol interference; being\\ncontrollable, appropriate measures can be taken at the receiver to account for it.\\nThis new pulse is denoted by g1(t), the Fourier transform of which is denoted by\\na. Plot the magnitude and phase spectrum of G1( f).\\nb. Show that the pulse g1(t) is defined by',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 516,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'This new pulse is denoted by g1(t), the Fourier transform of which is denoted by\\na. Plot the magnitude and phase spectrum of G1( f).\\nb. Show that the pulse g1(t) is defined by\\nPf n Tb ----- +     n  - =   Tb Tb 0  = 1 2Tb --------- p t sinc t Tb -----     = p tp t nTb -   dt 0 for n 1 2      = =  -   p t P f exp j2ft  df  -   = Pf n Tb ----- +     n  - =  Tb = Gf n Tb ----- +    2 n  - =   constant = G1 f 2 fTb  exp jfTb -   cos 0    f 1 2Tb     otherwise = g1 t Tb 2 sin t Tb    t Tb t -   -----------------------------------\\n=',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 516,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems and therefore justify the statement that the tails of g1(t) decay as\\n, which is faster than the\\nrate of decay that characterizes the sinc pulse. Comment on this advantage of g1(t) over the\\nsinc pulse.\\nc. Plot the waveform of g1(t) to demonstrate that g1(t) has only two distinguishable values at the\\nsampling instants; hence the reference to g1(t) as a duobinary code.\\nd. Signaling over a band-limited channel with the use of a duobinary code is referred to partial-\\nresponse signaling. Explain why. In this problem, we explore another form of partial-response signaling based on the modified\\nduobinary code. Let this second code be represented by the pulse g2(t) whose Fourier transform is\\ndefined by\\na. Plot the magnitude and phase spectra of G2( f).\\nb. Show that the modified duobinary pulse is itself defined by\\nand therefore demonstrate that it has three distinguishable levels at the sampling instants.\\nc. What is a practical advantage of the modified duobinary code over the duobinary code in terms of\\ntransmission over a band-limited channel?\\nMultichannel Line Codes Consider the passband basis functions defined in (8.56), where is itself defined by (8.57).\\nDemonstrate the validity of Properties 1, 2, and 3 of these passband basis functions. The water-filling solution for the loading problem is defined by (8.73) subject to the constraint of\\n(8.70). Using this pair of relations, formulate a recursive algorithm for computing the allocation of\\nthe transmit power P among the N subchannels. The algorithm should start with an initial total or\\nsum noise-to-signal ratio NSR(i) = 0 for iteration i = 0, and the subchannels sorted in terms of those\\nwith the smallest power allocation to the largest. The squared magnitude response of a linear channel, denoted by |H( f)|2, is shown in Figure P8.14.\\nAssuming that the gap and the noise variance for all subchannels, do the following:\\na. Derive the formulas for the optimum powers P1, P2, and P3, allocated to the three subchannels of',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 517,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Assuming that the gap and the noise variance for all subchannels, do the following:\\na. Derive the formulas for the optimum powers P1, P2, and P3, allocated to the three subchannels of\\nfrequency bands (0, W1), (W1, W2), and (W2, W).\\nb. Given that the total transmit power P = 10, l1 = 2/3, and l2 = 1/3, calculate the corresponding\\nvalues of P1, P2, and P3.\\n1 t  2 1 t  G2 f 2j 2fTb   exp j2fTb -   sin 0    f 1 2Tb   otherwise = g2 t 2Tb 2 sin t Tb    t 2Tb t -   --------------------------------------\\n= t  1 = n 2 1 = Figure P8.14 -W W -W2 W2 -W1 W1 |H( f )|2 t1 t2 1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 517,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Band-Limited Channels In this problem we explore the use of singular value decomposition (SVD) as an alternative to the\\nDFT for vector coding. This approach avoids the need for a cyclic prefix, with the channel matrix\\nbeing formulated as\\nwhere the sequence h0, h1, , hv denotes the sampled impulse response of the channel. The SVD of\\nthe matrix H is defined by\\nwhere U is an N-by-N unitary matrix and V is an (N + v)-by-(N + v) unitary matrix; that is,\\nwhere I is the identity matrix and the superscript denotes Hermitian transposition. The is an\\nN-by-N diagonal matrix with singular values\\n. The is an N-by-v matrix of zeros.\\na. Using this decomposition, show that the N subchannels resulting from the use of vector coding\\nare mathematically described by\\nThe Xn is an element of the matrix product\\n, where x is the received signal (channel output)\\nvector. An is the nth symbol an + jbn and Wn is a random variable due to channel noise.\\nb. Show that the SNR for vector coding as described herein is given by\\nwhere is the number of channels for each of which the allocated transmit power is\\nnonnegative, (SNR)n is the SNR of subchannel n, and is a prescribed gap.\\nc. As the block length N approaches infinity, the singular values approach the magnitudes of the\\nchannel Fourier transform. Using this result, comment on the relationship between vector coding\\nand discrete multitone.\\nComputer Experiments **8.16 In this computer-oriented problem, consisting of two parts, we demonstrate the effect of nonlinearity\\non eye patterns.\\na. Consider a 4-ary PAM system, operating under idealized conditions: no channel noise and no ISI.\\nthe specifications are as follows:\\nNyquist bandwidth, W = 0.5 Hz\\nRoll-off factor, = 0.5\\nSymbol duration, T = 2Tb for M = 4 and Tb is the bit duration.\\nCompute the eye pattern for this noiseless PAM system.\\nb. Repeat the computation, this time assuming that the channel is nonlinear with the following\\ninput-output relationship:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 518,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Compute the eye pattern for this noiseless PAM system.\\nb. Repeat the computation, this time assuming that the channel is nonlinear with the following\\ninput-output relationship:\\nH h0 h1 h2  h 0 0 0 h0 h1 h1 - h0     0 0 0  h0 h1 h = H = U :0N v   V UU I VV I = =   n n  1 2 N   = 0N v  Xn nAn Wn + = Ux SNR  vector coding  1 SNR  n  ------------------- +     n 1 = N  1 N v +     - = N   x t s t as2 t + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 518,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Notes where s(t) is the channel input and x(t) is the channel output (i.e., received signal); the a is a\\nconstant. Compute the eye pattern for the following three nonlinear conditions:\\nHence, discuss how varying the constant a affects the shape of the eye pattern for the 4-ary PAM\\nsystem.\\nNotes\\nThe criterion described in (8.11) or (8.15) was first formulated by Nyquist in the study of\\ntelegraph transmission theory; the Nyquist (1928b) paper is a classic. In the literature, this criterion\\nis referred to as Nyquists first criterion. In the 1928b paper, Nyquist described another method,\\nreferred to in the literature as Nyquists second criterion. The second method makes use of the\\ninstants of transition between unlike symbols in the received signal rather than centered samples. A\\ndiscussion of the first and second criteria is presented in Bennett (1970: 78-92) and in the paper by\\nGibby and Smith (1965). A third criterion attributed to Nyquist is discussed in Sunde (1969); see\\nalso the papers by Pasupathy (1974) and Sayar and Pasupathy (1987).\\nThe specifications described in Example 1 follow the book by Tranter et al. (2004).\\nThe SRRC pulse shaping is discussed in Chennakeshu and Saulnier (1993) in the context of /4-\\nshifted differential QPSK for digital cellular radio. It is also discussed in Anderson (2005: 27-29).\\nIn a strict sense, an eye pattern that is completely open occupies the range from -1 to +1. On this\\nbasis, zero intersymbol interference would correspond to an ideal eye opening of 2. However, for\\ntwo reasons, convenience of presentation and consistency with the literature, we have chosen an eye\\nopening of unity to refer to the ideal condition of zero intersymbol interference.\\nFor a detailed treatment of decision feedback equalizers, see the fifth edition of the classic book\\non Digital Communications by Proakis and Salehi (2008).\\nThe idea of an ADSL is attributed to Lechleider (1989) in having had the insight that such an',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 519,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'on Digital Communications by Proakis and Salehi (2008).\\nThe idea of an ADSL is attributed to Lechleider (1989) in having had the insight that such an\\narrangement offers the possibility of more than doubling the information capacity of a symmetric\\narrangement.\\nFor a detailed discussion of VDSL, see Chapter 7 of the book by Starr et al. (2003); see also the\\npaper by Cioffi et al. (1999).\\nThe method of Lagrange multipliers is discussed in Appendix D.\\na 0.05 0.1 0.2   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 519,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '501 CHAPTER 9 Signaling over Fading Channels In Chapters 7 and 8 we studied signaling over AWGN and band-limited channels, respec-\\ntively. In this chapter we go on to study a more complicated communications environment,\\nnamely a fading channel, which is at the very core of ever-expanding wireless communica-\\ntions. Fading refers to the fact that even though the distance separating a mobile receiver\\nfrom the transmitter is essentially constant, a relatively small movement of the receiver\\naway from the transmitter could result in a significant change in the received power. The\\nphysical phenomenon responsible for fading is multipath, which means that the transmitted\\nsignal reaches the mobile receiver via multiple paths with varying spatio-temporal charac-\\nteristics, hence the challenging nature of the wireless channel for reliable communication.\\nThis chapter consists of three related parts:\\nFirst we study signaling over a fading channel by characterizing its statistical behavior\\nin temporal as well as spacial terms. This statistical characterization is carried out from\\nthree different perspectives: physical, mathematical, and computational, each of which\\nenriches our understanding of the multipath phenomenon in its own way. This first part of\\nthe chapter finishes with:\\n\\nBER comparison of different modulation schemes for AWGN and Rayleigh fading\\nchannels.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 521,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'BER comparison of different modulation schemes for AWGN and Rayleigh fading\\nchannels.\\n\\nGraphical display of how different fading channels compare to a corresponding\\nAWGN channel using binary PSK.\\nThis evaluation then prompts the issue of how to combat the degrading effect of multipath\\nand thereby realize reliable communication over a fading channel. Indeed, the second part\\nof the chapter is devoted to this important practical issue. Specifically, we study the use of\\nspace diversity, which can be one of three kinds:\\nDiversity-on-receive, which involves the use of a single transmitter and multiple\\nreceivers, with each receiver having its own antenna.\\nDiversity-on-transmit, which involves the use of multiple transmitting antennas and\\na single receiver.\\nMultiple-input, multiple-output (MIMO) antenna system, which includes diversity\\non receive and diversity on transmit in a combined manner.\\nThe use of diversity-on-receive techniques is of long standing in the study of radio\\ncommunications. On the other hand, diversity-on-transmit and MIMO antenna systems are',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 521,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nof recent origin. The study of diversity is closely related to that of information capacity,\\nthe evaluation of which is also given special attention in the latter part of the chapter.\\nFor the third and final part of the chapter, we study spread-spectrum signals, which\\nprovide the basis of another novel way of thinking about how to mitigate the degrading\\neffects of the multipath phenomenon. In more specific terms, the use of spread-spectrum\\nsignaling leads to the formulation of code-division multiple access, a topic that was\\ncovered briefly in the introductory Chapter 1. Propagation Effects\\nThe major propagation problems1 encountered in the use of mobile radio in built-up areas\\nare due to the fact that the antenna of a mobile unit may lie well below the surrounding\\nbuildings. Simply put, there is no line-of-sight path to the base station. Instead, radio\\npropagation takes place mainly by way of scattering from the surfaces of the surrounding\\nbuildings and by diffraction over and/or around them, as illustrated in Figure 9.1. The\\nimportant point to note from Figure 9.1 is that energy reaches the receiving antenna via\\nmore than one path. Accordingly, we speak of a multipath phenomenon, in that the various\\nincoming radio waves reach their destination from different directions and with different\\ntime delays.\\nTo understand the nature of the multipath phenomenon, consider first a static\\nmultipath environment involving a stationary receiver and a transmitted signal that\\nconsists of a narrowband signal (e.g., unmodulated sinusoidal carrier). Let it be assumed\\nthat two attenuated versions of the transmitted signal arrive sequentially at the receiver.\\nThe effect of the differential time delay is to introduce a relative phase shift between any\\ntwo components of the received signal. We may then identify one of two extreme cases\\nthat can arise:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 522,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The relative phase shift is zero, in which case the two components add\\nconstructively, as illustrated in Figure 9.2a.\\n\\nThe relative phase shift is 180, in which case the two components add destructively,\\nas illustrated in Figure 9.2b.\\nFigure 9.1\\nIllustrating the mechanism of radio\\npropagation in urban areas.\\nBuilding Direction to elevated base station Obstructed line-of-sight path Mobile',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 522,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '2 Propagation Effects We may also use phasors to demonstrate the constructive and destructive effects of\\nmultipath, as shown in Figures 9.3a and 9.3b, respectively. Note that, in the static\\nmultipath environment described herein, the amplitude of the received signal does not vary\\nwith time.\\nConsider next a dynamic multipath environment in which the receiver is in motion and\\ntwo versions of the transmitted narrowband signal reach the receiver via paths of different\\nFigure 9.2 (a) Constructive and (b) destructive forms of the multipath\\nphenomenon for sinusoidal signals.\\nFigure 9.3 Phasor representations of (a) constructive and (b) destructive forms of multipath.\\nDirect-path signal Direct-path signal Reflected signal Reflected signal Composite signal Composite signal Time Time (a) (b) Phasor representing direct-transmission signal Phasor representing reflected signal Phasor representing direct-transmission signal Phasor representing reflected signal Phasor representing composite signal Phasor representing composite signal (a) (b)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 523,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nlengths. Owing to motion of the receiver, there is a continuous change in the length of each\\npropagation path. Hence, the relative phase shift between the two components of the\\nreceived signal is a function of spatial location of the receiver. As the receiver moves, we\\nnow find that the received amplitude (envelope) is no longer constant, as was the case in a\\nstatic environment; rather, it varies with distance, as illustrated in Figure 9.4. At the top of\\nthis figure, we have also included the phasor relationships for two components of the\\nreceived signal at various locations of the receiver. Figure 9.4 shows that there is\\nconstructive addition at some locations and almost complete cancellation at some other\\nlocations. This physical phenomenon is referred to as fast fading.\\nIn a mobile radio environment encountered in practice, there may of course be a\\nmultitude of propagation paths with different lengths and their contributions to the received\\nsignal could combine in a variety of ways. The net result is that the envelope of the received\\nsignal varies with location in a complicated fashion, as shown by the experimental record of\\nreceived signal envelope in an urban area that is presented in Figure 9.5. This figure clearly\\ndisplays the fading nature of the received signal. The received signal envelope in Figure 9.5\\nis measured in dBm. The unit dBm is defined as 10 log10(PP0), with P denoting the power\\nbeing measured and P0 = 1 mW as the frame of reference. In the case of Figure 9.5, P is the\\ninstantaneous power in the received signal envelope.\\nSignal fading is essentially a spatial phenomenon that manifests itself in the time\\ndomain as the receiver moves. These variations can be related to the motion of the receiver\\nas follows. Consider the situation illustrated in Figure 9.6, where the receiver is assumed\\nto be moving along the line AA with a constant velocity\\n. It is also assumed that the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 524,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'as follows. Consider the situation illustrated in Figure 9.6, where the receiver is assumed\\nto be moving along the line AA with a constant velocity\\n. It is also assumed that the\\nreceived signal is due to a radio wave from a scatterer labelled S. Let t denote the time\\ntaken for the receiver to move from point A to A. Using the notation described in Figure\\n6, the incremental change in the path length of the radio wave is deduced to be\\n(9.1)\\nFigure 9.4\\nIllustrating how the envelope fades as\\ntwo incoming signals combine with\\ndifferent phases. Fading envelope Amplitude Distance v l d  cos = vt  cos - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 524,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '2 Propagation Effects where is the spatial angle subtended between the incoming radio wave and the direction\\nof motion of the receiver. Correspondingly, the change in the phase angle of the received\\nsignal at point A with respect to that at point A is given by where  is the radio wavelength. The apparent change in frequency, or the Doppler shift, is\\ntherefore defined by\\n(9.2)\\nThe Doppler shift  is positive (resulting in an increase in frequency) when the radio\\nwaves arrive from ahead of the mobile unit and it is negative when the radio waves arrive\\nfrom behind the mobile unit.\\nFigure 9.5 Experimental record of received signal envelope in an urban area.\\n-100 -90 -80 -70 -60 0 5 10 15 20 Received signal envelope, dBm\\nDistance in meters\\nFigure 9.6\\nIllustrating the calculation\\nof Doppler shift.   2  ------l = 2vt  ---------------  cos - =  1 2 ------ t ------- - = v ---  cos = Direction of motion A A S l d',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 525,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels Jakes Model\\nTo illustrate fast fading due to a moving receiver, consider a dynamic multipath\\nenvironment that involves N iid fixed scatterers surrounding such a receiver. Let the\\ntransmitted signal be the complex sinusoidal function of unit amplitude and frequency fc,\\nas shown by\\nThen, the composite signal observed at the moving receiver, including relative effects of a\\nDoppler shift, is given by\\nwhere the amplitude An is contributed by the nth scatterer, is the corresponding Doppler\\nshift, and is some random phase. The complex envelope of the received signal is time\\nvarying, as shown by\\n(9.3)\\nCorrespondingly, the autocorrelation function of the complex envelope is defined by\\n(9.4)\\nwhere \\x02 is the expectation operator with respect to time t and the asterisk in\\ndenotes complex conjugation. Inserting (9.3) in (9.4) leads to a double summation, one\\nindexed by n and the other indexed by m. Then, simplifying the result under the iid\\nassumption, the autocorrelation function reduces to (9.5) At this point in the discussion, we make two observations:\\nThe effects of small changes in distances between the moving receiver and the nth\\nscatterer are small enough for all n for us to write\\n(9.6)\\nwhere n = 1, 2, , N.\\nThe Doppler shift is proportional to the cosine of the angle subtended\\nbetween the incoming radio wave from the nth scatterer and the direction of motion\\nof the receiver in Figure 9.6, which follows from (9.2).\\nWe may therefore write\\n(9.7) s t exp j2fct   = x0 t An exp j2fc n +  t jn +   n 1 = N  = n n x0 t An exp j2nt jn +   n 1 = N  = x0 t Rx0 t \\x02 x0* tx0 t  +     = x0* t Rx0  Rx0  \\x02 An 2 exp j2n     if m n =  n 1 = N  0 if m n          = \\x02 An 2 exp j2n     \\x02 An 2  \\x02 exp j2n     = n n n max n n  cos 1 2 N   = =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 526,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '3 Jakes Model 507 where is the maximum Doppler shift that occurs when the incoming radio waves\\npropogate in the same direction as the motion of the receiver. Accordingly, using (9.6) and\\n(9.7) in (9.5), we may write\\n(9.8)\\nwhere the multiplying factor\\n(9.9)\\nis the average signal power at the receiver input.\\nWe now make two final assumptions:\\nAll the radio waves arrive at the receiver from a horizontal direction (Clarke, 1968).\\nThe multipath is uniformly distributed over the range\\n, as shown by the\\nprobability density function (Jakes, 1974):\\n(9.10)\\nUnder these two assumptions, the remaining expectation in (9.8) becomes independent of\\nn and with it, that equation simplifies further as follows:\\nThe definite integral inside the brackets of this equation is recognized as the Bessel\\nfunction of the first kind of order zero,2 see Appendix C. By definition, for some argument\\nx, we have\\n(9.11)\\nWe may therefore express the autocorrelation function of the complex signal at the\\ninput of the moving receiver in the compact form\\n(9.12)\\nThe model described by the autocorrelation function of (9.12) is called the Jakes model.\\nFigure 9.7a shows a plot of the autocorrelation according to this model.\\nmax Rx0  P0 \\x02 exp j2max n  cos     for m n =  n 1 = N  0 for m n          = P0 An 2 n 1 = N  =   -   f  1 2 ------      -  0 otherwise       = Rx0  P0\\x02 exp j2max  cos          -  = P0 = f exp j2max  cos  d  -   P0 = 1 2 ------ exp j2max  cos  d  -   J0 x  1 2 ------ exp jx  cos  d  -   = x0 t Rx0  P0J0 2max   = Rx0',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 527,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nAccording to the Wiener-Khintchine relations for a weakly (wide-sense) stationary\\nprocess (discussed in Chapter 4), the autocorrelaton function and power spectrum form a\\nFourier-transform pair. Specifically, we may write\\n(9.13)\\nAt first sight, it might seem that a closed form solution of this transformation is\\nmathematically intractable; in reality, however, the exact solution is given in (Jakes, 1974):\\n(9.14)\\nFigure 9.7\\n(a) Autocorrelation of the complex envelope of\\nthe received signal according to the Jakes model.\\n(b) Power spectrum of the fading process for the\\nJakes model. 6 5 4 3 2 1 0 0 Normalized frequency, f\\nSpectrum (dB) (b) 1.0 0.8 0.6 0.4 0.2 0.0 -0.2 -0.4 6 -6 8 -8 10 -10 2 -2 0 Normalized delay ( max )\\nNormalized autocorrelation\\n4 -4  (a)  / max   / max  - Sx0   P0J0 2max     = Sx0   P0 1 max   2 - ---------------------------------------\\nfor max   0 for max        =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 528,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '3 Jakes Model and with it the model bears his name. Figure 9.7b plots the power spectrum in (9.14)\\nversus the Doppler shift  for\\n. This idealized graph has the shape of a bathtub,\\nexhibiting two symmetric integrable singularities at the end points\\n.\\nEXAMPLE\\nJakes Model Implemented as a FIR Filter\\nThe objective of this example is to compute a FIR (TDL) filter that models the power\\nspectrum of (9.14). To this end, we make use of the following relationships in light of\\nmaterial covered in Chapter 4 on stochastic processes:\\nThe autocorrelation function and power spectrum of a weakly stationary process\\nform a Fourier-transform pair, as already mentioned.\\nIn terms of stochastic processes, the input-output behavior of a linear system, in the\\nfrequency domain, is described by\\n(9.15)\\nwhere H(f) is the transfer function of the system, SX(f) is the power spectrum of the\\ninput process X(t), and SY(t) is the power spectrum of the output process Y(t), both\\nbeing weakly stationary.\\nIf the input process X(t) is Gaussian, then the output process Y(t) is also Gaussian.\\nIf the input X(t) is uncorrelated, then the ouput Y(t) will be correlated due to\\ndispersive behavior of the system.\\nThe issue at hand is to find the H(f) required to produce the desired power spectrum of\\n(9.14) using a white noise process of spectral density N02 as the input process X(t). Then,\\ngiven the SY(f) and setting the constant K = N02, we may solve (9.15) for H(f), obtaining\\n(9.16)\\nIn other words, H(f) is proportional to the square root of S(f). (From a practical\\nperspective, the constant K is determined by truncating the power-delay profile, an issue\\ndeferred to Section 9.14.)\\nIn light of (9.14) and (9.16), we may now say that the H(f) representing the desired\\nJakes FIR filter is given by (ignoring the constant K)\\n(9.17)\\nwhere\\n. Given this formula, we may then use inverse Fourier transformation\\nto compute the corresponding impulse response of the Jakes FIR filter.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 529,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '(9.17)\\nwhere\\n. Given this formula, we may then use inverse Fourier transformation\\nto compute the corresponding impulse response of the Jakes FIR filter.\\nHowever, before proceeding further, an important aspect of using Jakes model to\\nsimulate a fading channel is to pay particular attention to the following point:\\nThe sampling rate of the input signal applied to the Jakes model and the\\nsampled values of the fading process are highly different.\\nTo be specific, the former is a multiple of the symbol rate and the latter is a multiple of the\\nDoppler bandwidth,\\n. In other words, the sampling rate is much larger than\\n. It\\nfollows therefore that a multiple sampling rate with interpolation must be used in the\\nP0 1 =  max  = SY f H f2SX f = H f SY f K ------------ = H f 1 f 2 -   1 4  - for 1 f 1  -  0 otherwise     = f max  = max max',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 529,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nsimulation; the need for interpolation is to go from a discrete spectrum to its continuous\\nversion.\\nWith this point in mind, a 512-point inverse FFT algorithm is applied to the transfer\\nfunction of (9.17) for the following set of specifications:\\nmaximum Doppler shift,\\nsampling frequency,\\nWe thus obtain the discrete-time version of the truncated impulse response hn of the Jakes\\nFIR filter plotted in Figure 9.8a.\\nHaving computed hn, we may go on to use the FFT algorithm to compute the\\ncorresponding transfer function H(f) of the Jakes FIR filter; the result of this computation\\nis plotted in Figure 9.8b, which has a bathtub-like shape of its own, as expected.\\nEXAMPLE\\nIllustrative Generation of Fading Process Using the Jakes FIR Filter\\nTo expand the practical utility of the Jakes FIR filter computed in Example 1 to simulate\\nthe fading process, the next thing we do is to pass a complex white noise process through\\nthe filter, with the noise having uncorrelated samples. Figure 9.9a displays the power\\nmax 100 Hz = fs 16max = Figure 9.8 Jakes FIR filter. (a) Discrete impulse response. (b) Interpolated power spectral\\ndensity (PSD). 0.6 0.4 0.2 0 -0.5 0.01 0 0.02 0.03 0.04 0.05 Impulse Response Time 0.06 1.5 1 0.5 0 -600 -800 -400 -200 0 200 600 800 0.07 0.08 PSD Frequency 400 (a) (b)  10-3',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 530,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '4 Statistical Characterization of Wideband Wireless Channels spectrum of the resulting stochastic process at the filter output. Figure 9.9b shows the\\nenvelope of the output process, plotted on a logarithmic scale. This plot is typical of a\\nfading correlated signal. Statistical Characterization of Wideband Wireless Channels\\nPhysical characterization of the multipath environment described in Section 9.3 is\\nappropriate for narrowband mobile radio transmissions where the signal bandwidth is\\nsmall compared with the reciprocal of the spread in propagation path delays.\\nHowever, in real-life situations, we find that the signals radiated in a mobile radio\\nenvironment occupy a wide bandwidth, such that statistical characterization of the wireless\\nchannel requires more detailed mathematical considerations, which is the objective of this\\nsection. To this end, we follow the complex notations described in Chapter 2 to simplify\\nthe analysis.\\nTo be specific, we may express the transmitted band-pass signal as follows:\\n(9.18)\\nwhere is the complex (low-pass) envelope of x(t) and fc is the carrier frequency. Since\\nthe channel is time varying due to multipath effects, the impulse response of the channel is\\nFigure 9.9 Jakes FIR filter driven by white Gaussian noise. (a) Output power spectrum. (b) Envelope\\nof the output process.\\n0 -20 -10 -30 -40 -50 -500 0 PSD Frequency 0 -10 -5 -15 -20 0.1 0.15 0.2 0.3 500 Log Amplitude Time 0.25 (a) (b) x t Re x t j2fct   exp   = x t',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 531,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\ndelay dependent and, therefore, a time-varying function. Let the impulse response of the\\nchannel be expressed as\\n(9.19)\\nwhere is the complex low-pass impulse response of the channel and  is a delay\\nvariable. The complex low-pass impulse response is called the delay-spread\\nfunction of the channel. Correspondingly, the complex low-pass envelope of the channel\\noutput, namely\\n, is defined by the convolution integral\\n(9.20)\\nwhere the scaling factor 12 is the result of using complex notation; see Chapter 2 for\\ndetails. To be generic, the in Section 9.2 has been changed to\\n.\\nIn general, the behavior of a mobile radio channel can be described only in statistical\\nterms. For analytic purposes and mathematical tractability, the delay-spread function is modeled as a zero-mean complex-valued Gaussian process. Then, at any time t\\nthe envelope is Rayleigh distributed and the channel is therefore referred to as a\\nRayleigh fading channel. When, however, the mobile radio environment includes fixed\\nscatterers, we are no longer justified in using a zero-mean model to describe the delay-\\nspread function\\n. In such a case, it is more appropriate to use a Rician distribution\\nto describe the envelope and the channel is referred to as a Rician fading channel.\\nThe Rayleigh and Rician distributions for a real-valued stochastic process were considered\\nin Chapter 3. In the discussion presented in this chapter we focus largely, but not\\ncompletely, on a Rayleigh fading channel.\\nMultipath Correlation Function of the Channel\\nThe time-varying transfer function of the channel is defined as the Fourier transform of the\\ndelay-spread function with respect to the delay variable , as shown by\\n(9.21)\\nwhere f denotes the frequency variable. The time-varying transfer function may be\\nviewed as a frequency transmission characteristic of the channel.\\nFor a mathematically tractable statistical characterization of the channel, we make two',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 532,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'viewed as a frequency transmission characteristic of the channel.\\nFor a mathematically tractable statistical characterization of the channel, we make two\\nassumptions motivated by physical considerations; hence the practical importance of the\\nmodel resulting from these two assumptions.\\nASSUMPTION 1 Wide-Sense Stationarity\\nWith interest confined to fast fading in the short term, it is reasonable to assume\\nthat the complex impulse response is wide-sense stationary.\\nAs explained in Chapter 4, a stochastic process is said to be wide-sense (i.e., weakly)\\nstationary if its mean is time independent and its autocorrelation function is dependent\\nonly on the difference between two time instants at which the process is observed. In what\\nh t;   Re h t;   j2fct   exp   = h t;   h t;   y t y t 1 2--- h t;  x t  -   d  -   = x0 t x t h t;   h t;   h t;   h t;   h t;   H f t;   h t;   j2f -   exp  d  -   = H f t;   h t;',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 532,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Statistical Characterization of Wideband Wireless Channels follows we use the wide-sense stationary terminology because of its common use in the\\nwireless literature.\\nIn the context of the discussion presented herein, this first assumption means that\\n\\nThe expectation of with respect to time t is dependent only on the delay .',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 533,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Insofar as time t is concerned, the expectation of the product\\nis dependent only on the time difference\\n.\\nBecause Fourier transformation is a linear operation, it follows that if the complex delay-\\nspread function is a zero-mean Gaussian wide-sense stationary process, then the\\ncomplex time-varying transfer function has similar statistics.\\nASSUMPTION 2 Uncorrelated Scattering\\nThe channel is said to be an uncorrelated scattering channel, when contributions\\nfrom two or more scatterers with different propagation delays are uncorrelated.\\nIn other words, the second-order expectation with respect to time t satisfies the\\nrequirement\\nwhere is a Dirac-delta function defined in the delay domain. That is, the\\nautocorrelation function of is nonzero only when\\n.\\nIn the literature on statistical characterization of wireless channels, wide-sense\\nstationarity is abbreviated as WSS and uncorrelated scattering is abbreviated as US. Thus,\\nwhen both Assumptions 1 and 2 are satisfied simultaneously, the resulting channel model\\nis said to be the WSSUS model.\\nConsider then the correlation function3 of the delay-spread function\\n. Since is complex valued, we use the following definition for the correlation function: (9.22)\\nwhere \\x02 is the statistical expectation operator, the asterisk denotes complex conjugation, 1\\nand 2 are propagation delays of the two paths involved in the calculation, and t1 and t2 are\\nthe times at which the outputs of the two paths are observed. Under the combined WSSUS\\nchannel model, we may reformulate the correlation function in (9.22) as shown by\\n(9.23)\\nwhere t is the difference between the observation times t1 and t2 and (1-2) is the delta\\nfunction in the\\n-domain. Thus, using  in place of 1 for mathematical convenience, the\\nfunction in the second line of (9.23) is redefined as\\n(9.24)\\nThe function is called the multipath correlation profile of the channel. This new\\ncorrelation function provides a statistical measure of the extent to which the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 533,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '(9.24)\\nThe function is called the multipath correlation profile of the channel. This new\\ncorrelation function provides a statistical measure of the extent to which the\\nsignal is distorted in the time domain as a result of transmission through the channel.\\nh t;    h* 1 t1 ;   h 2 t2     t  t2 t1 - = h t;   H f t;   \\x02 h* 1 t1 ;  h 2 t2 ;     \\x02 h* 1 t1 ;  h 1 t2 ;    1 2 -   = 1 2 -   h t;   2 1  h t;   h t;   Rh 1 t1 2 t2  ;    \\x02 h* 1 t1 ;  h 2 t2 ;     = Rh 1 2 t ;    \\x02 h* 1 t;  h 2 t t + ;     = rh 1 t ;  1 2 -   =  rh t ;   \\x02 h* t;  h t t + ;     = rh t ;   rh  t  ;',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 533,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nSpaced-Frequency, Spaced-Time Correlation Function of\\nthe Channel\\nConsider next statistical characterization of the channel in terms of the complex time-\\nvarying transfer function\\n. Following a formulation similar to that described in\\n(9.22), the correlation function of is defined by (9.25) where f1 and f2 represent two frequencies in the spectrum of the transmitted signal. The\\ncorrelation function provides a statistical measure of the extent to which\\nthe signal is distorted in the frequency-domain by transmission through the channel. From\\n(9.21), (9.22), and (9.25), it is apparent that the correlation functions and form a two-dimensional Fourier-transform pair, defined as follows:\\n(9.26)\\nInvoking wide-sense stationarity in the time domain, we may reformulate (9.25) as\\n(9.27)\\nEquation (9.27) suggests that the correlation function may be measured by\\nusing pairs of spaced tones to carry out cross-correlation measurements on the resulting\\nchannel outputs. Such a measurement presumes stationarity in the time domain. If we also\\nassume stationarity in the frequency domain, we may go one step further and write\\n(9.28)\\nThe new correlation function\\n, introduced in the first line of (9.28), is in fact the\\nFourier transform of the multipath correlation profile with respect to the delay-\\ntime variable , as shown by\\n(9.29)\\nThe new function is called the spaced-frequency, spaced-time correlation\\nfunction of the channel, where the double use of spaced accounts for and\\n.\\nScattering Function of the Channel\\nFinally, we introduce another new function denoted by S() that forms a Fourier-\\ntransform pair with the multipath correlation profile with respect to the variable\\nt; that is, by definition, we have\\n(9.30)\\nfor the Fourier transform and\\n(9.31)\\nfor the inverse Fourier transform.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 534,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'transform pair with the multipath correlation profile with respect to the variable\\nt; that is, by definition, we have\\n(9.30)\\nfor the Fourier transform and\\n(9.31)\\nfor the inverse Fourier transform.\\nH f t;   H f t;   RH f1 t1 f2 t2  ;    \\x02 H * f1 t1 ;  H f2 t2 ;     = RH f1 t1 f2 t2  ;    RH f1 t1 f2 t2  ;    RH 1 t1 2 t2  ;    RH f1 t1 f2 t2  ;    Rh 1 t1 2 t2  ;    j - 2f11 f22 -     exp 1 d 2 d  -    -   RH f1 f2 t ;    \\x02 H * f1 t;  H f2 t t + ;     = RH f1 f2 t ;    RH f f f t ; +    rH f t ;   = \\x02 H * f t;  H f f t t + ; +     = rH f  ; t   rh  ; t   rH f t ;   rh  ; t   j2f -   exp  d  -   = rH f t ;   t f rh  ; t   S  ;   rh  ; t   j2t -   exp t   d  -   = rh  ; t   S ;   j2t   exp d  -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 534,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Statistical Characterization of Wideband Wireless Channels The function S() may also be defined in terms of by applying a form of\\ndouble Fourier transformation:\\nA Fourier transform with respect to the time variable t and an inverse Fourier\\ntransform with respect to the frequency variable f.\\nThat is to say,\\n(9.32)\\nFigure 9.10 displays the functional relationships between the three important functions:\\n,\\n, and S() in terms of the Fourier transform and its inverse.\\nThe function S() is called the scattering function of the channel. For a physical\\ninterpretation of it, consider the transmission of a single tone of frequency relative to\\nthe carrier. The complex envelope of the resulting filter output is\\n(9.33)\\nThe correlation function of is given by\\n(9.34)\\nwhere, in the last line, we made use of (9.28). Putting f = 0 in (9.29) and then using\\n(9.31), we may write\\n(9.35)\\nHence, we may view the integral inside the square brackets in (9.35), namely\\nFigure 9.10 Functional relationships between the multipath correlation profile\\n, the\\nspaced-frequency spaced-time correlation function\\n, and the scattering function S().\\nrH f t ;   S ;   rH f  ; t   j2t -   j2f   exp exp t   f   d d  -    -   = rh  ; t  rh f t ;   f  y t j2f t  H f t;   exp = y t \\x02 y* ty t t +     j2f t  \\x02  H * f t;  H * f t t + ;     exp = j2f t  rH 0 t ;   exp = rH 0 t ;   rh t ;   d  -   = S ;   d  -   j2t   exp d  -   = S ;   d  -   Ft[] Ft []: F [] -1 F [] Fourier transform with respect to delay\\nInverse Fourier transform with respect to frequency increment  f\\nInverse Fourier transform with respect to Doppler shift\\nFourier transform with respect to time increment  t\\n-1 Ff [] -1 Ff []: ~ ~ Spaced-frequency Spaced-time Correlation function rH ( f; t) Multipath autocorrelation profile rh ( ;t) Scattering function S( ; )        F []:  F []: -1  rh  ; t   rH f t ;',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 535,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nas the power spectral density of the channel output relative to the frequency of the\\ntransmitted tone with the Doppler shift  acting as the frequency variable. Generalizing\\nthis result, we may now make the statement:\\nThe scattering function S(; ) provides a statistical measure of the output\\npower of the channel, expressed as a function of the time delay  and the\\nDoppler shift .\\nPower-Delay Profile\\nWe continue statistical characterization of the wireless channel by putting t = 0 in (9.24)\\nto obtain (9.36) The function describes the intensity (averaged over the fading fluctuations) of the\\nscattering process at propagation delay  for the WSSUS channel. Accordingly, is\\ncalled the power-delay profile of the channel. In any event, this profile provides an estimate\\nof the average multipath power expressed as a function of the delay variable .\\nThe power-delay profile may also be defined in terms of the scattering function S()\\nby averaging it over all potentially possible Doppler shifts. Specifically, setting t = 0 in\\n(9.31) and then using the first line of (9.36), we obtain\\n(9.37)\\nFigure 9.11 shows an example of the power-delay profile that depicts a typical plot of the\\npower spectral density versus excess delay;4 the excess delay is measured with respect to\\nthe time delay for the shortest echo path. The threshold level K included in Figure 9.11\\ndefines the power level below which the receiver fails to operate satisfactorily.\\nFigure 9.11 Example of a power-delay profile for a mobile radio channel.\\nf  Ph  rh 0 ;   = \\x02 h t;   2   = Ph  Ph   Ph   S ;   d  -   = Delay Threshold level Power-delay profile, Ph( )\\n~  K 0',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 536,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '4 Statistical Characterization of Wideband Wireless Channels Central Moments of\\nTo characterize the power-delay profile of a WSSUS channel in statistical terms, we begin\\nwith the moment of order zero; that is, the integrated power averaged over the delay\\nvariable , as shown by\\n(9.38)\\nThe average delay, normalized with respect to Pav, is defined in terms of the first-order\\nmoment by the formula\\n(9.39)\\nCorrespondingly, the second-order central moment, normalized with respect to Pav, is\\ndefined by the root-mean-square (RMS) formula\\n(9.40)\\nThe new parameter is called the delay spread, which has acquired a special stature\\namong the parameters used to characterize the WSSUS channel.\\nFrom Chapter 2 on the representation of signals in a linear environment, we recall that\\nthe duration of a signal in the time domain is inversely related to the bandwidth of the\\nsignal in the frequency domain. Building on this time-frequency relationship, we may\\ndefine the coherence bandwidth Bcoherence of a WSSUS channel as follows:\\n(9.41)\\nIn words:\\nThe coherence bandwidth of the WSSUS channel is that band of frequencies for\\nwhich the frequency response of the channel is strongly correlated.\\nThis statement is intuitively satisfying.\\nDoppler Power Spectrum\\nConsider next the issue of relating Doppler effects to time variations of the channel. In\\ndirect contrast to the power-delay profile, this time we set f = 0, which corresponds to the\\ntransmission of a single tone (of some appropriate frequency) over the channel. Under this\\ncondition, the spaced-frequency, spaced-time correlation function of the channel,\\ndescribed in (9.29), reduces to\\n. Hence, evaluating the Fourier transform of this\\nfunction with respect to the time variable t, we may write\\n(9.42)\\nThe function defines the power spectrum of the channel output expressed as a\\nfunction of the Doppler shift ; it is therefore called the Doppler power spectrum of the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 537,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '(9.42)\\nThe function defines the power spectrum of the channel output expressed as a\\nfunction of the Doppler shift ; it is therefore called the Doppler power spectrum of the\\nchannel. Ph   Pav Ph d  -   = av 1 Pav -------- Ph  d  -   =  1 Pav --------  av -  2Ph  d  -   1 2  =  Bcoherence 1 av ------- = rH 0 t ;   SH   rH 0 t ;   j - 2t   t   d exp  -   = SH',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 537,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nThe Doppler-power spectrum of (9.42) may be interpreted in two insightful ways\\n(Molisch, 2011):\\nThe Doppler spectrum describes the frequency dispersion of a wireless channel,\\nwhich results in the occurrence of transmission errors in narrowband mobile\\nwireless communication systems.\\nThe Doppler spectrum provides a measure of temporal variability of the channel,\\nwhich, in mathematical terms, is described by the channels correlation function for\\n.\\nAs such, we may view the Doppler-power spectrum as another important statistical\\ncharacterization of WSSUS channels.\\nThe Doppler power spectrum may also be defined in terms of the scattering function by\\naveraging it over all possible propagation delays, as shown by\\n(9.43)\\nTypically, the Doppler shift  assumes positive and negative values with almost equal\\nlikelihood. The mean Doppler shift is therefore effectively zero. The square root of the\\nsecond moment of the Doppler spectrum is thus defined by\\n(9.44)\\nThe parameter  provides a measure of the width of the Doppler spectrum; therefore, it is\\ncalled the Doppler spread of the channel.\\nAnother useful parameter that is often used in radio propagation measurements is the\\nfade rate of the channel. For a Rayleigh fading channel, the average fade rate is related to\\nthe Doppler spread  by the empirical rule: crossings per second\\n(9.45)\\nAs the name implies, the fade rate provides a measure of the rapidity of the channel fading\\nphenomenon.\\nSome typical values encountered in a mobile radio environment are as follows:\\n\\nthe delay spread  amounts to about 20 s;',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 538,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the delay spread  amounts to about 20 s;\\n\\nthe Doppler spread  due to the motion of a vehicle may typically occupy the range\\n40-100 Hz, but sometimes may well exceed 100 Hz.\\nOne other parameter directly related to the Doppler spread is the coherence time of the\\nchannel. Here again, as with coherence bandwidth discussed previously, we may invoke\\nthe inverse time-frequency relationship to say that the coherence time of a multipath\\nwireless channel is inversely proportional to the Doppler spread, as shown by\\n(9.46) rH 0 t  ;   f  0 = SH   S ;   d  -   =  2SH   d  -   SH   d  -   --------------------------------------\\n           1 2  = ffade rate 1.475 = coherence 1  ------ = 0.3 2max --------------',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 538,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Statistical Characterization of Wideband Wireless Channels where is the maximum Doppler shift due to motion of the mobile unit. In words:\\nThe coherence time of the channel is that duration for which the time response\\nof the channel is strongly correlated.\\nHere again, this statement is intuitively satisfying.\\nClassification of Multipath Channels\\nThe particular form of fading experienced by a multipath channel depends on whether the\\nchannel characterization is viewed in the frequency domain or the time domain:\\nWhen the channel is viewed in the frequency domain, the parameter of concern is\\nthe channels coherence bandwidth Bcoherence, which is a measure of the\\ntransmission bandwidth for which signal distortion across the channel becomes\\nnoticeable. A multipath channel is said to be frequency selective if the coherence\\nbandwidth of the channel is small compared with the bandwidth of the transmitted\\nsignal. In such a situation, the channel has a filtering effect, in that two sinusoidal\\ncomponents with a frequency separation greater than the channels coherence\\nbandwidth are treated differently. If, however, the coherence bandwidth of the\\nchannel is large compared with the transmitted signal bandwidth, the fading is said\\nto be frequency nonselective, or frequency flat.\\nWhen the channel is viewed in the time domain, the parameter of concern is the\\ncoherence time coherence, which provides a measure of the transmitted signal\\nduration for which distortion across the channel becomes noticeable. The fading is\\nsaid to be time selective if the coherence time of the channel is small compared with\\nthe duration of the received signal (i.e., the time for which the signal is in flight). For\\nmax\\nFigure 9.12\\nIllustrating the four classes of multipath channels:\\nc = coherence time, Bc = coherence bandwidth.\\n0 Bandwidth c Time duration Time-flat Flat-flat Non-flat in both time and frequency Frequency-flat Bc',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 539,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\ndigital transmission, the received signals duration is taken as the symbol duration\\nplus the channels delay spread. If, however, the channels coherence time is large\\ncompared with the received signal duration, then the fading is said to be time\\nnonselective, or time flat, in the sense that the channel appears to the transmitted\\nsignal as time invariant.\\nIn light of this discussion, we may classify multipath channels as follows:\\n\\nFlat-flat channel, which is flat in both frequency and time.\\n\\nFrequency-flat channel, which is flat in frequency only.\\n\\nTime-flat channel, which is flat in time only.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 540,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Completely non-flat channel, which is flat neither in frequency nor in time; such a\\nchannel is also referred to as a doubly spread channel.\\nThe classification of multipath channels, based on this approach, is shown in Figure 9.12.\\nThe forbidden area, shown shaded in this figure, follows from the inverse relationship that\\nexists between bandwidth and time duration. FIR Modeling of Doubly Spread Channels\\nIn Section 9.4, statistical analysis of the doubly spread channel was carried out by focusing\\non two complex low-pass entities, namely the impulse response and the correspond-\\ning transfer function\\n. Therein, mathematical simplification was accomplished by\\ndisposing of the midband frequency fc of the actual band-pass character of the doubly\\nspread channel. Despite this simplification, the analytic approach used in Section 9.4 is\\nhighly demanding in mathematical terms. In this section, we will take an approximate\\napproach based on the use of a FIR filter to model the doubly spread channel.5 From an\\nengineering perspective. this new approach has a great deal of practical merit.\\nTo begin, we use the convolution integral to describe the input-output relationship of\\nthe system, as shown in (9.20), reproduced here for convenience of presentation\\n(9.47)\\nwhere is the complex low-pass input signal applied to the channel and is the\\nresulting complex low-pass output signal. Although this integral can be formulated in\\nanother equivalent way, the choice made in (9.47) befits modeling of a time-varying FIR\\nsystem, as we will see momentarily. Speaking of the input signal\\n, we assume that its\\nFourier transform satisfies the condition\\n(9.48)\\nwhere 2W denotes the original input band-pass signals bandwidth centered around the\\nmidband frequency fc. With FIR filtering in mind, it is logical to expand the delayed input\\nsignal using the sampling theorem, discussed in Chapter 6. Specifically, we write',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 540,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'midband frequency fc. With FIR filtering in mind, it is logical to expand the delayed input\\nsignal using the sampling theorem, discussed in Chapter 6. Specifically, we write\\n(9.49) h t;   H f t;   y t 1 2--- h t;  x t  -  d  -   = x t y t x t X f 0 = for f W  x t  -   x t  -   x t nT -  sinc  Ts ----- n -     n  - =   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 540,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': '5 FIR Modeling of Doubly Spread Channels where Ts is the sampling period of the FIR filter chosen in accordance with the sampling\\ntheorem as follows:\\n(9.50)\\nThe sinc function in (9.49) is defined by\\n(9.51)\\nFrom the standpoint of the sampling theorem we could set 1Ts = 2W, but the choice made\\nin (9.50) gives us more practical flexibility.\\nIn (9.49) it is important to note that we have done the following:\\n\\nDependence on the coordinate functions under the summation has been put on the\\ndelay variable in the sinc function.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 541,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Dependence on the coordinate functions under the summation has been put on the\\ndelay variable in the sinc function.\\n\\nDependence on the time-varying FIR coefficients has been put on time t.\\nThis separation of variables is the key to the FIR modeling of a linear time-varying\\nsystem. Note also that the sinc functions under the summation in (9.49) are orthogonal but\\nnot normalized.\\nThus, substituting (9.49) into (9.47) and interchanging the order of integration and\\nsummation, which is permitted as we are dealing with a linear system, we get\\n(9.52)\\nTo simplify matters, we now introduce the complex tap-coefficients6\\n, defined in\\nterms of the complex impulse response as follows:\\n(9.53)\\nAccordingly, we may rewrite (9.52) in the much simplified summation form:\\n(9.54)\\nExamining (9.54) for insight, we may make our first observation:\\nThe uniformly sampled functions are generated as tap-inputs by\\npassing the complex low-pass input signal through a TDL filter whose taps\\nare spaced T seconds apart.\\nTurning next to (9.53) for insight, refer to Figure 9.13, where this equation is sketched for\\nthree different settings of the function\\n; the area shaded in the figure\\nrefers to the complex impulse response that is assumed to be causal and occupying\\na finite duration. In light of the three different sketches shown in Figure 9.13, we may\\nmake our second observation.\\n1 Ts ----- 2W  sinc  Ts ----- n -     sin  Ts ----- n -      Ts ----- n -     ------------------------------------\\n=  y t x t Ts ----- n -     h t;  sinc t Ts ----- n -     d  -   n  - =   = cn t cn t 1 2--- h t;  sinc t Ts ----- n -    d  -   = y t x t Ts ----- n -    cn t n  - =   = x t T    n -   x t sinc t Ts    n -   h t;',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 541,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nAssuming that the integral in (9.53) is dominated by the mainlobe of the sinc\\nfunction, the complex time-varying tap-coefficient is essentially zero for\\nnegative values of discrete time n and all positive values of n greater than\\n.\\nIn accordance with these two observations, we may approximate (9.54) as follows:\\n(9.55)\\nwhere K is the number of taps.\\nEquation (9.55) defines a complex FIR model for the representation of a complex low-pass\\ntime-varying system characterized by the complex impulse response\\n. Figure 9.14\\ndepicts a block diagram representation of this model, based on (9.55).\\nFigure 9.13 Illustrating the way in which location of the sinc\\nweighting function shows up for varying n.\\n-1 1 2 K -1 1 2 K -1 0 0 0 1 2 K K + 1 ttth( ; t)  h( ; t)  h( ; t)  sinc(Ts) t sinc(Ts + 1) t sinc(Ts - (K + 1)) t (a) n = 0 (b) n = -1 (c) n = K + 1 cn t T  y t x t Ts ----- n -    cn t n 0 = K   h T ;',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 542,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '5 FIR Modeling of Doubly Spread Channels Some Practical Matters\\nTo model the doubly spread channel by means of a FIR filter in accordance with (9.55),\\nwe need to know the sampling rate 1Ts and the number of taps K in this equation. To\\nsatisfy these two practical requirements, we offer the following empirical points:\\nThe sampling rate of the FIR filter, 1Ts, is much higher than the maximum Doppler\\nbandwidth of the channel,\\nmax; typically, we find that 1Ts is eight to sixteen times\\nmax. Hence, knowing\\nmax, we may determine a desirable value of the sampling\\nrate 1Ts.\\nThe number of taps K in (9.55) may be determined by truncating the power-delay\\nprofile of the channel. Specifically, given a measurement of this profile, a\\nsuitable value of K is determined by choosing a threshold level below which the\\nreceiver fails to operate satisfactorily, as illustrated in Figure 9.11.\\nGeneration of the Tap-Coefficients\\nTo generate the tap-coefficients\\n, we may use the scheme shown in Figure 9.15 that\\ninvolves the following (Jeruchim et al., 2000):\\nA complex white Gaussian process of zero mean and unit variance is used as the\\ninput.\\nA complex low-pass filter of transfer function is chosen in such a way that it\\nproduces the desired Doppler power spectrum where we have used f in place\\nof the Doppler shift  for convenience of presentation. In other words, we may set\\nFigure 9.14 Complex FIR model of a complex low-pass time-varying channel.\\n Input signal c0(t) c1(t) c2(t) cK - 1(t) cK(t) Ts Ts Ts   x(t) x(t - Ts) x(t - 2Ts) x(t - K ) Output signal y(t)               Ph f cn t H f SH f Figure 9.15 Scheme for generating the\\nnth complex weighting\\ncoefficients in the\\nFIR model of Figure 9.14.\\ncn t H(f ) g(t) x(t - n )   Time-varying complex tap-coefficient cn(t)  Tap input Gain Complex Gaussian white noise process  n Complex low-pass linear filter',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 543,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\n(9.56)\\nwhere, in the second line, denotes the power spectral density of the white\\nnoise process, which is equal to unity by assumption.\\nThe filter is designed in such a way that its output has a normalized power of\\nunity.\\nThe static gain, denoted by\\n, accounts for different variances of the different\\ntap-coefficients. EXAMPLE 3 Rayleigh Processes For complex FIR modeling of a time-varying Rayleigh fading channel, we may use zero-\\nmean complex Gaussian processes to represent the time-varying tap-coefficients\\n,\\nwhich, in turn, means that the complex impulse response of the channel is also a\\nzero-mean Gaussian process in the variable t.\\nMoreover, under the assumption of a WSSUS channel, the tap-coefficients for\\nvarying n will be uncorrelated. The power spectral density of each tap-coefficient is\\nspecified by the Doppler spectrum. In particular, the variance of the nth weight\\nfunction is approximately given by\\n(9.57)\\nwhere Ts is the sampling period of the FIR and is a discrete version of the power-\\ndelay profile, . EXAMPLE 4 Rician-Jakes Doppler Spectrum Model\\nThe Jakes model, discussed in Example 1, is well suited for describing the Doppler\\nspectrum for a dense-scattering environment, exemplified by an urban area. However, in a\\nrural environment, there is a high likelihood for the presence of one strong direct line-of-\\nsight path, for which the FIR-based Rician model is an appropriate candidate. In such an\\nenvironment, we may use the Rician-Jakes Doppler spectrum that has the following form\\n(Tranter et al., 2004):\\n(9.58)\\nwhere is the maximum magnitude of the Doppler shift. This partially empirical for-\\nmula, plotted in Figure 9.16, consists of two components: the FIR Jakes filter of Example 1,\\nand two delta functions at representing a direct-line-of sight signal received.\\nTypically, the sequence defined by decreases with n in an approximate\\nexponential manner, eventually reaching a neglibly small value at some time\\n. This',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 544,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Typically, the sequence defined by decreases with n in an approximate\\nexponential manner, eventually reaching a neglibly small value at some time\\n. This\\nexponential approximation of the power-delay profile has been validated experimentally\\nby many measurements; see Note 4. In any event, the number of taps in the FIR filter, K, is\\nSc f SH f = Sw fH f 2 = H f 2 = Sw f g t n cn t h t;   cn t n 2 \\x02 cn t2   Ts 2p n    p n   Ph  S c f 0.41 1 f max   2 - --------------------------------------\\n= 0.91f 0.7max    + max 0.7max  p nTs   Tmax',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 544,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 Comparison of Modulation Schemes: Effects of Flat Fading approximately defined by the ratio\\n. The point made here on the number of taps\\nK substantiates what has been made previously on Jakes model in Example 1 and in point\\n2 under Some Practical Matters in this section. Comparison of Modulation Schemes: Effects of Flat Fading\\nWe bring this first part of the chapter to an end by presenting the effects of flat fading on\\nthe behavior of different modulation schemes for wireless communications.\\nIn Chapter 7 we studied the subject of signaling over AWGN channels using different\\nmodulation schemes and evaluated their performance under two different receiver condi-\\ntions: coherence and noncoherence. For the purpose of comparison, we have reproduced the\\nBER for a selected number of those modulation schemes in AWGN in Table 9.1.\\nFigure 9.16\\nIllustrating the Rician-Jakes\\nDoppler spectrum of (9.58).\\n-0.7 0 0.7 f Sd(f) -max max max max Tmax Ts  Table 9.1 Formulas for the BER of coherent and noncoherent digital receivers\\nBER Signaling scheme AWGN channel Flat Rayleigh fading channel\\n(a) Binary PSK, QPSK, MSK using coherent detection\\n(b) Binary FSK using coherent detection\\n(c) Binary DPSK\\n(d) Binary FSK using noncoherent detection\\nEb: transmitted energy per bit; N02: power spectral density of channel noise;\\n0: mean value of the received energy per bit-to-noise spectral density ratio.\\nQ 2Eb N0 ---------       1 2--- 1 0 1 0 + --------------- -       Q Eb N0 -------       1 2--- 1 0 2 0 + --------------- -       1 2--- Eb N0 ------- -       exp 1 2 1 0 +   -----------------------\\nEb 2N0 ---------- -       exp 1 2 0 + ---------------',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 545,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nTable 9.1 also includes the exact formulas for the BER for a flat Rayleigh fading\\nchannel, where the parameter\\n(9.59)\\nis the mean value of the received signal energy per bit-to-noise spectral density ratio. In\\n(9.59), the expectation \\x02[2] is the mean value of the Rayleigh-distributed random\\nvariable  characterizing the channel. The derivations of the fading-channel formulas\\nlisted in the last column of Table 9.1 are addressed in Problems 9.1 and 9.2.\\nComparing the formulas for a flat Rayleigh fading channel with the formulas for their\\nAWGN (i.e., nonfading) channel counterparts, we find that the Rayleigh fading process\\nresults in a severe degradation in the noise performance of a wireless communication\\nreceiver with the degradation measured in terms of decibels of additional mean SNR\\nspectral density ratio. In particular, the asymptotic decrease in the BER with follows an\\ninverse law. This form of asymptotic behavior is dramatically different from the case of a\\nnonfading channel, for which the asymptotic decrease in the BER with follows an\\nexponential law.\\nIn graphical terms, Figure 9.17 plots the formulas under part a of Table 9.1 compared\\nwith the BERs of binary PSK over the AWGN and Rayleigh fading channels. The figure\\nalso includes corresponding plots for the Rician fading channel with different values of the\\nRice factor K, discussed in Chapter 4. We see that as K increases from zero to infinity, the\\nbehavior of the receiver varies all the way from the Rayleigh channel to the AWGN\\nFigure 9.17 Comparison of performance of coherently detected binary PSK over\\ndifferent fading channels.\\n0 Eb N0 ------\\x02 2   = 0 0 0.1 1E-5 1E-4 1E-3 0.01 1 6 8 10 12 14 2 0 Bit error rate 4 16 18 20 Eb/N0 (dB) Gaussian Rayleigh Rician K = 12 Rician K = 6 Rician K =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 546,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '7 Diversity Techniques channel. The results plotted in Figure 9.17 for the Rician channel were obtained using\\nsimulations (Haykin and Moher, 2005). From Figure 9.17 we see that, as matters stand, we\\nhave a serious problem caused by channel fading. For example, at an SNR of 20 dB and\\nthe presence of Rayleigh fading, the use of binary PSK results in a BER of about 310-2,\\nwhich is not good enough for the transmission of speech or digital data over the wireless\\nchannel. 9.7 Diversity Techniques Up to now, we have emphasized the multipath fading phenomenon as an inherent\\ncharacteristic of a wireless channel, which indeed it is. Given this physical reality, how,\\nthen, do we make the communication process across the wireless channel into a reliable\\noperation? The answer to this fundamental question lies in the use of diversity, which may\\nbe viewed as a form of redundancy in a spatial context. In particular, if several replicas of\\nthe information-bearing signal can be transmitted simultaneously over independently\\nfading channels, then there is a good likelihood that at least one of the received signals will\\nnot be severely degraded by channel fading. There are several methods for making such a\\nprovision. In the context of the material covered in this book, we identify three approaches\\nto diversity:\\nFrequency diversity, in which the information-bearing signal is transmitted using\\nseveral carriers that are spaced sufficiently apart from each other to provide\\nindependently fading versions of the signal. This may be accomplished by choosing\\na frequency spacing equal to or larger than the coherence bandwidth of the channel.\\nTime diversity, in which the same information-bearing signal is transmitted in\\ndifferent time slots, with the interval between successive time slots being equal to or\\ngreater than the coherence time of the channel. We can still get some diversity if the\\ninterval is less than the coherence time of the channel, but at the expense of',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 547,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'greater than the coherence time of the channel. We can still get some diversity if the\\ninterval is less than the coherence time of the channel, but at the expense of\\ndegraded performance. In any event, time diversity may be likened to the use of a\\nrepetition code for error-control coding.\\nSpace diversity, in which multiple transmit or receive antennas, or both, are used\\nwith the spacing between adjacent antennas being chosen so as to ensure the\\nindependence of possible fading events occurring in the channel.\\nAmong these three kinds of diversity, space diversity is the subject of interest in the\\nsecond part of this chapter. Depending on which end of the wireless link is equipped with\\nmultiple antennas, we may identify three different forms of space diversity:\\nReceive diversity, which involves the use of a single transmit antenna and multiple\\nreceive antennas.\\nTransmit diversity, which involves the use of multiple transmit antennas and a single\\nreceive antenna.\\nDiversity on both transmit and receive, which combines the use of multiple antennas\\nat both the transmitter and receiver.\\nReceive diversity is the oldest one of the three, with the other two being of more recent\\norigin. In what follows, we will study these three different forms of diversity in this order.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 547,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels Space Diversity-on-Receive Systems\\nIn space diversity on receive, multiple receiving antennas are used with the spacing\\nbetween adjacent antennas being chosen so that their respective outputs are essentially\\nindependent of each other. This requirement may be satisfied by spacing the adjacent\\nreceiving antennas by as much as 10 to 20 radio wavelengths or less apart from each other.\\nTypically, an elemental spacing of several radio wavelengths is deemed to be adequate for\\nspace diversity on receive. The much larger spacing is needed for elevated base stations,\\nfor which the angle spread of the incoming radio waves is small; note that the spatial\\ncoherence distance is inversely proportional to the angle spread. Through the use of\\ndiversity on receive as described here, we create a corresponding set of fading channels\\nthat are essentially independent. The issue then becomes that of combining the outputs of\\nthese statistically independent fading channels in accordance with a criterion that will\\nprovide improved receiver performance. In this section, we describe three different\\ndiversity-combining systems that do share a common feature: they all involve the use of\\nlinear receivers; hence the relative ease of their mathematical tractability.\\nSelection Combining\\nThe block diagram of Figure 9.18 depicts a diversity-combining structure that consists of\\ntwo functional blocks: Nr linear receivers and a logic circuit. This diversity system is said\\nto be of a selection combining kind, in that given the Nr receiver outputs produced by a\\ncommon transmitted signal, the logic circuit selects the particular receiver output with the\\nlargest SNR as the received signal. In conceptual terms, selection combining is the\\nsimplest form of space-diversity-on-receive system.\\nTo describe the benefit of selection combining in statistical terms, we assume that the\\nwireless communication channel is described by a frequency-flat, slowly fading Rayleigh',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 548,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'To describe the benefit of selection combining in statistical terms, we assume that the\\nwireless communication channel is described by a frequency-flat, slowly fading Rayleigh\\nchannel. The implications of this assumption are threefold:\\nThe frequency-flat assumption means that all the frequency components constituting\\nthe transmitted signal experience the same random attenuation and phase shift.\\nThe slow-fading assumption means that fading remains essentially unchanged\\nduring the transmission of each symbol.\\nThe fading phenomenon is described by the Rayleigh distribution.\\nLet denote the complex envelope of the modulated signal transmitted during the\\nsymbol interval 0  t  T. Then, in light of the assumed channel, the complex envelope of\\nthe received signal of the kth diversity branch is defined by\\n(9.60)\\nwhere, for the kth diversity branch, the fading is represented by the multiplicative term and the additive channel noise is denoted by\\n. With the fading assumed\\nto be slowly varying relative to the symbol duration T, we should be able to estimate and then\\nremove the unknown phase shift k at each diversity branch with sufficient accuracy, in\\nwhich case (9.60) simplifies to\\n(9.61) s t xk t k jk  s t exp w k t + = 0 t T  k 1 2 Nr   = k jk   exp w k t xk t ks t w k t 0 t T  k 1 2 Nr   =  +',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 548,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Space Diversity-on-Receive Systems The signal component of is and the noise component is\\n. The average\\nSNR at the output of the kth receiver is therefore\\nOrdinarily, the mean-square value of is the same for all k. Accordingly, we may\\nexpress the (SNR)k as (9.62)\\nwhere E is the symbol energy and N02 is the noise spectral density. For binary data, E\\nequals the transmitted signal energy per bit Eb.\\nLet k denote the instantaneous SNR measured at the output of the kth receiver during\\nthe transmission of a given symbol. Then, replacing the mean-square value \\x02[|k|2] by the\\ninstantaneous value |k|2 in (9.62), we may write\\n(9.63)\\nUnder the assumption that the random amplitude k is Rayleigh distributed, the squared\\namplitude will be exponentially distributed7 (i.e., chi-squared with two degrees of\\nfreedom, discussed in Appendix A). If we further assume that the average SNR over the\\nshort-term fading is the same, namely av, for all the Nr diversity branches, then we may\\nexpress the probability density functions of the random variables pertaining to the\\nindividual branches as follows:\\n(9.64)\\nFigure 9.18 Block diagram of selection combiner, using Nr receive antennas.\\nReceiver 1 1 2 Nr Logic circuit Output Receiver 2 Multiple receive antennas Receiver Nr x1(t)  xN(t) x2(t)      xk t ks t w k t SNR  k \\x02 ks t2   \\x02 w k t2   -----------------------------\\n= \\x02 s t2   \\x02 w k t2   ---------------------------\\n     \\x02 k 2   k 1 2 Nr   = = w k t SNR  k E N0 ------\\x02 k 2   k  1 2 Nr   = = k E N0 ------k 2 k  1 2 Nr   = = k 2 k fk k  = 1 av ------- k av ------- -     k 0,  k 1 2 Nr   = exp',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 549,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nFor some SNR , the associated cumulative distributions of the individual branches are\\ndescribed by\\n(9.65)\\nfor k = 1, 2,, Nr. Since, by design, the Nr diversity branches are essentially statistically\\nindependent, the probability that all the diversity branches have an SNR less than the\\nthreshold  is the product of the individual probabilities that k <  for all k; thus, using\\n(9.64) in (9.65), we write\\n(9.66)\\nfor k = 1, 2, , Nr; note that the probability in (9.66) decreases with increasing Nr.\\nThe cumulative distribution function of (9.66) is the same as the cumulative\\ndistribution function of the random variable described by the sample value\\n(9.67)\\nwhich is less than the threshold  if, and only if, the individual SNRs are all\\nless than . Indeed, the cumulative distribution function of the selection combiner (i.e., the\\nprobability that all of the Nr diversity branches have an SNR less than ) is given by\\n(9.68)\\nBy definition, the probability density function is the derivative of the cumulative\\ndistribution function with respect to the argument sc. Hence, differentiating\\n(9.68) with respect to sc yields\\n(9.69)\\nFor convenience of graphical presentation, we use the scaled probability density function\\n\\x02 k     fk k  k d  -   = 1  av ------- -     exp - =  0  \\x02 k     \\x02 k     k 1 = Nr  = 1  av ------- -     exp - k 1 = Nr  = 1  av ------- -     exp - Nr  0   = sc sc max 1 2 Nr      = 1 2 Nr    Fsc   1 sc av ------- -     exp - Nr sc 0   = fsc   Fsc   fsc   d dsc ----------Fsc   = Nr av ------- sc av ------- -    1 sc av ------- -     exp - Nr 1 - sc 0   exp = fX x  av fsc sc   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 550,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '8 Space Diversity-on-Receive Systems where the sample value x of the normalized variable X is defined by\\nFigure 9.19 plots fX(x) versus x for varying number of receive-diversity branches Nr under\\nthe assumption that the short-term SNRs for all the Nr branches share the common value\\nav. From this figure we make two observations:\\nAs the number of diversity branches Nr is increased, the probability density function\\nfX(x) of the normalized random variable progressively moves to the\\nright.\\nThe probability density function fX(x) becomes more and more symmetrical and,\\ntherefore, Gaussian as Nr is increased.\\nStated in another way, a frequency-flat, slowly fading Rayleigh channel is modified through\\nthe use of selection combining into a Gaussian channel provided that the number of diversity\\nchannels Nr is sufficiently large. Realizing that a Gaussian channel is a digital communica-\\ntion theorists dream, we now see the practical benefit of using selection combining.\\nAccording to the theory described herein, the selection-combining procedure requires\\nthat we monitor the receiver outputs in a continuous manner and, at each instant of time,\\nselect the receiver with the strongest signal (i.e., the largest instantaneous SNR). From a\\npractical perspective, such a selective procedure is rather cumbersome. We may overcome\\nFigure 9.19 Normalized probability density function for a varying number Nr of receive antennas.\\nx sc av  = X av  = 6 0 1 2 3 4 5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 2 3 4 x fX(x) 6 10 Nr = 1 fX x  Nr exp x -  1 exp x -   -   Nr 1 - =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 551,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nthis practical difficulty by adopting a scanning version of the selection-combining\\nprocedure:\\n\\nStart the procedure by selecting the receiver with the strongest output signal.\\n\\nMaintain using the output of this particular receiver as the combiners output so long\\nas its instantaneous SNR does not drop below a prescribed threshold.\\n\\nAs soon as the instantaneous SNR of the combiner falls below the threshold, select a\\nnew receiver that offers the strongest output signal and continue the procedure.\\nThis technique has a performance very similar to the nonscanning version of selective\\ndiversity.\\nEXAMPLE\\nOutage Probability of Selection Combiner\\nThe outage probability of a diversity combiner is defined as the percentage of time the\\ninstantaneous output SNR of the combiner is below some prescribed level for a specified\\nnumber of branches. Using the cumulative distribution function of (9.68), Figure 9.20\\nplots the outage curves for the selection combiner with Nr as the running parameter. The\\nhorizontal axis of the figure represents the instantaneous output SNR of the combiner\\nrelative to 0 dB (i.e., the 50-percentile point for Nr = 1) and the vertical axis represents the\\noutage probability, expressed as a percentage. From the figure we observe the following:\\nThe fading depth introduced through the use of space diversity on receive\\ndiminishes rapidly with the increase in the number of diversity branches.\\nFigure 9.20\\nOutage probability for\\nselector combining for a\\nvarying number Nr of\\nreceive antennas. 2 3 4 Nr =1 6 10 0 -8 -6 -4 -2 0 2 4 6 8 10 100 90 80 70 60 50 40 30 20 10 0 Normalized SNR relative to single-channel median (Nr = 1) (dB)\\nOutage probability, percent',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 552,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '8 Space Diversity-on-Receive Systems Maximal-Ratio Combining\\nThe selection-combining technique just described is relatively straightforward to\\nimplement. However, from a performance point of view, it is not optimum, in that it\\nignores the information available from all the diversity branches except for the particular\\nbranch that produces the largest instantaneous power of its own demodulated signal.\\nThis limitation of the selection combiner is mitigated by the maximal-ratio combiner,8\\nthe composition of which is described by the block diagram of Figure 9.21 that consists of\\nNr linear receivers followed by a linear combiner. Using the complex envelope of the\\nreceived signal at the kth diversity branch given in (9.60), the corresponding complex\\nenvelope of the linear combiner output is defined by\\n(9.70)\\nwhere the ak are complex weighting parameters that characterize the linear combiner.\\nThese parameters are changed from instant to instant in accordance with signal variations\\nin the Nr diversity branches over the short-term fading process. The requirement is to\\ndesign the linear combiner so as to maximize the output SNR of the combiner at each\\ninstant of time. From (9.70), we note the following two points:\\nThe complex envelope of the output signal equals the first expression\\n.\\nThe complex envelope of the output noise equals the second expression\\n.\\nFigure 9.21 Block diagram of maximal-ratio combiner using Nr receive antennas.\\ny t akxk t k 1 = Nr  = ak k jk  s exp t w k t +   k 1 = Nr  = s t akk jk   exp akw k t k 1 = Nr  + k 1 = Nr  = s t akk jk   exp k 1 = Nr  akw k t k 1 = Nr  Receiver 1 1 2 Nr Linear combiner Output Receiver 2 Receiver Nr x1(t)  xN(t) x2(t)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 553,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nAssuming that the are mutually independent for k = 1, 2, , Nr, the output SNR of\\nthe linear combiner is therefore given by\\n(9.71)\\nwhere EN0 is the symbol energy-to-noise spectral density ratio.\\nLet c denote the instantaneous output SNR of the linear combiner. Then, using the two\\nterms and as the instantaneous values of the expectations in the numerator and denominator of\\n(9.71), respectively, we may write\\n(9.72)\\nThe requirement is to maximize c with respect to the ak. This maximization may be\\ncarried out by following the standard differentiation procedure, recognizing that the\\nweighting parameters ak are complex. However, we choose to follow a simpler procedure\\nbased on the Schwarz inequality, which was discussed in Chapter 7.\\nw k t SNR  c \\x02 s t akk jk   exp k 1 = Nr  2 \\x02 akw k t k 1 = Nr  2 ----------------------------------------------------------------------\\n= \\x02 s t2   \\x02 w k t2   ---------------------------\\n\\x02 akk jk   exp k 1 = Nr  2 \\x02 ak 2 k 1 = Nr  -----------------------------------------------------------\\n= E N0 ------ \\x02 akk jk   exp k 1 = Nr  2 \\x02 ak 2 k 1 = Nr  -----------------------------------------------------------\\n= akk jk   exp k 1 = Nr  2 ak 2 k 1 = Nr  c E N0 ------ akk jk   exp k 1 = Nr  2 ak 2 k 1 = Nr  ---------------------------------------------------\\n=',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 554,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '8 Space Diversity-on-Receive Systems Let ak and bk denote any two complex numbers for k = 1, 2, , Nr. According to the\\nSchwarz inequality for complex parameters, we have\\n(9.73)\\nwhich holds with equality for\\n, where c is some arbitrary complex constant and\\nthe asterisk denotes complex conjugation.\\nThus, applying the Schwarz inequality to the instantaneous output SNR of (9.72), with\\nak left intact and bk set equal to\\n, we obtain\\nCanceling common terms in the numerator and denominator, we readily obtain\\n(9.74)\\nEquation (9.74) proves that, in general, cannot exceed , where is as defined in (9.63). The equality in (9.74) holds for (9.75)\\nwhere c is some arbitrary complex constant.\\nEquation (9.75) defines the complex weighting parameters of the maximal-ratio\\ncombiner. Based on this equation, we may state that the optimal weighting factor ak for the\\nkth diversity branch has a magnitude proportional to the signal amplitude k and a phase\\nthat cancels the signal phase k to within some value that is identical for all the Nr diversity\\nbranches. The phase alignment just described has an important implication: it permits the\\nfully coherent addition of the Nr receiver outputs by the linear combiner.\\nEquation (9.74) with the equality sign defines the instantaneous output SNR of the\\nmaximal-ratio combiner, which is written as\\n(9.76)\\nAccording to (9.62), (EN0) is the instantaneous output SNR of the kth diversity\\nbranch. Hence, the maximal-ratio combiner produces an instantaneous output SNR that is\\nthe sum of the instantaneous SNRs of the individual branches; that is,\\n(9.77) akbk k 1 = Nr  2 ak 2 bk 2 k 1 = Nr  k 1 = Nr   ak cbk* = k jk   exp c E N0 ------ ak 2 k jk   exp 2 k 1 = Nr  k 1 = Nr  ak 2 k 1 = Nr  --------------------------------------------------------------------\\n c E N0 ------ k 2 k 1 = Nr   c k k  k ak c k jk   exp   = ck jk -   exp  = k 1 2 Nr   = mrc E N0 ------ k 2 k 1 = Nr  = k 2 mrc k k 1 = Nr  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 555,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nThe term maximal-ratio combiner has been coined to describe the combiner of Figure\\n21 that produces the optimum result given in (9.77). Indeed, we deduce from this result\\nthat the instantaneous output SNR of the maximal-ratio combiner can be large even when\\nthe SNRs of the individual branches are small. Since the instantaneous SNR produced by\\nthe selection combiner is simply the largest among the Nr terms of (9.77), it follows that:\\nThe selection combiner is clearly inferior in performance to the maximal-ratio\\ncombiner.\\nThe maximal SNR mrc is the sample value of a random variable denoted by\\n. According to\\n(9.76), is equal to the sum of Nr exponentially distributed random variables for a\\nfrequency-flat, slowly fading Rayleigh channel. From Appendix A, the probability density\\nfunction of such a sum is known to be chi-square with 2Nr degrees of freedom; that is,\\n(9.78)\\nNote that for Nr = 1, (9.69) and (9.78) assume the same value, which is to be expected.\\nFigure 9.22 plots the scaled probability density function,\\n, versus\\nthe normalized variable for varying Nr. Based on this figure, we may make\\nFigure 9.22 Normalized probability density function for a\\nvarying number of Nr receive antennas.\\n mrc fmrc   1 Nr 1 -  ! --------------------- mrc\\nNr 1 - av Nr ------------ mrc av ---------- -     exp = fX x  av fmrc   = x mrc av  = 2 3 4 6 10 0 10 9 8 7 6 5 4 3 2 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 0 x Nr = 1 fX(x) fX x  1 Nr 1 -   -------------------x Nr 1 - exp x -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 556,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '8 Space Diversity-on-Receive Systems observations similar to those for the selection combiner, except for the fact that for any Nr\\nwe find that the scaled probability density function for the maximal-ratio combiner is\\nradically different from its counterpart for the selection combiner.\\nEXAMPLE\\nOutage Probability for Maximal-Ratio Combiner\\nThe cumulative distribution function for the maximal-ratio combiner is defined by\\n(9.79)\\nwhere the probability density function is itself defined by (9.78). Using (9.79),\\nFigure 9.23 plots the outage probability for the maximal-ratio combiner with Nr as a running\\nparameter. Comparing this figure with that of Figure 9.20 for selection combining, we see\\nthat the outage-probability curves for these two diversity techniques are superficially similar.\\nThe diversity gain, defined as the EN0 saving at a given BER, provides a measure of the\\neffectiveness of a diversity technique on an outage-probability basis.\\nFigure 9.23 Outage probability of maximal-ratio combiner for a varying number Nr of\\nreceiver antennas. \\x03 mrc x    fmrc  mrc d 0 x  = 1 fmrc  mrc d x   - = fmrc   -15 -10 -5 2 3 4 Nr = 1 6 10 10 20 30 40 50 60 70 80 90 100 0 0 5 10 15 Normalized SNR relative to single-channel median ( Nr = 1) (dB)\\nOutage probability, percent',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 557,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nEqual-Gain Combining\\nIn a theoretical context, the maximal-ratio combiner is the optimum among linear diversity\\ncombining techniques, optimum in the sense that it produces the largest possible value of\\ninstantaneous output SNR. However, in practical terms, there are three important issues to\\nkeep in mind:9\\nSignificant instrumentation is needed to adjust the complex weighting parameters of\\nthe maximal-ratio combiner to their exact values, in accordance with (9.75).\\nThe additional improvement in output SNR gained by the maximal-ratio combiner\\nover the selection combiner is not that large, and it is quite likely that the additional\\nimprovement in receiver performance is lost in not being able to achieve the exact\\nsetting of the maximal-ratio combiner.\\nSo long as a linear combiner uses the diversity branch with the strongest signal, then\\nother details of the combiner may result in a minor improvement in overall receiver\\nperformance.\\nIssue 3 points to formulation of the so-called equal-gain combiner, in which all the\\ncomplex weighting parameters ak have their phase angles set opposite to those of their\\nrespective multipath branches in accordance with (9.75). But, unlike the ak in the\\nmaximal-ratio combiner, their magnitudes are set equal to some constant value, unity for\\nconvenience of use. Space Diversity-on-Transmit Systems\\nIn the wireless communications literature, space diversity-on-receive techniques are\\ncommonly referred to as orthogonal space-time block codes (Tarokh et al., 1999). This\\nterminology is justified on the following grounds:\\nThe transmitted symbols form an orthogonal set.\\nThe transmission of incoming data streams is carried out on a block-by-block basis.\\nSpace and time constitute the coordinates of each transmitted block of symbols.\\nIn a generic sense, Figure 9.24 presents the baseband diagram of a space-time block\\nencoder, which consists of two functional units: mapper and block encoder. The mapper',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 558,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'In a generic sense, Figure 9.24 presents the baseband diagram of a space-time block\\nencoder, which consists of two functional units: mapper and block encoder. The mapper\\ntakes the incoming binary data stream {bk}, where bk = 1, and generates a new sequence\\nof blocks with each block made up of multiple symbols that are complex. For example, the\\nmapper may be in the form of an M-ary PSK or M-ary QAM message constellation, which\\nare illustrated for M = 16 in the signal-space diagrams of Figure 9.25. All the symbols in a\\nparticular column of the transmission matrix are pulse-shaped (in accordance with the\\nFigure 9.24 Block diagram of orthogonal space-time block encoder.\\n{bk} {Sk} Constellation mapper Block encoder To transmit antennas',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 558,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '9 Space Diversity-on-Transmit Systems criteria described in Chapter 8) and then modulated into a form suitable for simultaneous\\ntransmission over the channel by the transmit antennas. The pulse shaper and modulator\\nare not shown in Figure 9.24 as the basic issue of interest is that of baseband data\\ntransmission with emphasis on the formulation of space-time block codes. The block\\nencoder converts each block of complex symbols produced by the mapper into an l-by-Nt\\ntransmission matrix S, where l and Nt are respectively the temporal dimension and spatial\\ndimension of the transmission matrix. The individual elements of the transmission\\nmatrix S are made up of linear combinations of and , where the are complex symbols and the are their complex conjugates.\\nEXAMPLE\\nQuadriphase Shift Keying\\nAs a simple example, consider the map portrayed by the QPSK, M = 4. This map is\\ndescribed in Table 9.2, where E is the transmitted signal energy per symbol.\\nThe input dibits (pairs of binary bits) are Gray encoded, wherein only one bit is flipped\\nas we move from one symbol to the next. (Gray encoding was discussed in Section 7.6\\nunder Quadriphase Shift Keying.) The mapped signal points lie on a circle of radius\\ncentered at the origin of the signal-space diagram.\\nFigure 9.25 (a) Signal constellation of 16-PSK. (b) Signal constellation of 16-QAM.\\n5 0 (a) (b) 2  1  2  1  0 sk sk * sk sk * Table 9.2 Gray-encoded QPSK mapper\\nDibit: i = 1, 2, 3,\\nCoordinates of mapped signal points: si, i = 1, 2, 3,\\n10 11 01 00 E E 2  1 1 -    E j74    exp = E 2  1 - 1 -    E j54    exp = E 2  1 - +1    E j34    exp = E 2  +1 +1    E j4    exp =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 559,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nAlamouti Code\\nExample 6 is illustrative of the Alamouti code, which is one of the first space-time block\\ncodes involving the use of two transmit antennas and one signal receive antenna (Alamouti,\\n1998). Figure 9.26 shows a baseband block diagram of this highly popular spatial code.\\nLet and denote the complex symbols produced by the codes mapper, which are\\nto be transmitted over the multipath wireless channel by two transmit antennas. Signal\\ntransmission over the channel proceeds as follows:\\nAt some arbitrary time t, antenna 1 transmits and simultaneously antenna\\ntransmits\\n.\\nAt time t + T, where T is the symbol duration, signal transmission is switched to\\ntransmitted by antenna 1 and simultaneously is transmitted by antenna 2.\\nThe resulting two-by-two space-time block code is written in matrix form as follows:\\n(9.80) s1 s2 s1 s2 s - 2* s1* S s1 s2 s - 2* s1* = Time Space Figure 9.26 Block diagram of\\nthe transceiver (transmitter\\nand receiver) for the Alamouti\\ncode. Note that t > t to allow\\nfor propagation delay.\\nTransmit at time t\\nTime t : noise = w1\\nTime t + T : noise = w2\\ns1\\nTransmit at time t + T\\nTransmit antenna 2 Transmit antenna 1 Recieve antenna Channel estimator Linear combiner Maximum-likelihood decoder Multiplicative path coefficients   s2 -s2  * h2 = r2ej 2 h2 h2 y2 y1 h1 s1 h1 s1* } s2  h1 = r1ej 1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 560,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '9 Space Diversity-on-Transmit Systems This transmission matrix is a complex-orthogonal matrix (quaternion) in that it satisfies\\nthe condition for orthogonality in both the spatial and temporal senses. To demonstrate\\nthis important property of the Alamouti, let (9.81)\\ndenote the Hermitian transpose of S, which involves both transposition and complex\\nconjugation. To demonstrate orthogonality in the spatial sense, we multiply the code\\nmatrix S by its Hermitian transpose on the right, obtaining\\n(9.82)\\nSince the right-hand side of (9.81) is real valued, it follows that the alternative matrix\\nproduct\\n, viewed in the temporal sense, yields exactly the same result. That is,\\n(9.83)\\nwhere I is the two-by-two identity matrix.\\nIn light of (9.80) and (9.83), we may now summarize three important properties of the\\nAlamouti code: PROPERTY 1 Unitarity (Complex Orthogonality)\\nThe Alamouti code is an orthogonal space-time block code, in that its transmission matrix\\nis a unitary matrix with the sum term being merely a scaling factor.\\nAs a consequence of this property, the Alamouti code achieves full diversity.\\nPROPERTY\\nFull-Rate Complex Code\\nThe Alamouti code (with two transmit antennas) is the only complex space-time block\\ncode with a code rate of unity in existence.\\nHence, for any signal constellation, full diversity of the code is achieved at the full\\ntransmission rate. S s1* s - 2 s2* s1 = Space Time S SS s1 s2 s - 2* s1* s1* s2 - s2* s1 = s1 2 + s2 2 s1s2 - + s2s1 s - 2*s1* + s1*s2* s2 2 + s1 2 = s1 2 + s2 2   = 1 0 0 1 SS SS SS s1 2 s2 2 +  I = = s1 2 s2 2 +',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 561,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nPROPERTY\\nLinearity\\nThe Alamouti code is linear in the transmitted symbols.\\nWe may therefore expand the transmission matrix S of the code as a linear combination of\\nthe transmitted symbols and their complex conjugates, as shown by\\n(9.84)\\nwhere the four constituent matrices are themselves defined as follows:\\nIn words, the Alamouti code is the only two-dimensional space-time code, the\\ntransmission matrix of which can be decomposed into the form described in (9.84).\\nReceiver Considerations of the Alamouti Code\\nThe discussion presented thus far has focused on the Alamouti code viewed from the\\ntransmitters perspective. We turn next to the design of the receiver for decoding the code.\\nTo this end, we assume that the channel is frequency-flat and slowly time varying, such\\nthat the complex multiplicative distribution introduced by the channel at time t is essentially\\nthe same as that at time t + T, where T is the symbol duration. As before, the multiplicative\\ndistortion is denoted by where we now have k = 1, 2, as indicated in Figure 9.25.\\nThus, with the symbols and transmitted simultaneously at time t, the complex\\nreceived signal at some time\\n, allowing for propagation delay, is described by\\n(9.85)\\nwhere is the complex channel noise at time\\n. Next, with the symbols and\\ntransmitted simultaneously at time t + T, the corresponding complex signal received at\\ntime is (9.86) where is the second complex channel noise at time\\n. To be more precise, the\\nnoise terms and are circularly-symmetric complex-valued uncorrelated Gaussian\\nrandom variables of zero mean and equal variance.\\nIn the course of time from to\\n, the channel estimator in the receiver has\\nsufficient time to produce estimates of the multiplicative distortion represented by',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 562,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': \"In the course of time from to\\n, the channel estimator in the receiver has\\nsufficient time to produce estimates of the multiplicative distortion represented by\\nS s111 s1 *12 s221 s2 *22 + + + = 11 1 0 0 0 = 12 0 0 0 1 = 21 0 1 0 0 = 22 0 0 1 - 0 = kejk s1 s2 t t  x1 1e j1s1 2e j2s2 w 1 + + = w 1 t s2* - s1* t T + x2 1e j1 - s2* 2e j2s1* w 2 + + = w 2 t T + w 1 w 2 t' t' T + kejk\",\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 562,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '9 Space Diversity-on-Transmit Systems for k = 1, 2. Hereafter, we assume that these two estimates are accurate enough for them to\\nbe treated as essentially exact; in other words, the receiver has knowledge of both\\nand\\n. Accordingly, we may formulate the combination of two variables, in (9.85)\\nand the complex conjugate of in (9.86), in matrix form as follows:\\n(9.87)\\nThe nice thing about this equation is that the original complex signals s1 and s2 appear as\\nthe vector of two unknowns. It is with this goal in mind that and were used for the\\nelements of the two-by-one received signal vector , in the manner shown on the right-\\nhand side of (9.87).\\nAccording to (9.87), the channel matrix of the transmit diversity in Figure 9.25 is\\ndefined by\\n(9.88)\\nIn a manner similar to the signal-transmission matrix\\n, we find that the channel matrix H\\nis also a unitary matrix, as shown by\\n(9.89)\\nwhere, as before, I is the identity matrix and the sum term is merely a scaling\\nfactor.\\nUsing the definition of (9.88) for the channel matrix, we may rewrite (9.87) in the\\ncompact matrix form (9.90) where (9.91) is the complex transmitted signal vector and\\n(9.92) 1ej1 2ej2 x1 x2 x x1 x2 * = 1 e j1 2e j2 2 e j - 2 1e j - 1 - s1 s2 w 1 w 2 + = x1 x2* x H h11 h12 h21 h22 = 1ej1 2ej2 2e j2 - 1e j1 - - = S HH 1 2 2 2 +  I = 1 2 2 2 + x Hs w + = s s1 s2 = w w 1 w 2 =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 563,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nis the additive complex channel noise vector. Note that the column vector in (9.91) is the\\nsame as the first row vector in the matrix of (9.80).\\nWe have now reached a point where we have to address the fundamental issue in\\ndesigning the receiver:\\nHow do we decode the Alamouti code, given the received signal vector ?\\nTo this end, we introduce a new complex two-by-one vector\\n, defined as the matrix\\nproduct of the received signal vector and the Hermition transpose of the channel matrix\\nH normalized with respect to the reciprocal sum term\\n; that is,\\n(9.93)\\nSubstituting (9.90) into (9.93) and then making use of the unitarity property of the channel\\nmatrix described in (9.89), we obtain the mathematical basis for decoding of the Alamouti\\ncode:\\n(9.94)\\nwhere is a modified form of the complex channel noise\\n, as shown by\\n(9.95)\\nSubstituting (9.88) and (9.92) into (9.95), the expanded form of the complex noise vector is defined as follows:\\n(9.96)\\nHence, we may go on to simply write\\n(9.97)\\nExamination of (9.97) leads us to make the following statement insofar as the receiver is\\nconcerned:\\nThe space-time channel is decoupled into a pair of scalar channels that are\\nstatistically independent of each other:\\nThe complex symbol at the output of the kth space-time channel is identical to\\nthe complex symbol transmitted by the kth antenna for k = 1, 2; the decoupling\\nshown clearly in (9.97) is attributed to complex orthogonality of the Alamouti code.\\nAssuming that the original channel noise is white Gaussian, then this\\nstatistical characterization is maintained in the modified noise appearing at\\nthe output of the kth space-time channel for k = 1, 2; this maintenance is\\nattributed to the processing performed in the receiver.\\ns S x y x 1 2 2 2 + y y1 y2 = 1 1 2 2 2 + -------------------    Hx = y s v + = v w v 1 1 2 2 2 + -------------------    Hw = v v1 v2 1 1 2 2 2 + ------------------- 1e j1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 564,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 's S x y x 1 2 2 2 + y y1 y2 = 1 1 2 2 2 + -------------------    Hx = y s v + = v w v 1 1 2 2 2 + -------------------    Hw = v v1 v2 1 1 2 2 2 + ------------------- 1e j1\\n- w 1 + 2ej2w 2* 2e j2 - w 1 - 1ej1w 2* = yk sk vk k  + 1 2  = = sk w k vk',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 564,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '9 Space Diversity-on-Transmit Systems This twofold statement hinges on the premise that the receiver has knowledge of the\\nchannel matrix H.\\nMoreover, with two transmit antennas and one receive antenna, the Alamouti code\\nachieves the same level of diversity as a corresponding system with one transmit antenna\\nand two receive antennas. It is in this sense that a wireless communication system based\\non the Alamouti code is said to enjoy a two-level diversity gain.\\nMaximum Likelihood Decoding\\nFigure 9.27 illustrates the signal-space diagram of an Alamouti-encoded system based on\\nthe QPSK constellation. The complex Gaussian noise clouds centered on the four signal\\npoints and with decreasing intensity illustrate the effects of complex noise term on the\\nlinear combiner output .\\nIn effect, the picture portrayed in Figure 9.27 is the graphical representation of (9.94) over\\ntwo successive symbol transmissions at times t and t + T, repeated a large number of times.\\nSuppose that the two signal constellations in the top half of the signal-space diagram in\\nFigure 9.27 represent the pair of symbols transmitted at time t, for which we write\\nFigure 9.27 Signal-space diagram for Alamouti code, using the\\nQPSK signal constellation. The signal points and and the\\ncorresponding linear normalized combiner outputs and are\\ndisplayed in the top half of the figure.\\nv y st s1 s2 = Imaginary Real 0 Noise v2 Noise v1 Observation y2 Observation y1  s2  s1  s1 s2 y1 y2',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 565,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nThen, the remaining two signal constellations positioned in the right half of Figure 9.27\\nrepresent the other pair of symbols transmitted at t + T, for which we write\\nOn this basis, we may now invoke the maximum likelihood decoding rule, discussed in\\nChapter 7, to make the three-fold statement:\\nCompute the composite squared Euclidean distance metric\\nproduced by sending signal vectors and\\n, respectively.\\nDo this computation for all four possible signal pairs in the QPSK constellation.\\nHence, the ML decoder selects the pair of signals for which the metric is the\\nsmallest.\\nThe metrics component in part 1 of this statement is illustrated in Figure 9.27. Multiple-Input, Multiple-Output Systems: Basic\\nConsiderations\\nIn Sections 9.8 and 9.9, we studied space-diversity wireless communication systems\\nemploying either multiple receive or multiple transmit antennas to combat the multipath\\nfading problem. In effect, fading was treated as a source that degrades performance, neces-\\nsitating the use of space diversity on receive or transmit to mitigate it. In this section, we\\ndiscuss MIMO wireless communication, which distinguishes itself in the following ways:10\\nThe fading phenomenon is viewed not as a nuisance but rather as an environmental\\nsource of enrichment to be exploited.\\nSpace diversity at both the transmit and receive ends of the wireless communication\\nlink may provide the basis for a significant increase in channel capacity.\\nUnlike conventional techniques, the increase in channel capacity is achieved by\\nincreasing computational complexity while maintaining the primary communication\\nresources (i.e., total transmit power and channel bandwidth) fixed.\\nCoantenna Interference\\nFigure 9.28 shows the block diagram of a MIMO wireless link. The signals transmitted by\\nthe Nt transmit antennas over the wireless channel are all chosen to lie inside a common',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 566,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Coantenna Interference\\nFigure 9.28 shows the block diagram of a MIMO wireless link. The signals transmitted by\\nthe Nt transmit antennas over the wireless channel are all chosen to lie inside a common\\nfrequency band. Naturally, the transmitted signals are scattered differently by the channel.\\nMoreover, owing to multiple signal transmissions, the system experiences a spatial form\\nof signal-dependent interference, called coantenna interference (CAI).\\nFigure 9.29 illustrates the effect of CAI for one, two, and eight simultaneous\\ntransmissions and a single receive antenna (i.e., Nt = 1, 2, 8 and Nr = 1) using binary PSK;\\nthe transmitted binary PSK signals used in the simulation resulting in this figure were\\nst T + s2* - +s1* = yt st - 2 yt T + st T + - 2 + st st T + yt st -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 566,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '10 Multiple-Input, Multiple-Output Systems: Basic Considerations different but they all had the same average power and occupied the same bandwidth.\\n(Sellathurai and Haykin, 2008). Figure 9.29 clearly shows the difficulty that arises due to\\nCAI when the number of transmit antennas Nt is large. In particular, with eight\\nsimultaneous signal transmissions, the eye pattern of the received signal is practically\\nclosed. The challenge for the receiver is how to mitigate the CAI problem and thereby\\nmake it possible to provide increased spectral efficiency.\\nIn a theoretical context, the spectral efficiency of a communication system is intimately\\nlinked to the channel capacity of the system. To proceed with evaluation of the channel\\ncapacity of MIMO wireless communication, we begin by formulating a baseband channel\\nmodel for the system as described next.\\nBasic Baseband Channel Model\\nConsider a MIMO narrowband wireless communication system built around a flat-fading\\nchannel, with Nt transmit antennas and Nr receive antennas. The antenna configuration is\\nhereafter referred to as the pair (Nt, Nr). For a statistical analysis of the MIMO system in\\nwhat follows, we use baseband representations of the transmitted and received signals as\\nwell as the channel. In particular, we introduce the following notation:\\n\\nThe spatial parameter\\n(9.98)\\ndefines new degrees of freedom introduced into the wireless communication system\\nby using a MIMO channel with Nt transmit antennas and Nr receive antennas.\\n The Nt-by-1 vector (9.99) Figure 9.28 Block diagram of MIMO wireless link with Nt transmit antennas and Nr\\nreceive antennas. 1 Transmit antennas Receive antennas Noise Noise Noise 2 1 Nt Nr 2    N min Nt Nr    = s n  s1 n s2 n sNt n      T =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 567,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nFigure 9.29 Effect of coantenna interference on the eye\\ndiagram for one receive antenna and different numbers\\nof transmit antennas. (a) Nt = 1, (b) Nt = 2, (c) Nt = 8.\\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 -5 -4 -3 -2 -1 0 1 2 Normalized time (a) (b) Amplitude 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 -1 -0.5 0 0.5 1 Normalized time Amplitude 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 -1.5 -1 -0.5 0 0.5 1 1.5 Normalized time (c) Amplitude',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 568,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '10 Multiple-Input, Multiple-Output Systems: Basic Considerations denotes the complex signal vector transmitted by the Nt antennas at discrete time n.\\nThe symbols constituting the vector are assumed to have zero mean and\\ncommon variance\\n. The total transmit power is fixed at the value (9.100)\\nFor P to be maintained constant, the variance (i.e., power radiated by each\\ntransmit antenna) must be inversely proportional to Nt.\\n\\nFor a flat-fading Rayleigh distributing channel, we may use to denote the\\nsampled complex gain of the channel coupling transmit antenna k to receive antenna\\ni at discrete time n, where i = 1, 2, , Nr and k = 1, 2, , Nt. We may thus express\\nthe Nr-by-Nt complex channel matrix as\\n(9.101)\\n\\nThe system of equations (9.102)\\ndefines the complex signal received at the ith antenna due to the transmitted symbol radiated by the kth antenna. The term denotes the additive complex\\nchannel noise perturbing\\n. Let the Nr-by-1 vector\\n(9.103)\\ndenote the complex received signal vector and the Nr-by-1 vector\\n(9.104)\\ndenote the complex channel noise vector. We may then rewrite the system of\\nequations (9.102) in the compact matrix form\\n(9.105)\\nEquation (9.105) describes the basic complex channel model for MIMO wireless\\ncommunications, assuming the use of a flat-fading channel. The equation describes the\\ninput-output behavior of the channel at discrete time n. To simplify the exposition,\\nhereafter we suppress the dependence on time n by simply writing\\n(9.106) s n  s 2 P Nts 2 = s 2 hik n  H n  h 11 n  h 12 n h1Nt n  h 21 n  h 22 n h2Nt n     hNr1 n  h Nr2 hNrNt n  =          Nr receive antennas Nt transmit antennas xi n  hik n sk n  w i n  i 1 2 Nr   = k 1 2 Nt   =    + k 1 = Nt  = sk n  w i n  xi n  x n  x1 n x2 n xNr n       = w n  w 1 n w 2 n w Nr n      T = x n  H n s n  w n  + = x Hs w + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 569,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nwhere it is understood that all four vector/matrix terms of the equation, s, H, w, and x, are\\nin actual fact dependent on the discrete time n. Figure 9.30 shows the basic channel model\\nof (9.106).\\nFor mathematical tractability, we assume a Gaussian model made up of three elements:\\nNt symbols, which constitute the transmitted signal vector drawn from a white\\ncomplex Gaussian codebook; that is, the symbols are iid complex\\nGaussian random variables with zero mean and common variance\\n. Hence, the\\ncorrelation matrix of the transmitted signal vector s is defined by\\n(9.107)\\nwhere is the Nt-by-Nt identity matrix.\\nNt  Nr elements of the channel matrix H, which are also drawn from an ensemble of\\niid complex random variables with zero mean and unit variance, as shown by the\\ncomplex distribution\\n(9.108)\\nwhere (...) denotes a real Gaussian distribution. On this basis, we find that the\\namplitude component hik is Rayleigh distributed. It is in this sense that we\\nsometimes speak of the MIMO channel as a rich Rayleigh scattering environment.\\nBy the same token, we also find that the squared amplitude component, namely\\n|hik|2, is a chi-squared random variable with the mean\\n(9.109)\\n(The chi-squared distribution is discussed in Appendix A.)\\nNr elements of the channel noise vector w, which are iid complex Gaussian random\\nvariables with zero mean and common variance\\n; that is, the correlation matrix\\nof the noise vector w is given by\\n(9.110)\\nwhere is the Nr-by-Nr identity matrix.\\nFigure 9.30 Depiction of the basic channel model of (9.106).\\ns s1 s2 sNt    s 2 Rs \\x02 ss   = s 2INt = INt hik: 0 1 2     j0 1 2     i 1 2 Nr   = k 1 2 Nt   =    + \\x02 hik 2   1 for all i and k = w 2 Rw \\x02 ww   = w 2 INr = INr Transmitted signal vector s Received signal vector x Channel noise vector w Flat-fading channel H',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 570,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '11 MIMO Capacity for Channel Known at the Receiver In light of (9.100) and the assumption that hik is a standard Gaussian random variable with\\nzero mean and unit variance, the average SNR at each receiver input of the MIMO channel\\nis given by\\n(9.111)\\nwhich is, for a prescribed noise variance\\n, fixed once the total transmit power P is fixed.\\nNote also that, first, all the Nt transmitted signals occupy a common channel bandwidth\\nand, second, the average SNR  is independent of Nr.\\nThe idealized Gaussian model just described of a MIMO wireless communication\\nsystem is applicable to indoor local area networks and other wireless environments, where\\nthe extent of user-terminal mobilities is limited.11 MIMO Capacity for Channel Known at the Receiver\\nWith the basic complex channel model of Figure 9.30 at our disposal, we are now ready to\\nfocus attention on the primary issue of interest: the channel capacity of a MIMO wireless\\nlink. In what follows, two special cases will be considered: the first case, entitled ergodic\\ncapacity, assumes that the MIMO channel is weakly (wide-sense) stationary and, therefore,\\nergodic. The second case, entitled outage capacity, considers a nonergodic MIMO channel\\nunder the assumption of quasi-stationarity from one burst of data transmission to the next.\\nErgodic Capacity\\nAccording to Shannons information capacity law discussed in Chapter 5, the capacity of a\\nreal AWGN channel, subject to the constraint of a fixed transmit power P, is defined by\\n(9.112)\\nwhere B is the channel bandwidth and is the noise variance measured over the\\nbandwidth B. Given a time-invariant channel, (9.112) defines the maximum data rate that\\ncan be transmitted over the channel with an arbitrarily small probability of error being\\nincurred as a result of the transmission. With the channel used K times for the transmission\\nof K symbols in T seconds, the transmission capacity per unit time is KT times the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 571,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'incurred as a result of the transmission. With the channel used K times for the transmission\\nof K symbols in T seconds, the transmission capacity per unit time is KT times the\\nformula for C given in (9.112). Recognizing that K = 2BT in accordance with the sampling\\ntheorem discussed in Chapter 6, we may express the information capacity of the AWGN\\nchannel in the equivalent form\\n(9.113)\\nNote that one bit per second per hertz corresponds to one bit per transmission.\\n P w 2 ------- = Nts 2 w 2 ------------ = w 2 C B 1 P w 2 ------- +       2 bits/s log = w 2 C 1 2--- 1 P w 2 ------- +       2 bits/(s Hz) log =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 571,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nWith wireless communications as the medium of interest, consider next the case of a\\ncomplex flat-fading channel with the receiver having perfect knowledge of the channel\\nstate. The capacity of such a channel is given by\\n(9.114)\\nwhere the expectation is taken over the gain of the channel and the channel is assumed\\nto be stationary and ergodic. In recognition of this assumption, C is commonly referred to\\nas the ergodic capacity of the flat-fading channel and the channel coding is applied across\\nfading intervals (i.e., over an ergodic interval of channel variation with time).\\nIt is important to note that the scaling factor of 12 is missing from the capacity\\nformula of (9.114). The reason for this omission is that this equation refers to a complex\\nbaseband channel, whereas (9.113) refers to a real channel. The fading channel covered by\\n(9.114) operates on a complex signal, namely a signal with in-phase and quadrature\\ncomponents. Therefore, such a complex channel is equivalent to two real channels with\\nequal capacities and operating in parallel; hence the result presented in (9.114).\\nEquation (9.114) applies to the simple case of a single-input, single-output (SISO)\\nflat-fading channel. Generalizing this formula to the case of a multiple-input, multiple-\\noutput MIMO flat-fading channel governed by the Gaussian model described in Figure\\n30, we find that the ergodic capacity of the MIMO channel is given by the following\\nformula:12\\n(9.115)\\nwhich is subject to the constraint where P is the constant transmit power and denotes the trace of the enclosed\\nmatrix. The expectation in (9.115) is over the random channel matrix H, and the\\nsuperscript dagger notes Hermitian transposition; Rs and Rw are respectively the\\ncorrelation matrices of the transmitted signal vector s and channel noise vector w. A\\ndetailed derivation of (9.115) is presented in Appendix E.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 572,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'correlation matrices of the transmitted signal vector s and channel noise vector w. A\\ndetailed derivation of (9.115) is presented in Appendix E.\\nIn general, it is difficult to evaluate (9.115) except for a Gaussian model. In particular,\\nsubstituting (9.107) and (9.110) into (9.115) and simplifying yields\\n(9.116)\\nNext, invoking the definition of the average SNR  introduced in (9.111), we may rewrite\\n(9.116) in the equivalent form\\n(9.117) C \\x02 1 h 2P w 2 ------------ +       2 log bits/(s Hz) = h 2 C \\x02 det Rw HRsH +   det Rw   ----------------------------------------------\\n      2 log bits/(s Hz) = max Rs tr Rs   P  tr   C \\x02 det INr s 2 w 2 -------HH +                 2 log bits/(s Hz) = C \\x02 det INr  Nt -----HH +           2 log bits/(s Hz), for Nt Nr  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 572,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '11 MIMO Capacity for Channel Known at the Receiver Equation (9.117), defining the ergodic capacity of a MIMO flat-fading channel, involves\\nthe determinant of an Nr-by-Nr sum matrix (inside the braces) followed by the logarithm to\\nbase 2. It is for this reason that this equation is referred to as the log-det capacity formula\\nfor a Gaussian MIMO channel.\\nAs indicated in (9.117), the log-det capacity formula therein assumes that Nt  Nr for the\\nmatrix product to be of full rank. The alternative case, Nr  Nt makes the Nt-by-Nt\\nmatrix product to be of full rank, in which case the log-det capacity formula of the\\nMIMO link takes the form\\n(9.118)\\nwhere, as before, the expectation is taken over the channel matrix H.\\nDespite the apparent differences between (9.117) and (9.118), they are equivalent in\\nthat either one of them applies to all {Nr, Nt} antenna configurations. The two formulas\\ndifferentiate themselves only when the full-rank issue is of concern.\\nClearly, the capacity formula of (9.114), pertaining to a complex, flat-fading link with a\\nsingle antenna at both ends of the link, is a special case of the log-det capacity formula.\\nSpecifically, for Nt = Nr = 1 (i.e., no spatial diversity),\\n, and H = h (with\\ndependence on discrete-time n suppressed, (9.116) reduces to that of (9.114).\\nAnother insightful result that follows from the log-det capacity formula is that if\\nNt = Nr = N, then, as N approaches infinity, the capacity C defined in (9.117) grows\\nasymptotically (at least) linearly with N; that is, (9.119)\\nIn words, the asymptotic formula of (9.119) may be stated as follows:\\nThe ergodic capacity of a MIMO flat-fading wireless link with an equal number\\nof transmit and receive antennas N grows roughly proportionately with N.\\nWhat this statement teaches us is that, by increasing computational complexity resulting\\nfrom the use of multiple antennas at both the transmit and receive ends of a wireless link,',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 573,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'What this statement teaches us is that, by increasing computational complexity resulting\\nfrom the use of multiple antennas at both the transmit and receive ends of a wireless link,\\nwe are able to increase the spectral efficiency of the link in a far greater manner than is\\npossible by conventional means (e.g., increasing the transmit SNR). The potential for this\\nvery sizable increase in the spectral efficiency of a MIMO wireless communication system\\nis attributed to the key parameter\\nN = min{Nt, Nr}\\nwhich defines the number of degrees of freedom provided by the system.\\nTwo Other Special Cases of the Log-Det Formula: Capacities of\\nReceive and Transmit Diversity Links\\nNaturally, the log-det capacity formula for the channel capacity of an Nt, Nr wireless link\\nincludes the channel capacities of receive and transmit diversity links as special cases:\\nHH HH C \\x02 det INt  Nr -----HH +           2 log bits/(s Hz), Nr Nt  =  P w 2  = N   lim C N---- constant',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 573,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nDiversity-on-receive channel. The log-det capacity formula (9.118) applies to this\\ncase. Specifically, for Nt = 1, the channel matrix H reduces to a column vector and\\nwith it (9.118) reduces to\\n(9.120)\\nCompared with the channel capacity of (9.114), for an SISO fading channel with\\n, the squared channel gain |h|2 is replaced by the sum of squared\\nmagnitudes |hi|2, i = 1, 2, , Nr. Equation (9.120) expresses the ergodic capacity\\ndue to the linear combination of the receive-antenna outputs, which is designed to\\nmaximize the information contained in the Nr received signals about the transmitted\\nsignal. This is simply a restatement of the maximal-ratio combining principle\\ndiscussed in Section 9.8.\\nDiversity-on-transmit channel. The log-det capacity formula of (9.117) applies to\\nthis second case. Specifically, for Nr = 1, the channel matrix H reduces to a row\\nvector, and with it (9.117) reduces to\\n(9.121)\\nwhere the matrix product HH is replaced by the sum of squared magnitudes |hk|2,\\nk = 1, 2, , Nt. Compared with case 1 on receive diversity, the capacity of the\\ndiversity-on-transmit channel is reduced because the total transmit power is being\\nheld constant, independent of the number of Nt transmit antennas.\\nOutage Capacity\\nTo realize the log-det capacity formula of (9.117), the MIMO channel must be described\\nby an ergodic process. In practice, however, the MIMO wireless channel is often\\nnonergodic and the requirement is to operate the channel under delay constraints. The\\nissue of interest is then summed up as follows:\\nHow much information can be transmitted across a nonergodic channel,\\nparticularly if the channel code is long enough to see just one random\\nchannel matrix?\\nIn the situation described here, the rate of reliable information transmission (i.e., the strict\\nShannon-sense capacity) is zero, since for any positive rate there exists a nonzero\\nprobability that the channel would not support such a rate.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 574,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Shannon-sense capacity) is zero, since for any positive rate there exists a nonzero\\nprobability that the channel would not support such a rate.\\nTo get around this serious difficulty, the notion of outage is introduced into\\ncharacterization of the MIMO link. (Outage was discussed previously in the context of\\ndiversity on receive in Section 9.8.) Specifically, we offer the following definition:\\nThe outage probability of a MIMO link is defined as the probability for which\\nthe link is in a state of outage (i.e., failure) for data transmitted across the link at\\na certain rate R, measured in bits per second per hertz.\\nC \\x02 log2 1  hi 2 i 1 = Nr  +                   = bits/(s Hz)  P   w 2 = C \\x02 log2 1  Nt ----- hk 2 k 1 = Nt  +         bits/(s Hz) =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 574,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '11 MIMO Capacity for Channel Known at the Receiver To proceed on this probabilistic basis, it is customary to operate the MIMO link by\\ntransmitting data in the form of bursts or frames and invoke a quasi-stationary model\\ngoverned by four points:\\nThe burst is long enough to accommodate the transmission of a large number of\\nsymbols, which, in turn, permits the use of an idealized infinite-time horizon basic to\\ninformation theory.\\nYet, the burst is short enough to treat the wireless link as quasi-stationary during\\neach burst; the slow variation is used to justify the assumption that the receiver has\\nperfect knowledge of the channel state.\\nThe channel matrix is permitted to change, from burst k to the next burst k + 1,\\nthereby accounting for statistical variations of the link.\\nDifferent realizations of the transmitted signal vector s are drawn from a white\\nGaussian codebook; that is, the correlation matrix of s is defined by (9.107).\\nPoints 1 and 4 pertain to signal transmission, whereas points 2 and 3 pertain to the MIMO\\nchannel itself.\\nTo proceed with the evaluation of outage probability under this model, we first note\\nthat, in light of the log-det capacity formula (9.117), we may view the random variable\\n(9.122)\\nas the expression for a sample realization of the MIMO link. In other words, with the\\nrandom-channel matrix Hk varying from one burst to the next, Ck will itself vary in a\\ncorresponding way. A consequence of this random behavior is that, occasionally, a sample\\ndrawn from the cumulative distribution function of the MIMO link results in a value for Ck\\nthat is inadequate to support reliable communication over the link. In this kind of situation\\nthe link is said to be in an outage state. Correspondingly, for a given transmission strategy,\\nwe define the outage probability at rate R as\\n(9.123)\\nEquivalently, we may write\\n(9.124)\\nOn this basis, we may offer the following definition:\\nThe outage capacity of the MIMO link is the maximum bit rate that can be',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 575,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '(9.123)\\nEquivalently, we may write\\n(9.124)\\nOn this basis, we may offer the following definition:\\nThe outage capacity of the MIMO link is the maximum bit rate that can be\\nmaintained across the link for all bursts of data transmissions (i.e., all possible\\nchannel states) for a prescribed outage probability.\\nBy the very nature of it, the study of outage capacity can only be conducted using Monte\\nCarlo simulation.\\nChannel Known at the Transmitter\\nThe log-det capacity formula of (9.117) is based on the premise that the transmitter has no\\nknowledge of the channel state. Knowledge of the channel state, however, can be made\\nCk det INr  Nt -----HkHk  +           2 bits/(s Hz) for burst k\\nlog = Poutage R  \\x03 Ck Rk   for some burst k = Poutage R  \\x03 det INr  Nt -----HkHk  +           R for some burst k  2 log       =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 575,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\navailable to the transmitter by first estimating the channel matrix H at the receiver and\\nthen sending this estimate to the transmitter via a feedback channel. In such a scenario, the\\ncapacity is optimized over the correlation matrix of the transmitted signal vector s, subject\\nto the power constraint; that is, the trace of this correlation matrix is less than or equal to\\nthe constant transmit power P. Naturally, formulation of the log-det capacity formula of a\\nMIMO channel for which the channel is known in both the transmitter and receiver is\\nmore challenging than when it is only known to the receiver. For details of this\\nformulation, the reader is referred to Appendix E. Orthogonal Frequency Division Multiplexing\\nIn Chapter 8 we introduced the DMT method as one discrete form of multichannel\\nmodulation for signaling over band-limited channels. Orthogonal frequency division\\nmultiplexing (OFDM)13 is another clearly related form of multifrequency modulation.\\nOFDM is particularly well suited for high data-rate transmission over delay-dispersive\\nchannels. In its own way, OFDM solves the problem by following the engineering\\nparadigm of divide and conquer. Specifically, a large number of closely spaced\\northogonal subcarriers (tones) is used to support the transmission. Correspondingly, the\\nincoming data stream is divided into a number of low data-rate substreams, one for each\\ncarrier, with the subchannels so formed operating in parallel. For the modulation process,\\na modulation scheme such as QPSK is used.\\nWhat we have just briefly described here is essentially the same as the procedure used\\nin DMT modulation. In other words, the underlying mathematical theory of DMT\\ndescribed in Chapter 8 applies equally well to OFDM, except for the fact that the signal\\nconstellation encoder does not include the use of loading for bit allocation. In addition,\\ntwo other changes have to be made in the implementation of OFDM:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 576,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'constellation encoder does not include the use of loading for bit allocation. In addition,\\ntwo other changes have to be made in the implementation of OFDM:\\nIn the transmitter, an upconverter is included after the digital-to-analog converter to\\nappropriately translate the transmitted frequency, so as to facilitate propogation of\\nthe transmitted signal over the radio channel.\\nIn the receiver, a downconverter is included before the analog-to-digital converter to\\nundo the frequency translation that was performed by the upconverter in the\\ntransmitter.\\nFigure 9.31 shows the block diagram of an OFDM system, the components of which are\\nconfigured to accommodate the transmission of a binary data stream at 36 Mbit/s as an\\nillustrative example. Parts a and b of the figure depict the transmitter and receiver of the\\nsystem, respectively. Specifically, pertinent values of data carrier rates as well as sub-\\ncarrier frequencies at the various functional blocks are included in part a of the figure\\ndealing with the transmitter. One last comment is in order: the front end of the transmitter\\nand the back end of the receiver are allocated to forward error-correction encoding and\\ndecoding, respectively, for improved reliability of the system. (Error-control coding of the\\nforward error-correction variety is discussed in Chapter 10.)\\nThe Peak-to-Average Power Ratio Problem\\nA compelling practical importance of OFDM to wireless communications is attributed to\\nthe computational benefits brought about by the FFT algorithm that plays a key role in its',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 576,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '13 Spread Spectrum Signals implementation. However, OFDM suffers from the so-called PAPR problem. This\\nproblem arises due to the statistical probabilities of a large number of independent\\nsubchannels in the OFDM becoming superimposed on each other in some unknown\\nfashion, thereby resulting in high peaks. For a detailed account of the PAPR problem and\\nhow to mitigate it, the reader is referred to Appendix G. Spread Spectrum Signals\\nIn previous sections of this chapter we described different methods for mitigating the\\neffect of multipath interference in signaling over fading channels. In this section of the\\nchapter, we describe another novel way of thinking about wireless communications, which\\nis based on a class of signals called spread spectrum signals.14\\nA signal is said to belong to this class of signals if it satisfies the following two\\nrequirements:\\nSpreading. Given an information-bearing signal, spreading of the signal is\\naccomplished in the transmitter by means of an independent spreading signal, such\\nthat the resulting spread spectrum signal occupies a bandwidth much larger than the\\nbandwidth of the original information-bearing signal: the larger the better.\\nDespreading. Given a noisy version of the transmitted spread spectrum signal,\\ndespreading (i.e., recovering the original information-bearing signal) is achieved by\\ncorrelating the received signal with a synchronized replica of the spreading signal in\\nthe receiver.\\nFigure 9.31 Block diagram of the typical implementation of an OFDM, illustrating the transmission\\nof binary data at 36 Mbit/s.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 577,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the receiver.\\nFigure 9.31 Block diagram of the typical implementation of an OFDM, illustrating the transmission\\nof binary data at 36 Mbit/s.\\n Forward error- correction encoder Input binary data stream 16-QAM modulator 36 Mbit/s 48 Mbit/s 12 MHz Subcarriers 250 kHz 250 kHz Data stream 16 MHz Serial- to- parallel converter 64-point inverse FFT 1 2 47 48 Parallel- to- serial converter Digital- to- analog converter Transmitted signal (a) Analog- to-digital converter Received signal Serial-to- parallel converter 64-point FFT algorithm Parallel- to-serial converter 16-QAM de- modulator Forward error- correction decoder Estimate of original binary data stream (b)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 577,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nIn effect, the information-bearing signal is spread (increased) in bandwidth before its\\ntransmission over the channel, and the received signal at the channel output is despread\\n(i.e., decreased) in bandwidth by the same amount.\\nTo explain the rationale of spread spectrum signals, consider, first, a scenario where\\nthere are no interfering signals at the channel output whatsoever. In this idealized scenario,\\nan exact replica of the original information-bearing signal is reproduced at the receiver\\noutput; this recovery follows from the combined action of spreading and despreading, in\\nthat order. We may thus say that the receiver performance is transparent with respect to\\nthe combined spreading-despreading process.\\nConsider, next, a practical scenario where an additive narrowband interference is\\nintroduced at the receiver input. Since the interfering signal is introduced into the\\ncommunication system after transmission of the information-bearing signal, its bandwidth\\nis increased by the spreading signal in the receiver, with the result that its power spectral\\ndensity is correspondingly reduced. Typically, at its output end, the receiver includes a\\nfilter whose bandwidth-occupancy matches that of the information-bearing signal.\\nConsequently, the average power of the interfering signal is reduced, and the output SNR\\nof the receiver is increased; hence, there is practical benefit in improved SNR to be gained\\nfrom using the spread spectrum technique when there is an interfering signal (e.g., due to\\nmultipath) to deal with. Of course, this benefit is obtained at the expense of increased\\nchannel bandwidth.\\nClassification of Spread Spectrum Signals\\nDepending on how the use of spread spectrum signals is carried out, we may classify them\\nas follows:\\nDirect Sequence-Spread Spectrum\\nOne method of spreading the bandwidth of an information-bearing signal is to use\\nthe so-called direct sequence-spread spectrum (DS-SS), wherein a pseudo-noise',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 578,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Direct Sequence-Spread Spectrum\\nOne method of spreading the bandwidth of an information-bearing signal is to use\\nthe so-called direct sequence-spread spectrum (DS-SS), wherein a pseudo-noise\\n(PN) sequence is employed as the spreading sequence (signal). The PN sequence is\\na periodic binary sequence with noise-like properties, details of which are\\npresented in Appendix J. The baseband modulated signal, representative of the\\nDS-SS method, is obtained by multiplying the information-bearing signal by the\\nPN sequence, whereby each information bit is chopped into a number of small\\ntime increments, called chips. The second stage of modulation is aimed at\\nconversion of the baseband DS-SS signal into a form suitable for transmission over\\na wireless channel, which is accomplished by using M-ary PSK, discussed in\\nChapter 7. The family of spread spectrum systems so formed is referred to simply\\nas DS/MPSK systems, a distinct characteristic of which is that spreading of the\\ntransmission bandwidth takes place instantaneously. Moreover, the signal-\\nprocessing capability of these systems to combat the effect of interferers,\\ncommonly referred to as jammers be they friendly or unfriendly, is a function of\\nthe PN sequence length. Unfortunately, this capability is limited by physical\\nconsiderations of the PN-sequence generator.\\nFrequency Hop-Spread Spectrum\\nTo overcome the physical limitations of DS/MPSK systems, we may resort to\\nalternative methods. One such method is to force the jammer to occupy a wider',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 578,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '13 Spread Spectrum Signals spectrum by randomly hopping the input data-modulated carrier from one frequency\\nto the next. In effect, the spectrum of the transmitter signal is spread sequentially\\nrather than instantaneously; the term sequentially refers to the pseudo-randomly\\nordered sequence of frequency hops. This second type of spread spectrum in which\\nthe carrier hops randomly from one frequency to another is called frequency hop-\\nspread spectrum. A commonly used modulation format used herein is that of M-ary\\nFSK, which was also discussed in Chapter 7. The combination of the two\\nmodulation techniques, namely frequency hopping and M-ary FSK, is referred to\\nsimply as FH/MFSK. Since frequency-hopping does not cover over the entire spread\\nspectrum instantaneously, we are led to consider the rate at which the hops occur. In\\nthis context, we may go on to identify two basic kinds of frequency hopping, which\\nare the converse of each other, as summarized here:\\n\\nFirst, slow-frequency hopping, in which the symbol rate of the M-ary FSK signal,\\ndenoted by Rs, is an integer multiple of the hop rate, denoted by Rh; that is, several\\nsymbols of the input data sequence are transmitted for each frequency hop.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 579,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Second, fast-frequency hopping, in which the hop rate Rh is an integer multiple of\\nthe M-ary FSK symbol rate Rs; that is, the carrier frequency will change (i.e.,\\nhop) several times during the transmission of one input-data symbol.\\nThe spread spectrum technique of the FH variety is particularly attractive for\\nmilitary applications. But, compared with the alternative spread spectrum technique,\\nDS/MPSK, the commercial use of FH/MFSK is insignificant, which is especially so\\nin regard to fast frequency hopping. The limiting factor behind this statement is the\\nexpense involved in the employment of frequency synthesizers, which are basic to\\nthe implementation of FH/MFSK systems. Accordingly, the FH/MFSK will not be\\nconsidered further.\\nProcessing Gain of the DS/BPSK\\nBefore closing this section on spread spectrum signals, it is informative to expand on the\\nimprovement in SNR gained at the receiver output, mentioned earlier on. To this end,\\nconsider the simple case of the DS/BPSK, in which the binary PSK, representing the\\nsecond stage of modulation in the transmitter, is coherent; that is, the receiver is\\nsynchronized with the transmitter in all of its features. In Problem 9.34, it is shown that the\\nprocessing gain of a spread spectrum signal compared to its unspread version is\\n(9.125)\\nwhere Tb is the bit duration and Tc is the chip duration. With PG expressed in decibels, in\\nProblem 9.34 it is also shown that\\n10 log10 (SNR)O = 10 log10 (SNR)I + 10 log10 (PG) dB\\n(9.126)\\nwhere (SNR)I and (SNR)O are the input SNR and output SNR, respectively. Furthermore,\\nrecognizing that the ratio TbTc is equal to the number of chips contained in a single bit\\nduration, it follows that the processing gain realized by the use of DS/BPSK increases\\nwith increasing length of a single period of the PN sequence, which was emphasized\\npreviously. PG Tb Tc ----- =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 579,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels Code-Division Multiple Access\\nModern wireless networks are commonly of a multiuser type, in that the multiple\\ncommunication links within the network are shared among multiple users. Specifically,\\neach individual user is permitted to share the available radio resources (i.e., time and\\nfrequency) with other users in the network and do so in an independent manner.\\nStated in another way, a multiple access technique permits the radio resources to be\\nshared among multiple users seeking to communicate with each other. In the context of\\ntime and frequency domains, we recall from Chapter 1 that frequency-division multiple\\naccess (FDMA) and time-division multiple access (TDMA) techniques allocate the radio\\nresources of a wireless channel through the use of disjointedness (i.e., orthogonality) in\\nfrequency and time, respectively. On the other hand, the code-division multiple access\\n(CDMA) technique, building on spread spectrum signals and benefiting from their\\nattributes, provides an alternative to the traditional techniques of FDMA and TDMA; it\\ndoes so by not requiring the bandwidth allocation of FDMA nor the time synchronization\\nneeded in TDMA. Rather, CDMA operates on the following principle:\\nThe users of a common wireless channel are permitted access to the channel\\nthrough the assignment of a spreading code to each individual user under the\\numbrella of spread spectrum modulation.\\nThis statement is testimony to what we said in the first paragraph of Section 9.13, namely\\nthat spread spectrum signals provide a novel way of thinking about wireless\\ncommunications.\\nTo elaborate on the way in which CDMA distinguishes itself from FDMA and TDMA\\nin graphical terms, consider Figure 9.32. Parts a and b of the figure depict the ways in\\nwhich the radio resources are distributed in FDMA and TDMA, respectively. To be\\nspecific:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 580,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'In FDMA, the channel bandwidth B is divided equally among a total number of K\\nusers, with each user being allotted a subband of width B/K and having the whole\\ntime resource T at its disposal.\\n\\nIn TDMA, the time resource T is divided equally among the K users, with each user\\nhaving total access to the frequency resource, namely the total channel bandwidth B,\\nbut for only T/K in each time frame.\\nIn a way, we may therefore think of FDMA and TDMA as the dual of each other.\\nTurning next to Figure 9.32c, we see that CDMA operates in a manner entirely\\ndifferent from both FDMA and TDMA. Graphically, we see that each CDMA user has full\\naccess to the entire radio resources at every point in time from one frame to the next.\\nNevertheless, for the full utilization of radio resources to be achievable, it is necessary that\\nthe spreading codes assigned to all the K users form an orthogonal set.\\nIn other words, orthogonality is a common requirement to the FDMA, TDMA, and\\nCDMA, each in its own specific way. However, this requirement is easier to implement\\npractically in FDMA and TDMA than it is in CDMA.\\nIn an ideal CDMA system, to satisfy the orthogonality requirement, the cross-\\ncorrelation between any two users of the system must be zero. Correspondingly, for this',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 580,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '14 Code-Division Multiple Access ideal condition to be satisfied, we require that the cross-correlation function between the\\nspreading sequences (codes) assigned to any two CDMA users of the system must be zero\\nfor all cyclic shifts in time. Unfortunately, ordinary PN sequences do not satisfy the\\northogonality requirement because of their relatively poor cross-correlation properties.\\nAccordingly, we have to look to alternative spreading codes to satisfy the orthogonality\\nrequirements. Fortunately, such an endeavor is mathematically feasible, depending on\\nwhether synchrony of the CDMA receiver to its transmitter is required or not. In what\\nfollows, we describe the use of Walsh-Hadamard sequences for the synchronous case and\\nGold sequences for the asynchronous case.\\nWalsh-Hadamard Sequences\\nConsider the case of a CDMA system, for which synchronization among users of the\\nsystem is permissible. Under this condition, perfect orthogonality of two spreading\\nsignals, cj(t) and ck(t), respectively assigned to users j and k for different time offsets,\\nnamely (9.127) reduces to (9.128) where the asterisk denotes complex conjugation. It turns out that, for the special case\\ndescribed in (9.128), the orthogonality requirement can be satisfied exactly, and the\\nresulting sequences are known as the Walsh-Hadamard sequences (codes).15\\nFigure 9.32 Resource distribution in (a) FDMA, (b) TDMA, and (c) CDMA. This figure shows the\\nessence of multiple access as in Figure 1.2 with a difference: Figure 9.32 is quantitative in its\\ndescription of multiple-access techniques.\\n1 2 k K   Bt B = Bt/M Tt = T Tt = T Tt T = Tt/M 1 2  k  K Bt = B Bt = BKk 2 1 (a) (b) (c) Rjk  Cj tCk t  -   dt 0 for j k  =  -   = Rjk 0  Cj tCk t dt 0 for j k and  0 =  =  -   =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 581,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nTo construct a Walsh-Hadamard sequence, we begin with a 2  2 matrix, denoted by\\nH2, for which the inner product of its two rows (or two columns) is zero. For example, we\\nmay choose the matrix\\n(9.129)\\nthe two rows of which are indeed orthogonal to each other. To go on and construct a\\nWalsh-Hadamard sequence of length 4 using H2, we construct the Kronecker product of\\nH2 with itself, as shown by\\nH4 = H2  H2\\n(9.130)\\nTo explain what we mean by the Kronecker product in a generic sense, let A = {ajk} and\\nB = (bjk} denote m  m and n  n matrices, respectively.16 Then, we may introduce the\\nfollowing rule:\\nThe Kronecker product of the two matrices A and B is made up of an mn  mn\\nmatrix, which is obtained from the matrix A by replacing its element ajk in\\nmatrix A with the scaled matrix ajk B.\\nEXAMPLE\\nConstruction of Hadamard-Walsh H4 from H2\\nFor the example of (9.129) on matrix H2, applying the Kronecker product rule, we may\\nexpress the H4 of (9.130) as follows:\\n(9.131)\\nThe four rows (and columns) of H4 defined in (9.131) are indeed orthogonal to each other.\\nCarrying on in this manner, we may go on to construct the Hadamard-Walsh sequences\\nH6, H8, and so on.\\nIn practical terms, a synchronous CDMA system is achievable provided that a single\\ntransmitter (e.g., the base station of a cellular network) transmits individual data streams\\nsimultaneously, with each data stream being addressed to a specific CDMA user (e.g.,\\nmobile unit).\\nGold Sequences\\nWhereas Walsh-Hadamard sequences are well suited for synchronous CDMA, Gold\\nsequences, on the other hand, are well suited for applications in asynchronous CDMA;\\nH2 + 1 1 + + 1 1 - = H4 + 1 H2  1 + H2  + 1 H2  1 - H2  = + 1 1 + 1 + 1 + + 1 1 - 1 + 1 - + 1 1 + 1 - 1 - + 1 1 - 1 - 1 + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 582,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '14 Code-Division Multiple Access therein, time- and phase-shifts between individual user signals, measured with respect to\\nthe base station in a cellular network, occur in a random manner; hence the adoption of\\nasynchrony.\\nGold sequences constitute a special class of maximal-length sequences, the generation\\nof which is embodied in Golds theorem, stated as follows:17\\nLet g1(X) and g2(X) be a preferred pair of primitive polynomials of degree n\\nwhose corresponding linear feedback shift registers generate maximal-length\\nsequences of period 2n - 1 and whose cross-correlation function has a\\nmagnitude less then or equal to\\n(9.132) or (9.133) Then, the linear feedback shift register corresponding to the product polynomial\\ng1(X) g2(X) will generage 2n + 1 different sequences, with each sequence\\nhaving a period of 2n = 1 and the cross-correlation between any pair of such\\nsequences satisfying the preceding condition.\\nTo understand Golds theorem, we need to define what we mean by a primitive\\npolynomial. Consider a polynomial g(X) defined over a binary field (i.e., a finite set of two\\nelements, 0 and 1, which is governed by the rules of binary arithmetic). The polynomial\\ng(X) is said to be an irreducible polynomial if it cannot be factored using any polynomials\\nfrom the binary field. An irreducible polynomial g(X) of degree m is said to be a primitive\\npolynomial if the smallest integer m for which the polynomial g(X) divides the factor\\nXn + 1 is n = 2m - 1. The topic of primitive polynomials is discussed in Chapter 10 on\\nerror-control coding.\\nEXAMPLE\\nCorrelation Properties of Gold Codes\\nAs an illustrative example, consider Gold sequences with period 27 - 1 = 127. To generate such\\na sequence for n = 7 we need a preferred pair of PN sequences that satisfy (9.132) (n odd), as\\nshown by\\nThis requirement is satisfied by the Gold-sequence generator shown in Figure 9.33 that\\ninvolves the modulo-2 addition of these two sequences. According to Golds theorem,\\nthere are a total of',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 583,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'shown by\\nThis requirement is satisfied by the Gold-sequence generator shown in Figure 9.33 that\\ninvolves the modulo-2 addition of these two sequences. According to Golds theorem,\\nthere are a total of\\nsequences that satisfy (9.132). The cross-correlation between any pair of such sequences is\\nshown in Figure 9.34, which is indeed in full accord with Golds theorem. In particular,\\nthe magnitude of the cross-correlation is less than or equal to 17.\\n2 n 1 +  2  1 for n odd + 2 n 1 +  2  1 for n even and n 0 mod 4  + 2 n 1 +  2  1 24 = 1 17 = + + 2n 1 27 = 1 129 = + +',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 583,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels The RAKE Receiver and Multipath Diversity\\nA discussion of wireless communications using CDMA would be incomplete without a\\ndescription of the RAKE receiver.18 The RAKE receiver was originally developed in the\\n1950s as a diversity receiver designed expressly to equalize the effect of multipath. First,\\nand foremost, it is recognized that useful information about the transmitted signal is\\ncontained in the multipath component of the received signal. Thus, taking the viewpoint\\nFigure 9.33 Generator for a Gold sequence of period 27 - 1 = 127.\\nFigure 9.34 Cross-correlation function R12 of a pair of Gold sequences based on the two PN\\nsequences [7,4] and [7,6,5,4].\\n1 2 3 4 5 Gold sequence Clock 6 7 1 2 3 4 5 6 7 100 50 0 Cross-correlation function R12( )\\n-20 -15 -10 -5 0 5 10 15 20 Delay -50 -100',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 584,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '15 The RAKE Receiver and Multipath Diversity that multipath may be approximated as a linear combination of differently delayed echoes,\\nas shown in the maximal ratio combiner of Figure 9.21, the RAKE receiver seeks to\\ncombat the effect of multipath by using a correlation method to detect the echo signals\\nindividually and then adding them algebraically. In this way, intersymbol interference due\\nto multipath is dealt with by reinserting different delays into the detected echoes so that\\nthey perform a constructive rather than destructive role.\\nFigure 9.35 shows the basic idea behind the RAKE receiver. The receiver consists of a\\nnumber of correlators connected in parallel and operating in a synchronous fashion with\\neach other. Each correlator has two inputs: (1) a delayed version of the received signal and\\n(2) a replica of the PN sequence used as the spreading code to generate the spread\\nspectrum-modulated signal at the transmitter. In effect, the PN sequence acts as a\\nreference signal. Let the nominal bandwidth of the PN sequence be denoted as W = 1Tc,\\nwhere Tc is the chip duration. From the discussion on PN sequences presented in\\nAppendix J, we find that the autocorrelation function of a PN sequence has a single peak\\nof width 1W, and it disappears toward zero elsewhere inside one period of the PN\\nsequence (i.e., one symbol period). Thus, we need only make the bandwidth W of the PN\\nsequence sufficiently large to identify the significant echoes in the received signal. To be\\nsure that the correlator outputs all add constructively, two other operations are performed\\nin the receiver by the functional blocks labeled phase and gain adjustors:\\nAn appropriate delay is introduced into each correlator output, so that the phase\\nangles of the correlator outputs are in agreement with each other.\\nThe correlator outputs are weighted so that the correlators responding to strong\\npaths in the multipath environment have their contributions accentuated, while the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 585,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The correlator outputs are weighted so that the correlators responding to strong\\npaths in the multipath environment have their contributions accentuated, while the\\ncorrelators not synchronizing with any significant path are correspondingly\\nsuppressed.\\nFigure 9.35 Block diagram of the RAKE receiver for CDMA over multipath channels.\\n Estimate of original bit stream Threshold y(t) Received signal Reference PN sequence Phase and gain adjustors \\x02 \\x02 \\x02 \\x02 \\x02 \\x02 Correlator 1 Correlator 2 Correlator 3 Correlator M Delay Tc Delay Tc Delay Tc \\x02 \\x02 \\x02 Decision device dt 0 Tb  1   1, 2   2, 3   3, M  M,',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 585,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nThe weighting coefficients ak are computed in accordance with the maximal ratio\\ncombining principle, discussed in Section 9.8. Specifically, we recall that the SNR of a\\nweighted sum, where each element of the sum consists of a signal plus additive noise of\\nfixed power, is maximized when the amplitude weighting is performed in proportion to the\\npertinent signal strength. That is, the linear combiner output is\\n(9.134)\\nwhere zk(t) is the phase-compensated output of the kth correlator and M is the number of\\ncorrelators in the receiver. Provided that we use enough correlators in the receiver to span\\na region of delays sufficiently wide to encompass all the significant echoes that are likely\\nto occur in the multipath environment, the output y(t) behaves essentially as though there\\nwas a single propagation path between the transmitter and receiver rather than a series of\\nmultiple paths spread in time.\\nTo simplify the presentation, the receiver of Figure 9.35 assumes the use of binary PSK in\\nperforming spread spectrum modulation at the transmitter. Thus, the final operation per-\\nformed in Figure 9.35 is that of integrating the linear combiner output y(t) over the bit dura-\\ntion Tb and then determining whether binary symbol 1 or 0 was transmitted in that bit interval.\\nThe RAKE receiver derives its name from the fact that the bank of parallel correlators\\nhas an appearance similar to the fingers of a rake; see Figure 9.36. Because spread\\nspectrum modulation is basic to the operation of CDMA wireless communications, it is\\nnatural for the RAKE receiver to be central to the design of the receiver used in this type of\\nmultiuser radio communication. Summary and Discussion\\nIn this chapter we discussed the topic of signaling over fading channels, which is at the\\nheart of wireless communications. There are three major sources of signal degradation in\\nwireless communications:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 586,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'co-channel interference,\\n fading, and  delay spread. The latter two are by-products of the multipath phenomenon. A common characteristic of\\nthese channel impairments is that they are all signal-dependent phenomena. As it is with\\nintersymbol interference that characterizes signaling over band-limited channels discussed\\nin Chapter 8, the degrading effects of interference and multipath in wireless\\ncommunications cannot be combated by simply increasing the transmitted signal, which is\\nwhat is done when noise is the only source of channel impairment as discussed in Chapter 7.\\nTo combat the effects of multipath and interference, we require the use of specialized\\ntechniques that are tailor-made for wireless communications. These specialized techniques\\ninclude space diversity, which occupied much of the material presented in this chapter.\\nFigure 9.36\\nPicture of a rake, symbolizing the bank of correlators.\\ny t akzk t k 1 = M  = . . .',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 586,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '16 Summary and Discussion We discussed different forms of space diversity, the main idea behind which is that two\\nor more propagation paths connecting the receiver to the transmitter are better than a\\nsingle propagation path. In historical terms, the first form of space diversity used to\\nmitigate the multipath fading problem was that of receive diversity, involving a single\\ntransmit antenna and multiple receive antennas. Under receive diversity, we discussed the\\nselection combiner, maximal-ratio combiner, and equal-gain combiner:\\n\\nThe selection combiner is the simplest form of receive diversity. It operates on the\\nprinciple that it is possible to select, among Nr receive-diversity branches, a\\nparticular branch with the largest output SNR; the branch so selected defines the\\ndesired received signal.\\n\\nThe maximal-ratio combiner is more powerful than the selection combiner by virtue\\nof the fact that it exploits the full information content of all the Nr receive-diversity\\nbranches about the transmitted signal of interest; it is characterized by a set of Nr\\nreceive-complex weighting factors that are chosen to maximize the output SNR of\\nthe combiner.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 587,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The equal-gain combiner is a simplified version of the maximal-ratio combiner.\\nWe also discussed diversity-on-transmit techniques, which may be viewed as the dual of\\ntheir respective diversity-on-receive techniques. Much of the discussion here focused on\\nthe Alamouti code, which is simple to design, yet powerful in performance, in that it\\nrealizes a two-level diversity gain: in other terms of performance, the Alamouti code is\\nequivalent to a linear diversity-on-receive system with a single antenna and two receive\\nantennas.\\nBy far, the most powerful form of space diversity is the use of multiple antennas at both\\nthe transmit and receive ends of the wireless link. The resulting configuration is referred to\\nas a MIMO wireless communication system, which includes the receive diversity and\\ntransmit diversity as special cases. The novel feature of the MIMO system is that, in a rich\\nscattering environment, it can provide a high spectral efficiency, which may be simply\\nexplained as follows. The signals transmitted simultaneously by the transmit antennas\\narrive at the input of each receive antenna in an uncorrelated manner due to the rich\\nscattering mechanism of the channel. The net result is a spectacular increase in the spectral\\nefficiency of the wireless link. Most importantly, the spectral efficiency increases roughly\\nlinearly with the number of transmit or receive antennas, whichever is the smaller one of\\nthe two. This important result assumes that the receiver has knowledge of the channel\\nstate. The spectral efficiency of the MIMO system can be further enhanced by including a\\nfeedback channel from the transmitter to the receiver, whereby the channel state is also\\nmade available to the transmitter and with it the transmitter is enabled to exercise control\\nover the transmitted signal.\\nMultiple Access Considerations\\nAn issue of paramount practical importance in wireless communications is that of multiple',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 587,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'over the transmitted signal.\\nMultiple Access Considerations\\nAn issue of paramount practical importance in wireless communications is that of multiple\\naccess to the wireless channel, in the context of which the following two approaches are\\nconsidered to be the dominant ones:\\nOrthogonal frequency division multiple access (OFDMA), which is the multi-user\\nversion of OFDM that was discussed in Section 9.12. In OFDMA multiple access is\\naccomplished through the assignment of subchannels (subcarriers) to individual users.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 587,\n",
       "   'chunk_idx': 2}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nNaturally, OFDMA inherits the distinctive features of OFDM. In particular, OFDMA\\nis well suited for high data-rate transmissions over delay-dispersive channels, realized\\nby exploiting the principle of divide and conquer. Accordingly, OFDMA is\\ncomputationally efficient in using the FFT algorithm. Moreover, OFDMA lends itself\\nto the combined use of MIMO, hence the ability to improve spectral efficiency and\\ntake advantage of channel flexibility.\\nCode-division multiple access (CDMA), which distinguishes itself by exploiting the\\nunderlying principle of spread spectrum signals, discussed in Section 9.13. To be\\nspecific, through the combined process of spectrum spreading in the transmitter and\\ncorresponding spectrum despreading in the receiver, a certain amount of processing\\ngain is obtained, hence the ability of CDMA users to occupy the same channel\\nbandwidth. Moreover, CDMA provides a flexible procedure for the allocation of\\nresources (i.e., PN codes) among a multiplicity of active users. Last but by no means\\nleast, in using the RAKE, viewed as an adaptive TDL filter, CDMA is enabled to\\nmatch the receiver input to the channel output by adjusting tap delays as well as tap\\nweights, thereby enhancing receiver performance in the presence of multipath.\\nTo conclude, OFDMA and CDMA provide two different approaches for the multiple\\naccess of active users to wireless channels, each one of which builds on its own distinctive\\nfeatures.\\nProblems\\nEffect of Flat Fading on the BER of Digital Communications Receivers Derive the BER formulas listed in the right-hand side of Table 9.2 for the following signaling\\nschemes over flat fading channels:\\na. Binary PSK using coherent detection\\nb. Binary FSK using coherent detection\\nc. Binary DPSK\\nd. Binary FSK using noncoherent detection Using the formulas derived in Problem 9.1, plot the BER charts for the schemes described therein.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 588,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'b. Binary FSK using coherent detection\\nc. Binary DPSK\\nd. Binary FSK using noncoherent detection Using the formulas derived in Problem 9.1, plot the BER charts for the schemes described therein.\\nSelective Channels Consider a time-selective channel, for which the modulated received signal is defined by\\nwhere m(t) is the message signal, is the result of angle modulation; the amplitude and\\nphase are contributed by the nth path, where n = 1, 2, , N.\\na. Using complex notation, show that the received signal is described as follows:\\nwhere\\nWhat is the formula for\\n? x t n tm t 2fct t n t + +   cos n 1 = N  = t n t n t x t  ts t =  t  n t n 1 = N  = s t',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 588,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems b. Show that the delay-spread function of the multipath channel is described by\\nwhere is the Dirac delta function in the\\n-domain. Hence, justify the statement that the\\nchannel described in this problem is a time-selective channel.\\nc. Let and denote the Fourier transforms of and\\n, respectively. What then is\\nthe Fourier transform of\\n?\\nd. Using the result of part c, justify the statement that the multipath channel described herein can be\\napproximately frequency-flat. What is the condition that would satisfy this description? In this problem, we consider a multipath channel embodying large-scale effects. Specifically, using\\ncomplex notation, the received signal at the channel output is described by\\nwhere and denote the amplitude and time delay associated with the lth path in the channel for\\nl = 1, 2, , L. Note that is assumed to be constant for all l.\\na. Show that the delay-spread function of the channel is described by\\nwhere is the Dirac delta function expressed in the -domain.\\nb. This channel is said to be time-nonselective. Why?\\nc. The channel does exhibit a frequency-dependent behavior. To illustrate this behavior, consider\\nthe following delay-spread function:\\nwhere is the time delay produced by the second path in the channel. Plot the magnitude\\n(amplitude) response of the channel for the following specifications:\\ni. ii. iii. where . Comment on your results. Expanding on the multipath channel considered in Problem 9.4, a more interesting case is\\ncharacterized by the scenario in which the received signal at the channel output is described as follows:\\nwhere the amplitude and time delay for the lth path are both time dependent for\\nl = 1, 2, , L.\\na. Show that the delay-spread function of the multipath channel described herein is given by\\nwhere is the Dirac delta function in the\\n-domain. This channel is said to exhibit both\\nlarge- and small-scale effects. Why?\\nb. The channel is also said to be both time selective and frequency selective. Why?',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 589,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '-domain. This channel is said to exhibit both\\nlarge- and small-scale effects. Why?\\nb. The channel is also said to be both time selective and frequency selective. Why?\\nh t;    t =   S f Ss f  t s t x t x t  ls t l -   l 1 = L  =  l l  l h t;    l l -   l 1 = L  =   h t;      2 2 -   + = 2  2 0.5 =  2 j 2  =  2 j - = j 1 - = x t  l ts t l t -   l 1 = L  =  l t l t h t;    l t l t -   l 1 = L  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 589,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nc. To illustrate the point made under b, consider the following channel description:\\nwhere and are both Rayleigh processes.\\nFor selected , and , do the following: i. At each time t = 0, compute the Fourier transform of\\n.\\nii. Hence, plot the magnitude spectrum of the channel, that is,\\n, expressed as a\\nfunction of both time t and frequency f.\\nComment on the results so obtained. Consider a multipath channel where the delay-spread function is described by\\nwhere the scattering processes attributed to the time-varying amplitude and fixed delay are\\nuncorrelated for l = 1, 2, , L.\\na. Determine the correlation function of the channel, namely\\n.\\nb. With a Jakes model for the scattering process described in (9.12), find the corresponding formula\\nfor the correlation function of the channel under part a of the problem.\\nc. Hence, justify the statement that the multipath channel described in this problem fits a WSSUS\\nmodel. Revisit the Jakes model for a fast fading channel described in (9.12). Let the coherence time be\\ndefined as that range of values over which the correlation function defined in (9.12) is greater\\nthan 0.5.\\nFor some prescribed maximum Doppler shift\\n, find the coherence time of the channel. Consider a multipath channel for which the delay-spread function is given by\\nwhere the amplitude is time varying but the time delay is fixed. As in Problem 9.4, the\\nscattering processes are described by the Jakes model in (9.12). Determine the power-delay profile\\nof the channel, . 9.9 In real-life situations, the wireless channel is nonstationary due to the presence of moving objects of\\ndifferent kinds and other physical elements that can significantly affect radio propagation. Naturally,\\ndifferent types of wireless channels have different degrees of nonstationarity.\\nEven though many wireless communication channels are indeed highly nonstationary, the WSSUS',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 590,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'different types of wireless channels have different degrees of nonstationarity.\\nEven though many wireless communication channels are indeed highly nonstationary, the WSSUS\\nmodel described in Section 9.4 still provides a reasonably accurate account of the statistical\\ncharacteristics of the channel. Elaborate on this statement.\\nSpace Diversity-on-Receive Systems Following the material presented on Rayleigh fading in Chapter 4, derive the probability density\\nfunction of (9.64). A receive-diversity system uses a selection combiner with two diversity paths. The outage occurs\\nwhen the instantaneous SNR  drops below 0.25av, where av is the average SNR.\\nDetermine the probability of outage experienced by the receiver.\\nh t;    1 t   2 tt 2 -   + =  1 t  2 t  1 t  2 t 2 h t;   H f t;   h t;    l t l -   l 1 = L  =  l t l Rh 1 t1 2 t2  ;    t  max h t;    l tt l -   l 1 = L  =  l t l Ph',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 590,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Problems 571 9.12 The average SNR in a selection combiner is 20 dB. Compute the probability that the instantaneous\\nSNR of the selection combiner drops below  = 10 dB for the following number of receive antennas:\\na. Nr = 1 b. Nr = 2 c. Nr = 3 d. Nr = 4. Comment on your results. Repeat Problem 9.12 for  = 15 dB. In Section 9.8 we derived the optimum values of (9.75) for complex weighting factors of the\\nmaximal-ratio combiner using the Cauchy-Schwartz inequality.\\nThis problem addresses the same issue, but this time we use the standard maximization procedure.\\nTo simplify matters, the number of diversity paths Nr is restricted to two, with the complex\\nweighting parameters denoted by a1 and a2. Let\\nThe complex derivative with respect to ak is defined by\\nApplying this formula to the combiners output SNR c of (9.71), derive the optimum in (9.75). As discussed in Section 9.8, an equal-gain combiner is a special form of the maximal-ratio combiner\\nfor which the weighting factors are all equal. For convenience of presentation, the weighting\\nparameters are set to unity.\\nAssuming that the instantaneous SNR is small compared with the average SNR\\n, derive an\\napproximate formula for the probability density function of the random variable represented by the sample . 9.16 Compare the performances of the following linear diversity-on-receive techniques:\\na. Selection combiner.\\nb. Maximal-ratio combiner.\\nc. Equal-gain combiner.\\nBase the comparison on signal-to-noise improvement, expressed in decibels for the following\\nnumber of diversity branches: Nr = 2, 3, 4, 5, 6. Show that the maximum-likelihood decision rule for the maximal-ratio combiner may be formulated\\nin the following two equivalent forms:\\na. If\\nthen choose symbol si over sk.\\nb. If, by the same token,\\nthen choose symbol si over sk. Here, d2(y1,si) denotes the squared Euclidean distance between\\nthe signal points y1 and si. It may be argued that, in a rather loose sense, transmit-diversity and receive-diversity antenna',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 591,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the signal points y1 and si. It may be argued that, in a rather loose sense, transmit-diversity and receive-diversity antenna\\nconfigurations are the dual of each other, as illustrated in Figure P9.18.\\na. Taking a general viewpoint, justify the mathematical basis for this duality.\\nak xk jyk k 1 2  =  + =  ak* -------- 1 2---  xk -------- j  yk -------- +     k 1 2  =  = mrc  av   1 2 2 2 +  si 2 y1si - y1si -   1 2 2 2 +  sk 2 y1sk - y1sk -   k i    1 2 2 2 1 - +  si 2 d2 y1 si    +   1 2 2 2 1 - +  sk 2 d2 y1 sk    +   k i',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 591,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nb. However, we may cite the example of frequency-division diplexing (FDD) for which, in a strict\\nsense, we find that the duality depicted in Figure P9.18 is violated. How is it possible for the\\nviolation to arise in this example?\\nSpace Diversity-on-Transmit Systems Show that the two-by-two channel matrix in (9.88), defined in terms of the multiplicative fading\\nfactors and\\n, is a unitary matrix, as shown by Derive the formula for the average probability of symbol error incurred by the Alamouti code. Figure P9.22 shows the extension of orthogonal space-time codes to the Alamouti code, using two\\nantennas on both transmit and receive. The sequence of signal encoding and transmissions is\\nidentical to that of the single-receiver case of Figure 9.18. Part a of the table below defines the\\nchannels between the transmit and receive antennas. Part b of the table defines the outputs of the\\nreceive antennas at times and\\n, where T is the symbol duration.\\na. Derive expressions for the received signals\\n, and\\n, including the respective additive\\nnoise components expressed in terms of the transmitted symbols.\\nb. Derive expressions for the line of combined outputs in terms of the received signals.\\nc. Derive the maximum-likelihood decision rule for the estimates and . 9.22 This problem explores a new interpretation of the Alamouti code. Let\\nFigure P9.18 1 2 2 Nr Diversity paths Diversity paths Multiple receive antennas Multiple transmit antennas Transmit antenna Receive antenna 1   Nr   1e j1 2e j2 1e j1 2e j2 2e j - 2 1e j - 1 -  1e j1 2e j2 2e j - 2 1e j - 1 - 1 2 2 2 +  1 0 0 1 = t t T + Receive antenna 1 Receive antenna 2 a. Transmit antenna 1 Transmit antenna 2 h1 h2 h3 h4 b. Time Time x1 x2 x3   x4 s1 s2 t t T + x1 x2 x3 x4 si si 1  jsi 2  i 1 2  =  + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 592,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Problems 573 where and are both real numbers. The complex entry in the 2-by-2 Alamouti code is\\nrepresented by the 2-by-2 real orthogonal matrix\\nLikewise, the complex-conjugated entry is represented by the 2-by-2 real orthogonal matrix\\na. Show that the 2-by-2 complex Alamouti code S is equivalent to the 4-by-4 real transmission\\nmatrix Figure P9.22   Transmit antenna 1 Transmit antenna 1 Receive antenna 1 Receive antenna 2 Interference and noise Interference and noise Channel estimator 1 Channel estimator 2 Linear combiner Maximum-likelihood decoder\\ns1,   -s2 s1 s2,  s1 y1 y2 s2 w1, w2 h1 h3 h2 h1 h2 h4 h1 h3 h2 h4 h3 h4 w3, w2 si 1  si 2  si si 1  si 2  si 2  - si 1  i 1 2  =  si* si 1  s - i 1  si 2  si 2  i 1 2  =  S4 s1 1  s1 2  s2 1  s2 2  s1 2  - s1 1  s2 2  - s2 1  s2 1  - s2 2  s1 1  s1 2  - s2 2  - s2 1  - s1 2  s1 1  =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 593,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nb. Show that S4 is an orthogonal matrix.\\nc. What is the advantage of the complex code S over the real code S4? For two transmit antennas and simple receive antenna, the Alamouti code is said to be the only\\noptimal space-time block. Using the log-det formula of (9.117), justify this statement. Show that the channel capacity of the Alamouti code is equal to the sum of the channel capacities of\\ntwo SISO systems with each one of them operating at half the original bit rate.\\nMIMO Wireless Communications Show that, at high SNRs, the capacity gain of a MIMO wireless communication system with the\\nchannel state known to the receiver is N = min{Nt,Nr} bits per second per hertz for every 3 dB\\nincrease in SNR. To calculate the outage probability of MIMO systems, we use the complementary cumulative distri-\\nbution function of the random channel matrix H rather than the cumulative probability function itself.\\nExplain this rationale for calculating the outage probability. Equation (9.120) defines the formula for the channel capacity of diversity-on-receive channel.\\nIn Section 9.8 we pointed out that the selection combiner is a special case of the maximal-ratio\\ncombiner. Using (9.120), formulate an expression for the channel capacity of wireless diversity\\nusing the selection combiner. For the special case of a MIMO system having Nt = Nr = N, show that the ergodic capacity of the\\nsystem scales linearly, rather than logarithmically, with increasing SNR as N approaches infinity. In this problem we continue with the solution to Problem 9.28, namely as\\nwhere Nt = Nr = N and is the average eigenvalue of the matrix produced\\n. What is\\nthe value of the constant?\\na. Justify the asymptotic result given in (9.119); that is,\\nb. What conclusion can you draw from this asymptotic result? Suppose that an additive, temporally stationary, Gaussian interference v(t) corrupts the basic',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 594,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'b. What conclusion can you draw from this asymptotic result? Suppose that an additive, temporally stationary, Gaussian interference v(t) corrupts the basic\\ncomplex channel model of (9.105). The interference v(t) has zero mean and correlation matrix Rv.\\nEvaluate the effect of the interference v(t) on the ergodic capacity of the MIMO link. Consider a MIMO link for which the channel may be considered to be essentially constant for k\\nusers of the channel.\\na. Starting with the basic channel model of (9.105), formulate the input-output relationship of this\\nlink with the input being described by the Nr-by-k matrix\\nb. How is the log-det capacity formula of the link correspondingly modified? In a MIMO channel, the ability to exploit space-division multiple-access techniques for spectrally\\nefficient wireless communications is determined by the rank of the complex channel matrix H. (The\\nrank of a matrix is defined by the number of independent columns in the matrix.) For a given (Nt, Nr)\\nantenna configuration, it is desirable that the rank of H equal the minimum one of Nt transmit and Nr\\nreceive antennas, for it is only then that we are able to exploit the full potential of the MIMO antenna\\nC av 2 e log -------------      N   av HH HH = C N---- constant  S s1 s2 sk      =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 594,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Notes configuration. Under special conditions, however, the rank of the channel matrix H is reduced to\\nunity, in which case the scattering (fading) energy flow across the MIMO link is effectively confined\\nto a very narrow pipe, and with it, the channel capacity is severely degraded.\\nUnder the special conditions just described, a physical phenomenon known as the keyhole channel or\\npinhole channel is known to arise. Using a propagation layout of the MIMO link, describe how this\\nphenomenon can be explained.\\nOFDMA and CDMA Parts a and b of Figure 9.31 show the block diagrams of the transmitter and receiver of an OFDM\\nsystem, formulated on the basis of digital signal processing. It is informative to construct an analog\\ninterpretation of the OFDM system, which is the objective of this problem.\\na. Construct the analog interpretations of parts a and b in Figure 9.31.\\nb. With this construction at hand, compare the advantages and disadvantages of the digital and\\nanalog implementations of OFDM. Figure P9.34 depicts the model of a DS/BPSK system, where the order of spectrum spreading and\\nBPSK in the actual system has been interchanged; this is feasible because both operations are linear.\\nFor system analysis, we build on signal-space theoretic ideas of Chapter 7, using this model and\\nassuming the presence of a jammer at the receiver input. Thus, whereas signal-space representation of\\nthe transmitted signal, x(t), is one-dimensional, that of the jammer, j(t), is two-dimensional.\\na. Derive the processing gain formula of (9.125).\\nb. Next, ignoring the benefit gained from coherent detection, derive the SNR formula of (9.126).\\nNotes\\nLocal propagation effects are discussed in Chapter 1 of the classic book by Jakes (1974). For a\\ncomprehensive treatment of this subject, see the books by Parsons (2000) and Molisch (2011).\\nBessel functions are discussed in Appendix C.\\nTo be precise, we should use the terminology autocorrelation function rather then correlation',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 595,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Bessel functions are discussed in Appendix C.\\nTo be precise, we should use the terminology autocorrelation function rather then correlation\\nfunction as we did in Section 9.3. However, to be consistent with the literature, hereafter we use the\\nterminology correlation function for the sake of simplicity.\\nOn the basis of many measurements, the power-delay profile may be approximated by the one-\\nsided exponential functions (Molisch, 2011):\\nFor a more generic model, the power-delay profile is viewed as the sum of several one-sided\\nexponential functions representing multiple clusters of interacting objects, as shown by\\nFigure P9.34  Data signal b(t) Estimate of b(t) s(t) x(t) u(t) y(t) Binary PSK modulator Carrier Local carrier j(t)   c(t) c(t) Transmitter PN code generator Local PN code generator Receiver Channel Coherent detector Ph  = exp   -   for  0   0 = otherwise',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 595,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': 'Chapter\\nSignaling over Fading Channels\\nwhere Pi,\\n, and are respectively the power, delay, and delay spread of the ith cluster.\\nThe approximate approach described in Section 9.5 follows Van Trees (1971).\\nThe complex tap-coefficient is also referred to as the tap-gain or tap-weight.\\nThe chi-squared distribution with two degrees of freedom is described in Appendix A.\\nThe term maximal-ratio combiner was coined in a classic paper on linear diversity combining\\ntechniques by Brennan (1959).\\nThe three-point exposition presented in this section on maximal-ratio combining follows the\\nchapter by Stein in Schwartz et al. (1966: 653-654).\\nThe idea of MIMO for wireless communications was first described in the literature by Foschini\\n(1996). In the same year, Teletar (1996) derived the capacity of multi-antenna Gaussian channels in\\na technical report.\\nAs a result of experimental measurements, the model is known to be decidedly non-Gaussian\\nowing to the impulsive nature of human-made electromagnetic interference and natural noise.\\nDetailed derivation of the ergodic capacity in (9.115) is presented in Appendix E.\\nThe idea of OFDM has a long history, dating back to Chang (1966). Then, Weinstein and Ebert\\n(1971) used the FFT algorithm and guard intervals for the first digital implementation of OFDM.\\nThe first use of OFDM for mobile communications is credited to Cemini (1985).\\nIn the meantime, OFDM has developed into an indispensable tool for broadband wireless\\ncommunications and digital audio broadcasting.\\nThe literature on spread spectrum communications is enormous. For classic papers on spread\\nspectrum communications, see the following two:\\n The paper by Scholtz (1982) describes the origins of spread spectrum communications.\\n The paper by Pickholtz, et al. (1982) addresses the fundamentals of spread spectrum\\ncommunications.\\nThe Walsh-Hadamard sequences (codes) are named in honor of two pioneering contributions:',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 596,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The paper by Pickholtz, et al. (1982) addresses the fundamentals of spread spectrum\\ncommunications.\\nThe Walsh-Hadamard sequences (codes) are named in honor of two pioneering contributions:\\n Joseph L. Walsh (1923) for finding a new set of orthogonal functions with entries\\n.\\n Jacques Hadamard (1893) for finding a new set of square matrices also with entries\\n, which\\nhad all their rows (and columns) orthogonal.\\nFor more detailed treatments of these two papers, see Harmuth (1970), and Seberry and Yamada\\n(1992), respectively.\\nTo be rigorous mathematically, we should speak of the matrices A and B to be over the Galois\\nfield, GF(2). To explain, for any prime p, there exists a finite field of p elements, denoted by GF(P).\\nFor any positive integer b, we may expand the finite field GF(p) to a field of pb elements, which is\\ncalled an extension field of GF(p) and denoted by GF(pb). Finite fields are also called Galois fields in\\nhonor of their discoverer.\\nThus, for the example of (9.129), we have a Galois field of p = 2 and thus write GF(2).\\nCorrespondingly, for the H4 in (9.130) we have the Galois field GF(22) = GF(4)\\nThe original papers on Gold sequences are Gold (1967, 1968). A detailed discussion of Gold\\nsequences is presented in Holmes (1982).\\nThe classic paper on the RAKE receiver is due to Price and Green (1958). For a good treatment\\nof the RAKE receiver, more detailed than that presented in Section 9.15, see Chapter 5 in the book\\nby Haykin and Mohr (2005). For application of the RAKE receiver in CDMA, see the book by\\nViterbi (1995). Ph  Pi i -----------      Ph  0 i -   i = 0 i i cn t 1  1',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 596,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '577 CHAPTER 10 Error-Control Coding In the previous three chapters we studied the important issue of data transmission over\\ncommunication channels under three different channel-impairment scenarios:\\n\\nIn Chapter 7 the focus of attention was on the kind of channels where AWGN is the\\nmain source of channel impairment. An example of this first scenario is a satellite-\\ncommunication channel.\\n\\nIn Chapter 8 the focus of attention was intersymbol interference as the main source\\nof channel impairment. An example of this second scenario is the telephone\\nchannel.\\n\\nThen, in Chapter 9, we focused on multipath as a source of channel impairment. An\\nexample for this third scenario is the wireless channel.\\nAlthough, indeed, these three scenarios are naturally quite different from each other, they\\ndo share a common practical shortcoming: reliability. This is where the need for error-\\ncontrol coding, the topic of this chapter, assumes paramount importance.\\nGiven these physical realities, the task facing the designer of a digital communication\\nsystem is that of providing a cost-effective facility for transmitting information from one\\nend of the system at a rate and level of reliability and quality that are acceptable to a user\\nat the other end.\\nFrom a communication theoretic perspective, the key system parameters available for\\nachieving these practical requirements are limited to two:\\n\\ntransmitted signal power, and',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 597,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'transmitted signal power, and\\n\\nchannel bandwidth.\\nThese two parameters, together with the power spectral density of receiver noise,\\ndetermine the signal energy per bit-to-noise power spectral density ratio, EbN0. In\\nChapter 7 we showed that this ratio uniquely determines the BER produced by a particular\\nmodulation scheme operating over a Gaussian noise channel. Practical considerations\\nusually place a limit on the value that we can assign to EbN0. To be specific, in practice,\\nwe often arrive at a modulation scheme and find that it is not possible to provide\\nacceptable data quality (i.e., low enough error performance). For a fixed EbN0, the only\\npractical option available for changing data quality from problematic to acceptable is to\\nuse error-control coding, which is the focus of attention in this chapter. In simple terms,\\nby incorporating a fixed number of redundant bits into the structure of a codeword at the\\ntransmitter, it is feasible to provide reliable communication over a noisy channel, provided',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 597,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '578 Chapter 10 Error-Control Coding that Shannons code theorem, discussed in Chapter 5, is satisfied. In effect, channel\\nbandwidth is traded off for reliable communication.\\nAnother practical motivation for the use of coding is to reduce the required EbN0 for a\\nfixed BER. This reduction in EbN0 may, in turn, be exploited to reduce the required\\ntransmitted power or reduce the hardware costs by requiring a smaller antenna size in the\\ncase of radio communications. Error Control Using Forward Error Correction\\nError control for data integrity may be exercised by means of forward error correction\\n(FEC).1 Figure 10.1a shows the model of a digital communication system using such an\\napproach. The discrete source generates information in the form of binary symbols. The\\nchannel encoder in the transmitter accepts message bits and adds redundancy according to\\na prescribed rule, thereby producing an encoded data stream at a higher bit rate. The\\nchannel decoder in the receiver exploits the redundancy to decide which message bits in\\nthe original data stream, given a noisy version of the encoded data stream, were actually\\ntransmitted. The combined goal of the channel encoder and decoder is to minimize the\\neffect of channel noise. That is, the number of errors between the channel encoder input\\n(derived from the source) and the channel decoder output (delivered to the user) is\\nminimized.\\nFor a fixed modulation scheme, the addition of redundancy in the coded messages\\nimplies the need for increased transmission bandwidth. Moreover, the use of error-control\\ncoding adds complexity to the system. Thus, the design trade-offs in the use of error-control\\ncoding to achieve acceptable error performance include considerations of bandwidth and\\nsystem complexity.\\nFigure 10.1 Simplified models of a digital communication system. (a) Coding and modulation\\nperformed separately. (b) Coding and modulation combined.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 598,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'system complexity.\\nFigure 10.1 Simplified models of a digital communication system. (a) Coding and modulation\\nperformed separately. (b) Coding and modulation combined.\\n(a) (b) Discrete source Modulator Channel encoder User Channel decoder Detector Waveform channel Noise Discrete source Encoder/modulator Detector/decoder User Waveform channel Noise Discrete channel',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 598,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '3 Discrete Memoryless Channels There are many different error-correcting codes (with roots in diverse mathematical\\ndisciplines) that we can use. Historically, these codes have been classified into block codes\\nand convolutional codes. The distinguishing feature for this particular classification is the\\npresence or absence of memory in the encoders for the two codes.\\nTo generate an (n, k) block code, the channel encoder accepts information in successive\\nk-bit blocks; for each block, it adds n - k redundant bits that are algebraically related to\\nthe k message bits, thereby producing an overall encoded block of n bits, where n > k. The\\nn-bit block is called a codeword, and n is called the block length of the code. The channel\\nencoder produces bits at the rate R0 = (nk)Rs, where Rs is the bit rate of the information\\nsource. The dimensionless ratio r = kn is called the code rate, where 0 < r < 1. The bit\\nrate R0, coming out of the encoder, is called the channel data rate. Thus, the code rate is a\\ndimensionless ratio, whereas the data rate produced by the source and the channel data\\nrate produced by the encoder are both measured in bits per second.\\nIn a convolutional code, the encoding operation may be viewed as the discrete-time\\nconvolution of the input sequence with the impulse response of the encoder. The duration\\nof the impulse response equals the memory of the encoder. Accordingly, the encoder for a\\nconvolutional code operates on the incoming message sequence, using a sliding window\\nequal in duration to its own memory. This, in turn, means that in a convolutional code,\\nunlike in a block code, the channel encoder accepts message bits as a continuous sequence\\nand thereby generates a continuous sequence of encoded bits at a higher rate.\\nIn the model depicted in Figure 10.1a, the operations of channel coding and modulation\\nare performed separately in the transmitter; and likewise for the operations of detection',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 599,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'In the model depicted in Figure 10.1a, the operations of channel coding and modulation\\nare performed separately in the transmitter; and likewise for the operations of detection\\nand decoding in the receiver. When, however, bandwidth efficiency is of major concern,\\nthe most effective method of implementing forward error-control correction coding is to\\ncombine it with modulation as a single function, as shown in Figure 10.1b. In this second\\napproach, coding is redefined as a process of imposing certain patterns on the transmitted\\nsignal and the resulting code is called a trellis code.\\nBlock codes, convolutional codes, and trellis codes represent the classical family of\\ncodes that follow traditional approaches rooted in algebraic mathematics in one form or\\nanother. In addition to these classical codes, we now have a new generation of coding\\ntechniques exemplified by turbo codes and low-density parity-check (LDPC) codes. These\\nnew codes are not only fundamentally different, but they have also already taken over the\\nlegacy coding schemes very quickly in many practical systems. Simply put, turbo codes\\nand LDPC codes are structured in such a way that decoding can be split into a number of\\nmanageable steps, thereby making it possible to construct powerful codes in a\\ncomputationally feasible manner, which is not attainable with the legacy codes. Turbo\\ncodes and LDPC codes are discussed in the latter part of the chapter. Discrete Memoryless Channels\\nReturning to the model of Figure 10.1a, the waveform channel is said to be memoryless if\\nin a given interval the detector output depends only on the signal transmitted in that\\ninterval and not on any previous transmission. Under this condition, we may model the\\ncombination of the modulator, the waveform channel, and the demodulator (detector) as a\\ndiscrete memoryless channel. Such a channel is completely described by the set of\\ntransition probabilities denoted by p(j|i), where i denotes a modulator input symbol, j',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 599,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '580 Chapter 10 Error-Control Coding denotes a demodulator output symbol, and p(j|i) is the probability of receiving symbol j\\ngiven that symbol i was sent. (Discrete memoryless channels were described previously at\\nsome length in Chapter 5 on information theory.)\\nThe simplest discrete memoryless channel results from the use of binary input and\\nbinary output symbols. When binary coding is used, the modulator has only the binary\\nsymbols 0 and 1 as inputs. Likewise, the decoder has only binary inputs if binary\\nquantization of the demodulator output is used; that is, a hard decision is made on the\\ndemodulator output as to which binary symbol was actually transmitted. In this situation,\\nwe have a binary symmetric channel with a transition probability diagram as shown in\\nFigure 10.2. From Chapter 5, we recall that the binary symmetric channel, assuming a\\nchannel noise modeled as AWGN, is completely described by the transition probability.\\nHard-decision decoding takes advantage of the special algebraic structure that is built into\\nthe design of channel codes; the decoding is therefore relatively easy to perform.\\nHowever, the use of hard decisions prior to decoding causes an irreversible loss of\\nvaluable information in the receiver. To reduce this loss, soft-decision coding can be used.\\nThis is achieved by including a multilevel quantizer at the demodulator output, as\\nillustrated in Figure 10.3a for the case of binary PSK signals. The input-output\\ncharacteristic of the quantizer is shown in Figure 10.3b. The modulator has only binary\\nsymbols 0 and 1 as inputs, but the demodulator output now has an alphabet with Q\\nsymbols. Assuming the use of the three-level quantizer described in Figure 10.3b, we have\\nQ = 8. Such a channel is called a binary input, Q-ary output discrete memoryless channel.\\nThe corresponding channel transition probability diagram is shown in Figure 10.3c. The\\nform of this distribution, and consequently the decoder performance, depends on the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 600,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'The corresponding channel transition probability diagram is shown in Figure 10.3c. The\\nform of this distribution, and consequently the decoder performance, depends on the\\nlocation of the representation levels of the quantizer, which, in turn, depends on the signal\\nlevel and noise variance. Accordingly, the demodulator must incorporate automatic gain\\ncontrol if an effective multilevel quantizer is to be realized. Moreover, the use of soft\\ndecisions complicates the implementation of the decoder. Nevertheless, soft-decision\\ndecoding offers significant improvement in performance over hard-decision decoding by\\ntaking a probabilistic rather than an algebraic approach. It is for this reason that soft-\\ndecision decoders are also referred to as probabilistic decoders.\\nChannel Coding Theorem Revisited\\nIn Chapter 5 on information theory we established the concept of channel capacity, which,\\nfor a discrete memoryless channel, represents the maximum amount of information that\\nFigure 10.2 Transition probability diagram of binary symmetric channel.\\nSymbol 1 Symbol 0 Symbol 1 Symbol 0 1 - p 1 - ppp Inputs Outputs',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 600,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '3 Discrete Memoryless Channels can be transmitted per channel use in a reliable manner. The channel coding theorem\\nstates:\\nIf a discrete memoryless channel has capacity C and a source generates\\ninformation at a rate less than C, then there exists a coding technique such that\\nthe output of the source may be transmitted over the channel with an arbitrarily\\nlow probability of symbol error.\\nFor the special case of a binary symmetric channel, the theorem teaches us that if the code\\nrate r is less than the channel capacity C, then it is possible to find a code that achieves\\nerror-free transmission over the channel. Conversely, it is not possible to find such a code\\nif the code rate r is greater than the channel capacity C. Thus, the channel coding theorem\\nspecifies the channel capacity C as a fundamental limit on the rate at which the\\ntransmission of reliable (error-free) messages can take place over a discrete memoryless\\nchannel. The issue that matters here is not the SNR, so long as it is large enough, but how\\nthe channel input is encoded.\\nThe most unsatisfactory feature of the channel coding theorem, however, is its\\nnonconstructive nature. The theorem asserts the existence of good codes but does not tell\\nus how to find them. By good codes we mean families of channel codes that are capable of\\nproviding reliable transmission of information (i.e., at arbitrarily small probability of\\nsymbol error) over a noisy channel of interest at bit rates up to a maximum value less than\\nthe capacity of that channel. The error-control coding techniques described in this chapter\\nprovide different methods of designing good codes.\\nFigure 10.3 Binary input, Q-ary output discrete memoryless channel. (a) Receiver for binary PSK.\\n(b) Transfer characteristic of a multilevel quantizer. (c) Channel transition probability diagram. Parts\\n(b) and (c) are illustrated for eight levels of quantization.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 601,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '(b) Transfer characteristic of a multilevel quantizer. (c) Channel transition probability diagram. Parts\\n(b) and (c) are illustrated for eight levels of quantization.\\n0 T   E + w(t) E = cos(2fct) dt 2 T Uniform multilevel quantizer b1 b2 b3 b4 b5 b6 b7 b8 (a) (b) (c) Output Input b1 b2 b3 b4 b5 b6 b7 b8 Symbol 1 transmitted E - Symbol 2 transmitted 1(t)  1(t)  Input Output',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 601,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '582 Chapter 10 Error-Control Coding Notation Many of the codes described in this chapter are binary codes, for which the alphabet\\nconsists only of binary symbols 0 and 1. In such a code, the encoding and decoding\\nfunctions involve the binary arithmetic operations of modulo-2 addition and multiplication\\nperformed on codewords in the code.\\nThroughout this chapter, we use the ordinary plus sign (+) to denote modulo-2 addition.\\nThe use of this terminology will not lead to confusion because the whole chapter relies on\\nbinary arithmetic. In so doing, we avoid use of the special symbol as we did in previous\\nparts of the book. Thus, according to the notation used in this chapter, the rules for\\nmodulo-2 addition are as follows:\\nBecause 1 + 1 = 0, it follows that 1 = -1. Hence, in binary arithmetic, subtraction is the\\nsame as addition. The rules for modulo-2 multiplication are as follows:\\nDivision is trivial, in that we have\\nand division by 0 is not permitted. Modulo-2 addition is the EXCLUSIVE-OR operation\\nin logic and modulo-2 multiplication is the AND operation.\\n4 Linear Block Codes By definition: A code is said to be linear if any two codewords in the code can be added in\\nmodulo-2 arithmetic to produce a third codeword in the code.\\nConsider, then, an (n,k) linear block code, in which k bits of the n code bits are always\\nidentical to the message sequence to be transmitted. The (n - k) bits in the remaining\\nportion are computed from the message bits in accordance with a prescribed encoding rule\\nthat determines the mathematical structure of the code. Accordingly, these (n - k) bits are\\nreferred to as parity-check bits. Block codes in which the message bits are transmitted in\\nunaltered form are called systematic codes. For applications requiring both error detection\\nand error correction, the use of systematic block codes simplifies implementation of the\\ndecoder.\\nLet m0, m1, , mk - 1 constitute a block of k arbitrary message bits. Thus, we have 2k',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 602,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'and error correction, the use of systematic block codes simplifies implementation of the\\ndecoder.\\nLet m0, m1, , mk - 1 constitute a block of k arbitrary message bits. Thus, we have 2k\\ndistinct message blocks. Let this sequence of message bits be applied to a linear block\\n0 0 0 = + 1 0 1 = + 0 1 1 = + 1 1 0 = + 0 0 0 =  1 0 0 =  0 1 0 =  1 1 1 =  1 1 1 =  0 1 0 =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 602,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Linear Block Codes encoder, producing an n-bit codeword whose elements are denoted by c0, c1, , cn - 1. Let\\nb0, b1, , bn - k - 1 denote the (n - k) parity-check bits in the codeword. For the code to\\npossess a systematic structure, a codeword is divided into two parts, one of which is\\noccupied by the message bits and the other by the parity-check bits. Clearly, we have the\\noption of sending the message bits of a codeword before the parity-check bits, or vice versa.\\nThe former option is illustrated in Figure 10.4, and its use is assumed in the following.\\nAccording to the representation of Figure 10.4, the (n - k) leftmost bits of a codeword\\nare identical to the corresponding parity-check bits and the k rightmost bits of the\\ncodeword are identical to the corresponding message bits. We may therefore write\\n(10.1)\\nThe (n - k) parity-check bits are linear sums of the k message bits, as shown by the\\ngeneralized relation\\n(10.2)\\nwhere the coefficients are defined as follows:\\n(10.3)\\nThe coefficients pij are chosen in such a way that the rows of the generator matrix are\\nlinearly independent and the parity-check equations are unique. The pij used here should\\nnot be confused with the p(j|i) introduced in Section 10.3.\\nThe system of (10.1) and (10.2) defines the mathematical structure of the (n,k) linear\\nblock code. This system of equations may be rewritten in a compact form using matrix\\nnotation. To proceed with this reformulation, we respectively define the 1-by-k message\\nvector m, the 1-by-(n - k) parity-check vector b, and the 1-by-n code vector c as follows:\\n(10.4) (10.5) (10.6) Note that all three vectors are row vectors. The use of row vectors is adopted in this\\nchapter for the sake of being consistent with the notation commonly used in the coding\\nliterature. We may thus rewrite the set of simultaneous equations defining the parity\\ncheck-bits in the compact matrix form\\nb = mP\\n(10.7)\\nFigure 10.4 Structure of systematic codeword.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 603,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'literature. We may thus rewrite the set of simultaneous equations defining the parity\\ncheck-bits in the compact matrix form\\nb = mP\\n(10.7)\\nFigure 10.4 Structure of systematic codeword.\\nci bi, i 0 1 n  k - 1 -  = mi+k n - , i n kn k - 1 n 1 -   +  - =      = bi p0im0 p1im1  pk 1 i - mk 1 - + + + = pij 1 if bi depends on mj 0 otherwise    = m m0 m1 mk 1 -      = b b0 b1 bn k 1 - -      = c c0 c1 cn 1 -      = b0, b1, . . ., bn - k -\\nm0, m1, . . ., mk -\\nParity-check bits\\nMessage bits',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 603,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '584 Chapter 10 Error-Control Coding The P in (10.7) is the k-by-(n - k) coefficient matrix defined by\\n(10.8)\\nwhere the element pij is 0 or 1.\\nFrom the definitions given in (10.4)-(10.6), we see that c may be expressed as a\\npartitioned row vector in terms of the vectors m and b as follows:\\n(10.9)\\nHence, substituting (10.7) into (10.9) and factoring out the common message vector m, we\\nget\\n(10.10)\\nwhere Ik is the k-by-k identity matrix:\\n(10.11)\\nDefine the k-by-n generator matrix\\n(10.12)\\nThe generator matrix G of (10.12) is said to be in the canonical form, in that its k rows are\\nlinearly independent; that is, it is not possible to express any row of the matrix G as a\\nlinear combination of the remaining rows. Using the definition of the generator matrix G,\\nwe may simplify (10.10) as\\n(10.13)\\nThe full set of codewords, referred to simply as the code, is generated in accordance\\nwith (10.13) by passing the message vector m range through the set of all 2k binary\\nk-tuples (1-by-k vectors). Moreover, the sum of any two codewords in the code is another\\ncodeword. This basic property of linear block codes is called closure. To prove its validity,\\nconsider a pair of code vectors ci and cj corresponding to a pair of message vectors mi and\\nmj, respectively. Using (10.13), we may express the sum of ci and cj as The modulo-2 sum of mi and mj represents a new message vector. Correspondingly, the\\nmodulo-2 sum of ci and cj represents a new code vector.\\nThere is another way of expressing the relationship between the message bits and\\nparity-check bits of a linear block code. Let H denote an (n - k)-by-n matrix, defined as\\nP p00 p01  p0 n k 1 - -  p10 p11  p1 n k 1 - -     pk 1 - 0  pk 1 1  - pk 1 - n k 1 - - = c b m = c mP Ik = Ik 1 0 0 0 1 0  0 0 1 = G P Ik = c mG = ci cj + miG mjG + = mi mj +  G =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 604,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '4 Linear Block Codes (10.14)\\nwhere PT is an (n - k)-by-k matrix, representing the transpose of the coefficient matrix P,\\nand In - k is the (n - k)-by-(n - k) identity matrix. Accordingly, we may perform the\\nfollowing multiplication of partitioned matrices:\\nwhere we have used the fact that multiplication of a rectangular matrix by an identity\\nmatrix of compatible dimensions leaves the matrix unchanged. In modulo-2 arithmetic,\\nthe matrix sum PT + PT is 0. We therefore have\\n(10.15)\\nEquivalently, we have GHT = 0, where 0 is a new null matrix. Postmultiplying both sides\\nof (10.13) by HT, the transpose of H, and then using (10.15), we get the inner product\\n(10.16)\\nThe matrix H is called the parity-check matrix of the code and the equations specified by\\n(10.16) are called parity-check equations.\\nThe generator equation (10.13) and the parity-check detector equation (10.16) are basic\\nto the description and operation of a linear block code. These two equations are depicted\\nin the form of block diagrams in Figure 10.5a and b, respectively.\\nSyndrome: Definition and Properties\\nThe generator matrix G is used in the encoding operation at the transmitter. On the other\\nhand, the parity-check matrix H is used in the decoding operation at the receiver. In the\\ncontext of the latter operation, let r denote the 1-by-n received vector that results from\\nFigure 10.5 Block diagram representations of the generator\\nequation (10.13) and the parity-check equation (10.16).\\nH In-k PT = HGT In-k PT PT  Ik = PT PT + = HGT 0 = cHT mGHT = 0 = Parity-check matrix H Null vector 0 Code vector c (b) Generator matrix G Code vector c Message vector m (a)',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 605,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '586 Chapter 10 Error-Control Coding sending the code vector c over a noisy binary channel. We express the vector r as the sum\\nof the original code vector c and a new vector e, as shown by\\n(10.17)\\nThe vector e is called the error vector or error pattern. The ith element of e equals 0 if the\\ncorresponding element of r is the same as that of c. On the other hand, the ith element of e\\nequals 1 if the corresponding element of r is different from that of c, in which case an\\nerror is said to have occurred in the ith location. That is, for i = 1, 2,, n, we have\\n(10.18)\\nThe receiver has the task of decoding the code vector c from the received vector r. The\\nalgorithm commonly used to perform this decoding operation starts with the computation\\nof a 1-by-(n - k) vector called the error-syndrome vector or simply the syndrome.2 The\\nimportance of the syndrome lies in the fact that it depends only upon the error pattern.\\nGiven a 1-by-n received vector r, the corresponding syndrome is formally defined as\\n(10.19)\\nAccordingly, the syndrome has the following important properties.\\nPROPERTY\\nThe syndrome depends only on the error pattern and not on the transmitted codeword.\\nTo prove this property, we first use (10.17) and (10.19), and then (10.16) to write\\n(10.20)\\nHence, the parity-check matrix H of a code permits us to compute the syndrome s, which\\ndepends only upon the error pattern e.\\nTo expand on Property 1, suppose that the error pattern e contains a pair of errors in\\nlocations i and j caused by the additive channel noise, as shown by\\nThen, substituting this error pattern into (10.20) yields the syndrome\\nwhere hi and hj are respectively the ith and jth rows of the matrix HT. In words, we may\\nstate the following corollary to Property 1:\\nFor a linear block code, the syndrome s is equal to the sum of those rows of\\nthe transposed parity-check matrix HT where errors have occurred due to\\nchannel noise.\\nPROPERTY\\nAll error patterns that differ by a codeword have the same syndrome.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 606,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the transposed parity-check matrix HT where errors have occurred due to\\nchannel noise.\\nPROPERTY\\nAll error patterns that differ by a codeword have the same syndrome.\\nFor k message bits, there are 2k distinct code vectors denoted as ci, where i = 0, 1, , 2k - 1.\\nCorrespondingly, for any error pattern e we define the 2k distinct vectors ei as follows for i = 0, 1, , 2k -\\n(10.21) rce + = ei 1 if an error has occurred in the ith location\\n0 otherwise    = s rHT = sce +  HT = cHT eHT + = eHT = e 001i001j00   = s hi hj + = ei e ci + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 606,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Linear Block Codes The set of vectors (ei, i = 0, 1, , 2k - 1) defined in (10.21) is called a coset of the code. In\\nother words, a coset has exactly 2k elements that differ at most by a code vector. Thus, an\\n(n,k) linear block code has 2n - k possible cosets. In any event, multiplying both sides of\\n(10.21) by the matrix HT and again using (10.16), we get\\n(10.22)\\nwhich is independent of the index i. Accordingly, we may say:\\nEach coset of the code is characterized by a unique syndrome.\\nWe may put Properties 1 and 2 in perspective by expanding (10.20). Specifically, with the\\nmatrix H having the systematic form given in (10.14), where the matrix P is itself defined\\nby (10.8), we find from (10.20) that the (n - k) elements of the syndrome s are linear\\ncombinations of the n elements of the error pattern e, as shown by\\n(10.23)\\nThis set of (n - k) linear equations clearly shows that the syndrome contains information\\nabout the error pattern and may, therefore, be used for error detection. However, it should\\nbe noted that the set of equations (10.23) is underdetermined, in that we have more\\nunknowns than equations. Accordingly, there is no unique solution for the error pattern.\\nRather, there are 2n error patterns that satisfy (10.23) and, therefore, result in the same\\nsyndrome, in accordance with Property 2 and (10.22). In particular, with 2n - k possible\\nsyndrome vectors, the information contained in the syndrome s about the error pattern e is\\nnot enough for the decoder to compute the exact value of the transmitted code vector.\\nNevertheless, knowledge of the syndrome s reduces the search for the true error pattern e\\nfrom 2n to 2n - k possibilities. Given these possibilities, the decoder has the task of making\\nthe best selection from the cosets corresponding to s.\\nMinimum Distance Considerations\\nConsider a pair of code vectors c1 and c2 that have the same number of elements. The\\nHamming distance, denoted by d(c1,c2), between such a pair of code vectors is defined as',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 607,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Consider a pair of code vectors c1 and c2 that have the same number of elements. The\\nHamming distance, denoted by d(c1,c2), between such a pair of code vectors is defined as\\nthe number of locations in which their respective elements differ.\\nThe Hamming weight w(c) of a code vector c is defined as the number of nonzero\\nelements in the code vector. Equivalently, we may state that the Hamming weight of a\\ncode vector is the distance between the code vector and the all-zero code vector. In a\\ncorresponding way, we may introduce a new parameter called the minimum distance dmin,\\nfor which we make the statement:\\nThe minimum distance dmin of a linear block code is the smallest Hamming\\ndistance between any pair of codewords.\\neiHT eHT ciHT + = eHT = s0 e0 en k - p00 en k - 1 + p10  en k - pk 1 0  - + + + + = s1 e1 en k - p01 en k - 1 + p11  en k - pk 1 1  - + + + + =  sn k - 1 - en k - 1 - en k - p0 n k - 1 -   en 1 - p k 1 n  - k - 1 +   + + + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 607,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '588 Chapter 10 Error-Control Coding That is, the minimum distance is the same as the smallest Hamming weight of the difference\\nbetween any pair of code vectors. From the closure property of linear block codes, the sum\\n(or difference) of two code vectors is another code vector. Accordingly, we may also state:\\nThe minimum distance of a linear block code is the smallest Hamming weight\\nof the nonzero code vectors in the code.\\nThe minimum distance dmin is related to the structure of the parity-check matrix H of the\\ncode in a fundamental way. From (10.16) we know that a linear block code is defined by\\nthe set of all code vectors for which cHT = 0, where HT is the transpose of the parity-check\\nmatrix H. Let the matrix H be expressed in terms of its columns as shown by\\n(10.24)\\nThen, for a code vector c to satisfy the condition cHT = 0, the vector c must have ones in\\nsuch positions that the corresponding rows of HT sum to the zero vector 0. However, by\\ndefinition, the number of ones in a code vector is the Hamming weight of the code vector.\\nMoreover, the smallest Hamming weight of the nonzero code vectors in a linear block\\ncode equals the minimum distance of the code. Hence, we have another useful result stated\\nas follows:\\nThe minimum distance of a linear block code is defined by the minimum\\nnumber of rows of the matrix HT whose sum is equal to the zero vector.\\nFrom this discussion, it is apparent that the minimum distance dmin of a linear block code\\nis an important parameter of the code. Specifically, dmin determines the error-correcting\\ncapability of the code. Suppose an (n,k) linear block code is required to detect and correct\\nall error patterns over a binary symmetric channel, and whose Hamming weight is less\\nthan or equal to t. That is, if a code vector ci in the code is transmitted and the received\\nvector is r = ci + e, we require that the decoder output whenever the error pattern e\\nhas a Hamming weight w(e)  t',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 608,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'vector is r = ci + e, we require that the decoder output whenever the error pattern e\\nhas a Hamming weight w(e)  t\\nWe assume that the 2k code vectors in the code are transmitted with equal probability. The\\nbest strategy for the decoder then is to pick the code vector closest to the received vector r;\\nthat is, the one for which the Hamming distance d(ci,r) is the smallest. With such a\\nstrategy, the decoder will be able to detect and correct all error patterns of Hamming weight\\nw(e), provided that the minimum distance of the code is equal to or greater than 2t + 1. We\\nmay demonstrate the validity of this requirement by adopting a geometric interpretation of\\nthe problem. In particular, the transmitted 1-by-n code vector and the 1-by-n received\\nvector are represented as points in an n-dimensional space. Suppose that we construct two\\nspheres, each of radius t, around the points that represent code vectors ci and cj under two\\ndifferent conditions:\\nLet these two spheres be disjoint, as depicted in Figure 10.6a. For this condition to\\nbe satisfied, we require that d(ci,cj)  2t + 1. If, then, the code vector ci is transmitted\\nand the Hamming distance d(ci,r)  t, it is clear that the decoder will pick ci, as it is\\nthe code vector closest to the received vector r.\\nIf, on the other hand, the Hamming distance d(ci,cj)  2t, the two spheres around ci\\nand cj intersect, as depicted in Figure 10.6b. In this second situation, we see that if ci\\nH h1 h2 hn      = c ci =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 608,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Linear Block Codes is transmitted, there exists a received vector r such that the Hamming distance\\nd(ci,r)  t, yet r is as close to cj as it is to ci. Clearly, there is now the possibility of\\nthe decoder picking the vector cj, which is wrong.\\nWe thus conclude the ideas presented thus far by saying:\\nAn (n,k) linear block code has the power to correct all error patterns of weight t\\nor less if, and only if, d(ci,cj) > 2t + 1, for all ci and cj.\\nBy definition, however, the smallest distance between any pair of code vectors in a code is\\nthe minimum distance dmin of the code. We may, therefore, go on to state:\\nAn (n,k) linear block code of minimum distance dmin can correct up to t errors\\nif, and only if, (10.25) where denotes the largest integer less than or equal to the enclosed\\nquantity.\\nThe condition described in (10.25) is important because it gives the error-correcting\\ncapability of a linear block code a quantitative meaning.\\nSyndrome Decoding\\nWe are now ready to describe a syndrome-based decoding scheme for linear block codes.\\nLet denote the 2k code vectors of an (n, k) linear block code. Let r denote\\nthe received vector, which may have one of 2n possible values. The receiver has the task of\\npartitioning the 2n possible received vectors into 2k disjoint subsets in\\nsuch a way that the ith subset Di corresponds to code vector ci for 1 < i < 2k. The received\\nvector r is decoded into ci if it is in the ith subset. For the decoding to be correct, r must be\\nin the subset that belongs to the code vector ci that was actually sent.\\nThe 2k subsets described herein constitute a standard array of the linear block code. To\\nconstruct it, we exploit the linear structure of the code by proceeding as follows:\\nThe 2k code vectors are placed in a row with the all-zero code vector c1 as the\\nleftmost element.\\nAn error pattern e2 is picked and placed under c1, and a second row is formed by\\nadding e2 to each of the remaining code vectors in the first row; it is important that',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 609,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'leftmost element.\\nAn error pattern e2 is picked and placed under c1, and a second row is formed by\\nadding e2 to each of the remaining code vectors in the first row; it is important that\\nFigure 10.6 (a) Hamming distance d(ci,cj)  2t + 1. (b) Hamming distance\\nd(ci,cj)  2t. The received vector is denoted by r.\\n(a) (b) ci cj r r ci cj ttttt 1 2--- dmin 1 -    c1 c2 c2k    D1 D2 D2k',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 609,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '590 Chapter 10 Error-Control Coding the error pattern chosen as the first element in a row has not previously appeared in\\nthe standard array.\\nStep 2 is repeated until all the possible error patterns have been accounted for.\\nFigure 10.7 illustrates the structure of the standard array so constructed. The 2k columns of\\nthis array represent the disjoint subsets\\n. The 2n-k rows of the array\\nrepresent the cosets of the code, and their first elements are called coset\\nleaders.\\nFor a given channel, the probability of decoding error is minimized when the most\\nlikely error patterns (i.e., those with the largest probability of occurrence) are chosen as\\nthe coset leaders. In the case of a binary symmetric channel, the smaller we make the\\nHamming weight of an error pattern, the more likely it is for an error to occur.\\nAccordingly, the standard array should be constructed with each coset leader having the\\nminimum Hamming weight in its coset.\\nWe are now ready to describe a decoding procedure for linear block codes:\\nFor the received vector r, compute the syndrome s = rHT.\\nWithin the coset characterized by the syndrome s, identify the coset leader (i.e., the\\nerror pattern with the largest probability of occurrence); call it e0.\\nCompute the code vector\\nc = r + e0\\n(10.26)\\nas the decoded version of the received vector r.\\nThis procedure is called syndrome decoding.\\nEXAMPLE\\nHamming Codes\\nFor any positive integer m > 3, there exists a linear block code with the following\\nparameters: code length n = 2m - 1 number of message bits\\nk = 2m - m -\\nnumber of parity-check bits\\nn - k = m\\nSuch a linear block code for which the error-correcting capability t = 1 is called a\\nHamming code.3 To be specific, consider the example of m = 3, yielding the (7, 4)\\nHamming code with n = 7 and k = 4. The generator of this code is defined by\\nFigure 10.7 Standard array for an (n,k) block code.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 610,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Hamming code with n = 7 and k = 4. The generator of this code is defined by\\nFigure 10.7 Standard array for an (n,k) block code.\\nc1 = 0 e2 . . . e3 ej . . . e2n - k c2 c2 + e2 . . . c2 + e3 c2 + ej . . . c2 + e2n - k c3 c3 + e2 . . . c3 + e3 c3 + ej . . . . . . . . . . . . . . . c3 + e2n - k ci ci + e2 . . . ci + e3 ci + ej . . . . . . . . . . . . . . . ci + e2n - k c2k c2k + e2 . . . c2k + e3 c2k + ej . . . c2k + e2n - k D1 D2 D2k    e2 e2n k -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 610,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '4 Linear Block Codes which conforms to the systematic structure of (10.12).\\nThe corresponding parity-check matrix is given by\\nThe operative property embodied in this equation is that the columns of the parity-check\\nmatrix P consist of all the nonzero m-tuples, where m = 3.\\nWith k = 4, there are 2k = 16 distinct message words, which are listed in Table 10.1. For\\na given message word, the corresponding codeword is obtained by using (10.13). Thus, the\\napplication of this equation results in the 16 codewords listed in Table 10.1.\\nIn Table 10.1, we have also listed the Hamming weights of the individual codewords in\\nthe (7,4) Hamming code. Since the smallest of the Hamming weights for the nonzero\\ncodewords is 3, it follows that the minimum distance of the code is 3, which is what it\\nshould be by definition. Indeed, all Hamming codes have the property that the minimum\\ndistance dmin = 3, independent of the value assigned to the number of parity bits m.\\nTo illustrate the relation between the minimum distance dmin and the structure of the\\nparity-check matrix H, consider the codeword 0110100. In matrix multiplication, defined\\nTable 10.1 Codewords of a (7,4) Hamming code\\nMessage word Codeword Weight of codeword Message word Codeword Weight of codeword 0000 0001 0010 0011 0100 0101 0110 0111 0000000 1010001 1110010 0100011 0110100 1100101 1000110 0010111 0 3 4 3 3 4 3 4 1000 1001 1010 1011 1100 1101 1110 1111 1101000 0111001 0011010 1001011 1011100 0001101 0101110 1111111 3 4 3 3 4 3 4 7 G 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 P Ik = { { H 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 = In k - PT { {',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 611,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '592 Chapter 10 Error-Control Coding by (10.16), the nonzero elements of this codeword sift out the second, third, and fifth\\ncolumns of the matrix H, yielding\\nWe may perform similar calculations for the remaining 14 nonzero codewords. We thus\\nfind that the smallest number of columns in H that sums to zero is 3, reconfirming the\\ndefining condition dmin = 3.\\nAn important property of binary Hamming codes is that they satisfy the condition of\\n(10.25) with the equality sign, assuming that t = 1. Thus, assuming single-error patterns,\\nwe may formulate the error patterns listed in the right-hand column of Table 10.2. The\\ncorresponding eight syndromes, listed in the left-hand column, are calculated in\\naccordance with (10.20). The zero syndrome signifies no transmission errors.\\nSuppose, for example, the code vector [1110010] is sent and the received vector is\\n[1100010] with an error in the third bit. Using (10.19), the syndrome is calculated to be\\nFrom Table 10.2 the corresponding coset leader (i.e., error pattern with the highest\\nprobability of occurrence) is found to be [0010000], indicating correctly that the third bit\\nof the received vector is erroneous. Thus, adding this error pattern to the received vector,\\nin accordance with (10.26), yields the correct code vector actually sent.\\nTable 10.2 Decoding table for the (7,4)\\nHamming code defined in Table 10.1\\nSyndrome Error pattern 000 100 010 001 110 011 111 101 0000000 1000000 0100000 0010000 0001000 0000100 0000010 0000001 0 1 0 0 0 1 0 1 1 + + 0 0 0 = s 1100010   1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 = 0 0 1 =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 612,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '5 Cyclic Codes 593 10.5 Cyclic Codes Cyclic codes form a subclass of linear block codes. Indeed, many of the important linear\\nblock codes discovered to date are either cyclic codes or closely related to cyclic codes.\\nAn advantage of cyclic codes over most other types of codes is that they are easy to\\nencode. Furthermore, cyclic codes possess a well-defined mathematical structure, which\\nhas led to the development of very efficient decoding schemes for them.\\nA binary code is said to be a cyclic code if it exhibits two fundamental properties: PROPERTY 1 Linearity Property\\nThe sum of any two codewords in the code is also a codeword.\\nPROPERTY\\nCyclic Property\\nAny cyclic shift of a codeword in the code is also a codeword.\\nProperty 1 restates the fact that a cyclic code is a linear block code (i.e., it can be described\\nas a parity-check code). To restate Property 2 in mathematical terms, let the n-tuple denote a codeword of an linear block code. The code is a cyclic\\ncode if the n-tuples\\nare all codewords in the code.\\nTo develop the algebraic properties of cyclic codes, we use the elements of a codeword to define the code polynomial\\n(10.27)\\nwhere X is an indeterminate. Naturally, for binary codes, the coefficients are 1s and 0s.\\nEach power of X in the polynomial represents a one-bit shift in time. Hence,\\nmultiplication of the polynomial by X may be viewed as a shift to the right. The key\\nquestion is: How do we make such a shift cyclic? The answer to this question is addressed\\nnext.\\nLet the code polynomial in (10.27) be multiplied by Xi, yielding\\nRecognizing, for example, that in modulo-2 addition, we may\\nmanipulate the preceding equation into the following compact form:\\n(10.28)\\nwhere the polynomial q(X) is defined by',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 613,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Recognizing, for example, that in modulo-2 addition, we may\\nmanipulate the preceding equation into the following compact form:\\n(10.28)\\nwhere the polynomial q(X) is defined by\\n(10.29) c0 c1 cn 1 -    n k    cn 1 - c0 cn 2 -      cn 2 - cn 1 - cn 3 -       c1 c2 cn 1 - c0       c0 c1 cn 1 -    c X  c0 c1X c2X2  cn 1 - Xn 1 - + + + + = c X  c X  c X  Xic X  c0Xi c1Xi 1 +  cn i - 1 - Xn 1 -  cn 1 - Xn i 1 - + + + + + + = cn i - cn i - + 0 = Xic X  q X Xn 1 +   c iX  + = q X  cn i - cn i - 1 + X  cn 1 - Xi 1 - + + + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 613,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '594 Chapter 10 Error-Control Coding As for the polynomial in (10.28), it is recognized as the code polynomial of the\\ncodeword obtained by applying i cyclic shifts to the\\ncodeword\\n. Moreover, from (10.28) we readily see that is the remainder that results from dividing by\\n. We may thus\\nformally state the cyclic property in polynomial notation as follows:\\nIf c(X) is a code polynomial, then the polynomial\\n(10.30)\\nis also a code polynomial for any cyclic shift i; the term mod is the abbreviation\\nfor modulo.\\nThe special form of polynomial multiplication described in (10.30) is referred to as\\nmultiplication modulo\\n. In effect, the multiplication is subject to the constraint\\n, the application of which restores the polynomial to order for all\\n. Note that, in modulo-2 arithmetic, has the same value as\\n. Generator Polynomial The polynomial and its factors play a major role in the generation of cyclic codes.\\nLet g(X) be a polynomial of degree that is a factor of\\n; as such, g(X) is the\\npolynomial of least degree in the code. In general, g(X) may be expanded as follows:\\n(10.31)\\nwhere the coefficient is equal to 0 or 1 for\\n. According to this\\nexpansion, the polynomial g(X) has two terms with coefficient 1 separated by\\nterms. The polynomial g(X) is called the generator polynomial of a cyclic code. A cyclic\\ncode is uniquely determined by the generator polynomial g(X) in that each code\\npolynomial in the code can be expressed in the form of a polynomial product as follows:\\n(10.32)\\nwhere a(X) is a polynomial in X with degree\\n. The c(X) so formed satisfies the\\ncondition of (10.30) since g(X) is a factor of\\n.\\nSuppose we are given the generator polynomial g(X) and the requirement is to encode\\nthe message sequence into an systematic cyclic code. That is,\\nthe message bits are transmitted in unaltered form, as shown by the following structure for\\na codeword (see Figure 10.4):\\nLet the message polynomial be defined by',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 614,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'the message bits are transmitted in unaltered form, as shown by the following structure for\\na codeword (see Figure 10.4):\\nLet the message polynomial be defined by\\n(10.33) and let (10.34) c iX cn i - cn 1 - c0 c1 cn i - 1 -         c0 c1 cn i - 1 - cn i - cn 1 -       c iX  Xic X  Xn 1 +   c iX  Xic X mod Xn 1 +   = Xn 1 + Xn 1 = Xic X  n 1 - i n  Xn 1 + Xn 1 - Xn 1 + n k - Xn 1 + g X  1 giXi Xn k - + i 1 = n k - 1 -  + = gi i 1 n k - 1 -   = n k - 1 - c X  a X g X  = k 1 - Xn 1 + m0 m1 mk 1 -      n k    b0 b1 bn k - 1 -    m0 m1 mk 1 -       n k parity-check bits - k message bits                 m X  m0 m1X  mk 1 - Xk 1 - + + + = b X  b0 b1X  bn k - 1 - Xn k - 1 - + + + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 614,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '5 Cyclic Codes Then, according to (10.1), we want the code polynomial to be in the form\\n(10.35)\\nTo this end, the use of (10.32) and (10.35) yields\\nEquivalently, invoking modulo-2 addition, we may also write\\n(10.36)\\nEquation (10.36) states that the polynomial b(X) is the remainder left over after dividing by g(X).\\nWe may now summarize the steps involved in the encoding procedure for an (n,k)\\ncyclic code, assured of a systematic structure. Specifically, we proceed as follows:\\nStep 1:\\nPremultiply the message polynomial m(X) by Xn-k.\\nStep 2:\\nDivide by the generator polynomial g(X), obtaining the remainder b(X).\\nStep 3:\\nAdd b(X) to Xn-km(X), obtaining the code polynomial c(X).\\nParity-Check Polynomial\\nAn (n,k) cyclic code is uniquely specified by its generator polynomial g(X) of order (n - k).\\nSuch a code is also uniquely specified by another polynomial of degree k, which is called\\nthe parity-check polynomial, defined by\\n(10.37)\\nwhere the coefficients hi are 0 or 1. The parity-check polynomial h(X) has a form similar\\nto the generator polynomial, in that there are two terms with coefficient 1, but separated by terms.\\nThe generator polynomial g(X) is equivalent to the generator matrix G as a description\\nof the code. Correspondingly, the parity-check polynomial h(X) is an equivalent\\nrepresentation of the parity-check matrix H. We thus find that the matrix relation HGT =\\npresented in (10.15) for linear block codes corresponds to the relationship\\n(10.38)\\nAccordingly, we may make the statement:\\nThe generator polynomial g(X) and the parity-check polynomial h(X) are\\nfactors of the polynomial Xn + 1, as shown by\\n(10.39)\\nThis statement provides the basis for selecting the generator or parity-check polynomial of\\na cyclic code. In particular, if g(X) is a polynomial of degree (n - k) and it is also a factor\\nof Xn + 1, then g(X) is the generator polynomial of an (n,k) cyclic code. Equivalently, if',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 615,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'a cyclic code. In particular, if g(X) is a polynomial of degree (n - k) and it is also a factor\\nof Xn + 1, then g(X) is the generator polynomial of an (n,k) cyclic code. Equivalently, if\\nc X  b X  Xn k - m X  + = a X g X  b X  Xn k - m X  + = Xn k - m X  g X  ---------------------------\\na X  b X  g x  ------------ + = Xn k - m X  Xn k - m X  h X  1 hiXi Xk + i 1 = k 1 -  + = k 1 - g X h X mod Xn 1 +   0 = g X h X  Xn 1 + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 615,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '596 Chapter 10 Error-Control Coding h(X) is a polynomial of degree k and it is also a factor of Xn + 1, then h(X) is the parity-\\ncheck polynomial of an (n,k) cyclic code.\\nA final comment is in order. Any factor of Xn + 1 with degree (n - k) can be used as a\\ngenerator polynomial. The fact of the matter is that, for large values of n, the polynomial\\nXn + 1 may have many factors of degree n - k. Some of these polynomial factors generate\\ngood cyclic codes, whereas some of them generate bad cyclic codes. The issue of how to\\nselect generator polynomials that produce good cyclic codes is very difficult to resolve.\\nIndeed, coding theorists have expended much effort in the search for good cyclic codes.\\nGenerator and Parity-Check Matrices\\nGiven the generator polynomial g(X) of an (n,k) cyclic code, we may construct the\\ngenerator matrix G of the code by noting that the k polynomials g(X), Xg(X), , Xk-1g(X)\\nspan the code. Hence, the n-tuples corresponding to these polynomials may be used as\\nrows of the k-by-n generator matrix G.\\nHowever, the construction of the parity-check matrix H of the cyclic code from the\\nparity-check polynomial h(X) requires special attention, as described here. Multiplying\\n(10.39) by a(x) and then using (10.32), we obtain\\n(10.40)\\nThe polynomials c(X) and h(X) are themselves defined by (10.27) and (10.37) respectively,\\nwhich means that their product on the left-hand side of (10.40) contains terms with powers\\nextending up to\\n. On the other hand, the polynomial a(X) has degree or less,\\nthe implication of which is that the powers of do not appear in the\\npolynomial on the right-hand side of (10.40). Thus, setting the coefficients of in the expansion of the product polynomial c(X)h(X) equal to zero, we\\nobtain the following set of equations:\\n(10.41)\\nComparing (10.41) with the corresponding relation (10.16), we may make the following\\nimportant observation:\\nThe coefficients of the parity-check polynomial h(X) involved in the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 616,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '(10.41)\\nComparing (10.41) with the corresponding relation (10.16), we may make the following\\nimportant observation:\\nThe coefficients of the parity-check polynomial h(X) involved in the\\npolynomial multiplication described in (10.41) are arranged in reversed order\\nwith respect to the coefficients of the parity-check matrix H involved in forming\\nthe inner product of vectors described in (10.16).\\nThis observation suggests that we define the reciprocal of the parity-check polynomial as\\nfollows: (10.42) c X h X  a X  Xna X  + = n k 1 - + k 1 - Xk Xk 1 + Xn 1 -    Xk Xk 1 - Xn 1 -    n k - cihk j i - + i j = j k +  0 for 0 jnk - 1 -  = Xkh X 1 -   Xk 1 hiX i - X k - + i 1 = k 1 -  +         = 1 = hk 1 - Xi Xk + i 1 = k 1 -  +',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 616,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '5 Cyclic Codes which is also a factor of\\n. The n-tuples pertaining to the polynomials may now be used in rows of the\\n-by-n\\nparity-check matrix H.\\nIn general, the generator matrix G and the parity-check matrix H constructed in the\\nmanner described here are not in their systematic forms. They can be put into their\\nsystematic forms by performing simple operations on their respective rows, as illustrated\\nin Example 1.\\nEncoding of Cyclic Codes\\nEarlier we showed that the encoding procedure for an (n,k) cyclic code in systematic form\\ninvolves three steps:\\n\\nmultiplication of the message polynomial m(X) by Xn - k,\\n\\ndivision of Xn - km(X) by the generator polynomial g(X) to obtain the remainder\\nb(X), and',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 617,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'multiplication of the message polynomial m(X) by Xn - k,\\n\\ndivision of Xn - km(X) by the generator polynomial g(X) to obtain the remainder\\nb(X), and\\n\\naddition of b(X) to Xn - km(X) to form the desired code polynomial.\\nThese three steps can be implemented by means of the encoder shown in Figure 10.8,\\nconsisting of a linear feedback shift register with stages.\\nThe boxes in Figure 10.8 represent flip-flops, or unit-delay elements. The flip-flop is a\\ndevice that resides in one of two possible states denoted by 0 and 1. An external clock (not\\nshown in Figure 10.8) controls the operation of all the flip-flops. Every time the clock ticks,\\nthe contents of the flip-flops (initially set to the state 0) are shifted out in the direction of the\\narrows. In addition to the flip-flops, the encoder of Figure 10.8 includes a second set of logic\\nelements, namely adders, which compute the modulo-2 sums of their respective inputs.\\nFinally, the multipliers multiply their respective inputs by the associated coefficients. In\\nparticular, if the coefficient\\n, the multiplier is just a direct connection. If, on the\\nother hand, the coefficient\\n, the multiplier is no connection.\\nThe operation of the encoder shown in Figure 10.8 proceeds as follows:\\nThe gate is switched on. Hence, the k message bits are shifted into the channel. As\\nsoon as the k message bits have entered the shift register, the resulting bits in\\nthe register form the parity-check bits. (Recall that the parity-check bits are the same\\nas the coefficients of the remainder b(X).)\\nXn 1 + n k -   Xkh X 1 -  Xk 1 + h X 1 -  Xn 1 - h X 1 -      n k -   n k -   gi 1 = gi 0 = n k -   Figure 10.8 Encoder for an (n,k) cyclic code.\\ng1 g2 gn - k - 1 . . . . . . Flip-flop Modulo-2 adder Parity bits Gate Codeword Message bits',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 617,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '598 Chapter 10 Error-Control Coding 2. The gate is switched off, thereby breaking the feedback connections.\\nThe contents of the shift register are read out into the channel.\\nCalculation of the Syndrome\\nSuppose the codeword is transmitted over a noisy channel, resulting in the\\nreceived word\\n. From Section 10.3, we recall that the first step in the decoding\\nof a linear block code is to calculate the syndrome for the received word. If the syndrome is\\nzero, there are no transmission errors in the received word. If, on the other hand, the syndrome\\nis nonzero, the received word contains transmission errors that require correction.\\nIn the case of a cyclic code in systematic form, the syndrome can be calculated easily.\\nLet the received vector be represented by a polynomial of degree or less, as shown by\\nLet q(X) denote the quotient and s(X) denote the remainder, which are the results of\\ndividing r(X) by the generator polynomial g(X). We may therefore express r(X) as follows:\\n(10.43)\\nThe remainder s(X) is a polynomial of degree or less, which is the result of interest.\\nIt is called the syndrome polynomial because its coefficients make up the\\n-by-1\\nsyndrome s.\\nFigure 10.9 shows a syndrome calculator that is identical to the encoder of Figure 10.8\\nexcept for the fact that the received bits are fed into the stages of the feedback shift\\nregister from the left. As soon as all the received bits have been shifted into the shift\\nregister, its contents define the syndrome s.\\nThe syndrome polynomial s(X) has the following useful properties that follow from the\\ndefinition given in (10.43). PROPERTY 1 The syndrome of a received word polynomial is also the syndrome of the corresponding\\nerror polynomial.\\nGiven that a cyclic code with polynomial c(X) is sent over a noisy channel, the received\\nword polynomial is defined by\\nwhere e(X) is the error polynomial. Equivalently, we may write',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 618,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'error polynomial.\\nGiven that a cyclic code with polynomial c(X) is sent over a noisy channel, the received\\nword polynomial is defined by\\nwhere e(X) is the error polynomial. Equivalently, we may write\\nc0 c1 cn 1 -      r0 r1 rn 1 -    n 1 - r X  r0 r1X  rn 1 - Xn 1 - + + + = r X  q X g X  s X  + = n k - 1 - n k -   n k - r X  c X  e X  + = e X  r X  c X  + = Figure 10.9 Syndrome computer for (n, k) cyclic code. g1 g2 gn - k - 1 . . . . . . Flip-flop Modulo-2 adder Gate Received bits',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 618,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '5 Cyclic Codes Hence, substituting (10.32) and (10.43) into the preceding equation, we get\\n(10.44)\\nwhere the quotient is\\n. Equation (10.44) shows that s(X) is also the\\nsyndrome of the error polynomial e(X). The implication of this property is that when the\\nsyndrome polynomial s(X) is nonzero, the presence of transmission errors in the received\\nvector is detected.\\nPROPERTY\\nLet s(X) be the syndrome of a received word polynomial r(X). Then, the syndrome of\\nXr(X), representing a cyclic shift of r(X), is Xs(X).\\nApplying a cyclic shift to both sides of (10.43), we get\\n(10.45)\\nfrom which we readily see that Xs(X) is the remainder of the division of Xr(X) by g(X).\\nHence, the syndrome of Xr(X) is Xs(X) as stated. We may generalize this result by stating\\nthat if s(X) is the syndrome of r(X), then is the syndrome of . PROPERTY 3 The syndrome polynomial s(X) is identical to the error polynomial e(X), assuming that the\\nerrors are confined to the parity-check bits of the received word polynomial r(X).\\nThe assumption made here is another way of saying that the degree of the error\\npolynomial e(X) is less than or equal to\\n. Since the generator polynomial g(X)\\nis of degree\\n, by definition, it follows that (10.44) can only be satisfied if the\\nquotient u(X) is zero. In other words, the error polynomial e(X) and the syndrome\\npolynomial s(X) are one and the same. The implication of Property 3 is that, under the\\naforementioned conditions, error correction can be accomplished simply by adding the\\nsyndrome polynomial s(X) to the received vector r(X).\\nEXAMPLE\\nHamming Codes Revisited\\nTo illustrate the issues relating to the polynomial representation of cyclic codes, we\\nconsider the generation of a (7,4) cyclic code. With the block length n = 7, we start by\\nfactorizing X7+ 1 into three irreducible polynomials:\\nBy an irreducible polynomial we mean a polynomial that cannot be factored using only\\npolynomials with coefficients from the binary field. An irreducible polynomial of degree',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 619,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'By an irreducible polynomial we mean a polynomial that cannot be factored using only\\npolynomials with coefficients from the binary field. An irreducible polynomial of degree\\nm is said to be primitive if the smallest positive integer n for which the polynomial divides\\n. For the example at hand, the two polynomials and are primitive. Let us take\\nas the generator polynomial, whose degree equals the number of parity-check bits. This\\nmeans that the parity-check polynomial is given by\\nwhose degree equals the number of message bits\\n. e X  u X g X  s X  + = u X  a X  q X  + = Xr X  Xq X g X  Xs X  + = Xis X  Xir X  n k -   n k - 1 -   n k -   X7 1 + 1 X +  1 X2 X3 + +  1 X X3 + +   = Xn 1 is n + 2m 1 - = 1 X2 X3 + +   1 X X3 + +   g X  1 X X3 + + = h X  1 X +  1 X2 X3 + +   = 1 = X X2 X4 + + + k 4 =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 619,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '600 Chapter 10 Error-Control Coding Next, we illustrate the procedure for the construction of a codeword by using this\\ngenerator polynomial to encode the message sequence 1001. The corresponding message\\nvector is given by\\nHence, multiplying m(X) by\\n, we get\\nThe second step is to divide by g(X), the details of which (for the example at\\nhand) are given below:\\nNote that in this long division we have treated subtraction the same as addition since we\\nare operating in modulo-2 arithmetic. We may thus write\\nThat is, the quotient a(X) and remainder b(X) are as follows, respectively:\\nHence, from (10.35) we find that the desired code vector is\\nThe codeword is therefore 0111001. The four rightmost bits, 1001, are the specified\\nmessage bits. The three leftmost bits, 011, are the parity-check bits. The codeword thus\\ngenerated is exactly the same as the corresponding one shown in Table 10.1 for a (7,4)\\nHamming code.\\nWe may generalize this result by stating that:\\nAny cyclic code generated by a primitive polynomial is a Hamming code of\\nminimum distance 3.\\nWe next show that the generator polynomial g(X) and the parity-check polynomial h(X)\\nuniquely specify the generator matrix G and the parity-check matrix H, respectively.\\nTo construct the 4-by-7 generator matrix G, we start with four vectors represented by\\ng(X) and three cyclic-shifted versions of it, as shown by\\nm X  1 X3 + = Xn k - X3 = Xn k - m X  X3 X6 + = Xn k - m X  X2 + X X4 + X2 + X X4 X6 X4 + X3 X6 + X3 X3 + X X3 + X + 1 X3 X6 + 1 X X3 + + -------------------------\\nX X3 X X2 + 1 X X3 + + -------------------------\\n+ + = a X  X X3 + = b X  X = X2 + c X  b X  Xn k - m X  + = X = X2 X3 X6 + + +',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 620,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '5 Cyclic Codes The vectors g(X), Xg(X), X2g(X), and X3g(X) represent code polynomials in the (7,4)\\nHamming code. If the coefficients of these polynomials are used as the elements of the\\nrows of a 4-by-7 matrix, we get the following generator matrix:\\nClearly, the generator matrix so constructed is not in systematic form. We can put it into\\na systematic form by adding the first row to the third row, and adding the sum of the first\\ntwo rows to the fourth row. These manipulations result in the desired generator matrix:\\nwhich is exactly the same as that in Example 1.\\nWe next show how to construct the 3-by-7 parity-check matrix H from the parity-check\\npolynomial h(X). To do this, we first take the reciprocal of h(X), namely X4h(X-1). For the\\nproblem at hand, we form three vectors represented by X4h(X-1) and two shifted versions\\nof it, as shown by\\nUsing the coefficients of these three vectors as the elements of the rows of the 3-by-7\\nparity-check matrix, we get\\nHere again we see that the matrix is not in systematic form. To put it into a systematic\\nform, we add the third row to the first row to obtain\\nwhich is exactly the same as that of Example 1.\\ng X  1 X X3 + + = XgX X X2 X4 + + = X2g X  X2 X3 X5 + + = X3g X  X3 X4 X6 + + = G= 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 1 G G 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 = X4h X 1 -   1 X2 X3 X4 X5h X 1 -   + + + X X3 X4 X5 X6h X 1 -   + + + X2 X4 X5 X6 + + + = = = H 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 = H H 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 621,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '602 Chapter 10 Error-Control Coding Figure 10.10 shows the encoder for the (7,4) cyclic Hamming code generated by the\\npolynomial\\n. To illustrate the operation of this encoder, consider the\\nmessage sequence (1001). The contents of the shift register are modified by the incoming\\nmessage bits as in Table 10.3. After four shifts, the contents of the shift register, and\\ntherefore the parity-check bits, are (011). Accordingly, appending these parity-check bits\\nto the message bits (1001), we get the codeword (0111001); this result is exactly the same\\nas that determined earlier in Example 1.\\nFigure 10.11 shows the corresponding syndrome calculator for the (7,4) Hamming\\ncode. Let the transmitted codeword be (0111001) and the received word be (0110001);\\nthat is, the middle bit is in error. As the received bits are fed into the shift register, initially\\nset to zero, its contents are modified as in Table 10.4. At the end of the seventh shift, the\\nsyndrome is identified from the contents of the shift register as 110. Since the syndrome is\\nnonzero, the received word is in error. Moreover, from Table 10.2, we see that the error\\npattern corresponding to this syndrome is 0001000. This indicates that the error is in the\\nmiddle bit of the received words, which is indeed the case.\\nFigure 10.10 Encoder for the (7,4) cyclic code generated by g(X) = 1 + X + X3.\\nTable 10.3 Contents of the shift register in the encoder\\nof Figure 10.10 for message sequence (1001)\\nShift\\nInput bit\\nContents of shift register\\n000 (initial state) 1 1 110 2 0 011 3 0 111 4 1 011 Flip-flop Modulo-2 adder Parity bits Gate Codeword Message bits g X  1 X X3 + + = Figure 10.11 Syndrome calculator for the (7,4) cyclic code generated by the polynomial g(X) = 1 + X + X3. Modulo-2 adder Flip-flop Received bits Gate',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 622,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '5 Cyclic Codes 603 EXAMPLE 3 Maximal-Length Codes For any positive integer\\n, there exists a maximal-length code4 with the following\\nparameters: block length: n = 2m - 1 number of message bits:\\nk = m\\nminimum distance:\\nMaximal-length codes are generated by vectors of the form\\n(10.46)\\nwhere h(X) is any primitive polynomial of degree m. Earlier we stated that any cyclic code\\ngenerated by a primitive polynomial is a Hamming code of minimum distance 3 (see Exam-\\nple 2). It follows, therefore, that maximal-length codes are the dual of Hamming codes.\\nThe polynomial h(X) defines the feedback connections of the encoder. The generator pol-\\nynomial g(X) defines one period of the maximal-length code, assuming that the encoder is in\\nthe initial state 00  01. To illustrate this, consider the example of a (7,3) maximal-length\\ncode, which is the dual of the (7,4) Hamming code described in Example 2. Thus, choosing\\nwe find that the generator polynomial of the (7,3) maximal-length code is\\nFigure 10.12 shows the encoder for the (7,3) maximal-length code. The period of the code\\nis\\n. Thus, assuming that the encoder is in the initial state 001, as indicated in Figure\\n12, we find the output sequence is described by\\nTable 10.4 Contents of the syndrome calculator\\nin Figure 10.11 for the received word (0110001)\\nShift\\nInput bit\\nContents of shift register\\n000 (initial state) 1 1 100 2 0 010 3 0 001 4 0 110 5 1 111 6 1 001 7 0 110 m 3  dmin 2m 1 - = g X  1 Xn + h X  --------------- = h X  1 X X3 + + = g X  1 X X2 X4 + + + = n 7 = 1 0 0 1 1 1 0 1 0 0 initial state g X  1 X X2 X4 + + + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 623,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '604 Chapter 10 Error-Control Coding This result is readily validated by cycling through the encoder of Figure 10.12.\\nNote that if we were to choose the other primitive polynomial\\nfor the (7,3) maximal-length code, we would simply get the image of the code described\\nabove, and the output sequence would be reversed in time.\\nReed-Solomon Codes\\nA study of cyclic codes for error control would be incomplete without a discussion of\\nReed-Solomon codes,5 albeit briefly.\\nUnlike the cyclic codes considered in this section, Reed-Solomon codes are nonbinary\\ncodes. A cyclic code is said to be nonbinary in that given the code vector\\nthe coefficients are not binary 0 or 1. Rather, the are themselves made up of\\nsequences of 0s and 1s, with each sequence being of length k. A Reed-Solomon code is\\ntherefore said to be a q-ary code, which means that the size of the alphabet used in\\nconstruction of the code is\\n. To be specific, a Reed-Solomon (n,k) code is used to\\nencode m-bit symbols into blocks consisting of symbols; that is,\\nbits, where\\n. Thus, the encoding algorithm expands a block of k symbols to n\\nsymbols by adding redundant symbols. When m is an integer power of 2, the m-bit\\nsymbols are called bytes. A popular value of m is 8; indeed, 8-bit Reed-Solomon codes are\\nextremely powerful.\\nA t-error-correcting Reed-Solomon code has the following parameters:\\nblock length message size parity-check size minimum distance The block length of the Reed-Solomon code is one less than the size of a code symbol,\\nand the minimum distance is one greater than the number of parity-check symbols. Reed-\\nSolomon codes make highly efficient use of redundancy; block lengths and symbol sizes\\nFigure 10.12 Encoder for the (7,3) maximal-length code;\\nthe initial state of the encoder is shown in the figure.\\n0 0 Modulo-2 adder Flip-flop 1 h X  1 X2 X3 + + = c c0 c1 cn 1 -      = ci  i 0 = n 1 - ci q 2k = n 2m 1 - = m 2m 1 -   m 1  n k - n 2m 1 symbols - = k symbols n k - 2t symbols = dmin 2t 1 symbols + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 624,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '6 Convolutional Codes can be adjusted readily to accommodate a wide range of message sizes. Moreover, Reed-\\nSolomon codes provide a wide range of code rates that can be chosen to optimize\\nperformance, and efficient techniques are available for their use in certain practical\\napplications. In particular, a distinctive feature of Reed-Solomon codes is their ability to\\ncorrect bursts of errors, hence their application in wireless communications to combat the\\nfading phenomenon. 10.6 Convolutional Codes In block coding, the encoder accepts a k-bit message block and generates an n-bit\\ncodeword, which contains n - k parity-check bits. Thus, codewords are produced on a\\nblock-by-block basis. Clearly, provision must be made in the encoder to buffer an entire\\nmessage block before generating the associated codeword. There are applications,\\nhowever, where the message bits come in serially rather than in large blocks, in which\\ncase the use of a buffer may be undesirable. In such situations, the use of convolutional\\ncoding may be the preferred method. A convolutional coder generates redundant bits by\\nusing modulo-2 convolutions; hence the name convolutional codes6.\\nThe encoder of a binary convolutional code with rate 1n, measured in bits per symbol,\\nmay be viewed as a finite-state machine that consists of an M-stage shift register with\\nprescribed connections to n modulo-2 adders and a multiplexer that serializes the outputs\\nof the adders. A sequence of message bits produces a coded output sequence of length\\nn(L + M) bits, where L is the length of the message sequence. The code rate is therefore\\ngiven by (10.47) Typically, we have L  M, in which case the code rate is approximately defined by bitssymbol\\n(10.48)\\nAn important characteristic of a convolutional code is its constraint length, which we\\ndefine as follows:\\nThe constraint length of a convolutional code, expressed in terms of message\\nbits, is the number of shifts over which a single incoming message bit can',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 625,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'define as follows:\\nThe constraint length of a convolutional code, expressed in terms of message\\nbits, is the number of shifts over which a single incoming message bit can\\ninfluence the encoder output.\\nIn an encoder with an M-stage shift register, the memory of the encoder equals M message\\nbits. Correspondingly, the constraint length, denoted by , equals M + 1 shifts that are\\nrequired for a message bit to enter the shift register and finally come out.\\nFigure 10.13 shows a convolutional encoder with the number of message bits n = 2 and\\nconstraint length = 3. In this example, the code rate of the encoder is 12. The encoder\\noperates on the incoming message sequence, one bit at a time, through a convolution\\nprocess; it is therefore said to be a nonsystematic code.\\nr L nL M +   ----------------------\\n= 1 n 1 M L  +   -----------------------------\\nbits/symbol = r 1 n---',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 625,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '606 Chapter 10 Error-Control Coding Each path connecting the output to the input of a convolutional encoder may be\\ncharacterized in terms of its impulse response, defined as follows:\\nThe impulse response of a particular path in the convolutional encoder is the\\nresponse of that path in the encoder to symbol 1 applied to its input, with each\\nflip-flop in the encoder set initially to the zero state.\\nEquivalently, we may characterize each path in terms of a generator polynomial, defined\\nas the unit-delay transform of the impulse response. To be specific, let the generator\\nsequence denote the impulse response of the ith path, where the\\ncoefficients equal symbol 0 or 1. Correspondingly, the generator\\npolynomial of the ith path is defined by\\n(10.49)\\nwhere D denotes the unit-delay variable. The complete convolutional encoder is described\\nby the set of generator polynomials\\n.\\nEXAMPLE\\nConvolutional Encoder\\nConsider again the convolutional encoder of Figure 10.13, which has two paths numbered\\n1 and 2 for convenience of reference. The impulse response of path 1 (i.e., upper path) is\\n(1, 1, 1). Hence, the generator polynomial of this path is\\ng(1)(D) = 1 + D + D2\\nThe impulse response of path 2 (i.e., lower path) is (1, 0, 1). The generator polynomial of\\nthis second path is\\ng(2)(D) = 1 + D2\\nFor an incoming message sequence given by (10011), for example, we have the\\npolynomial representation\\nm(D) = 1 + D3 + D4\\nFigure 10.13\\nConstraint length-3, rate -1/2\\nconvolutional encoder.\\nInput Output Flip-flop Modulo-2 adder Path 2 Path 1 (g0 ig1 ig2 igM i)     g0 ig1 ig2 igM i     g iD   g0 i g1 iD g2 iD2  gM iDM + + + + = g iD    i 1 = M',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 626,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '6 Convolutional Codes As with Fourier transformation, convolution in the time domain is transformed into\\nmultiplication in the D-domain. Hence, the output polynomial of path 1 is given by\\nwhere it is noted that the sums D4 + D4 and D5 + D5 are both zero in accordance with the\\nrules of binary arithmetic. We therefore immediately deduce that the output sequence of\\npath 1 is (1111001). Similarly, the output polynomial of path 2 is given by\\nThe output sequence of path 2 is therefore (1011111). Finally, multiplexing the two output\\nsequences of paths 1 and 2, we get the encoded sequence\\nc = (11, 10, 11, 11, 01, 01, 11)\\nNote that the message sequence of length L = 5 bits produces an encoded sequence of\\nlength n(L + - 1) = 14 bits. Note also that for the shift register to be restored to its initial\\nall-zero state, a terminating sequence of - 1 = 2 zeros is appended to the last input bit of\\nthe message sequence. The terminating sequence of - 1 zeros is called the tail of the\\nmessage.\\nCode Tree, Trellis Graph, and State Graph\\nTraditionally, the structural properties of a convolutional encoder are portrayed in\\ngraphical form by using any one of three equivalent graphs: code tree, trellis graph, and\\nstate graph.\\nAlthough, indeed, these three graphical representations of a convolutional encoder look\\ndifferent, their compositions follow the same underlying rule:\\nA code branch produced by input bit 0 is drawn as a solid line, whereas a code\\nbranch produced by input bit 1 is a dashed line.\\nHereafter, we refer to this convention as the graphical rule of a convolutional encoder.\\nWe will use the convolutional encoder of Figure 10.13 as a running example to\\nillustrate the insights that each one of these three diagrams provides.\\nCode Tree\\nWe begin the graphical representation of a convolutional encoder with the code tree of\\nFigure 10.14. Each branch of the tree represents an input bit, with the corresponding pair',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 627,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Code Tree\\nWe begin the graphical representation of a convolutional encoder with the code tree of\\nFigure 10.14. Each branch of the tree represents an input bit, with the corresponding pair\\nof output bits indicated on the branch. The convention used to distinguish the input bits\\nand 1 follows the graphical rule described above. Thus, a specific path in the tree is traced\\nfrom left to right in accordance with the message sequence. The corresponding coded bits\\nc 1 D   g 1 D  m D   = 1 D D2 + +  1  D3 + = D4 + 1 D D2 D3 D6 + + + + = c 2 D   g 2 D  m D   = 1 D2 +  1  D3 + = D4 + 1 D2 D3 D4 D5 D6 + + + + + =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 627,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '608 Chapter 10 Error-Control Coding on the branches of that path constitute the message sequence (10011) applied to the input\\nof the encoder of Figure 10.13. Following the procedure just described, we find that the\\ncorresponding encoded sequence is (11, 10, 11, 11, 01), which agrees with the first five\\npairs of bits in the encoded sequence {ci} that was derived in Example 4.\\nFigure 10.14 Code tree for the convolutional encoder of Figure 10.13.\\n00 11 00 00 00 00 11 10 01 aab 11 00 10 11 01 01 10 bcd 00 11 11 10 11 00 10 01 cab 11 00 01 01 10 01 10 dcd 00 11 00 11 10 11 11 10 01 aab 11 00 10 00 01 01 10 bcd 00 00 11 11 01 01 00 10 01 cab 11 01 10 10 01 10 dcd 0',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 628,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '6 Convolutional Codes Trellis Graph\\nFrom Figure 10.14, we observe that the tree becomes repetitive after the first three\\nbranches. Indeed, beyond the third branch, the two nodes labeled a are identical and so are\\nall the other node pairs that are identically labeled. We may establish this repetitive\\nproperty of the tree by examining the associated encoder of Figure 10.13. The encoder has\\nmemory M = - 1 = 2 message bits. We therefore find that, when the third message bit\\nenters the encoder, the first message bit is shifted out of the register. Consequently, after the\\nthird branch, the message sequences (100 m3m4) and (000 m3m4) generate the same\\ncode symbols, and the pair of nodes labeled a may be joined together. The same reasoning\\napplies to the other nodes in the code tree. Accordingly, we may collapse the code tree of\\nFigure 10.14 into the new form shown in Figure 10.15, which is called a trellis. It is so\\ncalled since a trellis is a treelike structure with re-emerging branches. The convention used\\nin Figure 10.15 to distinguish between input symbols 0 and 1 is as follows:\\nA code branch in a trellis produced by input binary symbol 0 is drawn as a solid\\nline, whereas a code branch produced by an input 1 is drawn as a dashed line.\\nAs before, each message sequence corresponds to a specific path through the trellis. For\\nexample, we readily see from Figure 10.15 that the message sequence (10011) produces\\nthe encoded output sequence (11, 10, 11, 11, 01), which agrees with our previous result.\\nThe Notion of State\\nIn conceptual terms, a trellis is more instructive than a tree. We say so because it brings out\\nexplicitly the fact that the associated convolutional encoder is in actual fact a finite-state\\nmachine. Basically, such a machine consists of a tapped shift register and, therefore, has a\\nfinite state; hence the name of the machine. Thus, we may conveniently say the following:\\nThe state of a rate 1/n convolutional encoder is determined by the smallest',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 629,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'finite state; hence the name of the machine. Thus, we may conveniently say the following:\\nThe state of a rate 1/n convolutional encoder is determined by the smallest\\nnumber of message bits stored in memory (i.e., the shift register).\\nFor example, the convolutional encoder of Figure 10.13 has a shift register made up of two\\nmemory cells. With the message bit stored in each memory cell being 0 or 1, it follows\\nthat this encoder can assume any one of 22 = 4 possible states, as described in Table 10.5.\\nFigure 10.15 Trellis for the convolutional encoder of Figure 10.13.\\n 00 00 00 00 11 11 11 11 00 1 2 3 4 5 L - 1 L L + 1 L + 2 00 11 00 10 10 10 10 00 abcd Level j = 0 11 00 00 00 10 10 10 10 10 01 01 01 01 01 01 01 01 01 11 11 11 00 11 11',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 629,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '610 Chapter 10 Error-Control Coding In describing a convolutional encoder, the notion of state is important in the following\\nsense:\\nGiven the current message bit and the state of the encoder, the codeword\\nproduced at the output of the encoder is completely determined.\\nTo illustrate this statement, consider the general case of a rate 1n convolutional encoder of\\nconstraint length . Let the state of the encoder at time-unit j be denoted by\\nThe jth codeword cj is completely determined by the state S together with the current\\nmessage bit mj.\\nNow that we understand the notion of state, the trellis graph of the simple convolutional\\nencoder of Figure 10.13 for is presented in Figure 10.15. From this latter figure, we\\nnow clearly see a unique characteristic of the trellis diagram:\\nThe trellis depicts the evolution of the convolutional encoders state across time.\\nTo be more specific, the first time-steps correspond to the encoders departure\\nfrom the initial zero state and the last time-steps correspond to the encoders\\nreturn to the initial zero state. Naturally, not all the states of the encoder can be reached in\\nthese two particular portions of the trellis. However, in the central portion of the trellis, for\\nwhich time-unit j lies in the range\\n, where L is the length of the incoming\\nmessage sequence, we do see that all the four possible states of the encoder are reachable.\\nNote also that the central portion of the trellis exhibits a fixed periodic structure, as\\nillustrated in Figure 10.16a.\\nState Graph\\nThe periodic structure characterizing the trellis leads us next to the state diagram of a\\nconvolutional encoder. To be specific, consider a central portion of the trellis\\ncorresponding to times j and j  1. We assume that for j2 in the example of Figure\\n13, it is possible for the current state of the encoder to be a, b, c, or d. For convenience\\nof presentation, we have reproduced this portion of the trellis in Figure 10.16a. The left',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 630,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '13, it is possible for the current state of the encoder to be a, b, c, or d. For convenience\\nof presentation, we have reproduced this portion of the trellis in Figure 10.16a. The left\\nnodes represent the four possible current states of the encoder, whereas the right nodes\\nrepresent the next states. Clearly, we may coalesce the left and right nodes. By so doing,\\nwe obtain the state graph of the encoder, shown in Figure 10.16b. The nodes of the figure\\nrepresent the four possible states of the encoder a, b, c, and d, with each node having two\\nincoming branches and two outgoing branches, following the graphical rule described\\npreviously.\\nTable 10.5 State table for the\\nconvolutional encoder of Figure 10.13\\nState Binary description abcd 00 10 01 11  S mj 1 - mj 2 - m  j  - 1 +     =  3 =  1 - 2 =  1 - 2 =  1 j L  -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 630,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '6 Convolutional Codes The binary label on each branch represents the encoders output as it moves from one\\nstate to another. Suppose, for example, the current state of the encoder is (01), which is\\nrepresented by node c. The application of input symbol 1 to the encoder of Figure 10.13\\nresults in the state (10) and the encoded output (00). Accordingly, with the help of this\\nstate diagram, we may readily determine the output of the encoder of Figure 10.13 for any\\nincoming message sequence. We simply start at state a, the initial all-zero state, and walk\\nthrough the state graph in accordance with the message sequence. We follow a solid\\nbranch if the input is bit 0 and a dashed branch if it is bit 1. As each branch is traversed, we\\noutput the corresponding binary label on the branch. Consider, for example, the message\\nsequence (10011). For this input, we follow the path abcabd, and therefore output the\\nsequence (11, 10, 11, 11, 01), which agrees exactly with our previous result. Thus, the\\ninput-output relation of a convolutional encoder is also completely described by its state\\ngraph.\\nRecursive Systematic Convolutional Codes\\nThe convolutional codes described thus far in this section have been feedforward\\nstructures of the nonsystematic variety. There is another type of linear convolutional codes\\nthat are the exact opposite, being recursive as well as systematic; they are called recursive\\nsystematic convolutional (RSC) codes.\\nFigure 10.16 (a) A portion of the central part of the trellis for the encoder of\\nFigure 10.13. (b) State graph of the convolutional encoder of Figure 10.13.\\na 00 a b 11 bccdbdacjj + 1 Depth 10 d 01 01 10 00 11 10 10 00 00 (b) (a) 01 11 11',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 631,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '612 Chapter 10 Error-Control Coding Figure 10.17 illustrates a simple example of an RSC code, two distinguishing features\\nof which stand out in the figure:\\nThe code is systematic, in that the incoming message vector mj at time-unit j defines\\nthe systematic part of the code vector cj at the output of the encoder.\\nThe code is recursive by virtue of the fact that the other constituent of the code\\nvector, namely the parity-check vector bj, is related to the message vector mj by the\\nmodulo-2 recursive equation\\n(10.50)\\nwhere bj-1 is the past value of bj stored in the memory of the encoder.\\nFrom an analytic point of view, in studying RSC codes, it is more convenient to work in\\nthe transform D-domain than the time domain. By definition, we have\\n(10.51)\\nand therefore rewrite (10.50) in the equivalent form:\\n(10.52)\\nwhere the transfer function 1(1 + D) operates on mj to produce bj. With the code vector cj\\nconsisting of the message vector mj followed by the parity-check vector bj, we may\\nexpress the code vector cj produced in response to the message vector mj as follows:\\n(10.53)\\nIt follows, therefore, that the code generator for the RSC code of Figure 10.17 is given by\\nthe matrix\\n(10.54)\\nGeneralizing, we may now make the statement:\\nFor recursive systematic convolutional codes, the transform-domain matrix\\nG(D) is easier to use as the code generator than the corresponding time-domain\\nmatrix G whose entries contain sequences of infinite length.\\nFigure 10.17 Example of a recursive systematic convolutional (RSC) encoder.\\nInput message vector mj Output code vector cj cj(0) cj(1) mj bj 1 - + bj = bj 1 - D bj   = bj 1 1 D + ------------- mj   = cj mj bj    = 1 1 1 D + -------------      = mj G D   1 1 1 D + -------------      =',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 632,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '7 Optimum Decoding of Convolutional Codes The same statement applies equally well to the parity-check generator H(D) compared\\nwith its time-domain counterpart H.\\nThe rationale behind making convolutional codes recursive is to feed one or more of\\nthe tap-outputs in the shift register back to the encoder input, which, in turn, makes the\\ninternal state of the shift register depend on past outputs. This modification, compared\\nwith a feedforward convolutional code, affects the behavior of error patterns in a profound\\nway, which is emphasized in the following statement:\\nA single error in the systematic bits of an RSC code produces an infinite\\nnumber of parity-check errors due to the use of feedback in the encoder.\\nThis property of recursive convolutional codes turns out to be one of the key factors\\nbehind the outstanding performance achieved by the class of turbo codes, to be discussed\\nin Section 10.12. Therein, we shall see that feedback plays a key role not only in the\\nencoder of turbo codes but also the decoder. For reasons that will become apparent later,\\nfurther work on turbo codes will be deferred to Section 10.12. Optimum Decoding of Convolutional Codes\\nIn the meantime, we resume the discussion on convolutional codes whose encoders are of\\nthe feedforward variety, aimed at the development of two different decoding algorithms,\\neach of which is optimum according to a criterion of its own.\\nThe first algorithm is the maximum likelihood (ML) decoding algorithm; the decoder is\\nitself referred to as the maximum likelihood decoder (maximum likelihood estimation was\\ndiscussed in Chapter 3). A distinctive feature of this decoder is that it produces a codeword\\nas output, the conditional probability of which is always maximized on the assumption\\nthat each codeword in the code is equiprobable. From Chapter 3 on probability theory, we\\nrecall that the conditional probability density function of a random variable X given a',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 633,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'that each codeword in the code is equiprobable. From Chapter 3 on probability theory, we\\nrecall that the conditional probability density function of a random variable X given a\\nquantity  can be rethought as the likelihood function of  with that function being\\ndependent on X, given a parameter . We may therefore make the statement:\\nIn the maximum likelihood decoding of a convolutional code, the metric to be\\nmaximized is the likelihood function of a codeword, expressed as a function of\\nthe noisy channel output.\\nThe second algorithm is the maximum a posteriori (MAP) probability decoding algorithm;\\nthe decoder is correspondingly referred to as a MAP decoder. In light of this second\\nalgorithms name, we may make the statement:\\nIn MAP decoding of a convolutional code, the metric to be maximized is the\\nposterior of a codeword, expressed as the product of the likelihood function of a\\ngiven bit and the a priori probability of that bit.\\nThese two decoding algorithms, optimal in accordance with their own respective criteria,\\nare distinguished from each other as follows:\\nThe ML decoding algorithm produces the most likely codeword as its output.\\nOn the other hand, the MAP decoding algorithm operates on the received\\nsequence on a bit-by-bit basis to produce the most likely symbol as output.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 633,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '614 Chapter 10 Error-Control Coding Stated in another way, we may say:\\nThe ML decoder minimizes the probability of selecting the wrong codeword,\\nwhereas the MAP decoder minimizes the decoded BER.\\nTypically, the ML decoder is simpler to implement; hence its popular use in practice.\\nHowever, the MAP decoding algorithm is preferred over the ML decoding algorithm in\\nthe following two situations:\\nThe information bits are not equally likely.\\nIterative decoding is used in the receiver, in which case the a priori probabilities of\\nthe message bits change from one iteration to the next; such a situation arises in\\nturbo decoding, which is discussed in Section 10.12.\\nApplications of the Two Decoding Algorithms\\nThe ML decoding algorithm is applied to convolutional codes in Section 10.8; in so doing,\\nwe are, in effect, opting for a simple approach to decode convolutional codes. This simple\\napproach is also applicable to another class of codes, called trellis-coded modulation,\\nwhich is discussed in Section 10.15.\\nThen, in Section 10.9 we move on to study the MAP decoding algorithm; the length of\\nthat section and the illustrative example in Section 10.10 are testimony to the complexity\\nof this second approach to decoding convolutional codes. Equipped with the MAP\\nalgorithm and its modified forms, Section 10.12 and 10.13 discuss their application to\\nturbo codes. It is in the material covered in those two sections that we find the practical\\nbenefits of feedback in decoding turbo codes. Maximum Likelihood Decoding of Convolutional Codes\\nWe begin the discussion of decoding convolutional codes by first describing the\\nunderlying theory of maximum likelihood decoding. The description is best understood by\\nfocusing on a trellis that represents each time step in the decoding process with a separate\\nstate graph.\\nLet m denote a message vector and c denote the corresponding code vector applied by\\nthe encoder to the input of a discrete memoryless channel. Let r denote the received',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 634,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'state graph.\\nLet m denote a message vector and c denote the corresponding code vector applied by\\nthe encoder to the input of a discrete memoryless channel. Let r denote the received\\nvector, which, in practice, will invariably differ from the transmitted code vector c due to\\nadditive channel noise. Given the received vector r, the decoder is required to make an\\nestimate of the message vector m. Since there is a one-to-one correspondence between\\nthe message vector m and the code vector c, the decoder may equivalently produce an\\nestimate of the code vector. We may then put = m if and only if = c\\nOtherwise, a decoding error is committed in the receiver. The decoding rule for choosing\\nthe estimate , given the received vector r, is said to be optimum when the probability of\\ndecoding error is minimized. In light of the material presented on signaling over AWGN\\nchannel in Chapter 7, we may state:\\nm c m c c',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 634,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Maximum Likelihood Decoding of Convolutional Codes For equiprobable messages, the probability of decoding error is minimized if\\nthe estimate is chosen to maximize the log-likelihood function.\\nLet \\x02(r|c) denote the conditional probability of receiving r, given that c was sent. The\\nlog-likelihood function equals ln\\x02(r|c), where ln denotes the natural logarithm. The\\nmaximum likelihood decoder for decision making is described as follows:\\nChoose the estimate for which the log-likelihood function ln\\x02(r|c)\\nis maximum.\\nConsider next the special case of a binary symmetric channel. In this case, both the\\ntransmitted code vector c and the received vector r represent binary sequences of some\\nlength N. Naturally, these two sequences may differ from each other in some locations\\nbecause of errors due to channel noise. Let ci and ri denote the ith elements of c and r,\\nrespectively. We then have\\nCorrespondingly, the log-likelihood function is\\n(10.55)\\nThe term p(ri|ci) in (10.55) denotes a transition probability, which is defined by\\n(10.56)\\nSuppose also that the received vector r differs from the transmitted code vector c in\\nexactly d places in the codeword, By definition, the number d is the Hamming distance\\nbetween the vectors r and c. Hence, we may rewrite the log-likelihood function in (10.55)\\nas follows:\\n(10.57)\\nIn general, the probability of an error occurring is low enough for us to assume p < 12. We\\nalso recognize that is a constant for all c. Accordingly, we may restate the\\nmaximum-likelihood decoding rule for the binary symmetric channel as follows:\\nChoose the estimate that minimizes the Hamming distance between the\\nreceived vector r and the transmitted vector c.\\nThat is, for the binary symmetric channel, the maximum-likelihood decoder for a\\nconvolutional code reduces to a minimum distance decoder. In such a decoder, the\\nreceived vector r is compared with each possible transmitted code vector c, and the',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 635,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'convolutional code reduces to a minimum distance decoder. In such a decoder, the\\nreceived vector r is compared with each possible transmitted code vector c, and the\\nparticular one closest to r is chosen as the correct transmitted code vector. The term\\nc c \\x02 r c   p ri ci   i 1 = N  = \\x02 ln r c   p ln ri ci   i 1 = N  = p ri ci   p, if ri ci  1 p, - if ri ci =      = p ln r c   dpNd -   1 p -   ln + ln = d p 1 p - ------------     N 1 p -   ln + ln = Nln 1 p -   c',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 635,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '616 Chapter 10 Error-Control Coding closest is used in the sense of minimum number of differing binary symbols (i.e.,\\nHamming distance) between the code and received vectors under investigation.\\nThe Viterbi Algorithm\\nThe equivalence between maximum likelihood decoding and minimum distance decoding\\nfor the binary symmetric channel implies that we may decode a convolutional code by\\nchoosing a path in the code tree whose coded sequence differs from the received sequence\\nin the fewest number of places. Since a code tree is equivalent to a trellis, we may equally\\nlimit our choice to the possible paths in the trellis representation of the code. The reason\\nfor preferring the trellis over the tree is that the number of nodes at each time instant does\\nnot continue to grow as the number of incoming message bits increases; rather, it remains\\nconstant at\\n, where is the constraint length of the code.\\nConsider, for example, the trellis diagram of Figure 10.15 for a convolutional code with\\nrate r = 12 and constraint length = 3. We observe that, at time-unit j = 3, there are two\\npaths entering any of the four nodes in the trellis. Moreover, these two paths will be\\nidentical onward from that point. Clearly, a minimum distance decoder may make a\\ndecision at that point as to which of those two paths to retain, without any loss of\\nperformance. A similar decision may be made at time-unit j = 4, and so on. This sequence\\nof decisions is exactly what the Viterbi algorithm7 does as it walks through the trellis. The\\nalgorithm operates by computing a metric (i.e., discrepancy) for every possible path in the\\ntrellis; hence the following statement:\\nThe metric for a particular path is defined as the Hamming distance between the\\ncoded sequence represented by that path and the received sequence.\\nThus, for each node (state) in the trellis of Figure 10.15 the algorithm compares the two\\npaths entering the node. The path with the lower metric is retained and the other path is',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 636,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Thus, for each node (state) in the trellis of Figure 10.15 the algorithm compares the two\\npaths entering the node. The path with the lower metric is retained and the other path is\\ndiscarded. This computation is repeated for every time-unit j of the trellis in the range\\nM < j < L, where is the encoders memory and L is the length of the\\nincoming message sequence. The paths that are retained by the algorithm are called\\nsurvivor or active paths. For a convolutional code of constraint length\\n, for\\nexample, no more than survivors and their metrics will ever be stored. The\\nlist of paths computed in the manner just described is always guaranteed to\\ncontain the maximum-likelihood choice.\\nA difficulty that may arise in the application of the Viterbi algorithm is the possibility\\nthat when the paths entering a state are compared, their metrics are found to be identical.\\nIn such a situation, we simply make the choice by flipping a fair coin (i.e., simply make a\\nrandom guess).\\nTo sum up:\\nThe Viterbi algorithm is a maximum-likelihood decoder, which is optimum for\\nan AWGN channel as well as a binary symmetric channel.\\nThe algorithm proceeds in a step-by-step fashion, as summarized in Table 10.6.\\n2 1 -   M  1 - =  3 = 2 1 - 4 = 2 1 -',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 636,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '8 Maximum Likelihood Decoding of Convolutional Codes EXAMPLE\\nCorrect Decoding of Received All-Zero Sequence\\nSuppose that the encoder of Figure 10.13 generates an all-zero sequence that is sent over a\\nbinary symmetric channel and that the received sequence is (0100010000 ). There are\\ntwo errors in the received sequence due to noise in the channel: one in the second bit and\\nthe other in the sixth bit. We wish to show that this double-error pattern is correctable\\nthrough the application of the Viterbi decoding algorithm.\\nIn Figure 10.18 we show the results of applying the algorithm for time-unit j = 1, 2, 3,\\n4, 5. We see that for j = 2 there are (for the first time) four paths, one for each of the four\\nstates of the encoder. The figure also includes the metric of each path for each level in the\\ncomputation.\\nIn the left side of Figure 10.18, for time-unit j = 3 we show the paths entering each of\\nthe states, together with their individual metrics. In the right side of the figure we show the\\nfour survivors that result from application of the algorithm for time-unit j = 3, 4, 5.\\nExamining the four survivors in the figure for j = 5, we see that the all-zero path has the\\nsmallest metric and will remain the path of smallest metric from this point forward. This\\nclearly shows that the all-zero sequence is indeed the maximum likelihood choice of the\\nViterbi decoding algorithm, which agrees exactly with the transmitted sequence.\\nTable 10.6 Summary of the Viterbi algorithm\\nThe Viterbi algorithm is a maximum likelihood decoder, which is optimal for any\\ndiscrete memoryless channel. It proceeds in three basic steps. In computational terms,\\nthe so-called add-compare-select (ACS) operation in Step 2 is at the heart of the\\nViterbi algorithm.\\nInitialization\\nSet the all-zero state of the trellis to zero.\\nComputation Step 1: time-unit j\\nStart the computation at some time-unit j and determine the metric for the path that',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 637,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': 'Viterbi algorithm.\\nInitialization\\nSet the all-zero state of the trellis to zero.\\nComputation Step 1: time-unit j\\nStart the computation at some time-unit j and determine the metric for the path that\\nenters each state of the trellis. Hence, identify the survivor and store the metric for each\\none of the states.\\nComputation Step 2: time-unit j +\\nFor the next time-unit j + 1, determine the metrics for all paths that enter a state\\nwhere is the constraint length of the convolutional encoder; hence do the following:\\na. Add the metrics entering the state to the metric of the survivor at the preceding\\ntime-unit j;\\nb. Compare the metrics of all paths entering the state;\\nc. Select the survivor with the largest metric, store it along with its metric, and\\ndiscard all other paths in the trellis.\\nComputation Step 3: continuation of the search to convergence\\nRepeat Step 2 for time-unit j < L +\\n, where L is the length of the message sequence\\nand is the length of the termination sequence.\\nStop the computation once the time-unit j = L + is reached. 2 1 -  2 L L L',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 637,\n",
       "   'chunk_idx': 1}},\n",
       " {'chunk': '618 Chapter 10 Error-Control Coding Figure 10.18 Illustrating steps in the Viterbi\\nalgorithm for Example 5.\\n1 1 2 2 2 1 2 2 4 3 5 2 3 3 4 3 4 3 2 01 Received sequence j = 5 j = 4 00 01 00 00 0 1 1 2 2 2 2 2 3 3 3 2 3 Survivors 0 1 1 2 2 1 2 3 4 4 2 2 3 3 4 4 2 3 01 Received sequence 00 01 00 0 1 1 1 2 2 2 2 2 3 3 2 Survivors Survivors 0 j = 3 1 1 2 1 3 2 3 3 2 5 2 2 3 4 01 00 01 Received sequence 0 j = 2 1 1 3 2 2 1 01 00 Received sequence 0 1 1 2 1 3 2 2 3 2 0 j = 1 1 1 01 Received sequence',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 638,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '8 Maximum Likelihood Decoding of Convolutional Codes EXAMPLE\\nIncorrect Decoding of Received All-Zero Sequence\\nSuppose next that the received sequence is (1100010000 ), which contains three errors\\ncompared with the transmitted all-zero sequence; two of the errors are adjacent to each\\nother and the third is some distance away.\\nIn Figure 10.19, we show the results of applying the Viterbi decoding algorithm for\\nlevels j = 1, 2, 3, 4. We see that in this second example on Viterbi decoding the correct\\npath has been eliminated by time-unit j = 3. Clearly, a triple-error pattern is uncorrectable\\nby the Viterbi algorithm when applied to a convolutional code of rate 12 and constraint\\nlength\\n. The exception to this algorithm is a triple-error pattern spread over a time\\nspan longer than one constraint length, in which case it is likely to be correctable.\\nFigure 10.19\\nIllustrating breakdown\\nof the Viterbi algorithm\\nin Example 6.  3 = 0 11 00 2 2 4 1 0 j = 2 Received sequence Received sequence j = 3 0 11 00 2 01 0 2 1 3 1 1 Received sequence 11 00 01 0 2 3 1 3 3 3 00 j = 4 1 2 0 11 2 0 j = 1 Received sequence',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 639,\n",
       "   'chunk_idx': 0}},\n",
       " {'chunk': '620 Chapter 10 Error-Control Coding What Have We Learned from Examples 5 and 6?\\nIn Example 5 there were two errors in the received sequence, whereas in Example 6 there\\nwere three errors, two of which were in adjacent symbols and the third one was some\\ndistance away. In both examples the encoder used to generate the transmitted sequence\\nwas the same. The difference between the two examples was attributed to the fact that the\\nnumber of errors in Example 6 was beyond the error-correcting capability of the\\nmaximum likelihood decoding algorithm, which is the next topic for discussion.\\nFree Distance of a Convolutional Code\\nThe performance of a convolutional code depends not only on the decoding algorithm\\nused but also on the distance properties of the code. In this context, the most important\\nsingle measure of a convolutional codes ability to combat errors due to channel noise is\\nthe free distance of the code, denoted by dfree; it is defined as follows:\\nThe free distance of a convolutional code is given by the minimum Hamming\\ndistance between any two codewords in the code.\\nA convolutional code with free distance dfree can, therefore, correct t errors if, and only if,\\ndfree is greater than 2t.\\nThe free distance can be obtained quite simply from the state graph of the convolutional\\nencoder. Consider, for example, Figure 10.16b, which shows the state graph of the encoder\\nof Figure 10.13. Any nonzero code sequence corresponds to a complete path beginning\\nand ending at the 00 state (i.e., node a). We thus find it useful to split this node in the\\nmanner shown in the modified state graph of Figure 10.20, which may be viewed as a\\nsignal-flow graph with a single input and single output.\\nA signal-flow graph consists of nodes and directed branches; it operates by the\\nfollowing set of rules:\\nA branch multiplies the signal at its input node by the transmittance characterizing\\nthat branch.\\nA node with incoming branches sums the signals produced by all of those branches.',\n",
       "  'metadata': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 640,\n",
       "   'chunk_idx': 0}},\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "docs_dir = \"/home/lyus4/yuheng/All_in_LLM/LLM_wireless_communication_expert/chunks_size_2000_overlap_200_min_50.json\"\n",
    "with open(docs_dir, \"r\", encoding=\"utf-8\") as f:\n",
    "    docs = json.load(f)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc7413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA:  55%|    | 109/200 [03:01<05:05,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] QA parsing error: Answer is too long\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA:  58%|    | 117/200 [03:19<03:56,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] QA parsing error: Answer is too long\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating QA: 100%|| 200/200 [05:55<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "random.seed(42)\n",
    "#  docs  200 \n",
    "sampled_docs = random.sample(docs, k=200)  #  len(docs) >= 200\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for doc in tqdm(sampled_docs, desc=\"Generating QA\"):\n",
    "    output_QA = qa_generator_llm(doc['chunk'], client)\n",
    "    try:\n",
    "        question = output_QA.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "        answer = output_QA.split(\"Answer: \")[-1]\n",
    "        assert len(answer) < 500, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": doc['chunk'],\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": doc['metadata'],\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] QA parsing error: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a697a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context': \"2\\tInitiation A UE capable of NR sidelink communication or NR sidelink discovery or NR sidelink U2N relay operation or NR sidelink U2U relay operation or NR sidelink positioning that is in RRC_CONNECTED may initiate the procedure to indicate it is (interested in) receiving or transmitting NR sidelink communication or NR sidelink discovery or NR sidelink U2N relay operation or NR sidelink U2U relay operation or SL-PRS transmission/reception in several cases including upon successful connection establishment or resuming, upon change of interest, upon changing QoS profile(s), upon receiving UECapabilityInformationSidelink from the associated peer UE, upon RLC mode information updated from the associated peer UE or upon change to a PCell providing SIB12 including sl-ConfigCommonNR, or upon change to a PCell providing SIB23 including sl-PosConfigCommonNR. A UE capable of NR sidelink communication may initiate the procedure to request assignment of dedicated sidelink DRB configuration and transmission resources for NR sidelink communication transmission. A UE capable of NR sidelink communication may initiate the procedure to report to the network that a sidelink radio link failure, sidelink RRC reconfiguration failure or sidelink carrier failure has been declared. A UE capable of NR sidelink discovery may initiate the procedure to request assignment of dedicated resources for NR sidelink discovery transmission or NR sidelink discovery reception. A UE capable of U2N relay operation may initiate the procedure to report/update parameters for acting as U2N Relay UE or U2N Remote UE (including L2 U2N Remote UE's source L2 ID). A UE capable of U2U relay operation may initiate the procedure to report/update parameters for acting as U2U Relay UE or U2U Remote UE. A UE capable of NR sidelink positioning may initiate the procedure to request it is interested or no longer interested in either transmitting SL-PRS or receiving SL-PRS\",\n",
       "  'question': 'What triggers a UE capable of NR sidelink communication to initiate a procedure to report a sidelink radio link failure?\\n',\n",
       "  'answer': 'A UE capable of NR sidelink communication may initiate the procedure to report to the network that a sidelink radio link failure has been declared, ensuring network awareness of connectivity issues and facilitating potential recovery or reconfiguration processes.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 732}},\n",
       " {'context': 'Chapter\\nSignaling over Fading Channels\\nNaturally, OFDMA inherits the distinctive features of OFDM. In particular, OFDMA\\nis well suited for high data-rate transmissions over delay-dispersive channels, realized\\nby exploiting the principle of divide and conquer. Accordingly, OFDMA is\\ncomputationally efficient in using the FFT algorithm. Moreover, OFDMA lends itself\\nto the combined use of MIMO, hence the ability to improve spectral efficiency and\\ntake advantage of channel flexibility.\\nCode-division multiple access (CDMA), which distinguishes itself by exploiting the\\nunderlying principle of spread spectrum signals, discussed in Section 9.13. To be\\nspecific, through the combined process of spectrum spreading in the transmitter and\\ncorresponding spectrum despreading in the receiver, a certain amount of processing\\ngain is obtained, hence the ability of CDMA users to occupy the same channel\\nbandwidth. Moreover, CDMA provides a flexible procedure for the allocation of\\nresources (i.e., PN codes) among a multiplicity of active users. Last but by no means\\nleast, in using the RAKE, viewed as an adaptive TDL filter, CDMA is enabled to\\nmatch the receiver input to the channel output by adjusting tap delays as well as tap\\nweights, thereby enhancing receiver performance in the presence of multipath.\\nTo conclude, OFDMA and CDMA provide two different approaches for the multiple\\naccess of active users to wireless channels, each one of which builds on its own distinctive\\nfeatures.\\nProblems\\nEffect of Flat Fading on the BER of Digital Communications Receivers Derive the BER formulas listed in the right-hand side of Table 9.2 for the following signaling\\nschemes over flat fading channels:\\na. Binary PSK using coherent detection\\nb. Binary FSK using coherent detection\\nc. Binary DPSK\\nd. Binary FSK using noncoherent detection Using the formulas derived in Problem 9.1, plot the BER charts for the schemes described therein.',\n",
       "  'question': 'What is the primary principle behind CDMA?\\n',\n",
       "  'answer': 'CDMA leverages the concept of spread spectrum signals through spectrum spreading in the transmitter and despreading in the receiver to achieve processing gain, allowing multiple users to share the same channel bandwidth efficiently.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 588,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '11 Bayesian Inference where the Xi are independent and uniformly distributed random variables on the interval\\nfrom -1 to +1. Suppose that we generate 20000 samples of the random variable Yn for\\nn = 10, and then compute the probability density function of Yn by forming a histogram of\\nthe results. Figure 3.11a compares the computed histogram (scaled for unit area) with the\\nprobability density function of a Gaussian random variable with the same mean and\\nvariance. The figure clearly illustrates that in this particular example the number of\\nindependent distributions n does not have to be large for the sum Yn to closely\\napproximate a Gaussian distribution. Indeed, the results of this example confirm how\\npowerful the central limit theorem is. Moreover, the results explain why Gaussian models\\nare so ubiquitous in the analysis of random signals not only in the study of communication\\nsystems, but also in so many other disciplines. Bayesian Inference\\nThe material covered up to this point in the chapter has largely addressed issues involved\\nin the mathematical description of probabilistic models. In the remaining part of the\\nchapter we will study the role of probability theory in probabilistic reasoning based on the\\nBayesian5 paradigm, which occupies a central place in statistical communication theory.\\nTo proceed with the discussion, consider Figure 3.12, which depicts two finite-\\ndimensional spaces: a parameter space and an observation space, with the parameter\\nspace being hidden from the observer. A parameter vector , drawn from the parameter\\nspace, is mapped probabilistically onto the observation space, producing the observation\\nvector x. The vector x is the sample value of a random vector X, which provides the\\nFigure 3.11 Simulation supporting validity of the central limit theorem.\\nProbability density fX (x)\\nx -4 -2 0 2 4 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 Simulated density of 5 uniforms\\nGaussian density with same mean\\nand variance',\n",
       "  'question': 'What does the simulation in Figure 3.11 support about the central limit theorem?\\n',\n",
       "  'answer': 'The simulation demonstrates that the sum of 10 independent uniformly distributed random variables closely approximates a Gaussian distribution, illustrating the power of the central limit theorem even with a relatively small number of distributions.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 139,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '.213 [13], clause 9.2.1. p0-nominal Power control parameter P0 for PUCCH transmissions. Value in dBm. Only even values (step size 2) allowed (see TS 38.213 [13], clause 7.2). pucch-GroupHopping Configuration of group- and sequence hopping for all the PUCCH formats 0, 1, 3 and 4. Value neither implies neither group or sequence hopping is enabled. Value enable enables group hopping and disables sequence hopping. Value disable disables group hopping and enables sequence hopping (see TS 38.211 [16], clause 6.3.2.2). pucch-ResourceCommon An entry into a 16-row table where each row configures a set of cell-specific PUCCH resources/parameters. The UE uses those PUCCH resources until it is provided with a dedicated PUCCH-Config (e.g. during initial access) on the initial uplink BWP. Once the network provides a dedicated PUCCH-Config for that bandwidth part the UE applies that one instead of the one provided in this field (see TS 38.213 [13], clause 9.2). pucch-ResourceCommonRedCap An entry into a 16-row table where each row configures a set of cell-specific PUCCH resources/parameters for (e)RedCap UEs. The UE uses those PUCCH resources until it is provided with a dedicated PUCCH-Config (e.g. during initial access) on the initial uplink BWP. Once the network provides a dedicated PUCCH-Config for that bandwidth part the UE applies that one instead of the one provided in this field (see TS 38.213 [13], clause 9.2). Conditional Presence Explanation InitialBWP-Only The field is mandatory present in the PUCCH-ConfigCommon of the initial BWP (BWP#0) in SIB1. It is absent in other BWPs including the RedCap-specific initial uplink BWP, if configured. InitialBWP-RedCap The field is mandatory present in the PUCCH-ConfigCommon of the RedCap-specific initial BWP. It is optional present, Need R, in the PUCCH-ConfigCommon of the initial BWP configured by initialUplinkBWP. It is absent in other BWPs',\n",
       "  'question': 'What are the only allowed values for the p0-nominal Power control parameter P0?\\n',\n",
       "  'answer': 'Even values in dBm with a step size of 2 are the only allowed values for the p0-nominal Power control parameter P0.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1569}},\n",
       " {'context': '1 Frame structure model\\nIn NR, the numerology concept is introduced to flexibly define the frame structure in such a way that it can work in\\nboth sub-6 GHz and mmWave bands. The flexible frame structure is defined by multiple numerologies formed by scaling\\nthe subcarrier spacing (SCS) of 15 kHz. The supported numerologies (0, 1, 2, 3, 4) correspond to SCSs of 15 kHz,\\nkHz, 60 kHz, 120 kHz, and 240 kHz. The SCS of 480 kHz is under study by 3GPP, but the simulator can support it.\\nTheoretically, not all SCS options are supported in all carrier frequencies and channels. For example, for sub 6 GHz, only\\n15 kHz, 30 kHz, and 60 kHz are defined. Above 6 GHz, the supported ones are 60 kHz, 120 kHz, and 240 kHz. Also,\\nfor numerology 2 (i.e., SCS = 60 KHz), two cyclic prefixes (CP) overheads are considered: normal and extended. For\\nthe rest of the numerologies, only the normal overhead is taken into consideration.\\nIdentifying components',\n",
       "  'question': 'What are the supported subcarrier spacings in the NR flexible frame structure?\\n',\n",
       "  'answer': 'The supported subcarrier spacings are 15 kHz, 30 kHz, 60 kHz, 120 kHz, and 240 kHz, with 480 kHz under study.',\n",
       "  'source_doc': {'document': 'documents/nrmodule.pdf',\n",
       "   'page': 11,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': 'value of the codeword dierence matrices.\\n For the ntnr MIMO channel, the universal criterion is to maximize the product\\nof the nmin smallest singular values of the codeword dierence matrices. With\\nnr nt, this criterion is the same as that arrived at by averaging over the i.i.d.\\nRayleigh statistics.',\n",
       "  'question': 'What is the universal criterion for the ntnr MIMO channel?\\n',\n",
       "  'answer': 'Maximize the product of the nmin smallest singular values of the codeword difference matrices.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 487,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications Figure 7.7: Beamforming patterns for dierent antenna array lengths. (Left) Lr =\\nand (right) Lr = 8. Antenna separation is fixed at half the carrier wavelength. The\\nlarger the length of the array, the narrower the beam.\\nthe two rows of H are linearly independent and the channel has rank 2, yielding\\ndegrees of freedom. The output of the channel spans a two-dimensional space as we\\nvary the transmitted signal at the transmit antenna array. In order to make H well-\\nconditioned, the angular separation t of the two receive antennas should be of the\\norder or larger than 1/Lt, where Lt := ntt is the length of the transmit antenna array,\\nnormalized to the carrier wavelength.\\nAnalogous to the receive beamforming pattern, one can also define a transmit beam-\\nforming pattern. This measures the amount of energy dissipated in other directions\\nwhen the transmitter attempts to focus its signal along a direction 0. The beam\\nwidth is 2/Lt; the longer the antenna array, the sharper the transmitter can focus the\\nenergy along a desired direction and the better it can spatially multiplex information\\nto the multiple receive antennas. Line-of-sight plus one reflected path\\nCan we get a similar eect to that of the example in Section 7.2.4, without putting\\neither the transmit antennas or the receive antennas far apart? Consider again the\\ntransmit and receive antenna arrays in that example, but now suppose in addition to\\na line-of-sight path there is another path reflected oa wall (see Figure 7.9(a)). Call\\nthe direct path, path 1 and the reflected path, path 2. Path i has an attenuation of\\nai, makes an angle of ti (ti := cos ti) with the transmit antenna array and an angle\\nof ri (ri := cos ri) with the receive antenna array. The channel H is given by the',\n",
       "  'question': 'How does the length of the antenna array affect the beam width in beamforming?\\n',\n",
       "  'answer': 'The beam width is inversely proportional to the length of the antenna array, meaning a longer array results in a narrower beam, allowing for more precise signal focus and better spatial multiplexing capabilities.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 360,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': 'A.3 The Nakagami Distribution\\nA9\\nIndeed, with this important point in mind, the plots presented in Figure A.3 actually\\ninclude points (denoted by crosses) that pertain to an arbitrarily selected wireless data.3\\nFigure A.4 provides further demonstration of the inherent flexibility of the Nakagami-\\nm distribution in approximating the log-normal distribution. It is clearly shown that the\\napproximation gets gradually better as the fading figure m is increased.\\nIt is not surprising, therefore, to find that the Nakagami-m distribution outperforms the\\nRayleigh and Rician distributions, particularly so in urban wireless communication\\nenvironments.4\\nNotes\\nThe visualization procedure described herein for the log-normal distribution follows Cavers\\n(2000).\\nTwo other procedures for visualizing the log-normal distribution are described in the literature, as\\nsummarized here:\\n\\nIn Proakis and Salehi (2008), the standard deviation and the mean are varied, with both and measured in volts.  In Goldstein (2005), a new random variable defined as the ratio of transmit-to-receive\\npower, is used in place of x, and a new formula for the log-normal distribution is derived. In so\\ndoing, the use of power measured in decibels plays a prominent role in a new formulation of\\nthe log-normal distribution. However, this new formulation takes values for\\n, which\\nraises a physically unacceptable scenario; specifically, for\\n, the receive-power assumes a\\nvalue greater than the transmit-power.',\n",
       "  'question': 'What distribution does the Nakagami-m distribution approximate?\\n',\n",
       "  'answer': 'The Nakagami-m distribution approximates the log-normal distribution, with the approximation quality improving as the fading figure m increases.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 729,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '..., [[ mpe-Reporting-FR2-r16 SetupRelease { MPE-Config-FR2-r16 } OPTIONAL -- Need M ]], [[ mpe-Reporting-FR2-r17 SetupRelease { MPE-Config-FR2-r17 } OPTIONAL, -- Need M twoPHRMode-r17 ENUMERATED {enabled} OPTIONAL -- Need R ]], [[ phr-AssumedPUSCH-Reporting-r18 ENUMERATED {enabled} OPTIONAL, -- Need R dpc-Reporting-FR1-r18 ENUMERATED {enabled} OPTIONAL -- Need R ]] } MPE-Config-FR2-r16 ::= SEQUENCE { mpe-ProhibitTimer-r16 ENUMERATED {sf0, sf10, sf20, sf50, sf100, sf200, sf500, sf1000}, mpe-Threshold-r16 ENUMERATED {dB3, dB6, dB9, dB12} } MPE-Config-FR2-r17 ::= SEQUENCE { mpe-ProhibitTimer-r17 ENUMERATED {sf0, sf10, sf20, sf50, sf100, sf200, sf500, sf1000}, mpe-Threshold-r17 ENUMERATED {dB3, dB6, dB9, dB12}, numberOfN-r17 INTEGER(1..4), ... } -- TAG-PHR-CONFIG-STOP -- ASN1STOP PHR-Config field descriptions dpc-Reporting-FR1 Indicates if the delta power class (DPC) is reported, as specified in TS 38.321 [3]. dummy This field is not used in this version of the specification and the UE ignores the received value. mpe-ProhibitTimer Value in number of subframes for MPE reporting, as specified in TS 38.321 [3]. Value sf10 corresponds to 10 subframes, and so on. mpe-Reporting-FR2 Indicates whether the UE shall report MPE P-MPR in the PHR MAC control element, as specified in TS 38.321 [3]. mpe-Threshold Value of the P-MPR threshold in dB for reporting MPE P-MPR when FR2 is configured, as specified in TS 38.321 [3]. The same value applies for each serving cell (although the associated functionality is performed independently for each cell). multiplePHR Indicates if power headroom shall be reported using the Single Entry PHR MAC control element or Multiple Entry PHR MAC control element defined in TS 38.321 [3]. True means to use Multiple Entry PHR MAC control element and False means to use the Single Entry PHR MAC control element defined in TS 38.321 [3]. The network configures this field to true for MR-DC and UL CA for NR, and to false in all other cases',\n",
       "  'question': 'What does the value sf10 correspond to in terms of subframes for MPE reporting?\\n',\n",
       "  'answer': 'The value sf10 corresponds to 10 subframes for MPE reporting, as specified in the TS 38.321 standard.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1528}},\n",
       " {'context': 'Chapter\\nSignaling over Fading Channels\\ndigital transmission, the received signals duration is taken as the symbol duration\\nplus the channels delay spread. If, however, the channels coherence time is large\\ncompared with the received signal duration, then the fading is said to be time\\nnonselective, or time flat, in the sense that the channel appears to the transmitted\\nsignal as time invariant.\\nIn light of this discussion, we may classify multipath channels as follows:\\n\\nFlat-flat channel, which is flat in both frequency and time.\\n\\nFrequency-flat channel, which is flat in frequency only.\\n\\nTime-flat channel, which is flat in time only.',\n",
       "  'question': \"What is the term for fading when the channel's coherence time is large compared to the received signal duration?\\n\",\n",
       "  'answer': 'Time nonselective fading, where the channel appears time invariant to the transmitted signal.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 540,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '. -\\tSidelinkUEInformationNR The SidelinkUEinformationNR message is used for the indication of NR sidelink UE information to the network. Signalling radio bearer: SRB1 RLC-SAP: AM Logical channel: DCCH Direction: UE to Network SidelinkUEInformationNR message -- ASN1START -- TAG-SIDELINKUEINFORMATIONNR-START SidelinkUEInformationNR-r16::= SEQUENCE { criticalExtensions CHOICE { sidelinkUEInformationNR-r16 SidelinkUEInformationNR-r16-IEs, criticalExtensionsFuture SEQUENCE {} } } SidelinkUEInformationNR-r16-IEs ::= SEQUENCE { sl-RxInterestedFreqList-r16 SL-InterestedFreqList-r16 OPTIONAL, sl-TxResourceReqList-r16 SL-TxResourceReqList-r16 OPTIONAL, sl-FailureList-r16 SL-FailureList-r16 OPTIONAL, lateNonCriticalExtension OCTET STRING OPTIONAL, nonCriticalExtension SidelinkUEInformationNR-v1700-IEs OPTIONAL } SidelinkUEInformationNR-v1700-IEs ::= SEQUENCE { sl-TxResourceReqList-v1700 SL-TxResourceReqList-v1700 OPTIONAL, sl-RxDRX-ReportList-v1700 SL-RxDRX-ReportList-v1700 OPTIONAL, sl-RxInterestedGC-BC-DestList-r17 SL-RxInterestedGC-BC-DestList-r17 OPTIONAL, sl-RxInterestedFreqListDisc-r17 SL-InterestedFreqList-r16 OPTIONAL, sl-TxResourceReqListDisc-r17 SL-TxResourceReqListDisc-r17 OPTIONAL, sl-TxResourceReqListCommRelay-r17 SL-TxResourceReqListCommRelay-r17 OPTIONAL, ue-Type-r17 ENUMERATED {relayUE, remoteUE} OPTIONAL, sl-SourceIdentityRemoteUE-r17 SL-SourceIdentity-r17 OPTIONAL, nonCriticalExtension SidelinkUEInformationNR-v1800-IEs OPTIONAL } SidelinkUEInformationNR-v1800-IEs ::= SEQUENCE { sl-CarrierFailureList-r18 SL-CarrierFailureList-r18 OPTIONAL, sl-TxResourceReqListL2-U2U-r18 SEQUENCE (SIZE (1.',\n",
       "  'question': 'What is the purpose of the SidelinkUEInformationNR message?\\n',\n",
       "  'answer': 'It is used for indicating NR sidelink UE information to the network, involving signaling radio bearer SRB1, RLC-SAP AM, and logical channel DCCH.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1038}},\n",
       " {'context': '. This field is not configured if ul-powerControl is configured in the BWP-UplinkDedicated in which the PUCCH-Config is included. pucch-PowerControl Configures power control parameters PUCCH transmission. resourceGroupToAddModList, resourceGroupToReleaseList Lists for adding and releasing groups of PUCCH resources that can be updated simultaneously for spatial relations with a MAC CE. resourceSetToAddModList, resourceSetToReleaseList Lists for adding and releasing PUCCH resource sets (see TS 38.213 [13], clause 9.2). resourceToAddModList, resourceToAddModListExt, resourceToReleaseList Lists for adding and releasing PUCCH resources applicable for the UL BWP and serving cell in which the PUCCH-Config is defined. The resources defined herein are referred to from other parts of the configuration to determine which resource the UE shall use for which report. If the network includes of resourceToAddModListExt, it includes the same number of entries, and listed in the same order, as in resourceToAddModList. secondTPCFieldDCI-1-1, secondTPCFieldDCI-1-2 A second TPC field can be configured via RRC for DCI-1-1 and DCI-1-2. Each TPC field is for each closed-loop index value respectively (i.e., 1st /2nd TPC fields correspond to \"closedLoopIndex\" value = 0 and 1. spatialRelationInfoToAddModList, spatialRelationInfoToAddModListSizeExt , spatialRelationInfoToAddModListExt Configuration of the spatial relation between a reference RS and PUCCH. Reference RS can be SSB/CSI-RS/SRS. If the list has more than one element, MAC-CE selects a single element (see TS 38.321 [3], clause 5.18.8 and TS 38.213 [13], clause 9.2.2). The UE shall consider entries in spatialRelationInfoToAddModList and in spatialRelationInfoToAddModListSizeExt as a single list, i.e. an entry created using spatialRelationInfoToAddModList can be modified using spatialRelationInfoToAddModListSizeExt (or deleted using spatialRelationInfoToReleaseListSizeExt) and vice-versa',\n",
       "  'question': 'What does PUCCH-Config define?\\n',\n",
       "  'answer': 'PUCCH-Config defines the power control parameters for PUCCH transmission, lists for adding and releasing PUCCH resources, and the spatial relations with a MAC CE for UL BWP and serving cell.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1562}},\n",
       " {'context': '1\\tBroadcast channel Data arrives to the coding unit in the form of a maximum of one transport block every 80ms. The following coding steps can be identified: -\\tPayload generation -\\tScrambling -\\tTransport block CRC attachment -\\tChannel coding -\\tRate matching\\n1\\tPBCH payload generation Denote the bits in a transport block delivered to layer 1 by , where is the payload size generated by higher layers. The lowest order information bit is mapped to the most significant bit of the transport block as defined in Clause 6.1.1 of [8, TS 38.321]. Generate the following additional timing related PBCH payload bits , where: - are the 4th, 3rd, 2nd, and 1st LSB of SFN, respectively; - is the half frame bit ; -\\tif as defined in Clause 4.1 of [5, TS38.213], is the MSB of as defined in Clause 7.4.3.1 of [4, TS 38.211]. is reserved. is the MSB of candidate SS/PBCH block index. -\\telse if as defined in Clause 4.1 of [5, TS38.213], is the MSB of as defined in Clause 7.4.3.1 of [4, TS 38.211]. , are the 5th and 4th bits of the candidate SS/PBCH block index, respectively. -\\telse if as defined in Clause 4.1 of [5, TS38.213], , , are the 6th, 5th, and 4th bits of the candidate SS/PBCH block index, respectively. -\\telse is the MSB of as defined in Clause 7.4.3.1 of [4, TS 38.211]. , are reserved. -\\tend if Let ; ; ; ; ; for to if is an SFN bit ; ; elseif is the half radio frame bit elseif ; ; else ; ; end if end for where is the number of candidate SS/PBCH blocks in a half frame according to Clause 4.1 of [5, TS38.213], and the value of is given by Table 7.1.1-1. Table 7.1.1-1: Value of PBCH payload interleaver pattern',\n",
       "  'question': 'How often does data arrive to the coding unit in the form of a transport block?\\n',\n",
       "  'answer': 'Data arrives in the form of a maximum of one transport block every 80 milliseconds.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38212-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 138}},\n",
       " {'context': '.321 RP-103 RP-240693 B Generalization of RACH-less handover for MAC spec [RACH-lessHO] RP-103 RP-240696 F Corrections to the MAC spec for R18 positioning RP-103 RP-240698 F Miscellaneous corrections on TS 38.321 for eRedCap RP-103 RP-240701 F Correction for SL resource pool usage for BRID/DAA transmission RP-103 RP-240651 A CR on termination of on-going RACH due to pending SR for SL-BSR RP-103 RP-240657 A Correction on CG-SDT initial transmission RP-103 RP-240700 F Miscellaneous MAC Corrections on SL Relay enhancements RP-103 RP-240665 F Clarification on Timing Advance Report MAC CE for NR ATG RP-103 RP-240688 F Miscellaneous corrections to eMBS in MAC RP-103 RP-240668 F Miscellaneous MAC corrections for CE RP-103 RP-240662 F Miscellaneous MAC corrections for network energy savings RP-103 RP-240694 - F Rapporteur CR to MT-SDT and CG-SDT enhanccement [CG-SDTenh] RP-103 RP-240651 A Miscellaneous corrections on TS 38.321 RP-103 RP-240656 A Miscellaneous corrections on TS 38.321 RP-103 RP-240689 B Introduction of Multi-carrier enhancements RP-103 RP-240695 - F Corrections to Rel-18 NTN enhancements RP-103 RP-240699 F MAC corrections on Rel-18 NR sidelink evolution RP-103 RP-240691 C Corrections on Rel-18 MIMOevo for TS 38',\n",
       "  'question': 'What is the subject of RP-240651?\\n',\n",
       "  'answer': 'The correction on termination of on-going RACH due to pending SR for SL-BSR.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38321-i50.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 610}},\n",
       " {'context': 'Problems the signal. In the absence of noise, the amplitude of the received sinusoidal wave for digit 1 or 0 is\\n1mV. Determine the average probability of symbol error for the following system configurations:\\na. binary FSK using coherent detection;\\nb. MSK using coherent detection;\\nc. binary FSK using noncoherent detection. In an FSK system using coherent detection, the signals s1(t) and s2(t) representing binary symbols\\nand 0, respectively, are defined by Assuming that fc > f, show that the correlation coefficient of the signals s1(t) and s2(t) is\\napproximately given by\\na. What is the minimum value of frequency shift f for which the signals s1(t) and s2(t) are\\northogonal?\\nb. What is the value of f that minimizes the average probability of symbol error?\\nc. For the value of f obtained in part c, determine the increase in EbN0 required so that this FSK\\nscheme has the same noise performance as a binary PSK scheme system, also using coherent\\ndetection. A binary FSK signal with discontinuous phase is defined by\\nwhere Eb is the signal energy per bit, Tb is the bit duration, and 1 and 2 are sample values of\\nuniformly distributed random variables over the interval 0 to 2. In effect, the two oscillators\\nsupplying the transmitted frequencies fc  f /2 operate independently of each other. Assume that\\nfc >>f.\\na. Evaluate the power spectral density of the FSK signal.\\nb. Show that, for frequencies far removed from the carrier frequency fc, the power spectral density\\nfalls off as the inverse square of frequency. How does this result compare with a binary FSK\\nsignal with continuous phase? Set up a block diagram for the generation of Sundes FSK signal s(t) with continuous phase by using\\nthe representation given in (7.170), which is reproduced here Discuss the similarities between MSK and offset QPSK, and the features that distinguish them. There are two ways of detecting an MSK signal. One way is to use a coherent receiver to take full',\n",
       "  'question': 'What is the minimum value of frequency shift f for which the signals s1(t) and s2(t) are orthogonal?\\n',\n",
       "  'answer': 'The minimum value of frequency shift f for which the signals s1(t) and s2(t) are orthogonal is when f equals the reciprocal of the bit duration Tb, ensuring the signals are distinguishable and non-overlapping in the time domain.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 459,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '. 1>\\tif the RRCRelease includes the measIdleConfig: 2>\\tif T331 is running: 3> stop timer T331; 3>\\tperform the actions as specified in 5.7.8.3; 2>\\tif the measIdleConfig is set to setup: 3>\\tstore the received measIdleDuration in VarMeasIdleConfig; 3>\\tstart timer T331 with the value set to measIdleDuration; 3>\\tif the measIdleConfig contains measIdleCarrierListNR: 4>\\tstore the received measIdleCarrierListNR in VarMeasIdleConfig; 3>\\tif the measIdleConfig contains measIdleCarrierListEUTRA: 4>\\tstore the received measIdleCarrierListEUTRA in VarMeasIdleConfig; 3>\\tif the measIdleConfig contains validityAreaList: 4>\\tstore the received validityAreaList in VarMeasIdleConfig; 3>\\tif the measIdleConfig contains measReselectionCarrierListNR: 4>\\tstore the received measReselectionCarrierListNR in VarMeasReselectionConfig; 3>\\tif the measIdleConfig contains measReselectionValidityDuration: 4>\\tstore the received measReselectionValidityDuration in VarMeasReselectionConfig; 3>\\tif the measIdleConfig contains measIdleValidityDuration: 4>\\tstore the received measIdleValidityDuration in VarEnhMeasIdleConfig; 1>\\tif the RRCRelease includes suspendConfig: 2>\\treset MAC and release the default MAC Cell Group configuration, if any; 2>\\tapply the received suspendConfig except the received nextHopChainingCount; 2>\\tif the sdt-Config is configured: 3>\\tfor each of the DRB in the sdt-DRB-List: 4>\\tconsider the DRB to be configured for SDT; 3>\\tif sdt-SRB2-Indication is configured: 4>\\tconsider the SRB2 to be configured for SDT; 3>\\tfor each RLC bearer (except those associated with broadcast MRBs and multicast MRBs) that is not suspended: 4>\\tre-establish the RLC entity as specified in TS 38.322 [4]; 3>\\tfor SRB2 (if it is resumed) and for SRB1: 4>\\ttrigger the PDCP entity to perform SDU discard as specified in TS 38',\n",
       "  'question': 'What happens if the RRCRelease includes the measIdleConfig and T331 is running?\\n',\n",
       "  'answer': 'The timer T331 is stopped, and actions specified in 5.7.8.3 are performed.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 320}},\n",
       " {'context': 'coresetPoolIndex with a value of 0 for first CORESETs on an active DL BWP of a serving cell, and -\\tis provided coresetPoolIndex with a value of 1 for second CORESETs on the active DL BWP of the serving cells, the first and second TCI-State or TCI-UL-State are specific to the first and second CORESETs, respectively',\n",
       "  'question': 'What is the value of coresetPoolIndex for first CORESETs on an active DL BWP?\\n',\n",
       "  'answer': 'The value of coresetPoolIndex for first CORESETs on an active DL BWP is 0.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38213-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 287}},\n",
       " {'context': '154 Chapter 4 Stochastic Processes EXAMPLE 3 Random Binary Wave Figure 4.6 shows the sample function x(t) of a weakly stationary process X(t) consisting of\\na random sequence of binary symbols 1 and 0. Three assumptions are made:\\nThe symbols 1 and 0 are represented by pulses of amplitude +A and -A volts\\nrespectively and duration T seconds.\\nThe pulses are not synchronized, so the starting time td of the first complete pulse\\nfor positive time is equally likely to lie anywhere between zero and T seconds. That\\nis, td is the sample value of a uniformly distributed random variable Td, whose\\nprobability density function is defined by\\nDuring any time interval (n - 1)T < t - td < nT, where n is a positive integer, the\\npresence of a 1 or a 0 is determined by tossing a fair coin. Specifically, if the\\noutcome is heads, we have a 1; if the outcome is tails, we have a 0. These two\\nsymbols are thus equally likely, and the presence of a 1 or 0 in any one interval is\\nindependent of all other intervals.\\nSince the amplitude levels -A and +A occur with equal probability, it follows immediately\\nthat \\x03[X(t)] = 0 for all t and the mean of the process is therefore zero.\\nTo find the autocorrelation function RXX(tk,ti), we have to evaluate the expectation\\n\\x03[X(tk)X(ti)], where X(tk) and X(ti) are random variables obtained by sampling the\\nstochastic process X(t) at times tk and ti respectively. To proceed further, we need to\\nconsider two distinct conditions:\\nCondition 1:\\n|tk - ti| > T\\nUnder this condition, the random variables X(tk) and X(ti) occur in different pulse intervals\\nand are therefore independent. We thus have\\nFigure 4.6 Sample function of random binary wave.\\nfTd td   1 T---, 0 td T   0, elsewhere      = \\x03 X tk  X ti    \\x03 X tk    \\x03 X ti    0, tk ti - T  = = td T x(t) t +A -A',\n",
       "  'question': 'What are the amplitudes of the pulses representing the binary symbols 1 and 0 in the weakly stationary process X(t)?\\n',\n",
       "  'answer': 'The symbol 1 is represented by a pulse of amplitude +A volts, while the symbol 0 is represented by a pulse of amplitude -A volts.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 174,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': 'Problems 143 3.39 In this problem we generalize the likelihood ratio test for simple binary hypotheses by including\\ncosts incurred in the decision-making process. Let Cij denote the cost incurred in deciding in favor of\\nhypothesis Hi when hypothesis Hj is true. Hence, show that the likelihood ratio test of (3.95) still\\nholds, except for the fact that the threshold of the test is now defined by Consider a binary hypothesis-testing procedure where the two hypotheses H0 and H1 are described\\nby different Poisson distributions, characterized by the parameters 0 and 1, respectively. The\\nobservation is simply a number of events k, depending on whether H0 or H1 is true. Specifically, for\\nthese two hypotheses, the probability mass functions are defined by\\nwhere i = 0 for hypothesis H0 and i = 1 for hypothesis H1. Determine the log-likelihood ratio test for\\nthis problem. Consider the binary hypothesis-testing problem\\nH1 : X = M + N\\nH0 : X = N\\nThe M and N are independent exponentially distributed random variables, as shown by\\nDetermine the likelihood ratio test for this problem. In this problem we revisit Example 8. But this time we assume that the mean m under hypothesis H1\\nis Gaussian distributed, as shown by\\na. Derive the likelihood ratio test for the composite hypothesis scenario just described.\\nb. Compare your result with that derived in Example 8.\\nFigure P3.38 C(e)   0 1.0   0 C10 C00 -   1 C01 C11 -   ----------------------------------\\n= pXi k  i  k k! ----------- i -   k  exp 0, 1, 2,  = = pM m   m m -  , m 0  exp 0 otherwise     = pN n  n n -  , n 0  exp 0 otherwise     = fM H1 m H1   1 2m ------------------ m2 2m 2 ---------- -       exp =',\n",
       "  'question': 'What is the probability mass function for hypothesis H0 in the given binary hypothesis-testing procedure?\\n',\n",
       "  'answer': 'The probability mass function for hypothesis H0 is defined as pXi k = (1/k!) * exp(0) when k = 0, 1, 2, ...',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 163,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': 'postcursors of a channel impulse response are illustrated in Figure 8.20. The idea of\\ndecision-feedback equalization is to use data decisions made on the basis of precursors of\\nFigure 8.20 Impulse response of a discrete-time channel, depicting\\nthe precursors and postcursors.\\nan an yn hkxn k - k = h0 xn hk xn k - hk xn k - k 0  + k 0  + = t h0 0 Postcursors Precursors',\n",
       "  'question': 'What is the purpose of decision-feedback equalization?\\n',\n",
       "  'answer': \"It uses data decisions based on precursors to mitigate the effects of a channel's impulse response.\",\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 493,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications\\nSolve the problem for the general case when the channel statistics and the power\\nconstraints of the users are arbitrary.\\nHint: Construct a Lagrangian for the convex optimization problem (6.42) with a\\nseparate Lagrange multiplier for each of the individual power constraint (6.43).\\nDo you think the sum capacity is a reasonable performance measure in the asym-\\nmetric case?\\nExercise 6.16. In Section 6.3.3, we have derived the optimal power allocation with\\nfull CSI in the symmetric uplink with the assumption that there is always a unique user\\nwith the strongest channel at any one time. This assumption holds with probability\\n1 when the fading distributions are continuous.\\nMoreover, under this assumption,\\nthe solution is unique. This is in contrast to the uplink AWGN channel where there\\nis a continuum of solution that achieves the optimal sum rate, of which only one is\\northogonal. We will see in this exercise that transmitting to only one user at a time\\nis not necessarily the unique optimal solution even for fading channels, if the fading\\ndistribution is discrete (to model measurement realities, such as the feedback of a finite\\nnumber of rate levels).\\nConsider the full CSI two user uplink with identical, independent, stationary and\\nergodic flat fading processes for the two users. The stationary distribution of the flat\\nfading for both of the users takes one of just two values: channel amplitude is either\\nat 0 or at 1 (with equal probability). Both of the users are individually average power\\nconstrained (by P). Calculate explicitly all the optimal joint power allocation and\\ndecoding policies to maximize the sum rate. Is the optimal solution unique? Hint:\\nClearly there is no benefit by allocating power to a user whose channel is fully faded\\n(the zero amplitude state).\\nExercise 6.17. In this exercise we further study the nature of the optimal power and',\n",
       "  'question': 'What is the probability of channel amplitude for each user in the stationary distribution of flat fading?\\n',\n",
       "  'answer': 'The channel amplitude is either at 0 or at 1 with an equal probability of 50%.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 334,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications\\n418 Decode Decode lator nt Decorre- Decorre- Stream nt Subtract Stream 1 stream 1 Decode lator 1 Decorre- stream 2 Subtract stream 1,2,...,nt 1 y[m] Stream 3 stream 3 Decode lator 3 Decorre- Stream 1,2 Subtract Stream 2 lator 2 Stream 1 stream nt Figure 8.11: Decorrelator-SIC: A bank of decorrelators with successive cancellation of\\nstreams.\\nsuccessful subtraction in each preceding stage). This decorrelator-SIC (decorrelator\\nwith successive interference cancellation) architecture is illustrated in Figure 8.11.\\nOne problem with this receiver structure is error propagation: an error in decod-\\ning the kth data stream means that the subtracted signal is incorrect and this error\\npropagates to all the streams further down, k + 1, . . . , nt. A careful analysis of the\\nperformance of this scheme is complicated, but can be made easier if we take the data\\nstreams to be well coded and the block length to be very large, so that streams are\\nsuccessfully cancelled with very high probability. With this assumption the kth data\\nstream sees only down-stream interference, i.e., from the streams k + 1, . . . , nt. Thus,\\nthe corresponding projection operation (denoted by Qk) is onto a higher dimensional\\nsubspace (one orthogonal to that spanned by hk+1, . . . , hnt, as opposed to being orthog-\\nonal to the span of h1, . . . , hk1, hk+1, . . . , hnt). As in the calculation of the previous\\nsection, the SNR of the kth data stream is (c.f. (8.44))\\nPkQkhk2 N0 . (8.55) While we clearly expect this to be an improvement over the simple bank of decor-\\nrelators, let us again turn to the i.i.d. Rayleigh fading model to see this concretely.',\n",
       "  'question': 'What is the effect of an error in decoding the kth data stream in a decorrelator-SIC architecture?\\n',\n",
       "  'answer': 'It causes incorrect subtraction and propagates the error to all subsequent streams, k + 1, . . . , nt, leading to degraded performance of the receiver structure.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 419,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '. NOTE 3Aa:\\tFor Multi-consecutive slots transmission as specified in clause 8.1.4 of TS 38.214 [7], during resource (re)selection, leave it to UE implementation, regarding whether to calculate the number of HARQ retransmissions from the allowed numbers based on the number of MCSt transmissions, or the number of slot(s) within Multi-consecutive slots transmission. NOTE 3Aa0:\\tWhen transmission is performed on Shared SL-PRS resource pool, the selected number of HARQ retransmissions also corresponds to the number of SL-PRS transmissions. 4>\\tselect an amount of frequency resources within the range, if configured by RRC, between sl-MinSubChannelNumPSSCH and sl-MaxSubchannelNumPSSCH included in sl-PSSCH-TxConfigList and, if configured by RRC, overlapped between sl-MinSubChannelNumPSSCH and sl-MaxSubchannelNumPSSCH indicated in sl-CBR-PriorityTxConfigList for the highest priority of the logical channel(s) and pending SL-PRS transmission(s), if available, allowed on the carrier and the CBR measured by lower layers according to clause 5.1.27 of TS 38.215 [24] if CBR measurement results are available or the corresponding sl-DefaultTxConfigIndex configured by RRC if CBR measurement results are not available or the corresponding sl-DefaultCBR-PartialSensing configured by RRC if partial sensing is selected and CBR measurement results are not available, or the corresponding sl-DefaultCBR-RandomSelection configured by RRC if random selection is selected and CBR measurement results are not available in case the sl-TxPoolExceptional is not used; 3>\\telse if the selected resource pool is Dedicated SL-PRS resource pool: 4>\\tselect one of the allowed values configured by RRC in sl-PRS-ResourceReservePeriodList and set the resource reservation interval, , with the selected value; 4>\\tselect the number of SL-PRS retransmissions from the allowed numbers, if configured by RRC, in sl-PRS-MaxNum-Transmissions included in sl-CBR-SL-PRS-TxConfigList',\n",
       "  'question': 'How many HARQ retransmissions correspond to the number of SL-PRS transmissions when transmission is performed on Shared SL-PRS resource pool?\\n',\n",
       "  'answer': 'The number of HARQ retransmissions matches the number of SL-PRS transmissions in Shared SL-PRS resource pool scenarios.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38321-i50.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 282}},\n",
       " {'context': '2\\tMeasurement identity removal The UE shall: 1>\\tfor each measId included in the received measIdToRemoveList that is part of the current UE configuration in VarMeasConfig: 2>\\tremove the entry with the matching measId from the measIdList within the VarMeasConfig; 2>\\tremove the measurement reporting entry for this measId from the VarMeasReportList, if included; 2>\\tstop the periodical reporting timer or timer T321 or timer T322, whichever one is running, and reset the associated information (e.g. timeToTrigger) for this measId. 2>\\tif the reportType is set to reportOnScellActivation in the reportConfig associated with this measId: 3>\\tindicate to lower layer to disable the measurement reporting for fast unknown SCell activation. NOTE:\\tThe UE does not consider the message as erroneous if the measIdToRemoveList includes any measId value that is not part of the current UE configuration.',\n",
       "  'question': 'What does the UE remove from VarMeasConfig for each measId in the received measIdToRemoveList?\\n',\n",
       "  'answer': 'The UE removes the entry with the matching measId from the measIdList within the VarMeasConfig.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 415}},\n",
       " {'context': '13 Hypothesis Testing dimension corresponds to the ensemble of channel outputs. Finally, the receiver performs\\nthe decision rule.\\nLikelihood Receiver\\nTo proceed with the solution to the binary hypothesis-testing problem, we introduce the\\nfollowing notations:\\n, which denotes the conditional density of the observation vector x\\ngiven that hypothesis H0 is true.\\n, denotes the conditional density of x given that the other hypothesis\\nH1 is true.\\n0 and 1 denote the priors of hypotheses H0 and H1, respectively.\\nIn the context of hypothesis testing, the two conditional probability density functions and are referred to as likelihood functions, or just simply\\nlikelihoods.\\nSuppose we perform a measurement on the transition mechanisms output, obtaining\\nthe observation vector x. In processing x, there are two kinds of errors that can be made by\\nthe decision rule:\\nError of the first kind. This arises when hypothesis H0 is true but the rule makes a\\ndecision in favor of H1, as illustrated in Figure 3.14.\\nError of the second kind. This arises when hypothesis H1 is true but the rule makes a\\ndecision in favor of H0.\\nThe conditional probability of an error of the first kind is\\nwhere Z1 is part of the observation space that corresponds to hypothesis H1. Similarly, the\\nconditional probability of an error of the second kind is\\nBy definition, an optimum decision rule is one for which a prescribed cost function is\\nminimized. A logical choice for the cost function in digital communications is the average\\nprobability of symbol error, which, in a Bayesian context, is referred to as the Bayes risk.\\nThus, with the probable occurrence of the two kinds of errors identified above, we define\\nthe Bayes risk for the binary hypothesis-testing problem as\\n(3.88)\\nwhere we have accounted for the prior probabilities for which hypotheses H0 and H1 are\\nknown to occur. Using the language of set theory, let the union of the disjoint subspaces Z0',\n",
       "  'question': 'What are the two types of errors that can be made by the decision rule in hypothesis testing?\\n',\n",
       "  'answer': 'The first kind occurs when hypothesis H0 is true but the rule decides in favor of H1, and the second kind occurs when hypothesis H1 is true but the rule decides in favor of H0.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 147,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '.304 [20]; 4>\\tperform cell re-selection to other cells on the same frequency as the barred cell as specified in TS 38.304 [20], upon which the procedure ends; 3>\\tif the UE is a fixed VSAT UE and the cellBarredFixedVSAT in the acquired SIB1 is set to barred or the cellBarredFixedVSAT is not included in the acquired SIB1, or 3>\\tif the UE is a mobile VSAT UE and the cellBarredMobileVSAT in the acquired SIB1 is set to barred or the cellBarredMobileVSAT is not included in the acquired SIB1: 4>\\tconsider the cell as barred in accordance with TS 38.304 [20]; 4>\\tperform cell re-selection to other cells on the same frequency as the barred cell as specified in TS 38.304 [20], upon which the procedure ends; 1>\\tif the access is for ATG: 2>\\tif the UE is in RRC_IDLE or in RRC_INACTIVE, or if the UE is in RRC_CONNECTED while T311 is running; and 2>\\tif the cellBarredATG in the acquired SIB1 is set to barred or the cellBarredATG is not included in the acquired SIB1: 3>\\tconsider the cell as barred in accordance with TS 38.304 [20]; 3>\\tperform cell re-selection to other cells on the same frequency as the barred cell as specified in TS 38.304 [20], upon which the procedure ends; 1>\\tif the UE is a RedCap UE and it is in RRC_IDLE or in RRC_INACTIVE, or if the RedCap UE is in RRC_CONNECTED while T311 is running: 2>\\tif intraFreqReselectionRedCap is not present in SIB1; or 2>\\tif the halfDuplexRedCapAllowed is not present in the acquired SIB1 and the UE supports only half-duplex FDD operation: 3>\\tconsider the cell as barred in accordance with TS 38.304 [20]; 3>\\tperform cell re-selection to other cells on the same frequency as the barred cell as specified in TS 38',\n",
       "  'question': 'What does the UE do when a cell is considered barred?\\n',\n",
       "  'answer': 'The UE performs cell re-selection to other cells on the same frequency as the barred cell as specified in TS 38.304 [20], upon which the procedure ends.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 75}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications of the sub-channels. An average transmit power constraint of P on the original channel\\ntranslates into a total power constraint of LP on the parallel channel.\\nFor a given realization of the channel, we have already seen in Section 5.3.3 that\\nthe optimal power allocation across the sub-channels is waterfilling. However, since\\nthe transmitter does not know what the channel gains are, a reasonable strategy is to\\nallocate equal power P to each of the sub-channels. In Section 5.3.3, it was mentioned\\nthat the maximum rate of reliable communication given the fading gains hs is\\nL X =1 log  1 + |h|2SNR  bits/s/Hz, (5.80) where SNR = P/N0. Hence, if the target rate is R bits/s/Hz per sub-channel, then\\noutage occurs when L X =1 log  1 + |h|2SNR  < LR. (5.81) Can one design a code to communicate reliably whenever\\nL X =1 log  1 + |h|2SNR  > LR ? (5.82) If so, an L-fold diversity is achieved for i.i.d. Rayleigh fading: outage occurs only if\\neach of the terms in the sum PL\\n=1 log(1 + |h|2SNR) is small.\\nThe term log (1 + |h|2SNR) is the capacity of an AWGN channel with received SNR\\nequal to |h|2SNR. Hence, a seemingly straightforward strategy, which was already used\\nin Section 5.3.3, would be to use a capacity-achieving AWGN code with rate\\nlog(1 + |h|2SNR)\\nfor the th coherence period, yielding an average rate of\\n1 LLX =1 log(1 + |h|2SNR) bits/s/Hz and meeting the target rate whenever condition (5.82) holds. The caveat is that this\\nstrategy requires the transmitter to know in advance the channel state during each of\\nthe coherence periods so that it can adapt the rate it allocates to each period. This\\nknowledge is not available. However, it turns out that such transmitter adaptation\\nis unnecessary: information theory guarantees that one can design a single code that\\ncommunicates reliably at rate R whenever the condition (5.82) is met.\\nHence, the\\noutage probability of the time-diversity channel is precisely',\n",
       "  'question': 'What is the average transmit power constraint on the original channel translated to the parallel channel?\\n',\n",
       "  'answer': 'The average transmit power constraint of P on the original channel translates into a total power constraint of LP on the parallel channel.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 229,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '... } HighSpeedConfigFR2-r17 ::= SEQUENCE { highSpeedMeasFlagFR2-r17 ENUMERATED {set1, set2} OPTIONAL, -- Need R highSpeedDeploymentTypeFR2-r17 ENUMERATED {unidirectional, bidirectional} OPTIONAL, -- Need R highSpeedLargeOneStepUL-TimingFR2-r17 ENUMERATED {true} OPTIONAL, -- Need R ... } -- TAG-HIGHSPEEDCONFIG-STOP -- ASN1STOP HighSpeedConfig field descriptions HighSpeedDemodCA-Scell If the field is present and UE supports demodulationEnhancementCA-r17, the UE shall apply the enhanced demodulation processing for HST-SFN joint transmission scheme with velocity up to 500km/h as specified in TS 38.101-4 [59]. This parameter only applies to SCell. highSpeedDemodFlag If the field is present and UE supports demodulationEnhancement-r16, the UE shall apply the enhanced demodulation processing for HST-SFN joint transmission scheme with velocity up to 500km/h as specified in TS 38.101-4 [59]. This parameter only applies to SpCell. highSpeedDeploymentTypeFR2 If the field is present, and field value is unidirectional, the UE shall assume uni-directional deployment or if field value is birectional the UE shall assume bidirectional deployment for FR2 up to 350km/h as specified in TS 38.133 [14]. highSpeedLargeOneStepUL-TimingFR2 If the field is present, large one step UE autonomous uplink transmit timing adjustment for FR2 up to 350km/h as specified in TS 38.133 [14] is enabled. highSpeedMeasCA-Scell If the field is present and UE supports measurementEnhancementCA-r17, the UE shall apply the enhanced RRM requirements to the serving frequency of SCell for carrier aggregation to support high speed up to 500 km/h as specified in TS 38.133 [14]. highSpeedMeasFlag If the field is present and UE supports measurementEnhancement-r16, the UE shall apply the enhanced intra-NR and inter-RAT EUTRAN RRM requirements to support high speed up to 500 km/h as specified in TS 38.133 [14]',\n",
       "  'question': 'What is the maximum velocity supported by the enhanced demodulation processing for HST-SFN joint transmission scheme?\\n',\n",
       "  'answer': 'The enhanced demodulation processing for HST-SFN joint transmission scheme supports a velocity up to 500km/h.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1360}},\n",
       " {'context': '1a.6.1\\tAdditional Sidelink RLC Bearer addition/modification conditions For NR sidelink communication, additional sidelink RLC bearer addition is initiated only in the following cases: 1>\\tfor unicast, for sidelink DRB, if SL-RLC-BearerConfig is received in sl-RLC-BearerToAddModList in the RRCReconfigurationSidelink for a slrb-PC5-ConfigIndex; or 1>\\tfor groupcast and broadcast, for sidelink DRB, if SL-RLC-BearerConfig is received in sl-RLC-BearerToAddModListSizeExt in sl-ConfigDedicatedNR for a sl-ServedRadioBearer; or 1>\\tfor unicast, for sidelink DRB, if SL-RLC-BearerConfig is received in sl-RLC-BearerToAddModListSizeExt in sl-ConfigDedicatedNR for a sl-ServedRadioBearer; or 1>\\tfor groupcast and broadcast, for sidelink DRB, if SL-RLC-BearerConfig is received in sl-RLC-BearerConfigListSizeExt in SIB12 or in SidelinkPreconfigNR for a sl-ServedRadioBearer, if the sidelink DRB has been established as in clause 5.8.9.1a.2 and has not been released as in clause 5.8.9.1a.1, and if the SL-TxProfile of all associated QoS flow(s) for the sl-ServedRadioBearer indicates backwardsIncompatible; or 1>\\tfor groupcast and broadcast, for sidelink DRB, if SL-RLC-BearerConfig is received in sl-RLC-BearerConfigListSizeExt in SIB12 or in SidelinkPreconfigNR for a sl-ServedRadioBearer, if the sidelink DRB has been established as in clause 5.8.9.1a.2 and has not been released as in clause 5.8.9.1a',\n",
       "  'question': 'What are the conditions for initiating additional sidelink RLC bearer addition in NR sidelink communication?\\n',\n",
       "  'answer': 'It is initiated for unicast, groupcast, and broadcast in sidelink DRB under specific configurations and conditions involving SL-RLC-BearerConfig and SL-TxProfile.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 818}},\n",
       " {'context': '.1.22.2.2-1 in TS 38.133 [14]. CLI-RSSI-Range information element -- ASN1START -- TAG-CLI-RSSI-RANGE-START CLI-RSSI-Range-r16 ::= INTEGER(0..76) -- TAG-CLI-RSSI-RANGE-STOP -- ASN1STOP -\\tClockQualityMetrics The IE ClockQualityMetrics is used to configure RAN timing synchronisation status information as specified in TS 38.473 [36] ClockQualityMetrics information element -- ASN1START -- TAG-CLOCKQUALITYMETRICS-START ClockQualityMetrics-r18 ::= SEQUENCE { synchronisationState-r18 ENUMERATED {locked, holdover, freerun, spare1} OPTIONAL, -- Need N tracebilityToUTC-r18 BOOLEAN OPTIONAL, -- Need N tracebilityToGNSS-r18 BOOLEAN OPTIONAL, -- Need N clockFrequencyStability-r18 BIT STRING (SIZE(16)) OPTIONAL, -- Need N clockAccuracy-r18 CHOICE { value INTEGER (1..40000000), index INTEGER (32..47) } OPTIONAL, -- Need N parentTimeSource-r18 ENUMERATED {syncE, pTP, gNSS,atomicClock, terrestialRadio, serialTimeCode, nTP, handset, other, spare7, spare6, spare5, spare4, spare3, spare2, spare1} OPTIONAL, -- Need N ... } -- TAG-CLOCKQUALITYMETRICS-STOP -- ASN1STOP -\\tCodebookConfig The IE CodebookConfig is used to configure codebooks of Type-I and Type-II (see TS 38.214 [19], clause 5.2.2',\n",
       "  'question': 'What is the range of values for CLI-RSSI-Range?\\n',\n",
       "  'answer': 'The CLI-RSSI-Range can have values from 0 to 76.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1240}},\n",
       " {'context': '.321 RP-99 RP-230688 F Correction to RA-SDT RP-99 RP-230692 F R17 MAC corrections RP-99 RP-230694 D Miscellaneous editorial changes for 38.321 RP-99 RP-230690 F MBS MAC Corrections 2023-06 RP-100 RP-231415 F Clarification on RA Resource Selection During CG-SDT RP-100 RP-231414 F Corrections on cfr-ConfigMulticast and Multicast DRX RP-100 RP-231415 F Correction to CG-SDT LCH restriction RP-100 RP-231414 F Corrections on SPS Initialization and Handling of Unknown, Unforeseen and Erroneous Protocol Data for MBS RP-100 RP-231415 F Corrections on SDT using NCD-SSB for RedCap RP-100 RP-231418 F Clarification on UL operation upon validity timer expiry RP-100 RP-231413 F Correction to MAC reset for eIAB RP-100 RP-231418 F Corrections to NR NTN for 38.321 RP-100 RP-231416 F Corrections on MAC reset regarding configured sidelink grant RP-100 RP-231416 F Correction on the usage of default CBR values for NR sidelink RP-100 RP-231412 F RA partition selection for Msg1 based SI request RP-100 RP-231412 F Correction to MAC spec for Positoning Enhancements RP-100 RP-231416 - F Correction on 38.321 for SL enhancements RP-100 RP-231415 F Correction on SDT with sparate initial BWP RP-100 RP-231416 - F Miscellaneous corrections on TS 38',\n",
       "  'question': 'What is the specific document number for the correction related to RA Resource Selection During CG-SDT?\\n',\n",
       "  'answer': 'RP-100 RP-231415 F is the document number for the correction on RA Resource Selection During CG-SDT.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38321-i50.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 607}},\n",
       " {'context': 'to a PDSCH reception and has not reported the HARQ-ACK information corresponding to the PDSCH reception if harq-ACK-SpatialBundlingPUCCH is not provided = HARQ-ACK information bit for TB for HARQ process number index in the set of numbers of HARQ processes of serving cell else = binary AND operation of the HARQ-ACK information bits corresponding to first and second transport blocks for HARQ process number index in the set of numbers of HARQ processes of serving cell',\n",
       "  'question': 'What is the condition for not reporting HARQ-ACK information in PDSCH reception?\\n',\n",
       "  'answer': 'HARQ-ACK information bit for TB for HARQ process number index is not provided if harq-ACK-SpatialBundlingPUCCH is not given.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38213-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 267}},\n",
       " {'context': 'Chapter\\nMIMO I: Spatial Multiplexing and\\nChannel Modeling\\nIn this book, we have seen several dierent uses of multiple antennas in wireless com-\\nmunication. In Chapter 3, multiple antennas were used to provide diversity gain and\\nincrease the reliability of wireless links. Both receive and transmit diversity were con-\\nsidered. Moreover, receive antennas can also provide a power gain. In Chapter 5, we\\nsaw that with channel knowledge at the transmitter, multiple transmit antennas can\\nalso provide a power gain via transmit beamforming. In Chapter 6, multiple transmit\\nantennas were used to induce channel variations, which can then be exploited by op-\\nportunistic communication techniques. The scheme can be interpreted as opportunistic\\nbeamforming and provides a power gain as well.\\nIn this and the next few chapters, we will study a new way to use multiple antennas.\\nWe will see that under suitable channel fading conditions, having both multiple transmit\\nand multiple receive antennas (i.e., a MIMO channel) provides an additional spatial\\ndimension for communication and yields a degree-of-freedom gain. These additional\\ndegrees of freedom can be exploited by spatially multiplexing several data streams onto\\nthe MIMO channel, and lead to an increase in the capacity: the capacity of such a\\nMIMO channel with n transmit and receive antennas is proportional to n.\\nHistorically, it has been known for a while that a multiple access system with mul-\\ntiple antennas at the base station allows several users to simultaneously communicate\\nwith the base station. The multiple antennas allow spatial separation of the signals\\nfrom the dierent users. It has been observed in the mid 1990s that a similar eect\\ncan occur for a point-to-point channel with multiple transmit and receive antennas,\\ni.e., even when the transmit antennas are not geographically far apart. This holds\\nprovided that the scattering environment is rich enough to allow the receive antennas',\n",
       "  'question': 'How does the number of antennas affect the capacity of a MIMO channel?\\n',\n",
       "  'answer': 'The capacity of a MIMO channel with n transmit and receive antennas is proportional to n, allowing for increased data transmission capacity.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 343,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '. If a UE is provided NonCellDefiningSSB in BWP-DownlinkDedicated for an active DL BWP, the UE assumes that the active DL BWP includes the SS/PBCH blocks provided by NonCellDefiningSSB. The SS/PBCH blocks provided by NonCellDefiningSSB and the SS/PBCH blocks that the UE used to obtain SIB1 have same QCL properties if they have a same index. Unless otherwise stated, handling of overlapping between downlink receptions or uplink transmissions and the SS/PBCH blocks provided by NonCellDefiningSSB is same as handling of overlapping between downlink receptions or uplink transmissions and the SS/PBCH blocks provided by ssb-PositionsInBurst in SIB1 or in ServingCellConfigCommon. The SS/PBCH blocks in Clause 11.1 for resolving directional collisions for a set of serving cells among multiple serving cells, when the UE is provided directionalCollisionHandling and indicates support of half-DuplexTDD-CA-SameSCS, correspond to the SS/PBCH blocks the UE used to obtain SIB1. A UE does not expect to monitor PDCCH when the UE performs RRM measurements [10, TS 38.133] over a bandwidth that is not within the active DL BWP for the UE. 13\\tUE procedure for monitoring Type0-PDCCH CSS sets If during cell search a UE determines from MIB that a CORESET for Type0-PDCCH CSS set is present, as described in clause 4.1, the UE determines a number of consecutive resource blocks and a number of consecutive symbols for the CORESET of the Type0-PDCCH CSS set from controlResourceSetZero in pdcch-ConfigSIB1, as described in Tables 13-0 through 13-10, for operation without shared spectrum channel access in FR1, FR2-1 and FR2-NTN, or as described in Tables 13-1A and 13-4A for operation with shared spectrum channel access in FR1, or as described in Table 13-10A for FR2-2, and determines PDCCH monitoring occasions from searchSpaceZero in pdcch-ConfigSIB1, included in MIB, as described in Tables 13-11 through 13-15A',\n",
       "  'question': 'What does a UE assume when provided NonCellDefiningSSB in BWP-DownlinkDedicated for an active DL BWP?\\n',\n",
       "  'answer': 'The UE assumes that the active DL BWP includes the SS/PBCH blocks provided by NonCellDefiningSSB, which have the same QCL properties as the blocks it used to obtain SIB1 if they share the same index.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38213-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 510}},\n",
       " {'context': '.e., ciphering shall be applied to all subsequent messages received and sent by the UE, including the message used to indicate the successful completion of the procedure; 1>\\trelease the measurement gap configuration indicated by the measGapConfig, if configured; 1>\\trelease the MUSIM gap configuration indicated by the musim-GapConfig, if configured; 1>\\trelease the FR2 UL gap configuration indicated by the ul-GapFR2-Config, if configured; 1>\\tperform the L2 U2N Remote UE configuration procedure in accordance with the received sl-L2RemoteUE-Config as specified in 5.3.5',\n",
       "  'question': 'What is released if the measGapConfig is configured?\\n',\n",
       "  'answer': 'The measurement gap configuration is released if it is configured.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 310}},\n",
       " {'context': 'NR Module, .3\\n(i.e., slots number 1 and 6 in the figure). In 5G-LENA, flexible slots consist of DL/UL control symbols and a variable\\nnumber of DL and UL symbols for data; DL slots carry only DL control and DL data; and UL slots consist of UL data\\nand UL control parts.\\nFig. 7: Example of SRS transmissions of 4 different UEs (maximum 1 UE SRS transmission per slot, as per 5G-LENA\\ndesign), considering SRS periodicity equal to 20 slots. Numerology considered is  = 0. F stands for frame and SF for\\nsubframe.\\nIn 5G NR, SRS parameters, such as periodicity and offset, are typically configured by RRC and notified to UE [TS38331].\\nAnother option is to have gNB MAC scheduler to determine the SRS periodicity/offset and then to notify UE through\\nDCI format 2_3 on which resources SRS should be transmitted [TS38212]. The latter option, scheduling-based SRS, is\\na more dynamic approach and allows more flexible SRS parameter and periodicity assignment, e.g., when there are less\\nUEs, a lower periodicity value can be used, while when there are more UEs, the gNB MAC scheduler can dynamically\\nincrease the periodicity and then update the offsets accordingly. We have implemented scheduling-based SRS.\\nTo allow dynamic SRS scheduling and adjustment of SRS periodicity/offset of all UEs, we introduced NrMacSched-\\nulerSrs and NrMacSchedulerSrsDefault into 5G-LENA model.\\nNrMacSchedulerSrs is an interface that is used by the NR gNB MAC scheduler to obtain the SRS offset/periodicity\\nfor a UE. There can be various implementations of this interface that would simulate different algorithms for SRS off-\\nset/periodicity generation. In NrMacSchedulerSrsDefault, we provide one possible implementation. Each time\\na new UE is attached it is called the function AddUe that returns the offset/periodicity configuration. When scheduler\\ndetects that the SRS periodicity is too small for the number of UEs it calls the IncreasePeriodicity, which picks',\n",
       "  'question': 'What is the purpose of the NrMacSchedulerSrs interface in 5G-LENA?\\n',\n",
       "  'answer': 'The NrMacSchedulerSrs interface is used by the NR gNB MAC scheduler to obtain the SRS offset/periodicity for a UE, allowing dynamic SRS scheduling and adjustment of SRS periodicity/offset for all UEs.',\n",
       "  'source_doc': {'document': 'documents/nrmodule.pdf',\n",
       "   'page': 22,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '. EUTRA-MultiBandInfoList information element -- ASN1START -- TAG-EUTRA-MULTIBANDINFOLIST-START EUTRA-MultiBandInfoList ::= SEQUENCE (SIZE (1..maxMultiBands)) OF EUTRA-MultiBandInfo EUTRA-MultiBandInfo ::= SEQUENCE { eutra-FreqBandIndicator FreqBandIndicatorEUTRA, eutra-NS-PmaxList EUTRA-NS-PmaxList OPTIONAL -- Need R } -- TAG-EUTRA-MULTIBANDINFOLIST-STOP -- ASN1STOP -\\tEUTRA-MultiBandInfoListAerial The IE EUTRA-MultiBandInfoListAerial indicates the list of frequency bands for aerial UE in addition to the band represented by CarrierFreq for which cell reselection parameters are common, and a list of additionalPmax and additionalSpectrumEmission. EUTRA-MultiBandInfoListAerial information element -- ASN1START -- TAG-EUTRA-MULTIBANDINFOLISTAERIAL-START EUTRA-MultiBandInfoListAerial-r18 ::= SEQUENCE (SIZE (1..maxMultiBands)) OF EUTRA-MultiBandInfoAerial-r18 EUTRA-MultiBandInfoAerial-r18 ::= SEQUENCE { eutra-FreqBandIndicator-r18 FreqBandIndicatorEUTRA, eutra-NS-PmaxListAerial-r18 EUTRA-NS-PmaxList OPTIONAL -- Need R } -- TAG-EUTRA-MULTIBANDINFOLISTAERIAL-STOP -- ASN1STOP -\\tEUTRA-NS-PmaxList The IE EUTRA-NS-PmaxList concerns a list of additionalPmax and additionalSpectrumEmission, as defined in TS 36.101 [22], clause 6, for a given frequency band. EUTRA-NS-PmaxList information element -- ASN1START -- TAG-EUTRA-NS-PMAXLIST-START EUTRA-NS-PmaxList ::= SEQUENCE (SIZE (1..maxEUTRA-NS-Pmax)) OF EUTRA-NS-PmaxValue EUTRA-NS-PmaxValue ::= SEQUENCE { additionalPmax INTEGER (-30..33) OPTIONAL, -- Need R additionalSpectrumEmission INTEGER (1..288) OPTIONAL -- Need R } -- TAG-EUTRA-NS-PMAXLIST-STOP -- ASN1STOP -\\tEUTRA-PhysCellId The IE EUTRA-PhysCellId is used to indicate the physical layer identity of the cell, as defined in TS 36.211 [31]. EUTRA-PhysCellId information element -- ASN1START -- TAG-EUTRA-PHYSCELLID-START EUTRA-PhysCellId ::= INTEGER (0.',\n",
       "  'question': 'What does the EUTRA-MultiBandInfoList information element represent?\\n',\n",
       "  'answer': 'It represents a list of frequency bands for aerial UE in addition to the band represented by CarrierFreq.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 2133}},\n",
       " {'context': '2\\tConstants a) Window_Size This constant indicates the size of the reordering window. The value equals to 2[pdcp-SN-SizeDL] - 1 for SRB/DRB/MRB and 2[sl-PDCP-SN-Size] - 1 for SLRB.\\n3\\tTimers The transmitting PDCP entity shall maintain the following timers: a) discardTimer This timer is configured only for DRBs. The duration of the timer is configured by upper layers TS 38.331 [3]. In the transmitter, a new timer is started upon reception of an SDU from upper layer as specified in clause 5.2.1. b) discardTimerForLowImportance This timer is configured only for DRBs. The duration of the timer is configured by upper layers TS 38.331 [3]. In the transmitter, a new timer is started upon reception of an SDU belonging to a low importance PDU Set from upper layer as specified in clause 5.2.1. The receiving PDCP entity shall maintain the following timers: c) t-Reordering The duration of the timer is configured by upper layers TS 38.331 [3], except for the case of NR sidelink communication or sidelink SRB4. For NR sidelink communication or sidelink SRB4, the t-Reordering timer is determined by the UE implementation. This timer is used to detect loss of PDCP Data PDUs as specified in clause 5.2.2. If t-Reordering is running, t-Reordering shall not be started additionally, i.e. only one t-Reordering per receiving PDCP entity is running at a given time. Annex A (normative):',\n",
       "  'question': 'What does the discardTimerForLowImportance timer do?\\n',\n",
       "  'answer': 'It is configured only for DRBs and starts a new timer upon receiving an SDU from a low importance PDU Set from upper layer as specified in clause 5.2.1.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38323-i50.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 63}},\n",
       " {'context': '3 The Fourier Transform The Fourier transform provides the mathematical tool for measuring the frequency\\ncontent, or spectrum, of a signal. For this reason, the terms Fourier transform and\\nspectrum are used interchangeably. Thus, given a signal g(t) with Fourier transform G(f),\\nwe may refer to G(f) as the spectrum of the signal g(t). By the same token, we refer to\\n|G(f)| as the magnitude spectrum of the signal g(t), and refer to arg[G(f)] as its phase\\nspectrum.\\nIf the signal g(t) is real valued, then the magnitude spectrum of the signal is an even\\nfunction of frequency f, while the phase spectrum is an odd function of f. In such a case,\\nknowledge of the spectrum of the signal for positive frequencies uniquely defines the\\nspectrum for negative frequencies.\\nNotations\\nFor convenience of presentation, it is customary to express (2.17) in the short-hand form\\nwhere F plays the role of an operator. In a corresponding way, (2.18) is expressed in the\\nshort-hand form\\nwhere F-1 plays the role of an inverse operator.\\nThe time function g(t) and the corresponding frequency function G(f) are said to\\nconstitute a Fourier-transform pair. To emphasize this point, we write\\nwhere the top arrow indicates the forward transformation from g(t) to G(f) and the bottom\\narrow indicates the inverse transformation. One other notation: the asterisk is used to\\ndenote complex conjugation.\\nTables of Fourier Tranformations\\nTo assist the user of this book, two tables of Fourier transformations are included:\\nTable 2.1 on summarizes the properties of Fourier transforms; proofs of\\nthem are presented as end-of-chapter problems.\\nTable 2.2 on presents a list of Fourier-transform pairs, where the items\\nlisted on the left-hand side of the table are time functions and those in the center\\ncolumn are their Fourier transforms.\\nEXAMPLE\\nBinary Sequence for Energy Calculations\\nConsider the five-digit binary sequence 10010. This sequence is represented by two',\n",
       "  'question': 'What is the magnitude spectrum of a real-valued signal?\\n',\n",
       "  'answer': 'It is an even function of frequency f.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 39,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '..maxNrofBWPs)) OF BWP-Id OPTIONAL, -- Need N downlinkBWP-ToAddModList SEQUENCE (SIZE (1..maxNrofBWPs)) OF BWP-Downlink OPTIONAL, -- Need N firstActiveDownlinkBWP-Id BWP-Id OPTIONAL, -- Cond SyncAndCellAdd bwp-InactivityTimer ENUMERATED {ms2, ms3, ms4, ms5, ms6, ms8, ms10, ms20, ms30, ms40,ms50, ms60, ms80,ms100, ms200,ms300, ms500, ms750, ms1280, ms1920, ms2560, spare10, spare9, spare8, spare7, spare6, spare5, spare4, spare3, spare2, spare1 } OPTIONAL, --Need R defaultDownlinkBWP-Id BWP-Id OPTIONAL, -- Need S uplinkConfig UplinkConfig OPTIONAL, -- Need M supplementaryUplink UplinkConfig OPTIONAL, -- Need M pdcch-ServingCellConfig SetupRelease { PDCCH-ServingCellConfig } OPTIONAL, -- Need M pdsch-ServingCellConfig SetupRelease { PDSCH-ServingCellConfig } OPTIONAL, -- Need M csi-MeasConfig SetupRelease { CSI-MeasConfig } OPTIONAL, -- Need M sCellDeactivationTimer ENUMERATED {ms20, ms40, ms80, ms160, ms200, ms240, ms320, ms400, ms480, ms520, ms640, ms720, ms840, ms1280, spare2,spare1} OPTIONAL, -- Cond ServingCellWithoutPUCCH crossCarrierSchedulingConfig CrossCarrierSchedulingConfig OPTIONAL, -- Need M tag-Id TAG-Id, dummy1 ENUMERATED {enabled} OPTIONAL, -- Need R pathlossReferenceLinking ENUMERATED {spCell, sCell} OPTIONAL, -- Cond SCellOnly servingCellMO MeasObjectId OPTIONAL, -- Cond MeasObject ..., [[ lte-CRS-ToMatchAround SetupRelease { RateMatchPatternLTE-CRS } OPTIONAL, -- Need M rateMatchPatternToAddModList SEQUENCE (SIZE (1..maxNrofRateMatchPatterns)) OF RateMatchPattern OPTIONAL, -- Need N rateMatchPatternToReleaseList SEQUENCE (SIZE (1..maxNrofRateMatchPatterns)) OF RateMatchPatternId OPTIONAL, -- Need N downlinkChannelBW-PerSCS-List SEQUENCE (SIZE (1.',\n",
       "  'question': 'What are the possible values for the bwp-InactivityTimer?\\n',\n",
       "  'answer': 'The bwp-InactivityTimer can be one of several values including ms2, ms3, ms4, ms5, ms6, ms8, ms10, ms20, ms30, ms40, ms50, ms60, ms80, ms100, ms200, ms300, ms500, ms750, ms1280, ms1920, ms2560, spare10, spare9, spare8, spare7, spare6, spare5, spare4, spare3, spare2, and spare1.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1711}},\n",
       " {'context': '. SimultaneousRxTxPerBandPair information element -- ASN1START -- TAG-SIMULTANEOUSRXTXPERBANDPAIR-START SimultaneousRxTxPerBandPair ::= BIT STRING (SIZE (3..496)) -- TAG-SIMULTANEOUSRXTXPERBANDPAIR-STOP -- ASN1STOP -\\tSON-Parameters The IE SON-Parameters contains SON related parameters. SON-Parameters information element -- ASN1START -- TAG-SON-PARAMETERS-START SON-Parameters-r16 ::= SEQUENCE { rach-Report-r16 ENUMERATED {supported} OPTIONAL, ..., [[ rlfReportCHO-r17 ENUMERATED {supported} OPTIONAL, rlfReportDAPS-r17 ENUMERATED {supported} OPTIONAL, success-HO-Report-r17 ENUMERATED {supported} OPTIONAL, twoStepRACH-Report-r17 ENUMERATED {supported} OPTIONAL, pscell-MHI-Report-r17 ENUMERATED {supported} OPTIONAL, onDemandSI-Report-r17 ENUMERATED {supported} OPTIONAL ]], [[ cef-ReportRedCap-r17 ENUMERATED {supported} OPTIONAL, rlf-ReportRedCap-r17 ENUMERATED {supported} OPTIONAL ]], [[ spr-Report-r18 ENUMERATED {supported} OPTIONAL, successIRAT-HO-Report-r18 ENUMERATED {supported} OPTIONAL ]] } -- TAG-SON-PARAMETERS-STOP -- ASN1STOP -\\tSpatialRelationsSRS-Pos The IE SpatialRelationsSRS-Pos is used to convey spatial relation for SRS for positioning related parameters. SpatialRelationsSRS-Pos information element -- ASN1START -- TAG-SPATIALRELATIONSSRS-POS-START SpatialRelationsSRS-Pos-r16 ::= SEQUENCE { spatialRelation-SRS-PosBasedOnSSB-Serving-r16 ENUMERATED {supported} OPTIONAL, spatialRelation-SRS-PosBasedOnCSI-RS-Serving-r16 ENUMERATED {supported} OPTIONAL, spatialRelation-SRS-PosBasedOnPRS-Serving-r16 ENUMERATED {supported} OPTIONAL, spatialRelation-SRS-PosBasedOnSRS-r16 ENUMERATED {supported} OPTIONAL, spatialRelation-SRS-PosBasedOnSSB-Neigh-r16 ENUMERATED {supported} OPTIONAL, spatialRelation-SRS-PosBasedOnPRS-Neigh-r16 ENUMERATED {supported} OPTIONAL } --TAG-SPATIALRELATIONSSRS-POS-STOP -- ASN1STOP -\\tSRS-AllPosResourcesRRC-Inactive The IE SRS-AllPosResourcesRRC-Inactive is used to convey SRS positioning related parameters specific for a certain band',\n",
       "  'question': 'What does the SON-Parameters information element contain?\\n',\n",
       "  'answer': 'The SON-Parameters information element contains SON related parameters such as rach-Report-r16, rlfReportCHO-r17, rlfReportDAPS-r17, success-HO-Report-r17, twoStepRACH-Report-r17, pscell-MHI-Report-r17, onDemandSI-Report-r17, cef-ReportRedCap-r17, rlf-ReportRedCap-r17, spr-Report-r18, and successIRAT-HO-Report-r18.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 2104}},\n",
       " {'context': 'obstacles.\\nIn the present situation, if we assume that the wall is very large, the reflected wave\\nat a given point is the same (except for a sign change) as the free space wave that would\\nexist on the opposite side of the wall if the wall were not present (see Figure 2.3). This\\nmeans that the reflected wave from the wall has the intensity of a free space wave at\\na distance equal to the distance to the wall and then back to the receive antenna, i.e.,\\n2d r. Using (2.2) for both the direct and the reflected wave, and assuming the same',\n",
       "  'question': 'What is the intensity of the reflected wave from the wall?\\n',\n",
       "  'answer': 'The intensity of the reflected wave from the wall is the same as a free space wave at a distance of 2dr, where dr is the distance to the wall and back to the receive antenna.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 26,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': '. The configured cellDTX-DRX-Cycle is an integer multiple of configured drx-longCycle or vice versa. cellDTX-DRX-onDurationTimer Value in multiples of 1/32 ms (subMilliSeconds) or in ms (milliSecond). For the latter, value ms1 corresponds to 1 ms, value ms2 corresponds to 2 ms, and so on. cellDTX-DRX-SlotOffset Value in 1/32 ms. Value 0 corresponds to 0 ms, value 1 corresponds to 1/32 ms, value 2 corresponds to 2/32 ms, and so on. cellDTX-DRX-ActivationStatus Initial activation status of cell DTX/DRX indicating whether the UE shall activate the configuration according to the received parameters. This field is only used upon setup of a cell DTX/DRX configuration. cellDTX-DRX-ConfigType Indicates whether the configuration is for cell DTX only, cell DRX only, or joint cell DTX/DRX configuration. -\\tCellGroupConfig The CellGroupConfig IE is used to configure a master cell group (MCG) or secondary cell group (SCG). A cell group comprises of one MAC entity, a set of logical channels with associated RLC entities and of a primary cell (SpCell) and one or more secondary cells (SCells). For an NCR-MT, the CellGroupConfig IE is also used to provide the configuration of side control information for the NCR-Fwd access link. CellGroupConfig information element -- ASN1START -- TAG-CELLGROUPCONFIG-START -- Configuration of one Cell-Group: CellGroupConfig ::= SEQUENCE { cellGroupId CellGroupId, rlc-BearerToAddModList SEQUENCE (SIZE(1..maxLC-ID)) OF RLC-BearerConfig OPTIONAL, -- Need N rlc-BearerToReleaseList SEQUENCE (SIZE(1..maxLC-ID)) OF LogicalChannelIdentity OPTIONAL, -- Need N mac-CellGroupConfig MAC-CellGroupConfig OPTIONAL, -- Need M physicalCellGroupConfig PhysicalCellGroupConfig OPTIONAL, -- Need M spCellConfig SpCellConfig OPTIONAL, -- Need M sCellToAddModList SEQUENCE (SIZE (1..maxNrofSCells)) OF SCellConfig OPTIONAL, -- Need N sCellToReleaseList SEQUENCE (SIZE (1..maxNrofSCells)) OF SCellIndex OPTIONAL, -- Need N ..',\n",
       "  'question': 'What are the units of measurement for the cellDTX-DRX-onDurationTimer value?\\n',\n",
       "  'answer': 'The cellDTX-DRX-onDurationTimer value can be measured in multiples of 1/32 milliseconds or in whole milliseconds, with each whole millisecond value corresponding to an increment of one millisecond.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1214}},\n",
       " {'context': '.A, after determining a set of PUCCH resources for HARQ-ACK information bits, as described in clause 9.2.1. The PUCCH resource determination is based on a PUCCH resource indicator field [5, TS 38.212], if present, in a last DCI format, excluding the SPS activation DCI, among the DCI formats that have a value of a PDSCH-to-HARQ_feedback timing indicator field, if present, or a value of dl-DataToUL-ACK, or dl-DataToUL-ACK-r16, or dl-DataToUL-ACK-DCI-1-2, or dl-DataToUL-ACK-r17, or dl-DataToUL-ACK-DCI-1-2-r17, or dl-DataToUL-ACK-MulticastDCI-Format4-1, or dl-DataToUL-ACK-v1700, indicating a same slot for the PUCCH transmission, that the UE detects and for which the UE transmits corresponding HARQ-ACK information in the PUCCH. The PUCCH resource indicator field values map to values of a set of PUCCH resource indexes, as defined in Table 9.2.3-2 for a PUCCH resource indicator field of 3 bits, provided by resourceList for PUCCH resources from a set of PUCCH resources provided by PUCCH-ResourceSet with a maximum of eight PUCCH resources. If the PUCCH resource indicator field includes 1 bit or 2 bits, the values map to the first two values or the first four values, respectively, of Table 9.2.3-2. If the last DCI format does not include a PUCCH resource indicator field, the first value of Table 9.2.3-2 is used',\n",
       "  'question': 'What values indicate a same slot for the PUCCH transmission?\\n',\n",
       "  'answer': 'dl-DataToUL-ACK, dl-DataToUL-ACK-r16, dl-DataToUL-ACK-DCI-1-2, dl-DataToUL-ACK-r17, dl-DataToUL-ACK-DCI-1-2-r17, dl-DataToUL-ACK-MulticastDCI-Format4-1, or dl-DataToUL-ACK-v1700.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38213-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 293}},\n",
       " {'context': \"1\\tTransport Block repetition for uplink transmissions of PUSCH repetition Type A with a configured grant The procedures described in this clause apply to PUSCH transmissions of PUSCH repetition Type A with a Type 1 or Type 2 configured grant. The higher layer parameter repK-RV defines the redundancy version pattern to be applied to the repetitions. If cg-RetransmissionTimer is provided, the redundancy version for uplink transmission with a configured grant is determined by the UE. If the parameter repK-RV is not provided in the configuredGrantConfig and cg-RetransmissionTimer is not provided, the redundancy version for uplink transmissions with a configured grant shall be set to 0. If the parameter repK-RV is provided in the configuredGrantConfig and cg-RetransmissionTimer is not provided, for the nth transmission occasion among K repetitions, n=1, 2, , K, it is associated with (mod(((n-mod(n, N))/N)-1,4)+1)th value in the configured RV sequence, where N=1. If a configured grant configuration is configured with startingFromRV0 set to 'off', the initial transmission of a transport block may only start at the first transmission occasion of the K repetitions. Otherwise, the initial transmission of a transport block may start at -\\tthe first transmission occasion of the K repetitions if the configured RV sequence is {0,2,3,1}, -\\tany of the transmission occasions of the K repetitions that are associated with RV=0 if the configured RV sequence is {0,3,0,3}, -\\tany of the transmission occasions of the K repetitions if the configured RV sequence is {0,0,0,0}, except the last transmission occasion when K8\",\n",
       "  'question': 'What determines the redundancy version pattern for PUSCH repetitions with a configured grant?\\n',\n",
       "  'answer': 'The higher layer parameter repK-RV defines the redundancy version pattern to be applied to the repetitions.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38214-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 418}},\n",
       " {'context': 'the abstraction model explained in the following section. Finally, CalculateTheEstimatedLongTermMetric\\ncalculates the metric that is used to select the best BF pair.\\nIn Figure Diagram of beamforming model, dependencies on 3GPP channel related classes, and NrGnbPhy., we show the\\ndiagram of the classes that are used for realistic BF based on SRS measurements, the dependencies among classes, and the\\nmost important methods. E.g., we can see that RealisticBeamformingAlgorithm needs to access to ThreeGppChannelModel\\nto obtain the channel matrix in order to perform the estimation of the channel based on SRS report.\\nAbstraction model for SRS-based channel estimation\\nAssume a single-antenna system. Let h denote the (complex-valued) small-scale fading channel between a UE and a\\ngNB. Then, the estimation of the small-scale fading channel at the gNB can be modeled as in [SigProc5G] :\\nh = (h + e),\\nwhere  is a scaling factor to maintain normalization of the estimated channel, and e is the white complex Gaussian\\nchannel estimation error. The estimation error is assumed to be characterized by zero-mean and variance 2\\ne.\\nThe variance of the error is given by:\\n2 e = 1 (SINR+), where SINR is the received SINR (or SNR) of SRS at the gNB and is the gain obtained from time-domain filtering\\nduring the channel estimation. According to 3GPP analysis of SRS transmission, is set to 9 dB [SigProc5G]. The\\nscaling factor is given by:\\n =  1 (1+2e). Then, the channel matrix estimate can be used to compute transmit/receive BF vectors, as part of the beam management.\\n10 SRS transmission and reception\\nSRS transmission typically spans over 1, 2 or 4 consecutive OFDM symbols at the end of the NR slot. 5G-LENA\\nimplements such behaviour in the time domain by allowing different configurations. In the frequency domain, in order to\\nallow frequency multiplexing, SRS is typically transmitted over only a subset of subcarriers, defined by the configuration,',\n",
       "  'question': 'What is the variance of the error in the estimation of the small-scale fading channel at the gNB?\\n',\n",
       "  'answer': 'The variance of the error is given by 1/SINR+ where SINR is the received SINR (or SNR) of SRS at the gNB and is the gain obtained from time-domain filtering during the channel estimation, with being set to 9 dB.',\n",
       "  'source_doc': {'document': 'documents/nrmodule.pdf',\n",
       "   'page': 21,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': 'B.8 Outage Formulation\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\nB.9 Multiple Access Channel . . . . . . . . . . . . . . . . . . . . . . . . . .\\nB.9.1\\nCapacity Region\\n. . . . . . . . . . . . . . . . . . . . . . . . . .\\nB.9.2\\nCorner Points of the Capacity Region . . . . . . . . . . . . . . .\\nB.9.3\\nFast Fading Uplink . . . . . . . . . . . . . . . . . . . . . . . . .\\nB.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       "  'question': 'What is the section title immediately following B.9.2 in the context?\\n',\n",
       "  'answer': 'The section title is \"B.10 Exercises.\"',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 8,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': 'of the even smarter transmit beamforming.\\nHow about in a fast Rayleigh fading environment? In this case, we have observed\\nthat dumb antennas have no eect on the overall channel as the full multiuser diversity\\ngain has already been realized. Space-time codes, on the other hand, increase the\\ndiversity of the point-to-point links and consequently decrease the channel fluctuations\\nand hence the multiuser diversity gain . (Exercise 6.31 makes this more precise.) Thus,\\nthe use of space-time codes as a point-to-point technology in a multiuser downlink with\\nrate control and scheduling can actually be harmful, in the sense that even the naturally\\npresent multiuser diversity is removed.\\nThe performance impact of using transmit\\nbeamforming is not so clear: on the one hand it reduces the channel fluctuation and\\nhence the multiuser diversity gain, but on the other hand it provides an array power\\ngain. However, in an FDD system the fast fading channel may make it very dicult\\nto feed back so much information to enable coherent beamforming.\\nThe comparison between the three schemes is summarized in Table 6.1. All three\\ntechniques use the multiple antennas to transmit to only one user at a time. With full\\nchannel knowledge at the transmitter, an even smarter scheme can transmit to multiple\\nusers simultaneously, exploiting the multiple degrees of freedom existing inherently in\\nthe multiple antenna channel. We will discuss this in Chapter 10.',\n",
       "  'question': 'What effect do dumb antennas have in a fast Rayleigh fading environment?\\n',\n",
       "  'answer': 'They have no effect on the overall channel as the full multiuser diversity gain has already been realized.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 318,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': '.213 [48]. If the field is configured as \"semiStatic\", the UE shall apply the channel access procedures for semi-static channel occupancy as described in clause 4.3 in TS 37.213. If the field is configured as \"dynamic\", the UE shall apply the channel access procedures as defined in TS 37.213, clause 4.1 and 4.2. channelAccessMode2 If present, the UE shall apply channel access procedures for operation with shared spectrum channel access in accordance with TS 37.213 [48], clause 4.4 for FR2-2. If absent, the UE shall not apply any channel access procedure. The network always configures this field if channel access procedures are required for the serving cell within this region by regulations. dmrs-TypeA-Position Position of (first) DM-RS for downlink (see TS 38.211 [16], clause 7.4.1.1.1) and uplink (TS 38.211 [16], clause 6.4.1.1.3). downlinkConfigCommon The common downlink configuration of the serving cell, including the frequency information configuration and the initial downlink BWP common configuration. The parameters provided herein should match the parameters configured by MIB and SIB1 (if provided) of the serving cell, with the exception of controlResourceSetZero and searchSpaceZero which can be configured in ServingCellConfigCommon even if MIB indicates that they are absent. discoveryBurstWindowLength Indicates the window length of the discovery burst in ms (see TS 37.213 [48]). The field discoveryBurstWindowLength-r17 is applicable to SCS 480 kHz and SCS 960 kHz. featurePriorities Indicates priorities for features, such as (e)RedCap, Slicing, SDT, MSG1-Repetitions, and MSG3-Repetitions for Coverage Enhancements. These priorities are used to determine which FeatureCombinationPreambles the UE shall use when a feature maps to more than one FeatureCombinationPreambles, as specified in TS 38.321 [3]. A lower value means a higher priority. The network does not signal the same priority for more than one feature',\n",
       "  'question': 'What is the position of the first DM-RS for downlink and uplink?\\n',\n",
       "  'answer': 'The position of the first DM-RS for downlink is specified in TS 38.211 clause 7.4.1.1.1, and for uplink in TS 38.211 clause 6.4.1.1.3.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1749}},\n",
       " {'context': \". For PUSCH repetition Type B carrying semi-persistent CSI report(s) without a corresponding PDCCH after being activated on PUSCH by a 'CSI request' field on a DCI, if the first nominal repetition is not the same as the first actual repetition, the first nominal repetition is omitted; otherwise, the first nominal repetition is omitted according to the conditions in Clause 9, Clause 11.1, Clause 11.2A, Clause 15 and Clause 17.2 of [6, TS 38.213], and Clause 5.34.3 of [10, TS 38.321]. For PUSCH repetition Type B, when a UE is scheduled to transmit a transport block and aperiodic CSI report(s) on PUSCH by a 'CSI request' field on a DCI, the CSI report(s) is multiplexed only on the first actual repetition. The UE does not expect that the first actual repetition has a single symbol duration. For pusch-TimeDomainAllocationListForMultiPUSCH in pusch-Config, if a row indicates resource allocation for two to eight contiguous PUSCHs and extendedK2 is not configured, K2 given by k2-r16 indicates the slot where UE shall transmit the first PUSCH of the multiple PUSCHs. Each PUSCH has a separate SLIV and mapping type. The number of scheduled PUSCHs is signalled by the number of indicated valid SLIVs in the row of the pusch-TimeDomainAllocationListForMultiPUSCH signalled in DCI format 0_1. For pusch-TimeDomainAllocationListForMultiPUSCH in pusch-Config, if a row indicates resource allocation of more than one PUSCH and extendedK2 is configured, each PUSCH has a separate SLIV, mapping type and K2 given by extendedK2. If a row indicates resource allocation of a single PUSCH, the PUSCH has a single SLIV, mapping type, and K2, where K2 is given by extendedK2, if configured, otherwise K2 is given by k2-r16. The number of scheduled PUSCHs is signalled by the number of indicated SLIVs in the row of the pusch-TimeDomainAllocationListForMultiPUSCH signalled in DCI format 0_1\",\n",
       "  'question': \"What happens to the first nominal repetition in PUSCH repetition Type B if it's not the same as the first actual repetition?\\n\",\n",
       "  'answer': 'The first nominal repetition is omitted based on specific conditions outlined in several clauses of TS 38.213 and TS 38.321.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38214-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 388}},\n",
       " {'context': 'the delay spread  amounts to about 20 s;\\n\\nthe Doppler spread  due to the motion of a vehicle may typically occupy the range\\n40-100 Hz, but sometimes may well exceed 100 Hz.\\nOne other parameter directly related to the Doppler spread is the coherence time of the\\nchannel. Here again, as with coherence bandwidth discussed previously, we may invoke\\nthe inverse time-frequency relationship to say that the coherence time of a multipath\\nwireless channel is inversely proportional to the Doppler spread, as shown by\\n(9.46) rH 0 t  ;   f  0 = SH   S ;   d  -   =  2SH   d  -   SH   d  -   --------------------------------------\\n           1 2  = ffade rate 1.475 = coherence 1  ------ = 0.3 2max --------------',\n",
       "  'question': 'What is the typical range of Doppler spread due to vehicle motion?\\n',\n",
       "  'answer': 'The Doppler spread due to vehicle motion typically occupies the range of 40-100 Hz, but it can sometimes exceed 100 Hz.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 538,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': '9 Adaptive Equalization limited condition. Specifically, the channel is now modeled by a low-pass Butterworth\\nfilter, whose frequency response is defined by\\nwhere N is the order of the filter, and f0 is the 3-dB cutoff frequency of the filter. For the\\nresults displayed in Figure 8.16, the following filter parameter values were used:\\nWith the roll-off factor and Nyquist bandwidth\\n, for binary PAM,\\nthe use of (8.24) defines the transmission bandwidth of the PAM transmission system to be\\nAlthough the channel bandwidth cutoff frequency is greater than absolutely necessary, its\\neffect on the passband is observed in a decrease in the size of the eye opening. Instead of\\nthe distinct values at time t = 1s, shown in Figure 8.15a and b, now there is a blurred\\nregion. If the channel bandwidth were to be reduced further, the eye would close even\\nmore until finally no distinct eye opening would be recognizable. Adaptive Equalization\\nIn this section we develop a simple and yet effective algorithm for the adaptive equaliza-\\ntion of a linear channel of unknown characteristics. Figure 8.17 shows the structure of an\\nadaptive synchronous equalizer, which incorporates the matched filtering action. The\\nalgorithm used to adjust the equalizer coefficients assumes the availability of a desired\\nresponse. Ones first reaction to the availability of a replica of the transmitted signal is: If\\nsuch a signal is available at the receiver, why do we need adaptive equalization? To answer\\nthis question, we first note that a typical telephone channel changes little during an aver-\\nage data call. Accordingly, prior to data transmission, the equalizer is adjusted under the\\nH f 1 1 f f0   2N + ------------------------------\\n= N 3 and f0  0.6 Hz for binary PAM\\nN 3 and f0  0.3 Hz for 4-PAM = = = =  0.5 = W 0.5 Hz = BT 0.5 1 0.5 +   0.75 Hz = = Figure 8.17 Block diagram of adaptive equalizer using an adjustable TDL filter.',\n",
       "  'question': 'What is the effect of reducing the channel bandwidth on the eye opening in a transmission system?\\n',\n",
       "  'answer': 'Reducing the channel bandwidth causes the eye opening to close more, leading to a blurred region and eventual unrecognizability of distinct eye openings.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 489,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '. -\\tfor all other PDSCH transmissions, the set of resource blocks in bandwidth part with starting position are divided into resource-block bundles in increasing order of the resource-block number and bundle number where is the bundle size for bandwidth part provided by the higher-layer parameter vrb-ToPRB-Interleaver for DCI formats 1_0, 1_1, and 1_3 in a UE-specific search space, or vrb-ToPRB-InterleaverDCI-1-2 for DCI format 1_2, and -\\tresource block bundle 0 consists of resource blocks, -\\tresource block bundle consists of resource blocks if and resource blocks otherwise, -\\tall other resource block bundles consists of resource blocks. -\\tVirtual resource blocks in the interval are mapped to physical resource blocks according to -\\tvirtual resource block bundle is mapped to physical resource block bundle -\\tvirtual resource block bundle is mapped to physical resource block bundle where -\\tThe UE is not expected to be configured with simultaneously with a PRG size of 4 as defined in [6, TS 38.214] The UE may assume that the same precoding in the frequency domain is used within a PRB bundle and the bundle size is determined by clause 5.1.2.3 in [6, TS 38.214]. The UE shall not make any assumption that the same precoding is used for different bundles of common resource blocks. For PDSCH transmissions scheduled by DCI format 4_1 or 4_2, and using G-RNTI or G-CS-RNTI, the quantities and in this clause are replaced by and , respectively, and is the bundle size for the common MBS frequency resource provided by the higher-layer parameter vrb-ToPRB-Interleaver in pdsch-ConfigMulticast. For PDSCH transmissions scheduled by DCI format 4_0, and using G-RNTI for broadcast, MCCH-RNTI, or Multicast-MCCH-RNTI, the quantities and in this clause are replaced by and , respectively, and .',\n",
       "  'question': 'What is the size of resource block bundle 0?\\n',\n",
       "  'answer': 'It consists of resource blocks.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38211-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 97}},\n",
       " {'context': 'Chapter\\nSignaling over Band-Limited Channels\\nEquations (8.85) and (8.86) follow from discretizing the continuous-time Fourier\\ntransform both in time and frequency, as discussed in Chapter 2 with one difference: the\\nDFT in (8.65) and its inverse in (8.66) have the same scaling factor, for the purpose\\nof symmetry.\\nAlthough the DFT and IDFT appear to be similar in their mathematical formulations,\\ntheir interpretations are different, as discussed previously in Chapter 2. As a reminder, we\\nmay interpret the DFT process described in (8.85) as a system of N complex heterodyning\\nand averaging operations, as shown in Figure 2.32a. In the picture depicted in this part of\\nthe figure, heterodyning refers to the multiplication of the data sequence xn by one of N\\ncomplex exponentials, exp(-j2knN). As such, (8.85) may be viewed as the analysis\\nequation. For the interpretation of (8.86), we may view it as the synthesis equation:\\nspecifically, the complex Fourier coefficient Xk is weighted by one of N complex\\nexponentials exp(-j2knN). At time n, the output xn is formed by summing the weighted\\ncomplex Fourier coefficients, as shown in Figure 2.32b.\\nAn important property of a circulant matrix, exemplified by the channel matrix H of\\n(8.83), is that it permits the spectral decomposition defined by\\n(8.87)\\nwhere the superscript  denotes Hermitian transposition (i.e., the combination of complex\\nconjugation and ordinary matrix transposition). Descriptions of the matrices Q and are\\npresented in the following in that order. The matrix Q is a square matrix defined in terms\\nof the kernel of the N-point DFT as shown by\\n(8.88)\\nFrom this definition, we readily see that the klth element of the N-by-N matrix, Q, starting\\nfrom the bottom right at k = 0 and l = 0 and counting up step-by-step, is\\n(8.89)\\nThe matrix Q is an orthonormal matrix or unitary matrix, in the sense that it satisfies the\\ncondition\\n(8.90)\\nwhere I is the identity matrix. That is, the inverse matrix of Q is equal to the Hermitian',\n",
       "  'question': 'What is the interpretation of the DFT process described in (8.85)?\\n',\n",
       "  'answer': 'The DFT process is interpreted as a system of N complex heterodyning and averaging operations.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 510,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': \". - is selected by UE where . When the UE performs at least contiguous partial sensing and if , the UE selects a set of candidate slots with corresponding PBPS and/or CPS results (if available). If the number of candidate slots is smaller than , it is up to UE implementation to include other candidate slots. If the higher layer parameter sl-TransmissionStructureForPSCCHandPSSCH is set to 'contiguousRB', the UE shall exclude candidate single-slot or candidate multi-slot resources with the sub-channel with the smallest index including resource blocks of the intra-cell guardband PRBs, configured by higher layer parameter, sl-IntraCellGuardBandsSL-List, or determined according to the nominal intra-cell guard band and RB set pattern as specified in [8, TS 38.101-1] when higher layer parameter, sl-IntraCellGuardBandsSL-List, is not configured. If rbSetsWithConsecutiveLBTFailure is provided, the UE shall exclude candidate single-slot resources or candidate multi-slot resources, whose associated one or more RB set(s) is included in the rbSetsWithConsecutiveLBTFailure parameter. The total number of remaining candidate single-slot resources or candidate multi-slot resources is denoted by . 2)\\tThe sensing window is defined by the range of slots [), when the UE performs full sensing, where is defined above and is defined in slots in Table 8.1.4-1 where is the SCS configuration of the SL BWP. The UE shall monitor slots which belongs to a sidelink resource pool within the sensing window except for those in which its own transmissions occur. The UE shall perform the behaviour in the following steps based on PSCCH decoded and RSRP measured in these slots. When the UE performs periodic-based partial sensing, the UE shall monitor slots at , where is a slot of the selected candidate slots and is converted to units of logical slot according to clause 8.1.7. The UE shall perform the behaviour in the following steps based on PSCCH decoded and RSRP measured in these slots\",\n",
       "  'question': 'What parameter determines the exclusion of candidate single-slot or multi-slot resources with the sub-channel having the smallest index?\\n',\n",
       "  'answer': \"The higher layer parameter sl-TransmissionStructureForPSCCHandPSSCH is set to 'contiguousRB' when excluding these resources.\",\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38214-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 574}},\n",
       " {'context': '. In the procedural specification, in field descriptions as well as in headings suffices are not used, unless there is a clear need to distinguish the extension from the original field. -\\tMore generally, in case there is a need to distinguish different variants of an ASN.1 field or IE, a suffix should be added at the end of the identifiers e.g. MeasObjectUTRA, ConfigCommon. When there is no particular need to distinguish the fields (e.g. because the field is included in different IEs), a common field identifier name may be used. This may be attractive e.g. in case the procedural specification is the same for the different variants. -\\tIt should be avoided to use field identifiers with the same name within the elements of a CHOICE, including using a CHOICE inside a SEQUENCE (to avoid certain compiler errors). Table A.3.1.2-1: Examples of typical abbreviations used in ASN.1 identifiers Abbreviation Abbreviated word Config Configuration DL Downlink Ext Extension Freq Frequency Id Identity Ind Indication Meas Measurement MIB MasterInformationBlock Neigh Neighbour(ing) Param(s) Parameter(s) Phys Physical PCI Physical Cell Id Proc Process Reconfig Reconfiguration Reest Re-establishment Req Request Rx Reception Sched Scheduling SIB SystemInformationBlock Sync Synchronisation Thr Threshold Tx Transmission UL Uplink NOTE:\\tThe table A.3.1.2.1-1 is not exhaustive. Additional abbreviations may be used in ASN.1 identifiers when needed. A.3.1.3\\tText references using ASN.1 identifiers A text reference into the RRC PDU contents description from other parts of the specification is made using the ASN.1 field identifier of the referenced type. The ASN.1 field and type identifiers used in text references should be in the italic font style. The \"do not check spelling and grammar\" attribute in Word should be set. Quotation marks (i.e., \"\") should not be used around the ASN.1 field or type identifier. A reference to an RRC PDU should be made using the corresponding ASN',\n",
       "  'question': 'What should be avoided when using field identifiers within elements of a CHOICE?\\n',\n",
       "  'answer': 'Using field identifiers with the same name within elements of a CHOICE to avoid certain compiler errors.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 2447}},\n",
       " {'context': \"1\\tUplink switching for EN-DC For a UE indicating a capability for uplink switching with BandCombination-UplinkTxSwitch for a band combination, and if it is for that band combination configured with a MCG using E-UTRA radio access and with a SCG using NR radio access (EN-DC), if the UE is configured with uplink switching with parameter uplinkTxSwitching, -\\tfor the UE configured with switchedUL by the parameter uplinkTxSwitchingOption, when the UE is to transmit in the uplink based on DCI(s) received before or based on a higher layer configuration(s): -\\twhen the UE is to transmit an NR uplink that takes place after an E-UTRA uplink on another uplink carrier then the UE is not expected to transmit for the duration of on any of the two carriers. -\\twhen the UE is to transmit an E-UTRA uplink that takes place after an NR uplink on another uplink carrier then the UE is not expected to transmit for the duration of on any of the two carriers. -\\tthe UE is not expected to transmit simultaneously on the NR uplink and the E-UTRA uplink. If the UE is scheduled or configured to transmit any NR uplink transmission overlapping with an E-UTRA uplink transmission, the NR uplink transmission is dropped, -\\tfor the UE configured with uplinkTxSwitchingOption set to 'dualUL', when the UE is to transmit in the uplink based on DCI(s) received before or based on a higher layer configuration(s): -\\twhen the UE is to transmit an NR two-port uplink that takes place after an E-UTRA uplink on another uplink carrier then the UE is not expected to transmit for the duration of on any of the two carriers. -\\twhen the UE is to transmit an E-UTRA uplink that takes place after an NR two-port uplink on another uplink carrier then the UE is not expected to transmit for the duration of on any of the two carriers. -\\tthe UE is not expected to transmit simultaneously a two- port transmission on the NR uplink and the E-UTRA uplink\",\n",
       "  'question': 'What happens if a UE is scheduled to transmit an NR uplink overlapping with an E-UTRA uplink?\\n',\n",
       "  'answer': 'The NR uplink transmission is dropped, and the UE is not expected to transmit simultaneously on both bands.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38214-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 448}},\n",
       " {'context': '10\\tReference signal measurement timing configuration The UE shall setup the first SS/PBCH block measurement timing configuration (SMTC) in accordance with the received periodicityAndOffset parameter (providing Periodicity and Offset value for the following condition) in the SSB-MTC configuration. The first subframe of each SMTC occasion occurs at an SFN and subframe of the NR SpCell meeting the following condition: SFN mod T = (FLOOR (Offset/10)); if the Periodicity is larger than sf5: subframe = Offset mod 10; else: subframe = Offset or (Offset +5); with T = CEIL(Periodicity/10). If smtc2 is present, for cells indicated in the pci-List parameter in smtc2 in the same MeasObjectNR, the UE shall setup an additional SS/PBCH block measurement timing configuration (SMTC) in accordance with the received periodicity parameter in the smtc2 configuration and use the Offset (derived from parameter periodicityAndOffset) and duration parameter from the smtc1 configuration. The first subframe of each SMTC occasion occurs at an SFN and subframe of the NR SpCell meeting the above condition. If smtc2-LP is present, for cells indicated in the pci-List parameter in smtc2-LP in the same frequency (for intra frequency cell reselection) or different frequency (for inter frequency cell reselection), the UE shall setup an additional SS/PBCH block measurement timing configuration (SMTC) in accordance with the received periodicity parameter in the smtc2-LP configuration and use the Offset (derived from parameter periodicityAndOffset) and duration parameter from the smtc configuration for that frequency. The first subframe of each SMTC occasion occurs at an SFN and subframe of the NR SpCell or serving cell (for cell reselection) meeting the above condition',\n",
       "  'question': 'What is the condition for the first subframe of each SMTC occasion?\\n',\n",
       "  'answer': 'It occurs at an SFN and subframe of the NR SpCell meeting the condition SFN mod T = (FLOOR (Offset/10)).',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 430}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications\\n589 uA uB UA UB y2 y1 y y Figure A.6: Projecting the received vector y onto the signal direction v reduces the\\nvector detection problem to the scalar one.\\nMore formally, we are viewing the received vector in a dierent orthonormal basis:\\nthe first direction is that given by v, and the other directions are orthogonal to each\\nother and to the first one. In other words, we form an orthogonal matrix O whose first\\nrow is v, and the other rows are orthogonal to each other and to the first one and have\\nunit norm. Then O  y 1 2 (uA + uB)  =   xuA uB 0 ... 0  + Ow. (A.42) Since Ow N (0, (N0/2)I) (c.f. (A.8)), this means that all but the first component\\nof the vector O  y 1 2 (uA + uB)  are independent of the transmit symbol x and the\\nnoise in the first component. Thus it suces to make a decision on the transmit symbol\\nx, using only the first component, which is precisely (A.41).\\nThis important observation can be summarized:\\nIn technical jargon, the scalar y in (A.41) is called a sucient statistic of the\\nreceived vector y to detect the transmit symbol u.\\nThe sucient statistic y is a projection of the received signal in the signal di-\\nrection v: in the literature on communication theory, this operation is called a\\nmatched filter; the linear filter at the receiver is matched to the direction of\\nthe transmit signal.\\nThis argument explains why the error probability depends on uA and uB only\\nthrough the distance between them: the noise is isotropic and the entire detection\\nproblem is rotationally invariant.',\n",
       "  'question': 'What is the term for the scalar y in equation A.41 in communication theory?\\n',\n",
       "  'answer': 'The scalar y is called a sufficient statistic of the received vector y to detect the transmit symbol u.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 590,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '.maxFeatureSetCombinations)) OF FeatureSetCombination OPTIONAL, pdcp-ParametersMRDC-v1530 PDCP-ParametersMRDC OPTIONAL, lateNonCriticalExtension OCTET STRING (CONTAINING UE-MRDC-Capability-v15g0) OPTIONAL, nonCriticalExtension UE-MRDC-Capability-v1560 OPTIONAL } -- Regular non-critical extensions: UE-MRDC-Capability-v1560 ::= SEQUENCE { receivedFilters OCTET STRING (CONTAINING UECapabilityEnquiry-v1560-IEs) OPTIONAL, measAndMobParametersMRDC-v1560 MeasAndMobParametersMRDC-v1560 OPTIONAL, fdd-Add-UE-MRDC-Capabilities-v1560 UE-MRDC-CapabilityAddXDD-Mode-v1560 OPTIONAL, tdd-Add-UE-MRDC-Capabilities-v1560 UE-MRDC-CapabilityAddXDD-Mode-v1560 OPTIONAL, nonCriticalExtension UE-MRDC-Capability-v1610 OPTIONAL } UE-MRDC-Capability-v1610 ::= SEQUENCE { measAndMobParametersMRDC-v1610 MeasAndMobParametersMRDC-v1610 OPTIONAL, generalParametersMRDC-v1610 GeneralParametersMRDC-v1610 OPTIONAL, pdcp-ParametersMRDC-v1610 PDCP-ParametersMRDC-v1610 OPTIONAL, nonCriticalExtension UE-MRDC-Capability-v1700 OPTIONAL } UE-MRDC-Capability-v1700 ::= SEQUENCE { measAndMobParametersMRDC-v1700 MeasAndMobParametersMRDC-v1700, nonCriticalExtension UE-MRDC-Capability-v1730 OPTIONAL } UE-MRDC-Capability-v1730 ::= SEQUENCE { measAndMobParametersMRDC-v1730 MeasAndMobParametersMRDC-v1730 OPTIONAL, nonCriticalExtension UE-MRDC-Capability-v1800 OPTIONAL } UE-MRDC-Capability-v1800 ::= SEQUENCE { -- R4 33-2: Support network control of requirementnetwork applicability for UE supporting interBandMRDC-WithOverlapDL-Bands-r16 requirementTypeIndication-r18 ENUMERATED {supported} OPTIONAL, measAndMobParametersMRDC-v1810 MeasAndMobParametersMRDC-v1810 OPTIONAL, nonCriticalExtension SEQUENCE {} OPTIONAL } -- Late non-critical extensions: UE-MRDC-Capability-v15g0 ::= SEQUENCE { rf-ParametersMRDC-v15g0 RF-ParametersMRDC-v15g0 OPTIONAL, nonCriticalExtension UE-MRDC-Capability-v15n0 OPTIONAL } UE-MRDC-Capability-v15n0 ::= SEQUENCE { rf-ParametersMRDC-v15n0 RF-ParametersMRDC-v15n0 OPTIONAL, -- Following field is only',\n",
       "  'question': 'What is the purpose of the lateNonCriticalExtension in the context of UE-MRDC-Capability-v15g0?\\n',\n",
       "  'answer': 'The lateNonCriticalExtension in UE-MRDC-Capability-v15g0 is used for containing additional non-critical capabilities that can be added later, specifically related to RF parameters and further extensions to UE-MRDC-Capability-v15n0.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 2114}},\n",
       " {'context': 'For efficient signal transmission, the redundant information should, therefore, be\\nremoved from the signal prior to transmission. This operation, with no loss of information,\\nis ordinarily performed on a signal in digital form, in which case we refer to the operation\\nas lossless data compression. The code resulting from such an operation provides a\\nrepresentation of the source output that is not only efficient in terms of the average number\\nof bits per symbol, but also exact in the sense that the original data can be reconstructed\\nwith no loss of information. The entropy of the source establishes the fundamental limit on\\nthe removal of redundancy from the data. Basically, lossless data compression is achieved\\nL L pklk k 0 = K 1 -  = L  Lmin L ----------- = L Lmin  L L HS    H S  L ------------ =',\n",
       "  'question': 'What is the term for the operation of removing redundant information from a digital signal without losing any data?\\n',\n",
       "  'answer': 'Lossless data compression is the process of efficiently transmitting signals by eliminating unnecessary bits while ensuring the original data can be perfectly reconstructed.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 235,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': '. When this field is included in ServingCellConfigCommon within dedicated signalling, it indicates TA reporting is enabled during reconfiguration with sync (see TS 38.321 [3], clause 5.4.8). Conditional Presence Explanation SIB19 The field is mandatory present for the serving cell in SIB19. The field is optionally present, Need R, otherwise. -\\tNZP-CSI-RS-Resource The IE NZP-CSI-RS-Resource is used to configure Non-Zero-Power (NZP) CSI-RS transmitted in the cell where the IE is included, which the UE may be configured to measure on (see TS 38.214 [19], clause 5.2.2.3.1). A change of configuration between periodic, semi-persistent or aperiodic for an NZP-CSI-RS-Resource is not supported without a release and add. NZP-CSI-RS-Resource information element -- ASN1START -- TAG-NZP-CSI-RS-RESOURCE-START NZP-CSI-RS-Resource ::= SEQUENCE { nzp-CSI-RS-ResourceId NZP-CSI-RS-ResourceId, resourceMapping CSI-RS-ResourceMapping, powerControlOffset INTEGER (-8..15), powerControlOffsetSS ENUMERATED{db-3, db0, db3, db6} OPTIONAL, -- Need R scramblingID ScramblingId, periodicityAndOffset CSI-ResourcePeriodicityAndOffset OPTIONAL, -- Cond PeriodicOrSemiPersistent qcl-InfoPeriodicCSI-RS TCI-StateId OPTIONAL, -- Cond Periodic ..., [[ subcarrierSpacing-r18 SubcarrierSpacing OPTIONAL, -- Cond LTM absoluteFrequencyPointA-r18 ARFCN-ValueNR OPTIONAL, -- Cond LTM cyclicPrefix-r18 ENUMERATED {extended} OPTIONAL -- Cond LTM ]] } -- TAG-NZP-CSI-RS-RESOURCE-STOP -- ASN1STOP NZP-CSI-RS-Resource field descriptions periodicityAndOffset Periodicity and slot offset sl1 corresponds to a periodicity of 1 slot, sl2 to a periodicity of two slots, and so on. The corresponding offset is also given in number of slots (see TS 38.214 [19], clause 5.2.2.3.1). Network always configures the UE with a value for this field for periodic and semi-persistent NZP-CSI-RS-Resource (as indicated in CSI-ResourceConfig). powerControlOffset Power offset of PDSCH EPRE to NZP CSI-RS EPRE. Value in dB (see TS 38',\n",
       "  'question': 'What does the NZP-CSI-RS-Resource IE configure?\\n',\n",
       "  'answer': 'The NZP-CSI-RS-Resource information element configures Non-Zero-Power CSI-RS transmitted in the cell where it is included, specifying parameters like resource mapping, power control offset, and periodicity for UE measurement.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1472}},\n",
       " {'context': '. The UE generates a NACK when, due to prioritization, as described in clause 16.2.4, the UE does not receive PSFCH in any PSFCH reception occasion associated with a PSSCH transmission in a resource provided by a DCI format 3_0 and the UE transmitted PSSCH in the resource or, for a configured grant, in a resource provided in a single period and for which the UE is provided a PUCCH resource to report HARQ-ACK information and the UE transmitted PSSCH in the resource. The priority value of the NACK is same as the priority value of the PSSCH transmission. The UE generates a NACK when, due to prioritization as described in clause 16.2.4, or due to a failed channel access procedure [15, TS 37.213] for operation with shared spectrum channel access, the UE does not transmit a PSSCH in any of the resources provided by a DCI format 3_0 or, for a configured grant, in any of the resources provided in a single period and for which the UE is provided a PUCCH resource to report HARQ-ACK information. The priority value of the NACK is same as the priority value of the PSSCH that was not transmitted due to prioritization or due to the failed channel access procedure. The UE generates an ACK if the UE does not transmit a PSCCH with a SCI format 1-A scheduling a PSSCH in any of the resources provided by a configured grant in a single period and for which the UE is provided a PUCCH resource to report HARQ-ACK information. The priority value of the ACK is same as the largest priority value among the possible priority values for the configured grant. The UE generates an ACK if the UE does not transmit a PSCCH with a SCI format 1-A scheduling a PSSCH in any of the resources provided by a DCI format 3_0 and for which the UE is provided a PUCCH resource to report HARQ-ACK information. The priority value of the ACK is same as the largest priority value among the possible priority values for the dynamic grant',\n",
       "  'question': 'What is the priority value of a NACK generated due to prioritization?\\n',\n",
       "  'answer': 'The priority value of a NACK generated due to prioritization is the same as the priority value of the PSSCH transmission it relates to.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38213-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 594}},\n",
       " {'context': '. If the total number N of beams included in the MAC CE is odd, the 4 right-most bits in the final octet of the IAB-MT Resource set are disregarded by the UE. The length of the field is 4 bits; -\\tDU resource configuration: when this field is set to 00, the provided power adjustment is applied on FDM resources where the simultaneous MT and DU signals are non-overlapping in the frequency-domain; when this field is set to 01, the provided power adjustment is applied on non-FDM resources where the simultaneous MT and DU signals may overlap in the frequency-domain for a given (MT CC, DU cell); when this field is set to 10, the provided power adjustment is applied on FDM resources where the simultaneous MT and DU signals are non-overlapping in the frequency-domain, and on non-FDM resources where the simultaneous MT and DU signals may overlap in the frequency-domain for a given (MT CC, DU cell). The length of the field is 2 bits; -\\tCell info: indicates the cell configuration associated with the information contained in the MAC CE. IAB-DU cell is contained in the first nine bits of the field, while the Child IAB-MT Serving Cell index into the next 5 bits of the field. The length of the field is 14 bits. Figure 6.1.3.64-1: Desired IAB-MT PSD range MAC CE',\n",
       "  'question': 'How many bits are disregarded by the UE when the total number of beams in the MAC CE is odd?\\n',\n",
       "  'answer': 'The 4 right-most bits in the final octet of the IAB-MT Resource set are disregarded when the total number of beams in the MAC CE is odd.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38321-i50.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 535}},\n",
       " {'context': 'j  0 1 L 1 -   = = Bj Es N0 ----------      n Es N0 ------ rj 2 n +   - j  exp 0 1 L 1 -   = = \\x02 mj   1 and La mj    j   L L 1 K 1 -   +  = = = j ss    Bj 1 2---Lc rj Tcj       j  exp L L 1 K 1 -   +  = = f ss r    Aj and Bj j 0 = K 1 -  j 0 = L 1 -',\n",
       "  'question': 'What is the value of K in the given context?\\n',\n",
       "  'answer': 'K has a value of 1 in the provided mathematical context.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 653,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': '. -\\tSL PRS request - 1 bit as defined in clause 8.4.4 of [6, TS 38.214] when the higher layer parameter sl-SCI-basedSL-PRS-TxTriggerSCI1-B is provided; 0 bit otherwise. -\\tReserved - bits as configured by higher layer parameter sl-NumReservedBitsSCI1B-DedicatedSL-PRS-RP, with value set to zero. Table 8.3.1.2-1: Cast type indicator Value of Cast type indicator Cast type Broadcast Groupcast Unicast Reserved',\n",
       "  'question': 'What does a 1 bit signify in SL PRS request?\\n',\n",
       "  'answer': 'A 1 bit in SL PRS request indicates the higher layer parameter sl-SCI-basedSL-PRS-TxTriggerSCI1-B is provided, as defined in clause 8.4.4 of [6, TS 38.214].',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38212-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 385}},\n",
       " {'context': '2\\tSequence generation The sounding reference signal sequence for an SRS resource, or if numberOfHops for SRS-PosResource is provided, for a given hop within an SRS resource, shall be generated according to where is given by clause 6.4.1.4.3, is given by clause 5.2.2 with and the transmission comb number is contained in the higher-layer parameter transmissionComb. The quantity is the OFDM symbol number within the SRS resource. The quantity is given by -\\tif the higher-layer parameter nrofSRS-Ports-n8 equals ports8tdm -\\totherwise The cyclic shift for antenna port is given as where where is contained in the higher layer parameter transmissionComb. The maximum number of cyclic shifts is given by Table 6.4.1.4.2-1. The quantities and are given by -\\tif the higher-layer parameter nrofSRS-Ports-n8 equals ports8tdm -\\totherwise The quantity is given by -\\tif the higher-layer parameter cyclicShiftHopping is not configured: -\\tif the higher-layer parameter cyclicShiftHopping is configured: where and is the th entry and the cardinality of the set respectively, where is given by the higher-layer parameter hoppingSubset in the cyclicShiftHopping IE if configured, otherwise . The higher-layer parameter hoppingSubset in the cyclicShiftHopping IE includes a bitmap of bits with non-zero bits, where if the th non-zero bit is the :th bit in the bitmap, then . The pseudo-random sequence is defined by clause 5.2.1 and shall be initialized with at the beginning of each radio frame for which , where the cyclic-shift hopping identity is contained in the higher-layer parameter cyclicShiftHopping. If the higher-layer parameter hoppingFinerGranularity is configured, , otherwise . The sequence group and the sequence number in clause 5.2.2 depends on the higher-layer parameter groupOrSequenceHopping in the SRS-Resource IE or the SRS-PosResource IE. The SRS sequence identity is given by the higher layer parameter sequenceId in the SRS-Resource IE',\n",
       "  'question': 'What is the maximum number of cyclic shifts for antenna port?\\n',\n",
       "  'answer': 'The maximum number of cyclic shifts is given by Table 6.4.1.4.2-1.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38211-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 85}},\n",
       " {'context': '35 for M = 2, 4, 8.\\nBandwidth Efficiency of M-ary FSK Signals\\nWhen the orthogonal signals of an M-ary FSK signal are detected coherently, the adjacent\\nsignals need only be separated from each other by a frequency difference 12T so as to\\nmaintain orthogonality. Hence, we may define the channel bandwidth required to transmit\\nM-ary FSK signals as\\n(7.207)\\nFor multilevels with frequency assignments that make the frequency spacing uniform and\\nequal to 1/2T, the bandwidth B of (7.207) contains a large fraction of the signal power.\\nsi tsj t dt 0 T  0 i j   = i t 1 E -------si t for 0 t T  and i 1 2 M   = = 2E Pe M 1 -  Q E N0 ------      B M 2T ------ =',\n",
       "  'question': 'What is the frequency difference required to maintain orthogonality between adjacent signals in an M-ary FSK signal detected coherently?\\n',\n",
       "  'answer': 'The adjacent signals need only be separated by a frequency difference of 12T to maintain orthogonality when detected coherently in an M-ary FSK signal.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 416,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': 'Technical Specification 3rd Generation Partnership Project; Technical Specification Group Radio Access Network; NR; Radio Resource Control (RRC) protocol specification () The present document has been developed within the 3rd Generation Partnership Project (3GPP TM) and may be further elaborated for the purposes of 3GPP.\\nThe present document has not been subject to any approval process by the 3GPP Organizational Partners and shall not be implemented.\\nThis Specification is provided for future development work within 3GPP only. The Organizational Partners accept no liability for any use of this Specification.',\n",
       "  'question': 'What is the purpose of the Technical Specification Group Radio Access Network?\\n',\n",
       "  'answer': 'It develops the Radio Resource Control (RRC) protocol specification within the 3rd Generation Partnership Project (3GPP).',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '1\\tResource allocation in time domain When the UE is scheduled to receive PDSCH by a DCI, the Time domain resource assignment field value m for the scheduled PDSCH on the serving cell provides a row index m + 1 to a resource allocation table. The determination of the used resource allocation table is defined in Clause 5.1.2.1.1. The indexed row defines the slot offset K0, the start and length indicator SLIV, or directly the start symbol S and the allocation length L, and the PDSCH mapping type to be assumed in the PDSCH reception. Given the parameter values of the indexed row: -\\tThe slot allocated for the PDSCH is Ks, where , if UE is configured with ca-SlotOffset for at least one of the scheduled and scheduling cell, and Ks = , otherwise, and where n is the slot with the scheduling DCI, and K0 is based on the numerology of PDSCH, and and are the subcarrier spacing configurations for PDSCH and PDCCH, respectively, and - and are the and the, respectively, which are determined by higher-layer configured ca-SlotOffset, for the cell receiving the PDCCH respectively, and are the and the, respectively, which are determined by higher-layer configured ca-SlotOffset for the cell receiving the PDSCH, as defined in clause 4.5 of [4, TS 38.211]. -\\tThe reference point S0 for starting symbol S is defined as: -\\tif configured with referenceOfSLIVDCI-1-2, and when receiving PDSCH scheduled by DCI format 1_2 with CRC scrambled by C-RNTI, MCS-C-RNTI, CS-RNTI with K0=0, and PDSCH mapping Type B, the starting symbol S is relative to the starting symbol S0 of the PDCCH monitoring occasion where DCI format 1_2 is detected; when the PDCCH reception includes two PDCCH candidates from two respective search space sets, as described in clause 10.1 of [6, TS 38.213], the PDCCH candidate that starts later in time is used for the purpose of determining the starting symbol S0; -\\totherwise, the starting symbol S is relative to the start of the slot using S0=0',\n",
       "  'question': 'What does the Time domain resource assignment field value m provide when UE is scheduled to receive PDSCH?\\n',\n",
       "  'answer': 'It provides a row index m + 1 to a resource allocation table which defines slot offset K0, start and length indicator SLIV, or directly the start symbol S and the allocation length L, and the PDSCH mapping type to be assumed in the PDSCH reception.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38214-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 27}},\n",
       " {'context': '. maxNrofTCI-States-1 INTEGER ::= 127 -- Maximum number of TCI states minus 1. maxUL-TCI-r17 INTEGER ::= 64 -- Maximum number of TCI states. maxUL-TCI-1-r17 INTEGER ::= 63 -- Maximum number of TCI states minus 1. maxNrofAdditionalPCI-r17 INTEGER ::= 7 -- Maximum number of additional PCI maxNrofAdditionalPRACHConfigs-r18 INTEGER ::= 7 -- Maximum number of additional PRACH configurations for 2TA maxNrofdelayD-r18 INTEGER ::= 4 -- Maximum number of delayD values. maxMPE-Resources-r17 INTEGER ::= 64 -- Maximum number of pooled MPE resources maxNrofUL-Allocations INTEGER ::= 16 -- Maximum number of PUSCH time domain resource allocations',\n",
       "  'question': 'What is the maximum number of TCI states?\\n',\n",
       "  'answer': 'The maximum number of TCI states is 127.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 2298}},\n",
       " {'context': '. 4>\\tif the UE did not provide psi-Identification since it was configured to provide UL traffic information, or if the information previously provided in psi-Identification has changed since the last transmission of the UEAssistanceInformation message containing psi-Identification: 5>\\tif the UE is able to identify PSI(s) for the QoS flow: 6>\\tset psi-Identification to true; 5>\\telse: 6>\\tset psi-Identification to false. 1>\\tif transmission of the UEAssistanceInformation message is initiated to report relay UE information with non-3GPP connection(s) according to 5.7.4.2: 2>\\tinclude n3c-relayUE-InfoList in the UEAssistanceInformation message; The UE shall set the contents of the UEAssistanceInformation message for configured grant assistance information for NR sidelink communication or NR sidelink positioning: 1>\\tif configured to provide configured grant assistance information for NR sidelink: 2>\\tinclude the sl-UE-AssistanceInformationNR; 1>\\tif configured to provide configured grant assistance information for NR sidelink positioning: 2>\\tinclude the sl-PRS-UE-AssistanceInformationNR; NOTE 4:\\tIt is up to UE implementation when and how to trigger configured grant assistance information for NR sidelink communication or NR sidelink positioning. The UE shall: 1>\\tif the procedure was triggered to provide configured grant assistance information for NR sidelink communication by an NR RRCReconfiguration message that was embedded within an E-UTRA RRCConnectionReconfiguration: 2>\\tsubmit the UEAssistanceInformation to lower layers via SRB1, embedded in E-UTRA RRC message ULInformationTransferIRAT as specified in TS 36.331 [10], clause 5.6',\n",
       "  'question': 'What message is used to transfer UEAssistanceInformation when triggered by an NR RRCReconfiguration message embedded within an E-UTRA RRCConnectionReconfiguration?\\n',\n",
       "  'answer': 'The UEAssistanceInformation is submitted to lower layers via SRB1, embedded in an E-UTRA RRC message ULInformationTransferIRAT as specified in TS 36.331, clause 5.6.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 636}},\n",
       " {'context': '. 2>\\tif sidelinkRequest is received: 3>\\tfor a sidelink band combination the UE included in supportedBandCombinationListSidelinkEUTRA-NR, supportedBandCombinationListSL-RelayDiscovery, supportedBandCombinationListSL-U2U-RelayDiscovery or supportedBandCombinationListSL-NonRelayDiscovery: 4>\\tif the UE supports partial sensing for a band of the sidelink band combination, include the partial sensing capabilities for the band using the sl-TransmissionMode2-PartialSensing-r17; 3>\\tset sidelinkRequested to true; 2>\\tinclude into featureSets the feature sets referenced from the \"candidate feature set combinations\" and may exclude the feature sets with the parameters that exceed any of maxBandwidthRequestedDL, maxBandwidthRequestedUL, maxCarriersRequestedDL or maxCarriersRequestedUL, whichever are received; 1>\\telse, if the requested rat-Type is eutra-nr: 2>\\tinclude into supportedBandCombinationList and/or supportedBandCombinationListNEDC-Only as many E-UTRA-NR band combinations as possible from the list of \"candidate band combinations\", starting from the first entry; 3>\\tif srs-SwitchingTimeRequest is received: 4>\\tif SRS carrier switching is supported; 5>\\tinclude srs-SwitchingTimesListNR, srs-SwitchingTimesListEUTRA and srs-SwitchingAffectedBandsListNR for each band combination; 4>\\tset srs-SwitchingTimeRequested to true; 2>\\tinclude, into featureSetCombinations, the feature set combinations referenced from the supported band combinations as included in supportedBandCombinationList according to the previous; 2>\\tif uplinkTxSwitchRequest is received: 3>\\tinclude into supportedBandCombinationList-UplinkTxSwitch as many E-UTRA-NR band combinations that supported UL TX switching as possible from the list of \"candidate band combinations\", starting from the first entry; 4>\\tif srs-SwitchingTimeRequest is received: 5>\\tif SRS carrier switching is supported; 6>\\tinclude srs-SwitchingTimesListNR, srs-SwitchingTimesListEUTRA and srs-SwitchingAffectedBandsListNR for each band combination; 5>\\tset',\n",
       "  'question': 'What is the process for handling a sidelinkRequest in the given context?\\n',\n",
       "  'answer': 'When a sidelinkRequest is received, the UE includes the feature sets referenced from the \"candidate feature set combinations\" and may exclude feature sets with parameters exceeding any of maxBandwidthRequestedDL, maxBandwidthRequestedUL, maxCarriersRequestedDL or maxCarriersRequestedUL.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 553}},\n",
       " {'context': '.3.1.1.2-3B, if associated with the first CRI in CSI part 1, numberOfSingleTRP-CSI-Mode1 = 2 and if reported PMI wideband information fields , from left to right as in Tables 6.3.1.1.2-1, if associated with CRI in CSI part 1, numberOfSingleTRP-CSI-Mode1 = 1 and if reported; PMI wideband information fields , from left to right as in Tables 6.3.1.1.2-1, if associated with the first CRI in CSI part 1, numberOfSingleTRP-CSI-Mode1 = 2 and if reported PMI wideband information fields , from left to right as in Tables 6.3.1.1.2-1, or codebook index for 2 antenna ports according to Clause 5.2.2.2.1 in [6, TS38.214], if associated with CRI in CSI part 1, pmi-FormatIndicator= widebandPMI, numberOfSingleTRP-CSI-Mode1 = 1 and if reported; PMI wideband information fields , from left to right as in Tables 6.3.1.1.2-1, or codebook index for 2 antenna ports according to Clause 5.2.2.2.1 in [6, TS38.214], if associated with the first CRI in CSI part 1, pmi-FormatIndicator= widebandPMI, numberOfSingleTRP-CSI-Mode1 = 2 and if reported Wideband CQI for the second TB as in Tables 6.3.1.1.2-3B, if associated with the second CRI in CSI part 1, numberOfSingleTRP-CSI-Mode1 = 2 and if reported Layer Indicator as in Table 6.3.1.1.2-3B, if associated with the second CRI in CSI part 1, numberOfSingleTRP-CSI-Mode1 = 2 and if reported PMI wideband information fields , from left to right as in Tables 6.3.1.1.2-1, if associated with the second CRI in CSI part 1, numberOfSingleTRP-CSI-Mode1 = 2 and if reported PMI wideband information fields , from left to right as in Tables 6.3.1.1.2-1, or codebook index for 2 antenna ports according to Clause 5.2.2.2.1 in [6, TS38.214], if associated with the second CRI in CSI part 1, pmi-FormatIndicator= widebandPMI, numberOfSingleTRP-CSI-Mode1 = 2 and if reported Table 6.3.2.1.2-4B: Mapping order of CSI fields of one CSI report, CSI part 2 wideband,',\n",
       "  'question': 'What is the value of numberOfSingleTRP-CSI-Mode1 when associated with the first CRI in CSI part 1 and PMI wideband information fields are reported?\\n',\n",
       "  'answer': 'The value of numberOfSingleTRP-CSI-Mode1 is 2 when associated with the first CRI in CSI part 1 and PMI wideband information fields are reported.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38212-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 83}},\n",
       " {'context': 'Technical Specification 3rd Generation Partnership Project; Technical Specification Group Radio Access Network; NR; Packet Data Convergence Protocol (PDCP) specification () The present document has been developed within the 3rd Generation Partnership Project (3GPP TM) and may be further elaborated for the purposes of 3GPP.\\nThe present document has not been subject to any approval process by the 3GPP Organizational Partners and shall not be implemented.\\nThis Specification is provided for future development work within 3GPP only. The Organizational Partners accept no liability for any use of this Specification.',\n",
       "  'question': 'What is the full name of the organization that developed the Technical Specification mentioned?\\n',\n",
       "  'answer': 'The 3rd Generation Partnership Project (3GPP) developed the Technical Specification.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38323-i50.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '. In the case of dedicated SL PRS resource pool, that PSCCH carries the SCI format 1-B associated with the SL PRS transmission. The UE may report the association information between the already transmitted SL PRSs of SL PRS resources and UE Tx ARP ID. The association information includes ARP ID(s) indicated by sl-POS-ARP-ID-Tx, SL PRS transmission timestamp(s) indicated by sl-TimeStamp, and optional SL PRS resource ID(s) indicated by sl-PRS-ResourceID.',\n",
       "  'question': 'What format does the PSCCH carry for SL PRS transmission?\\n',\n",
       "  'answer': 'The PSCCH carries the SCI format 1-B associated with the SL PRS transmission.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38214-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 596}},\n",
       " {'context': '. NOTE 5: Ciphering and integrity protection can be enabled or disabled for a DRB. The enabling/disabling of ciphering or integrity protection can be changed only by releasing and adding the DRB. NOTE 6:\\tIn DAPS handover, the UE may perform PDCP entity re-establishment (if reestablishPDCP is set) or the PDCP data recovery (if recoverPDCP is set) for a non-DAPS bearer when indication of successful completion of random access towards target cell is received from lower layers as specified in TS 38.321 [3]. In this case, the UE suspends data transmission and reception for all non-DAPS bearers in the source MCG for duration of the DAPS handover.',\n",
       "  'question': 'What can be enabled or disabled for a DRB?\\n',\n",
       "  'answer': 'Ciphering and integrity protection can be enabled or disabled for a DRB.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 209}},\n",
       " {'context': 'of base stations among which it is in soft hando. Formulate the power con-\\ntrol problem to meet the error probability requirement for each mobile in the\\ndownlink.\\nExercise 4.14. In this problem we consider the design of hopping patterns of neigh-\\nboring cells in the OFDM system. Based on the design principles arrived at in Sec-\\ntion 4.4.2, we want the hopping patterns to be latin squares and further require these\\nlatin squares to be orthogonal.\\nAnother way to say the orthogonality of a pair of\\nlatin squares is the following. For the two latin squares, the N\\nc ordered pairs (n1, n2),\\nwhere n1 and n2 are the entries (sub-carrier or virtual channel indices) from the same\\nposition in the respective latin squares, exhaust the N\\nc possibilities, i.e., every ordered\\npair occurs exactly once.\\n13Note that this is dierent from the downlink of IS-95, where each user is assigned an orthogonal\\nsequence.',\n",
       "  'question': 'What is the requirement for two latin squares to be orthogonal?\\n',\n",
       "  'answer': 'The ordered pairs of entries from the same position in the respective latin squares must exhaust all possible combinations, with each ordered pair occurring exactly once.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 192,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': \".e., not applicable for a UE configured with UE FFP parameters (e.g. period, offset) regardless whether the UE would initiate its own COT or would share gNB's COT. cg-UCI-Multiplexing If present, this field indicates that in the case of PUCCH overlapping with CG-PUSCH(s) including CG-UCI within a PUCCH group, HARQ-ACK is multiplexed on the CG-PUSCH including CG-UCI (see TS 38.213 [13], clause 9). configuredGrantConfigIndex Indicates the index of the Configured Grant configurations within the BWP. configuredGrantConfigIndexMAC Indicates the index of the Configured Grant configurations within the MAC entity. disableCG-RetransmissionMonitoring Indicates that the UE shall disable waking-up to monitor possible grants for retransmissions corresponding to this ConfiguredGrantConfig when DRX is configured. When this field is configured, the UE does not start the drx-HARQ-RTT-TimerUL for PUSCH transmissions using configured uplink grants corresponding to this ConfiguredGrantConfig. See TS 38.321 [3], clause 5.7. configuredGrantTimer Indicates the initial value of the configured grant timer (see TS 38.321 [3]) in multiples of periodicity. When cg-RetransmissonTimer is configured, if HARQ processes are shared among different configured grants on the same BWP, configuredGrantTimer * periodicity is set to the same value for the configurations that share HARQ processes on this BWP. The value of the extension configuredGrantTimer is 2 times the configured value. dmrs-SeqInitialization The network configures this field if transformPrecoder is disabled or when the value of sdt-NrofDMRS-Sequences is set to 1. Otherwise, the field is absent. frequencyDomainAllocation Indicates the frequency domain resource allocation, see TS 38.214 [19], clause 6.1.2, and TS 38.212 [17], clause 7.3.1). frequencyHopping The value intraSlot enables 'Intra-slot frequency hopping' and the value interSlot enables 'Inter-slot frequency hopping'. If the field is absent, frequency hopping is not configured\",\n",
       "  'question': \"What does the field 'cg-UCI-Multiplexing' indicate?\\n\",\n",
       "  'answer': 'It shows that HARQ-ACK is multiplexed on the CG-PUSCH including CG-UCI when PUCCH overlaps with CG-PUSCH(s) in a PUCCH group.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1266}},\n",
       " {'context': '5 Probability of Error To elaborate, consider first the invariance of Pe with respect to rotation. The effect of a\\nrotation applied to all the message points in a constellation is equivalent to multiplying the\\nN-dimensional signal vector si by an N-by-N orthonormal matrix denoted by Q for all i.\\nBy definition, the matrix Q satisfies the condition\\n(7.64)\\nwhere the superscript T denotes matrix transposition and I is the identity matrix whose\\ndiagonal elements are all unity and its off-diagonal elements are all zero. According to\\n(7.64), the inverse of the real-valued orthonormal matrix Q is equal to its own transpose.\\nThus, in dealing with rotation, the message vector si is replaced by its rotated version\\n(7.65)\\nCorrespondingly, the N-by-1 noise vector w is replaced by its rotated version\\n(7.66)\\nHowever, the statistical characteristics of the noise vector are unaffected by this rotation\\nfor three reasons:\\nFrom Chapter 4 we recall that a linear combination of Gaussian random variables is\\nalso Gaussian. Since the noise vector w is Gaussian, by assumption, then it follows\\nthat the rotated noise vector wrotate is also Gaussian.\\nSince the noise vector w has zero mean, the rotated noise vector wrotate also has zero\\nmean, as shown by\\n(7.67)\\nThe covariance matrix of the noise vector w is equal to (N02)I, where N02 is the\\npower spectral density of the AWGN w(t) and I is the identity matrix; that is\\n(7.68)\\nHence, the covariance matrix of the rotated noise vector is\\n(7.69)\\nwhere, in the last two lines, we have made use of (7.68) and (7.64).\\nIn light of these three reasons, we may, therefore, express the observation vector in the\\nrotated message constellation as\\n(7.70) QQT I = si rotate  Qsi i 1 2 M   =  = wrotate Qw = \\x03 wrotate   \\x03 Qw   = Q\\x03 w   = 0 = \\x03 wwT   N0 2 ------I = \\x03 wrotatewrotate T   \\x03 Qw Qw  T   = \\x03 QwwTQT   = Q\\x03 wwT  QT = N0 2 ------QQT = N0 2 ------I = xrotate Qsi w i 1 2 M   =  + =',\n",
       "  'question': 'What is the effect of rotation on the statistical characteristics of the noise vector?\\n',\n",
       "  'answer': 'The statistical characteristics of the noise vector remain unaffected by rotation because the rotated noise vector is also Gaussian, has zero mean, and its covariance matrix is equal to the original covariance matrix scaled by the identity matrix.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 365,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': 'We may, therefore, go on to make the statement:\\nThe mutual information I(X;Y) is a measure of the uncertainty about the\\nchannel input, which is resolved by observing the channel output.\\nH(X Y yk) p xj yk   1 p xj yk   -------------------     2 log j 0 = J 1 -  = = H(X Y) H(X Y yk) = k 0 = K 1 -  = p yk   p xj yk  p yk   1 p xj yk   -------------------     2 log j 0 = J 1 -  k 0 = K 1 -  = p xj yk    1 p xj yk   -------------------     2 log j 0 = J 1 -  k 0 = K 1 -  = p xj yk    p xj yk  p yk   = IXY ;   H X  HXY   - =',\n",
       "  'question': 'What is the mutual information I(X;Y) a measure of?\\n',\n",
       "  'answer': 'Mutual information I(X;Y) measures the uncertainty about the channel input resolved by observing the channel output.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 246,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': '4\\tMBS services of interest determination The UE shall: 1>\\tconsider an MBS service to be part of the MBS services of interest if the following conditions are met: 2>\\tthe UE is receiving or interested to receive this service via a broadcast MRB; and 2>\\tthe session of this service is ongoing or about to start; and 2>\\tone or more MBS FSAIs in the USD for this service is included in SIB21 acquired from the PCell for a frequency belonging to the set of MBS frequencies of interest, determined according to 5.9.4.3 or SIB21 acquired from the PCell does not provide the frequency mapping for the concerned service but that frequency is included in the USD for this service. NOTE:\\tThe UE may determine whether the session is ongoing or about to start from the start and stop time indicated in the User Service Description (USD), see TS 38.300 [2] or TS 23.247 [67].',\n",
       "  'question': 'What are the conditions for a UE to consider an MBS service as part of its MBS services of interest?\\n',\n",
       "  'answer': 'The UE must be receiving or interested in receiving the service via a broadcast MRB, the session must be ongoing or about to start, and one or more MBS FSAIs in the USD for the service must be included in SIB21 acquired from the PCell for a frequency belonging to the set of MBS frequencies of interest.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 912}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications Bibliographical Notes\\nThe angular domain approach to MIMO channel modeling is based on works by Sayeed\\n[105] and Poon et. al. [90, 92]. [105] considered an array of discrete antenna elements,\\nwhile [90, 92] considered a continuum of antenna elements to emphasize that spatial\\nmultiplexability is limited not by the number of antenna elements but by the size of the\\nantenna array. We considered only linear arrays in this chapter, but [90] also treated\\nother antenna array configurations such as circular rings and spherical surfaces. The\\ndegree-of-freedom formula 7.78 is derived in [90] for the clustered response model.\\nOther related approaches to MIMO channel modeling are by Raleigh and Cio[97],\\nby Gesbert et. al. [47] and by Shiu et. al. [111]. The latter work used a Clarke-like\\nmodel but with two rings of scatterers, one around the transmitter and one around the\\nreceiver, to derive the MIMO channel statistics.\\n5 Exercises Exercise 7.1. 1. For the SIMO channel with uniform linear array in Section 7.2.1,\\ngive an exact expression for the distance between the transmit antenna and the\\nith receive antenna. Make precise in what sense is (7.19) an approximation.\\nRepeat the analysis for the approximation (7.27) in the MIMO case.\\nExercise 7.2. Verify that the unit vector er(r), defined in (7.21), is periodic with\\nperiod r and within one period never repeats itself.\\nExercise 7.3. Verify (7.35).\\nExercise 7.4. In an earlier work on MIMO communication [97], it is stated that the\\nnumber of degrees of freedom in a MIMO channel with nt transmit, nr receive antennas\\nand K multipaths is given by:\\nmin{nt, nr, K}\\n(7.82)\\nand this is the key parameter that determines the multiplexing capability of the chan-\\nnel. What are the problems with this statement?\\nExercise 7.5. In this question we study the role of antenna spacing in the angular\\nrepresentation of the MIMO channel.',\n",
       "  'question': 'What is the key parameter that determines the multiplexing capability of a MIMO channel?\\n',\n",
       "  'answer': 'The minimum of the number of transmit, receive antennas, and multipaths.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 391,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '.213 [13], clause 11.2A). Conditional Presence Explanation SUL-Only The field is optionally present, Need R, if supplementaryUplink is configured in ServingCellConfig. It is absent otherwise. SymbolPeriodicity This field is mandatory present if the configured UL CI monitoring periodicity indicated by monitoringSlotPeriodicityAndOffset for DCI format 2_4 is 1 slot with more than one monitoring occasion, otherwise absent. -\\tUplinkConfigCommon The IE UplinkConfigCommon provides common uplink parameters of a cell. UplinkConfigCommon information element -- ASN1START -- TAG-UPLINKCONFIGCOMMON-START UplinkConfigCommon ::= SEQUENCE { frequencyInfoUL FrequencyInfoUL OPTIONAL, -- Cond InterFreqHOAndServCellAdd initialUplinkBWP BWP-UplinkCommon OPTIONAL, -- Cond ServCellAdd dummy TimeAlignmentTimer } UplinkConfigCommon-v1700 ::= SEQUENCE { initialUplinkBWP-RedCap-r17 BWP-UplinkCommon OPTIONAL -- Need R } -- TAG-UPLINKCONFIGCOMMON-STOP -- ASN1STOP UplinkConfigCommon field descriptions frequencyInfoUL Absolute uplink frequency configuration and subcarrier specific virtual carriers. initialUplinkBWP The initial uplink BWP configuration for a serving cell (see TS 38.213 [13], clause 12). initialUplinkBWP-RedCap If present, (e)RedCap UEs use this UL BWP instead of initialUplinkBWP. If absent, (e)RedCap UEs use initialUplinkBWP provided that it does not exceed the (e)RedCap UE maximum bandwidth (see also clause 5.2.2.4.2). Conditional Presence Explanation InterFreqHOAndServCellAdd This field is mandatory present for inter-frequency handover and upon serving cell (PSCell/SCell) addition. Otherwise, the field is optionally present, Need M. ServCellAdd This field is mandatory present upon serving cell addition (for PSCell and SCell) and upon handover from E-UTRA to NR. It is optionally present, Need M otherwise. -\\tUplinkConfigCommonSIB The IE UplinkConfigCommonSIB provides common uplink parameters of a cell',\n",
       "  'question': \"What is the Conditional Presence Explanation for the field 'InterFreqHOAndServCellAdd'?\\n\",\n",
       "  'answer': 'It is mandatory present for inter-frequency handover and upon serving cell addition, otherwise it is optionally present, Need M.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1827}},\n",
       " {'context': 'BIBLIOGRAPHY\\n[mmwave-module] NYU WIRELESS, University of Padova, ns-3 module for simulating mmwave-based cellular sys-\\ntems, Available at \\n[TR38900]\\n3GPP TR 38.900 Study on channel model for frequency above 6GHz, () TR 38.912v14.0.0\\n(2016-12), 3rd Generation Partnership Project, 2016.\\n[end-to-end-mezz] Marco Mezzavilla, Menglei Zhang, Michele Polese, Russell Ford, Sourjya Dutta, Sundeep Rangan,\\nMichele Zorzi, End-to-End Simulation of 5G mmWave Networks,, in IEEE Communication Surveys and\\nTutorials, vol. 13, No 20, pp. 2237-2263, April 2018.\\n[WNS32018-NR]\\nB. Bojovic, S. Lagen, L. Giupponi, Implementation and Evaluation of Frequency Division Multi-\\nplexing of Numerologies for 5G New Radio in ns-3 , in Workshop on ns-3, June 2018, Mangalore,\\nIndia.\\n[CAMAD2018-NR]\\nN. Patriciello, S. Lagen, L. Giupponi, B. Bojovic, 5G New Radio Numerologies and their Im-\\npact on the End-To-End Latency , in Proceedings of IEEE International Workshop on Computer-\\nAided Modeling Analysis and Design of Communication Links and Networks (IEEE CAMAD), 17-\\n19 September 2018, Barcelona (Spain).\\n[CA-WNS32017]\\nB. Bojovic, D. Abrignani Melchiorre, M. Miozzo, L. Giupponi, N. Baldo, Towards LTE-Advanced\\nand LTE-A Pro Network Simulations: Implementing Carrier Aggregation in LTE Module of ns-3, in\\nProceedings of the Workshop on ns-3, Porto, Portugal, June 2017.\\n[ff-api]\\nFemtoForum , LTE MAC Scheduler Interface v1.11, Document number: FF_Tech_001_v1.11 , Date\\nissued: 12-10-2010.\\n[TS38300]\\n3GPP TS 38.300, TSG RAN; NR; Overall description; Stage 2 (), v16.0.0, Dec.\\n[TS38211]\\n3GPP TS 38.211, TSG RAN; NR; Physical channels and modulation (), v18.4.0, Sep. 2024.\\n[TS38212]\\n3GPP TS 38.212, TSG RAN; NR; Multiplexing and channel coding (), v16.0.0, Dec. 2019.\\n[TS38213]\\n3GPP TS 38.213, TSG RAN; NR; Physical layer procedures for control (), v16.0.0, Dec. 2019.\\n[TS38214]\\n3GPP TS 38.214, TSG RAN; NR; Physical layer procedures for data (), v16.0.0, Dec. 2019.',\n",
       "  'question': 'What is the document number for the LTE MAC Scheduler Interface v1.11 issued by FemtoForum?\\n',\n",
       "  'answer': 'The document number for the LTE MAC Scheduler Interface v1.11 issued by FemtoForum is FF_Tech_001_v1.11.',\n",
       "  'source_doc': {'document': 'documents/nrmodule.pdf',\n",
       "   'page': 69,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': 'Chapter\\nSignaling over AWGN Channels\\nFrom Appendix C on Bessel functions, we recognize the integral of (7.219) as the\\nmodified Bessel function of zero order, written in the compact form\\n(7.220)\\nUsing this formula, we may correspondingly express the likelihood function for the\\nsignal-detection problem described herein in the compact form\\n(7.221)\\nWith binary transmission as the issue of interest, there are two hypotheses to be\\nconsidered: hypothesis H1, that signal s1(t) was sent, and hypothesis H2, that signal s2 was\\nsent. In light of (7.221), the binary-hypothesis test may now be formulated as follows:\\nThe modified Bessel function I() is a monotonically increasing function of its argument.\\nHence, we may simplify the hypothesis test by focusing on i for given E/N0T. For\\nconvenience of implementation, however, the simplified hypothesis test is carried out in\\nterms of rather than i; that is to say:\\n(7.222)\\nFor obvious reasons, a receiver based on (7.222) is known as the quadratic receiver. In\\nlight of the definition of i given in (7.216), the receiver structure for computing i is as\\nshown in Figure 7.38a. Since the test described in (7.222) is independent of the symbol\\nenergy E, this hypothesis test is said to be uniformly most powerful with respect to E.\\nTwo Equivalent Forms of the Quadratic Receiver\\nWe next derive two equivalent forms of the quadrature receiver shown in Figure 7.38a.\\nThe first form is obtained by replacing each correlator in this receiver with a\\ncorresponding equivalent matched filter. We thus obtain the alternative form of quadrature\\nreceiver shown in Figure 7.38b. In one branch of this receiver, we have a filter matched to\\nthe signal cos(2fit) and in the other branch we have a filter matched to sin(2fit), both of\\nwhich are defined for the signaling interval 0  t  T. At time t = T, the filter outputs are\\nsampled, squared, and then added together.\\nTo obtain the second equivalent form of the quadrature receiver, suppose we have a fil-',\n",
       "  'question': 'What is the name of the receiver based on the formula (7.222)?\\n',\n",
       "  'answer': 'The quadratic receiver is known as a receiver based on the formula (7.222) because it simplifies the hypothesis test by focusing on i for given E/N0T.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 422,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '.331) RP-87 RP-200334 F CR to 38.331 on support of 70MHz channel bandwidth RP-87 RP-200335 F Clarification on the PLMN-IdentityInfoList RP-87 RP-200334 F Correction on removal of NR-DC and NE-DC band combinations when capabilityRequestFilterCommon is absent RP-87 RP-200334 F Correction on reporting of uplink TX direct current RP-87 RP-200334 F Corrections to the Location measurement indication procedure RP-87 RP-200334 - F Introduction of provisions for late non-critical extensions RP-87 RP-200334 F Correction on p-maxNR-FR1 for NE-DC RP-87 RP-200334 - F Correction on SFTD frequency list in INM RP-87 RP-200335 F Miscellaneous non-controversial corrections Set V RP-87 RP-200335 F Capability coordination for NE-DC RP-87 RP-200335 F CR on fallback BC reporting RP-87 RP-200334 F CR on overheating assistance reporting in handover case RP-87 RP-200334 F Correction on NZP-CSI-RS-ResourceSet RP-87 RP-200335 F UE capability of intra-band requirements for inter-band EN-DC/NE-DC 03/2020 RP-87 RP-200335 F Correction on usage of access category 2 for UAC for RNA update RP-87 RP-200358 F NAS handling error of nas-Container for security key derivation RP-87 RP-200356 F CR on capability of maxUplinkDutyCycle for inter-band EN-DC PC2 UE RP-87 RP-200357 F Support of releasing UL configuration RP-87 RP-200357 B Introduction of a second SMTC per frequency carrier in idle/inactive RP-87 RP-200358 C Introduction of voice fallback indication RP-87 RP-200358 C CR to 38.331 on CSI-RS inter-node message RP-87 RP-200335 B PRACH prioritization parameters for MPS and MCS RP-87 RP-200358 B Introduction of downgraded configuration for SRS antenna switching RP-87 RP-200355 B Introducing autonomous gap in CGI reporting RP-87 RP-200351 B Introduction of UECapabilityInformation segmentation in TS38',\n",
       "  'question': 'What is the channel bandwidth on support of 70MHz?\\n',\n",
       "  'answer': 'The channel bandwidth on support of 70MHz is 38.331.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 2511}},\n",
       " {'context': 'SNR, it is really far away from the capacity at low SNRs. What is going on here?\\nTo get some more insight, let us plot the performance of a bank of matched filters,\\nthe kth filter being matched to the spatial signature hk of transmit antenna k. From\\nFigure 8.13 we see the performance of the bank of matched filters is far superior to the\\ndecorrelator bank at low SNR (although far inferior at high SNR).',\n",
       "  'question': 'What does the performance of a bank of matched filters show at low SNR compared to a decorrelator bank?\\n',\n",
       "  'answer': 'The performance of a bank of matched filters is far superior to the decorrelator bank at low SNR.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 420,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': '9\\tIntegrity protection and verification The integrity protection function includes both integrity protection and integrity verification and is performed in PDCP, if configured. The data unit that is integrity protected is the PDU header and the data part of the PDU before ciphering. The integrity protection is always applied to PDCP Data PDUs of SRBs. The integrity protection is applied to sidelink SRB1, SRB2 and SRB3. The integrity protection is applied to PDCP Data PDUs of DRBs (including sidelink DRBs for unicast) for which integrity protection is configured. The integrity protection is not applicable to PDCP Control PDUs. For downlink and uplink, the integrity protection algorithm and key to be used by the PDCP entity are configured by upper layers TS 38.331 [3] and the integrity protection method shall be applied as specified in TS 33.501 [6] for NR and in TS 33.401 [17] for E-UTRA/EPC. The integrity protection function is activated/suspended/resumed by upper layers TS 38.331 [3]. When security is activated and not suspended, the integrity protection function shall be applied to all PDUs including and subsequent to the PDU indicated by upper layers TS 38.331 [3] for the downlink and the uplink, respectively. NOTE 1:\\tAs the RRC message which activates the integrity protection function is itself integrity protected with the configuration included in this RRC message, this message needs first be decoded by RRC before the integrity protection verification could be performed for the PDU in which the message was received. NOTE 2:\\tAs the PC5-S message which activates the integrity protection function is itself integrity protected with the configuration included in this PC5-S message, this message needs first be decoded by upper layer before the integrity protection verification could be performed for the PDU in which the message was received',\n",
       "  'question': 'What data units are integrity protected in PDCP?\\n',\n",
       "  'answer': 'The PDU header and the data part of the PDU before ciphering are integrity protected in PDCP.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38323-i50.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 40}},\n",
       " {'context': '4 Statistical Characterization of Wideband Wireless Channels follows we use the wide-sense stationary terminology because of its common use in the\\nwireless literature.\\nIn the context of the discussion presented herein, this first assumption means that\\n\\nThe expectation of with respect to time t is dependent only on the delay .',\n",
       "  'question': 'What does the expectation of a variable depend on in the context of wide-sense stationary terminology?\\n',\n",
       "  'answer': 'The expectation of the variable depends solely on the delay in the context of wide-sense stationary terminology.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 533,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '2\\tPhysical downlink control channel (PDCCH)\\n1\\tControl-channel element (CCE) A physical downlink control channel consists of one or more control-channel elements (CCEs) as indicated in Table 7.3.2.1-1. Table 7.3.2.1-1: Supported PDCCH aggregation levels. Aggregation level Number of CCEs',\n",
       "  'question': 'How many control-channel elements are in a physical downlink control channel?\\n',\n",
       "  'answer': 'A physical downlink control channel consists of one or more control-channel elements (CCEs).',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38211-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 98}},\n",
       " {'context': 'allow frequency multiplexing, SRS is typically transmitted over only a subset of subcarriers, defined by the configuration,\\ne.g., each 2nd or each 4th subcarrier is used for SRS transmission. However, since the minimum transmission granularity\\nin 5G-LENA module is a RB in frequency domain, all subcarriers are used for SRS transmission. Figure Example of\\nSRS transmissions of 4 different UEs (maximum 1 UE SRS transmission per slot, as per 5G-LENA design), considering SRS\\nperiodicity equal to 20 slots. Numerology considered is \\\\mu=0. F stands for frame and SF for subframe. shows the slot\\nstructure and the symbols over which the SRS transmission spans, assuming a repeated TDD pattern structure of [DL F\\nUL UL UL] (i.e., one DL slot, followed by one flexible slot and three UL slots and that SRS transmissions occur in F slots\\nPHY layer',\n",
       "  'question': 'What is the minimum transmission granularity in 5G-LENA module in frequency domain?\\n',\n",
       "  'answer': 'The minimum transmission granularity in 5G-LENA module in frequency domain is a Resource Block (RB).',\n",
       "  'source_doc': {'document': 'documents/nrmodule.pdf',\n",
       "   'page': 21,\n",
       "   'chunk_idx': 2}},\n",
       " {'context': '2\\tPower control\\n0\\tS-SS/PSBCH blocks A UE determines a power for an S-SS/PSBCH block transmission occasion in slot , in the anchor RB-set if applicable, on active SL BWP of carrier as [dBm] where - is defined in [8-1, TS 38.101-1] - is a value of dl-P0-PSBCH-r17 if using the parameter is supported by the UE and the parameter is provided; else dl-P0-PSBCH-r16 if provided; otherwise, - is a value of dl-Alpha-PSBCH, if provided; else, - when the active SL BWP is on a serving cell , as described in clause 7.1.1 except that -\\tthe RS resource is the one the UE uses for determining a power of a PUSCH transmission scheduled by a DCI format 0_0 in serving cell when the UE is configured to monitor PDCCH for detection of DCI format 0_0 in serving cell -\\tthe RS resource is the one corresponding to the SS/PBCH block the UE uses to obtain MIB when the UE is not configured to monitor PDCCH for detection of DCI format 0_0 in serving cell - is a number of resource blocks for a S-SS/PSBCH block transmission with SCS configuration - is a value of sl-SSSBPowerOffsetOfAnchorRBSet, if provided; otherwise, . For operation with shared spectrum channel access, after allocating power for transmission of each S-SS/PSBCH block in the anchor RB-set, the UE equally allocates power remaining from , if any, for transmission of each S-SS/PSBCH block in all non-anchor RB-sets within the SL BWP -\\tif dl-P0-PSBCH is not provided, a power for transmission of each S-SS/PSBCH block in a non-anchor RB-set is -\\totherwise, a power for transmission of each S-SS/PSBCH block in a non-anchor RB-set is .',\n",
       "  'question': 'What is the power level for S-SS/PSBCH block transmission in dBm?\\n',\n",
       "  'answer': 'The power level for S-SS/PSBCH block transmission is determined based on various parameters such as dl-P0-PSBCH-r17, dl-P0-PSBCH-r16, dl-Alpha-PSBCH, and sl-SSSBPowerOffsetOfAnchorRBSet, with specific values depending on the configuration and support by the UE.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38213-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 545}},\n",
       " {'context': 'the measResultNeighFreqListRSSI to the linear average of the available RSSI sample value(s) provided by lower layers for the associated neighbouring frequency up to the moment the UE sends the EUTRA RRCConnectionReconfigurationComplete message: 7>\\tfor each neighbour frequency included, include the optional fields that are available; 4>\\tif measurements are available for the measObjectNR: 5>\\tif the SS/PBCH block-based measurement quantities are available: 6>\\tset the measResultListNR in measResultNeighCells to include all the available measurement quantities of the best measured cells, other than the source PCell or target PCell, ordered such that the cell with highest SS/PBCH block RSRP is listed first if SS/PBCH block RSRP measurement results are available, otherwise the cell with highest SS/PBCH block RSRQ is listed first if SS/PBCH block RSRQ measurement results are available, otherwise the cell with highest SS/PBCH block SINR is listed first, based on the available SS/PBCH block based measurements collected up to the moment the UE sends the RRCReconfigurationComplete message if the procedure is triggered due to successful completion of reconfiguration with sync, or up to the moment the UE sends the EUTRA RRCConnectionReconfigurationComplete message if the procedure is triggered due to successful completion of Mobility from NR to E-UTRA; 6>\\tfor each neighbour cell included, include the optional fields that are available; NOTE 1:\\tFor the neighboring cells set included in measResultListNR in measResultNeighCells ordered based on the SS/PBCH block measurement quantities, the UE includes also the CSI-RS based measurement quantities, if available',\n",
       "  'question': 'What does the UE include in measResultListNR for each neighbour cell?\\n',\n",
       "  'answer': 'The UE includes the CSI-RS based measurement quantities if they are available for each neighbour cell.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 698}},\n",
       " {'context': '. If the T312 is kept in SCG, Inform E-UTRAN/NR about the SCG radio link failure by initiating the SCG failure information procedure.as specified in 5.7.3. T316 Upon transmission of the MCGFailureInformation message Upon receiving RRCRelease, RRCReconfiguration with reconfigurationwithSync for the PCell, MobilityFromNRCommand, or upon initiating the re-establishment procedure Perform the actions as specified in 5.7.3b.5. T319 Upon transmission of RRCResumeRequest or RRCResumeRequest1 when the resume procedure is not initiated for SDT. Upon reception of RRCResume, RRCSetup, RRCRelease, RRCRelease with suspendConfig or RRCReject message, upon cell re-selection or upon relay (re)selection. Perform the actions as specified in 5.3.13.5. T319a Upon transmission of RRCResumeRequest or RRCResumeRequest1 when the resume procedure is initiated for SDT. Upon reception of RRCResume, RRCSetup, RRCRelease, RRCReject message or upon failure to resume RRC connection for SDT as specified in 5.3.13.5 or upon cell reselection. Perform the actions as specified in 5.3.13.5. T320 Upon reception of t320 or upon cell (re)selection to NR from another RAT with validity time configured for dedicated priorities (in which case the remaining validity time is applied). Upon entering RRC_CONNECTED, upon reception of RRCRelease, when PLMN selection or SNPN selection is performed on request by NAS, when the UE enters RRC_IDLE from RRC_INACTIVE, or upon cell (re)selection to another RAT (in which case the timer is carried on to the other RAT). Discard the cell reselection priority information provided by dedicated signalling. T321 Upon receiving measConfig including a reportConfig with the reportType set to reportCGI Upon acquiring the information needed to set all fields of cgi-info, upon receiving measConfig that includes removal of the reportConfig with the reportType set to reportCGI and upon detecting that a cell is not broadcasting SIB1',\n",
       "  'question': 'What triggers the transmission of the MCGFailureInformation message?\\n',\n",
       "  'answer': 'The transmission of the MCGFailureInformation message is triggered upon receiving RRCRelease, RRCReconfiguration with reconfigurationwithSync for the PCell, MobilityFromNRCommand, or upon initiating the re-establishment procedure.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 2338}},\n",
       " {'context': '.214 including alignment of terminology across specifications in RAN1#98bis and RAN1#99 2019-12 RAN#86 RP-192634 B Introduction of UE behaviour for SRS measurements for CLI 2019-12 RAN#86 RP-192635 - B Introduction of two-step RACH 2019-12 RAN#86 RP-192636 - B Introduction of NR - U 2019-12 RAN#86 RP-192637 - B Introduction of integrated access and backhaul for NR 2019-12 RAN#86 RP-192638 - B Introduction of NR V2X 2019-12 RAN#86 RP-192639 - B Introduction of NR URLLC support 2019-12 RAN#86 RP-192641 - B Introduction of NR enhanced MIMO 2019-12 RAN#86 RP-192642 - B Introduction of cross-slot scheduling restriction 2019-12 RAN#86 RP-192643 - B Introduction of NR positioning support 2019-12 RAN#86 RP-192645 - B Introduction of Cross-carrier Scheduling with Different Numerologies 2019-12 RAN#86 RP-192646 - B Introduction of multiple LTE CRS rate matching patterns 2019-12 RAN#86 RP-192646 - B Aperiodic CSI-RS Triggering for UE reporting beamSwitchTiming values of 224 and 2019-12 RAN#86 RP-192646 - B Behaviour for triggered with a CSI report for non-active BWP 2019-12 RAN#86 RP-192646 - B Introduction of downgraded configurations for SRS antenna switching 2019-12 RAN#86 RP-192646 - B Introduction of one-slot periodic TRS configuration for FR1 under a certain condition 2019-12 RAN#86 RP-192640 - B Introduction of Industrial IoT 2020-03 RAN#87-e RP-200185 - F Corrections on NR - U 2020-03 RAN#87-e RP-200187 - F Corrections on NR V2X 2020-03 RAN#87-e RP-200483 F Corrections on Cross-carrier Scheduling with Different Numerologies 2020-03 RAN#87-e RP-200188 - F Corrections on NR URLLC support 2020-03 RAN#87-e RP-200190 - F Corrections on NR enhanced MIMO 2020-03 RAN#87-e RP-200189 - F Corrections on Industrial IoT 2020-03 RAN#87-e RP-200191 - F Corrections of cross-slot scheduling restriction and CSI/L1-RSRP measurement outside active time 2020-03 RAN#87-e RP-200184 - F Corrections on two-step RACH after RAN1#100-e 2020-03 RAN#87-e RP-200192 - F Corrections of NR positioning',\n",
       "  'question': 'What are some of the key introductions in the RAN#86 RP-192634 to RP-192646?\\n',\n",
       "  'answer': 'The key introductions include UE behavior for SRS measurements, two-step RACH, NR-U, integrated access and backhaul for NR, NR V2X, URLLC support, enhanced MIMO, cross-slot scheduling restriction, and positioning support.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38214-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 622}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications\\n42 XXXX \\x00\\x02\\x01 \\x03\\x05\\x04 \\x06 \\x01 \\x07\\t\\x08\\x05\\x03\\x05\\x04 \\x00 \\x0f\\x0e \\x0e \\x10\\x11 \\x00 \\x0f\\x0e \\x0e \\x12 \\x10\\x11 \\x00\\x14\\x13\\x15\\x01 \\x03\\x05\\x04 \\x0e \\x00\\x14\\x13\\x15\\x01 \\x03\\x05\\x04 \\x0e \\x16 \\x17\\x19\\x18\\x1b\\x1a \\x02\\x17\\x15 \"! \\x03 #%$ & #\\x19$ & #%$ & #\\x19$ & \\'(\\x01 \\x03\\x05\\x04 \\x12 ) #%* & ) \\x17 \\x17 + \\x01 \\x03\\x05\\x04 \\x10\\x11 +\\x02\\x13,\\x01 \\x03\\x05\\x04 \\x0e .- /\\t\\x18 \\x01102\\x03%3\\n\\x04 .- /\\t\\x18 \\x01102\\x03%3\\n\\x04 + \\x02\\x0e \\x0e \\x10\\x11 + \\x0f\\x0e \\x0e +\\x02\\x13\\x15\\x01 \\x03\\x05\\x04 \\x0e \\x16 \\x17\\x19\\x18\\x1b\\x1a \\x02\\x17,\\x1e%\\x1f\"! \\x03 #%* & Figure 2.12: A complete system diagram.\\ntime, moreover, the real and imaginary components are i.i.d. Gaussians with variances\\nN0/2. A complex Gaussian random variable X whose real and imaginary components\\nare i.i.d. satisfies a circular symmetry\\nproperty: ejX has the same distribution as\\nX for any . We shall call such a random variable circular symmetric complex Gaus-\\nsian, denoted by CN(0, 2), where 2 = E[|X|2]. The concept of circular symmetry is\\ndiscussed further in Section A.1.3 of Appendix A.\\nThe assumption of AWGN essentially means that we are assuming that the primary\\nsource of the noise is at the receiver or is radiation impinging on the receiver that is\\nindependent of the paths over which the signal is being received. This is normally a\\nvery good assumption for most communication situations. Time and Frequency Coherence Doppler Spread and Coherence Time\\nAn important channel parameter is the time-scale of the variation of the channel. How\\nfast do the taps h[m] vary as a function of time m? Recall that\\nh[m] = X i ab i(m/W)sinc [i(m/W)W] ,\\n= X i ai(m/W)ej2fci(m/W)sinc [i(m/W)W] .\\n(2.43)\\nLet us look at this expression term by term. From Section 2.2.2 we gather that signif-\\nicant changes in ai occur over periods of seconds or more. Significant changes in the\\nphase of the ith path occur at intervals of 1/(4Di), where Di = fc \\ni(t) is the Doppler\\nshift for that path. When the dierent paths contributing to the th tap have dierent',\n",
       "  'question': 'What are the real and imaginary components of a complex Gaussian random variable X assumed to be?\\n',\n",
       "  'answer': 'The real and imaginary components are independent and identically distributed (i.i.d.) Gaussian random variables with variances of N0/2.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 43,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '1\\t/2-BPSK In case of /2-BPSK modulation, bit is mapped to complex-valued modulation symbol according to\\n2\\tBPSK In case of BPSK modulation, bit is mapped to complex-valued modulation symbol according to\\n3\\tQPSK In case of QPSK modulation, pairs of bits, , are mapped to complex-valued modulation symbols according to\\n4\\t16QAM In case of 16QAM modulation, quadruplets of bits, , are mapped to complex-valued modulation symbols according to\\n5\\t64QAM In case of 64QAM modulation, hextuplets of bits, , are mapped to complex-valued modulation symbols according to\\n6\\t256QAM In case of 256QAM modulation, octuplets of bits, , are mapped to complex-valued modulation symbols according to\\n7\\t1024QAM In case of 1024QAM modulation, 10-tuplets of bits, , are mapped to complex-valued modulation symbols according to\\n2\\tSequence generation\\n1\\tPseudo-random sequence generation Generic pseudo-random sequences are defined by a length-31 Gold sequence. The output sequence of length, where, is defined by where and the first m-sequence shall be initialized with. The initialization of the second m-sequence, , is denoted by with the value depending on the application of the sequence. The low-PAPR sequence is defined by a cyclic shift of a base sequence according to where is the length of the sequence. Multiple sequences are defined from a single base sequence through different values of and . Base sequences are divided into groups, where is the group number and is the base sequence number within the group, such that each group contains one base sequence () of each length , and two base sequences () of each length , . The definition of the base sequence depends on the sequence length .',\n",
       "  'question': 'What is the length of the output sequence defined by a length-31 Gold sequence?\\n',\n",
       "  'answer': 'The output sequence has a length of 31.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38211-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 16}},\n",
       " {'context': '2\\tFrequency hopping for PUSCH repetition Type B For PUSCH repetition Type B (as determined according to procedures defined in Clause 6.1.2.1 for scheduled PUSCH, or Clause 6.1.2.3 for configured PUSCH), a UE is configured for frequency hopping by the higher layer parameter frequencyHoppingDCI-0-2 in pusch-Config for PUSCH transmission scheduled by DCI format 0_2, by frequencyHoppingDCI-0-1 provided in pusch-Config for PUSCH transmission scheduled by DCI format 0_1, and by frequencyHoppingPUSCH-RepTypeB provided in rrc-ConfiguredUplinkGrant for Type 1 configured PUSCH transmission. The frequency hopping mode for Type 2 configured PUSCH transmission follows the configuration of the activating DCI format. One of two frequency hopping modes can be configured: -\\tInter-repetition frequency hopping -\\tInter-slot frequency hopping For operation with shared spectrum channel access in FR1, the UE does not expect that two hops of a PUSCH transmission are in different RB sets. In case of resource allocation type 1, whether or not transform precoding is enabled for PUSCH transmission, the UE may perform PUSCH frequency hopping, if the frequency hopping field in a corresponding detected DCI format is set to 1, or if for a Type 1 PUSCH transmission with a configured grant the higher layer parameter frequencyHoppingPUSCH-RepTypeB is provided, otherwise no PUSCH frequency hopping is performed. When frequency hopping is enabled for PUSCH, the RE mapping is defined in clause 6.3.1.6 of [4, TS 38.211]. For a PUSCH scheduled by DCI format 0_1 or a PUSCH based on a Type 2 configured UL grant activated by DCI format 0_1 and for resource allocation type 1, frequency offsets are configured by higher layer parameter frequencyHoppingOffsetLists in pusch-Config',\n",
       "  'question': 'What are the two frequency hopping modes for Type 2 configured PUSCH transmission?\\n',\n",
       "  'answer': 'Inter-repetition frequency hopping and inter-slot frequency hopping are the two frequency hopping modes for Type 2 configured PUSCH transmission.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38214-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 544}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications\\n287 5 10 15 20 25 Csym  C 30 35 40 SNR (dB) 0.5 0 -5 -10 0.8 0.75 0.7 0.65 0.6 0.55 Figure 6.10: Plot of the symmetric -outage capacity of the 2-user Rayleigh slow fading\\nuplink as compared to C, the corresponding performance of a point-to-point Rayleigh\\nslow fading channel. Csym  for K = 2 users as compared to C, for Rayleigh fading, in Figure 6.10. As SNR\\nincreases, the ratio of Csym\\n\\nto C increases; thus the eect of the inter-user interference\\nis becoming smaller. However, as SNR becomes very large, the ratio starts to decrease;\\nthe inter-user interference begins to dominate. In fact, at very large SNRs the ratio\\ndrops back to 1/K (Exercise 6.14). We will obtain a deeper understanding of this\\nbehavior when we study outage in the uplink with multiple antennas in Section 10.1.4. Fast Fading Channel\\nLet us now turn to the fast fading scenario, where each {hk[m]}m is modelled as a\\ntime-varying ergodic process. With the ability to code over multiple coherence time\\nintervals, we can have a meaningful definition of the capacity region of the uplink fading\\nchannel. With only receiver CSI, the transmitters cannot track the channel and there\\nis no dynamic power allocation. Analogous to the discussion in the point-to-point case\\n(c.f. Section 5.4.5 and, in particular, (5.89)), the sum capacity of the uplink fast fading\\nchannel can be expressed as:',\n",
       "  'question': 'What happens to the ratio of Csym to C as SNR becomes very large in the 2-user Rayleigh slow fading uplink?\\n',\n",
       "  'answer': 'The ratio drops back to 1/K, indicating that inter-user interference begins to dominate at very high SNRs.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 288,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '. -\\tRRCReject The RRCReject message is used to reject an RRC connection establishment or an RRC connection resumption. Signalling radio bearer: SRB0 RLC-SAP: TM Logical channel: CCCH Direction: Network to UE RRCReject message -- ASN1START -- TAG-RRCREJECT-START RRCReject ::= SEQUENCE { criticalExtensions CHOICE { rrcReject RRCReject-IEs, criticalExtensionsFuture SEQUENCE {} } } RRCReject-IEs ::= SEQUENCE { waitTime RejectWaitTime OPTIONAL, -- Need N lateNonCriticalExtension OCTET STRING OPTIONAL, nonCriticalExtension SEQUENCE{} OPTIONAL } -- TAG-RRCREJECT-STOP -- ASN1STOP RRCReject-IEs field descriptions waitTime Wait time value in seconds. The field is always included. -\\tRRCRelease The RRCRelease message is used to command the release of an RRC connection or the suspension of the RRC connection',\n",
       "  'question': 'What is the wait time value in seconds for the RRCReject-IEs field?\\n',\n",
       "  'answer': 'The wait time value in seconds is always included in the RRCReject-IEs field.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 985}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications B.3.4\\nOperational Interpretation\\nThere is a common misconception which needs to be pointed out. In solving the input\\ndistribution optimization problem (B.22) for the capacity C, it was remarked that\\nat the optimal solution, the outputs y[m]s should be independent, and one way to\\nachieve this is for the inputs x[m]s to be independent. Does that imply no coding is\\nneeded to achieve capacity? For example, in the binary symmetric channel, the optimal\\ninput yields i.i.d. equally likely symbols; does it mean then we can send equally likely\\ninformation bits raw across the channel and still achieve capacity?\\nOf course not: to get very small error probability one needs to code over many\\nsymbols. The fallacy of the above argument is that reliable communication cannot be\\nachieved at exactly the rate C and when the outputs are exactly independent. Indeed,\\nwhen the outputs and inputs are i.i.d,\\nH(x|y) = NXm=1 H(x[m]|y[m]) = NH(x[m]|y[m]),\\n(B.32)\\nand there is a lot of uncertainty in the input given the output: the communication\\nis hardly reliable. But once one shoots for a rate strictly less than C, no matter how\\nclose, the coding theorem guarantees that reliable communication is possible. The\\nmutual information I(x; y)/N per symbol is close to C, the outputs y[m]s are almost\\nindependent, but now the conditional entropy H(x|y) is reduced abruptly to (close\\nto) zero since reliable decoding is possible. But to achieve this performance, coding is\\ncrucial; indeed the entropy per input symbol is close to I(x; y)/N, less than H(x[m])\\nunder uncoded transmission. For the binary symmetric channel, the entropy per coded\\nsymbol is 1 H(), rather than 1 for uncoded symbols.\\nThe bottomline is that while the value of the input optimization problem (B.22) has\\noperational meaning as the maximum rate of reliable communication, it is incorrect\\nto interpret the i.i.d. input distribution which attains that value as the statistics of',\n",
       "  'question': 'What is the misconception about achieving capacity in the input distribution optimization problem?\\n',\n",
       "  'answer': 'The misconception is that independent inputs automatically achieve capacity without coding, which is false for reliable communication.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 610,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': 'BIT STRING (SIZE (32)), four-three-TypeI-SinglePanel-Restriction2-r17 BIT STRING (SIZE (192)), six-two-TypeI-SinglePanel-Restriction2-r17 BIT STRING (SIZE (192)), twelve-one-TypeI-SinglePanel-Restriction2-r17 BIT STRING (SIZE (48)), four-four-TypeI-SinglePanel-Restriction2-r17 BIT STRING (SIZE (256)), eight-two-TypeI-SinglePanel-Restriction2-r17 BIT STRING (SIZE (256)), sixteen-one-TypeI-SinglePanel-Restriction2-r17 BIT STRING (SIZE (64)) } } } } OPTIONAL, -- Need R typeI-SinglePanel-ri-RestrictionSTRP-r17 BIT STRING (SIZE (8)) OPTIONAL, -- Need R typeI-SinglePanel-ri-RestrictionSDM-r17 BIT STRING (SIZE (4)) OPTIONAL -- Need R }, type2 SEQUENCE { typeII-PortSelection-r17 SEQUENCE { paramCombination-r17 INTEGER (1',\n",
       "  'question': 'What is the size of the BIT STRING with the identifier \"four-three-TypeI-SinglePanel-Restriction2-r17\"?\\n',\n",
       "  'answer': 'The BIT STRING with the identifier \"four-three-TypeI-SinglePanel-Restriction2-r17\" has a size of 192 bits.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1244}},\n",
       " {'context': '. timeSinceFailure This field is used to indicate the time that elapsed since the connection (establishment or resume) failure. Value in seconds. The maximum value 172800 means 172800s or longer. RA-InformationCommon field descriptions absoluteFrequencyPointA This field indicates the absolute frequency position of the reference resource block (Common RB 0). allPreamblesBlocked This field is included when the all the preamble transmission attempts in the corresponding beam (SSB or CSI-RS) are blocked by failed LBT. attemptedBWP-InfoList This field indicates locationAndBandwidth and subcarrierSpacing of all the bandwidth parts in which the consistent LBT failures are triggered at the moment of successful RA completion. locationAndBandwidth Frequency domain location and bandwidth of the bandwidth part associated to the random-access resources used by the UE or of the bandwidth part in which the consistent LBT failures is triggered and not cancelled prior to successful completion of random access procedure (if this field is included in attemptedBWP-InfoList) or prior to RLF/HOF (if this field is included in attemptedBWP-InfoList or bwp-Info). numberOfLBT-Failures This field is used to indicate the total number of preamble transmission attempts for which LBT failure indication is received in the RA procedure. If the number of LBT failure indications received from lower layers during the RA procedure exceeds or equals to 128, UE sets the field to 128.This field is optional present when there is at least one preamble transmission attempt for which LBT failure indication is received during the RA procedure, otherwise it is absent. numberOfPreamblesPerSSB-ForThisPartition This field determines how many consecutive preambles are associated to the used feature or combination of features starting from the starting preamble(s) per SSB',\n",
       "  'question': 'What does the timeSinceFailure field indicate?\\n',\n",
       "  'answer': 'The timeSinceFailure field indicates the time elapsed since the connection failure, measured in seconds, with a maximum value of 172800 representing 172800 seconds or longer.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1094}},\n",
       " {'context': '.8.9.1b.2; 1>\\tif the RRCReconfigurationSidelink message includes the sl-MeasConfig: 2>\\tperform the sidelink measurement configuration procedure as specified in 5.8.10; 1>\\tif the RRCReconfigurationSidelink message includes the sl-CSI-RS-Config: 2>\\tapply the sidelink CSI-RS configuration; 1>\\tif the RRCReconfigurationSidelink message includes the sl-LatencyBoundCSI-Report: 2>\\tapply the configured sidelink CSI report latency bound; 1>\\tif the RRCReconfigurationSidelink includes the sl-RLC-ChannelToReleaseListPC5: 2>\\tfor each SL-RLC-ChannelID value included in the sl-RLC-ChannelToReleaseListPC5 that is part of the current UE sidelink configuration; 3>\\tperform the PC5 Relay RLC channel release procedure, according to clause 5.8.9.7.1; 1>\\tif the RRCReconfigurationSidelink includes the sl-RLC-ChannelToAddModListPC5: 2>\\tfor each sl-RLC-ChannelID-PC5 value included in the sl-RLC-ChannelToAddModListPC5 that is not part of the current UE sidelink configuration: 3>\\tperform the PC5 Relay RLC channel addition procedure, according to clause 5.8.9.7.2; 2>\\tfor each sl-RLC-ChannelID-PC5 value included in the sl-RLC-ChannelToAddModListPC5 that is part of the current UE sidelink configuration: 3>\\tperform the PC5 Relay RLC channel modification procedure according to clause 5.8.9.7.2; 1>\\tif the RRCReconfigurationSidelink message includes the sl-DRX-ConfigUC-PC5; and 1>\\tif the UE accepts the sl-DRX-ConfigUC-PC5: 2>\\tconfigure lower layers to perform sidelink DRX operation according to sl-DRX-ConfigUC-PC5 for the associated destination as defined in TS 38.321 [3]; 1>\\tif the RRCReconfigurationSidelink message includes the sl-LatencyBoundIUC-Report: 2>\\tapply the configured sidelink IUC report latency bound; 1>\\tif the RRCReconfigurationSidelink message includes the sl-LocalID-PairToAddModList: 2>\\tconfigure SRAP entity to perform NR sidelink L2 U2U relay operation accordingly for the end-to-end PC5 connection with the peer L2 U2U Remote UE as defined in TS 38',\n",
       "  'question': 'What does the RRCReconfigurationSidelink message include if it has the sl-MeasConfig?\\n',\n",
       "  'answer': 'The sidelink measurement configuration procedure as specified in 5.8.10 is performed.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 803}},\n",
       " {'context': '6 Phase-Shift Keying Techniques Using Coherent Detection To calculate the probability of making an error of the first kind, we note from Figure 7.13a\\nthat the decision region associated with symbol 1 or signal s1(t) is described by\\nwhere the observable element x1 is related to the received signal x(t) by\\n(7.102)\\nThe conditional probability density function of random variable X1, given that symbol\\n(i.e., signal s2(t)) was transmitted, is defined by\\n(7.103)\\nUsing (7.101) in this equation yields\\n(7.104)\\nThe conditional probability of the receiver deciding in favor of symbol 1, given that\\nsymbol 0 was transmitted, is therefore\\n(7.105) Putting (7.106) Figure 7.14 Block diagrams for (a) binary PSK transmitter and (b) coherent\\nbinary PSK receiver.\\nBinary PSK signal s(t) Product modulator Polar nonreturn- to-zero level encoder Binary data sequence Decision device Tb x(t) Choose 1 if x1 > 0 Choose 0 if x1 < 0 = 2 Tb cos (2 fct ) 0 dt \\x02 Threshold = 0 Correlator x1 (a) (b) 1(t)  1(t)   Z1:0 x1    x1 x t1 t dt 0 Tb = fX1 x1 0   1 N0 -------------- 1 N0 ------ x1 s21 -  2 - exp = fX1 x1 0   1 N0 -------------- 1 N0 ------ x1 Eb +   2 - exp = p10 1 N0 -------------- exp 1 N0 ------ x1 Eb +   2 - dx1 0   = z 2 N0 ------ x1 Eb +   =',\n",
       "  'question': 'What is the description of the decision region associated with symbol 1 or signal s1(t)?\\n',\n",
       "  'answer': 'The decision region is described by a specific mathematical equation involving the observable element x1 related to the received signal x(t).',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 375,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '6\\tEvent A5 (SpCell becomes worse than threshold1 and neighbour becomes better than threshold2) The UE shall: 1>\\tconsider the entering condition for this event to be satisfied when both condition A5-1 and condition A5-2, as specified below, are fulfilled; 1>\\tconsider the leaving condition for this event to be satisfied when condition A5-3 or condition A5-4, i.e. at least one of the two, as specified below, is fulfilled; 1>\\tuse the SpCell for Mp. NOTE 1:\\tThe parameters of the reference signal(s) of the cell(s) that triggers the event are indicated in the measObjectNR associated to the event which may be different from the measObjectNR of the NR SpCell. Inequality A5-1 (Entering condition 1) Mp + Hys < Thresh1 Inequality A5-2 (Entering condition 2) Mn + Ofn + Ocn - Hys > Thresh2 Inequality A5-3 (Leaving condition 1) Mp - Hys > Thresh1 Inequality A5-4 (Leaving condition 2) Mn + Ofn + Ocn + Hys < Thresh2 The variables in the formula are defined as follows: Mp is the measurement result of the NR SpCell, not taking into account any offsets. Mn is the measurement result of the neighbouring cell, not taking into account any offsets. Ofn is the measurement object specific offset of the neighbour cell (i.e. offsetMO as defined within measObjectNR corresponding to the frequency of the neighbour cell). Ocn is the cell specific offset of the neighbour cell (i.e. cellIndividualOffset as defined within measObjectNR corresponding to the frequency of the neighbour cell, or cellIndividualOffset as defined within reportConfigNR), and set to zero if not configured for the neighbour cell. Hys is the hysteresis parameter for this event (i.e. hysteresis as defined within reportConfigNR for this event). Thresh1 is the threshold parameter for this event (i.e. a5-Threshold1 as defined within reportConfigNR for this event). Thresh2 is the threshold parameter for this event (i.e. a5-Threshold2 as defined within reportConfigNR for this event)',\n",
       "  'question': 'What is the hysteresis parameter for Event A5?\\n',\n",
       "  'answer': 'The hysteresis parameter for Event A5 is defined within reportConfigNR for this event.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 475}},\n",
       " {'context': '., including the registered SNPN identity), if available; 3>\\tfor intra-NR handover, set the c-RNTI to the C-RNTI assigned by the target PCell of the handover; 3>\\tif the procedure is triggered due to successful completion of reconfiguration with sync, for the source PCell in which the last RRCReconfiguration message including reconfigurationWithSync was applied; or 3>\\tif the procedure is triggered due to successful completion of Mobility from NR to E-UTRA, for the source PCell in which the last MobilityFromNRCommand concerning an inter-RAT handover from NR to E-UTRA was applied: 4>\\tset the sourcePCellID in sourceCellInfo to the global cell identity and tracking area code, if available, of the source PCell; 4>\\tset the sourceCellMeas in sourceCellInfo to include the cell level RSRP, RSRQ and the available SINR, of the source PCell based on the available SSB and CSI-RS measurements collected up to the moment the UE sends RRCReconfigurationComplete message if the procedure is triggered due to successful completion of reconfiguration with sync, or up to the moment the UE sends the EUTRA RRCConnectionReconfigurationComplete message if the procedure is triggered due to successful completion of Mobility from NR to E-UTRA; 4>\\tset the rsIndexResults in sourceCellMeas to include all the available SSB and CSI-RS measurement quantities of the source PCell collected up to the moment the UE sends RRCReconfigurationComplete message if the procedure is triggered due to successful completion of reconfiguration with sync, or up to the moment the UE sends the EUTRA RRCConnectionReconfigurationComplete message if the procedure is triggered due to successful completion of Mobility from NR to E-UTRA; 4>\\tif the last executed handover was a DAPS handover and if an RLF occurred at the source PCell during the DAPS handover while T304 was running: 5>\\tset the rlf-InSourceDAPS in sourceCellInfo to true; 3>\\tif the procedure is triggered due to successful completion of reconfiguration with sync,',\n",
       "  'question': 'What triggers the setting of the c-RNTI to the C-RNTI assigned by the target PCell of the handover?\\n',\n",
       "  'answer': 'Intra-NR handover procedure.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 692}},\n",
       " {'context': '.331 [5]. The length of the field is 4 bits; -\\tTCI State Serving Cell IDi: This field indicates the identity of the Serving Cell on which the TCI State used for SRS resource i is located. The length of the field is 5 bits; -\\tTCI State BWP IDi: This field indicates a BWP as the codepoint of the DCI bandwidth part indicator field as specified in TS 38.212 [9], on which the TCI State used for SRS resource i is located. If value of unifiedTCI-StateType in the Serving Cell indicated by TCI State Serving Cell IDi is joint, this field indicates a DL BWP. If value of unifiedTCI-StateType in the Serving Cell indicated by TCI State Serving Cell IDi is separate, this field indicates a UL BWP. The length of the field is 2 bits; -\\tTCI State IDi: This field contains an identifier of the TCI state used for SRS resource i. TCI State ID0 refers to the first SRS resource within the resource set, TCI State ID1 refers to the second one and so on. If joint/downlink TCI State is used, 7-bits length TCI state ID i.e. TCI-StateId as specified in TS 38.331 [5] is used. If separate downlink and uplink TCI State is used, the most significant bit of TCI state ID is considered as a reserved bit and the remaining 6 bits indicate the TCI-UL-State-Id as specified in TS 38.331 [5]. The length of the field is 7 bits. This field is only present if MAC CE is used for activation of SP SRS resource set, i.e. the A/D field is set to 1, or for AP SRS resource set; -\\tR: Reserved bit, set to 0. Figure 6.1.3.59-1: SP/AP SRS TCI State Indication MAC CE',\n",
       "  'question': 'How many bits are in the field that indicates the identity of the Serving Cell on which the TCI State used for SRS resource i is located?\\n',\n",
       "  'answer': 'The field indicating the identity of the Serving Cell for the TCI State used for SRS resource i is 5 bits long.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38321-i50.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 518}},\n",
       " {'context': '..maxNR-NS-Pmax)) OF NR-NS-PmaxValueAerial-r18 NR-NS-PmaxValueAerial-r18 ::= SEQUENCE { additionalPmax-r18 P-Max OPTIONAL, -- Need N additionalSpectrumEmission-r18 AdditionalSpectrumEmission-r18 } -- TAG-NR-NS-PMAXLIST-STOP -- ASN1STOP -\\tNSAG-ID The IE NSAG-ID is used to identify an NSAG (TS 23.501 [32]) for slice-based cell reselection or slice-based random access. NSAG-ID information element -- ASN1START -- TAG-NSAG-ID-START NSAG-ID-r17 ::= BIT STRING (SIZE (8)) -- TAG-NSAG-ID-STOP -- ASN1STOP -\\tNSAG-IdentityInfo The IE NSAG-IdentityInfo is used to identify an NSAG (TS 23.501 [32]) for slice-based cell reselection. NSAG-IdentityInfo information element -- ASN1START -- TAG-NSAG-IDENTITYINFO-START NSAG-IdentityInfo-r17 ::= SEQUENCE { nsag-ID-r17 NSAG-ID-r17, trackingAreaCode-r17 TrackingAreaCode OPTIONAL -- Need R } -- TAG-NSAG-IDENTITYINFO-STOP -- ASN1STOP NSAG-IdentityInfo field descriptions trackingAreaCode If absent, UE assumes the trackingAreaCode of the serving cell. -\\tNTN-Config The IE NTN-Config provides parameters needed for the UE to access NR via NTN access. NTN-Config information element -- ASN1START -- TAG-NTN-CONFIG-START NTN-Config-r17 ::= SEQUENCE { epochTime-r17 EpochTime-r17 OPTIONAL, -- Need R ntn-UlSyncValidityDuration-r17 ENUMERATED{ s5, s10, s15, s20, s25, s30, s35, s40, s45, s50, s55, s60, s120, s180, s240, s900} OPTIONAL, -- Cond SIB19 cellSpecificKoffset-r17 INTEGER(1..1023) OPTIONAL, -- Need R kmac-r17 INTEGER(1..512) OPTIONAL, -- Need R ta-Info-r17 TA-Info-r17 OPTIONAL, -- Need R ntn-PolarizationDL-r17 ENUMERATED {rhcp,lhcp,linear} OPTIONAL, -- Need R ntn-PolarizationUL-r17 ENUMERATED {rhcp,lhcp,linear} OPTIONAL, -- Need S ephemerisInfo-r17 EphemerisInfo-r17 OPTIONAL, -- Need R ta-Report-r17 ENUMERATED {enabled} OPTIONAL, -- Need R ... } TA-Info-r17 ::= SEQUENCE { ta-Common-r17 INTEGER(0..66485757), ta-CommonDrift-r17 INTEGER(-257303..257303) OPTIONAL, -- Need R ta-CommonDriftVariant-r17 INTEGER(0.',\n",
       "  'question': 'What is the size of the NSAG-ID in bits?\\n',\n",
       "  'answer': 'The NSAG-ID is a bit string with a size of 8 bits.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1468}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications Suppose we broadcast the common information at rate R using a space-time code\\nthat satisfies (9.79) for an nt  nt MIMO channel. Since this code is\\napproximately universal for every nt  nr MIMO channel, the diversity seen by\\neach user is simultaneously the best possible at rate R. To summarize: the\\ndiversity gain obtained by each user is the best possible with respect to both,\\n the number of receive antennas it has, and\\n the statistics of the fading channel the user is currently experiencing.\\nChapter 9: The Main Plot\\nFor a slow fading channel at high SNR, the tradeobetween data rate and error\\nprobability is captured by the tradeobetween multiplexing and diversity gains.\\nThe optimal diversity gain d(r) is the rate at which outage probability decays\\nwith increasing SNR when the data rate is increasing as r log SNR. The classical\\ndiversity gain is the diversity gain at a fixed rate, i.e., the multiplexing gain r = 0.\\nThe optimal diversity gain d(r) is determined by the outage probability of the\\nchannel at a data rate of r log SNR bits/s/Hz. The operational interpretation is\\nvia the existence of a universal code that achieves reliable communication\\nsimultaneously over all channels that are not in outage.\\nThe universal code viewpoint provides a new code design criterion. Instead of\\naveraging over the channel statistics, we consider the performance of a code over\\nthe worst-case channel that is not in outage.\\n For the parallel channel, the universal criterion is to maximize the product of the\\ncodeword dierences. Somewhat surprisingly, this is the same as the criterion\\narrived at by averaging over the Rayleigh channel statistics.\\n For the MISO channel, the universal criterion is to maximize the smallest singular\\nvalue of the codeword dierence matrices.\\n For the ntnr MIMO channel, the universal criterion is to maximize the product\\nof the nmin smallest singular values of the codeword dierence matrices. With',\n",
       "  'question': 'What is the best diversity gain for each user in a slow fading channel at high SNR?\\n',\n",
       "  'answer': 'The optimal diversity gain d(r) is the rate at which outage probability decays with increasing SNR when the data rate is increasing as r log SNR.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 487,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications\\n46 10 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 60 50 40 30 20 10 0 0.65 0.66 0.67 0.68 0.69 0.7 0.71 0.72 0.73 0.74 0.75 0.76 0.45 0 10 20 0.001 0.0008 0.0006 0.0004 0.0002 0 0.0002 0.0004 0.0006 0.0008 0.001 0 50 100 150 200 250 300 350 400 450 500 550 30 40 50 60 70 0.006 0.005 0.004 0.003 0.002 0.001 0 0.001 0.002 0.003 0.004 50 100 150 200 250 300 350 400 450 500 550 0 0.5 (d) Power Specturm Amplitude (linear scale) Amplitude (linear scale) 200MHz (dB) Power Spectrum (b) time (ns) time (ns) (a) (c) 40MHz frequency (GHz) frequency (GHz) (dB) Figure 2.13: (a) A channel over 200 MHz is frequency-selective, and the impulse re-\\nsponse has many taps. (b) The spectral content of the same channel. (c) The same\\nchannel over 40 MHz is flatter, and has much fewer taps. (d) The spectral contents\\nof the same channel, limited to 40 MHz bandwidth. At larger bandwidths, the same\\nphysical paths are resolved into a finer resolution.\\nKey Channel Parameters and Time Scales\\nSymbol\\nRepresentative Values\\ncarrier frequency fc 1 GHz communication bandwidth\\nW\\n1 MHz\\ndistance between transmitter and receiver\\nd 1 km velocity of mobile v 64 km/h Doppler shift for a path\\nD = fcv c 50 Hz Doppler spread of paths corresponding to a tap\\nDs\\n100 Hz\\ntime scale for change of path amplitude\\nd v 1 minute time scale for change of path phase\\n1 4D 5 ms time scale for a path to move over a tap\\nc vW 20 s coherence time Tc = 1 4Ds 2.5 ms delay spread Td 1  s coherence bandwidth Wc = 1 2Td 500 kHz Table 2.1: A summary of the physical parameters of the channel and the time scale of\\nchange of the key parameters in its discrete-time baseband model.',\n",
       "  'question': 'What is the Doppler shift for a path in a channel?\\n',\n",
       "  'answer': 'The Doppler shift for a path in a channel is 50 Hz.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 47,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': \"1A\\tSecond procedures for RedCap UE In this clause, the term 'UE' refers to a RedCap UE that indicates supportOfERedCap. A UE that has not indicated eRedCapNotReducedBB-BW does not expect to transmit a PUSCH over a bandwidth that is larger than 25 PRBs for 15 kHz SCS, or larger than 12 PRBs for 30 kHz SCS, per hop in a slot. A UE that has not indicated eRedCapNotReducedBB-BW does not expect to process a PDSCH reception that is scheduled by a DCI format with CRC scrambled by a C-RNTI, CS-RNTI, MCS-C-RNTI, G-RNTI for multicast, or G-CS-RNTI, or is associated with a SPS PDSCH configuration activated by a DCI format with CRC scrambled by CS-RNTI or G-CS-RNTI, over a number of PRBs that is larger than 25 PRBs for 15 kHz SCS, or larger than 12 PRBs for 30 kHz SCS, in a slot. A UE that has not indicated eRedCapNotReducedBB-BW is not required to process a PDSCH reception in slot that is scheduled by a DCI format with CRC scrambled by a G-RNTI for broadcast or a MCCH-RNTI over a number of PRBs that is larger than 25 PRBs for 15 kHz SCS, or larger than 12 PRBs for 30 kHz SCS, when the PDSCH reception is with repetitions or when the UE receives another PDSCH in slot . A UE that has not indicated eRedCapNotReducedBB-BW is not required to process a PDSCH reception that is scheduled by a DCI format with CRC scrambled by Multicast MCCH-RNTI or G-RNTI for multicast in RRC_INACTIVE state over a number of PRBs that is larger than 25 PRBs for 15 kHz SCS, or larger than 12 PRBs for 30 kHz SCS, in a slot. A UE is not required to process a PDSCH reception that is scheduled by a DCI format with CRC scrambled by a TC-RNTI over a number of PRBs that is larger than 25 PRBs for 15 kHz SCS, or larger than 12 PRBs for 30 kHz SCS, in a slot\",\n",
       "  'question': 'What is the maximum number of Physical Resource Blocks (PRBs) a UE can transmit over for 15 kHz SCS if it has not indicated eRedCapNotReducedBB-BW?\\n',\n",
       "  'answer': 'A UE that has not indicated eRedCapNotReducedBB-BW does not expect to transmit a PUSCH over a bandwidth that is larger than 25 PRBs for 15 kHz SCS.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38213-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 617}},\n",
       " {'context': '2\\tModulation For each codeword , the UE shall assume the block of scrambled bits are modulated as described in clause5.1 using one of the modulation schemes in Table 7.3.1.2-1, resulting in a block of complex-valued modulation symbols . Table 7.3.1.2-1: Supported modulation schemes. Modulation scheme Modulation order QPSK 16QAM 64QAM 256QAM 1024QAM\\n3\\tLayer mapping The UE shall assume that complex-valued modulation symbols for each of the codewords to be transmitted are mapped onto one or several layers according to Table 7.3.1.3-1. Complex-valued modulation symbols for codeword shall be mapped onto the layers , where is the number of layers and is the number of modulation symbols per layer. Table 7.3.1.3-1: Codeword-to-layer mapping for spatial multiplexing. Number of layers Number of codewords Codeword-to-layer mapping\\n4\\tAntenna port mapping The block of vectors , shall be mapped to antenna ports according to where , . The set of antenna ports shall be determined according to the procedure in [4, TS 38.212].',\n",
       "  'question': 'What modulation schemes are supported in the given context?\\n',\n",
       "  'answer': 'The supported modulation schemes include QPSK, 16QAM, 64QAM, 256QAM, and 1024QAM.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38211-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 93}},\n",
       " {'context': 'tion: E [xxt] = K. Show that the jointly Gaussian random vector with covariance\\nK has the maximal dierential entropy among this set of covariance constrained\\nrandom variables.\\nNow consider a complex random variable x. Show that among the class of contin-\\nuous complex random variables x with the second moment condition E [|x|2] P,\\nthe circularly symmetric Gaussian complex random variable has the maximal dif-\\nferential entropy. Hint: View x as a length 2 vector of real random variables and\\nuse the previous part of this question.',\n",
       "  'question': 'What type of Gaussian random variable has the maximal differential entropy among continuous complex random variables with a second moment condition?\\n',\n",
       "  'answer': 'The circularly symmetric Gaussian complex random variable has the maximal differential entropy among continuous complex random variables with a second moment condition, as it maximizes the entropy given the constraint on the second moment E[|x|^2] = P. This result follows from treating the complex random variable as a length 2 vector of real random variables and applying the principles of differential entropy maximization for Gaussian distributions under covariance constraints.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 627,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': '. ssb-PositionQCL Indicates the QCL relation between SS/PBCH blocks for a specific neighbor cell as specified in TS 38.213 [13], clause 4.1. If provided, the cell specific value overwrites the common value signalled by ssb-PositionQCL-Common in SIB4 for the indicated cell. ssb-PositionQCL-Common Indicates the QCL relation between SS/PBCH blocks for inter-frequency neighbor cells as specified in TS 38.213 [13], clause 4.1. ssb-ToMeasure The set of SS blocks to be measured within the SMTC measurement duration (see TS 38.215 [9]). When the field is absent the UE measures on all SS-blocks. ssbSubcarrierSpacing Subcarrier spacing of SSB. Only the following values are applicable depending on the used frequency: FR1: 15 or 30 kHz FR2-1/FR2-NTN: 120 or 240 kHz FR2-2: 120, 480, or 960 kHz threshX-HighP Parameter \"ThreshX, HighP\" in TS 38.304 [20]. threshX-HighQ Parameter \"ThreshX, HighQ\" in TS 38.304 [20]. threshX-LowP Parameter \"ThreshX, LowP\" in TS 38.304 [20]. threshX-LowQ Parameter \"ThreshX, LowQ\" in TS 38.304 [20]. tn-AreaIdList List of TN area identifiers. The associated coverage information is provided in SIB25. t-ReselectionNR Parameter \"TreselectionNR\" in TS 38.304 [20]. t-ReselectionNR-SF Parameter \"Speed dependent ScalingFactor for TreselectionNR\" in TS 38.304 [20]. If the field is absent, the UE behaviour is specified in TS 38.304 [20]. Conditional Presence Explanation LessThan5MHz The field is mandatory present if the carrierBandwidth in SIB1 indicates UL or DL transmission bandwidth other than 15 PRB and the corresponding neighbour cell(s) support(s) 12 PRB, 15 PRB or 20 PRB transmission bandwidth configuration as defined in TS 38.101-1 [15], TS 38.211 [16] and TS 38.213 [13]. Otherwise, the field is optional, Need R. Mandatory The field is mandatory present in SIB4. RSRQ The field is mandatory present if threshServingLowQ is present in SIB2; otherwise it is absent',\n",
       "  'question': 'What does ssb-PositionQCL indicate?\\n',\n",
       "  'answer': 'It indicates the QCL relation between SS/PBCH blocks for a specific neighbor cell as specified in TS 38.213, clause 4.1.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1132}},\n",
       " {'context': \"1\\tGeneral\\n1\\tIntroduction This clause covers the general requirements.\\n2\\tGeneral requirements The UE shall: 1>\\tprocess the received messages in order of reception by RRC, i.e. the processing of a message shall be completed before starting the processing of a subsequent message; NOTE:\\tNetwork may initiate a subsequent procedure prior to receiving the UE's response of a previously initiated procedure. 1>\\twithin a clause execute the steps according to the order specified in the procedural description; 1>\\tconsider the term 'radio bearer' (RB) to cover SRBs, DRBs and MRBs unless explicitly stated otherwise; 1>\\tset the rrc-TransactionIdentifier in the response message, if included, to the same value as included in the message received from the network that triggered the response message; 1>\\tupon receiving a choice value set to setup: 2>\\tapply the corresponding received configuration and start using the associated resources, unless explicitly specified otherwise; 1>\\tupon receiving a choice value set to release: 2>\\tclear the corresponding configuration and stop using the associated resources; 1>\\tin case the size of a list is extended, upon receiving an extension field comprising the entries in addition to the ones carried by the original field (regardless of whether the network signals more entries in total); apply the following generic behaviour unless explicitly stated otherwise: 2>\\tcreate a combined list by concatenating the additional entries included in the extension field to the original field while maintaining the order among both the original and the additional entries; 2>\\tfor the combined list, created according to the previous, apply the same behaviour as defined for the original field.\",\n",
       "  'question': 'What does the UE process received messages in order of?\\n',\n",
       "  'answer': 'The UE processes received messages in the order they are received by RRC, completing the processing of one message before starting the next.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 40}},\n",
       " {'context': 'Exercise 6.27. Consider the two user symmetric downlink fading channel with re-\\nceiver CSI alone (c.f. (6.50)). We have seen that the capacity region of the down-\\nlink channel does not depend on the correlation between the additive noise processes\\nz1[m] and z2[m] (c.f. Exercise 6.24(1)).\\nConsider the following specific correlation:\\n(z1[m], z2[m]) are CN (0, K[m]) and independent in time m. To preserve the marginal\\nvariance, the diagonal entries of the covariance matrix K[m] must be N0 each. Let us\\ndenote the o-diagonal term by [m]N0 (with |[m]| 1). Suppose now we let the two\\nusers cooperate.',\n",
       "  'question': 'What must the diagonal entries of the covariance matrix K[m] be to preserve the marginal variance?\\n',\n",
       "  'answer': 'The diagonal entries of the covariance matrix K[m] must be N0 each to preserve the marginal variance.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 339,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': \". For intra-NR handover nrPreviousCell is included and for the handover from EUTRA to NR eutraPreviousCell is included. pSCellId This field is used to indicate the PSCell in which the UE failed to perform fast MCG recovery procedure or the UE successfully performed fast MCG recovery procedure. ra-InformationCommon This field is optionally included when connectionFailureType is set to 'hof' or when connectionFailureType is set to 'rlf' and the rlf-Cause equals to 'randomAccessProblem' or 'beamRecoveryFailure'; otherwise this field is absent. reconnectCellId This field is used to indicate the cell in which the UE comes back to connected after connection failure and after failing to perform reestablishment, or to indicate the suitable cell in which the UE reconnects after failure in performing MobilityFromNRCommand for voice fallback (without initiating re-establishment procedure). If the UE comes back to RRC CONNECTED in an NR cell then nrReconnectCellID is included and if the UE comes back to RRC CONNECTED in an LTE cell then eutraReconnectCellID is included. reestablishmentCellId If the UE was not configured with conditionalReconfiguration at the time of re-establishment attempt, or if the cell selected for the re-establishment attempt is not a candidate target cell for conditional reconfiguration, this field is used to indicate the cell in which the re-establishment attempt was made after connection failure. rlf-Cause This field is used to indicate the cause of the last radio link failure that was detected. In case of handover failure information reporting (i.e., the connectionFailureType is set to 'hof'), the UE is allowed to set this field to any value, except for the case in which a radio link failure was detected in the source PCell while performing a DAPS handover. scg-FailedAfterMCG This field is set if for the SCG failure is detected after MCG failure while T316 is running\",\n",
       "  'question': 'What is the purpose of the pSCellId field?\\n',\n",
       "  'answer': 'The pSCellId field indicates the PSCell where the UE failed to perform fast MCG recovery or successfully completed it.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1103}},\n",
       " {'context': \". The PMI value for the selected CSI-RS resources corresponds to the codebook indices of and where The precoding matrices indicated by the PMI are determined from vectors, where are the indices of the selected CSI-RS resources in increasing order, such that , and , , where are the corresponding values from the selected combination of . ports are selected from the ports of the -th selected CSI-RS resource, for , based on vectors, , , which are indicated by , where Let the index , for the -th selected CSI-RS resource, , is obtained from the elements of , as described in Clause 5.2.2.2.7 for the indicator , obtained from the elements of . Vector is a -element column vector containing a value of 1 in the element of index and zeros elsewhere, and where the first element is the element of index 0. -\\tIf for the -th selected CSI-RS resource, , for , and is not reported. The vectors, , , are common for all the selected CSI-RS resources and are identified by , where with the indices assigned such that increases with . is indicated by the index , when and , where -\\tIf , or and , is not reported. -\\tIf and , the nonzero offset between and is reported with assuming that (reference for the offset) is 0. The nonzero offset values are mapped to the index values of in increasing order with offset value 1 mapped to index value '0'. The vectors' elements are given by for , and . If the higher layer parameter codebookMode is set to 'mode1', an offset is reported for the -th selected CSI-RS resource, with , relative to the first of the selected CSI-RS resources. The reported offsets are common for all layers and are indicated by , given by where the value of is configured by higher layer parameter numberOfO3. The offsets are represented by If codebookMode is set to 'mode2', the offset indicator, , is not reported and for\",\n",
       "  'question': 'What is the meaning of the PMI value in the context of CSI-RS resources?\\n',\n",
       "  'answer': 'The PMI value corresponds to the codebook indices of CSI-RS resources and determines the precoding matrices from vectors.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38214-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 286}},\n",
       " {'context': '.. maxNrofServingCells-1)) OF ARFCN-ValueNR OPTIONAL, nonCriticalExtension CG-ConfigInfo-v1610-IEs OPTIONAL } CG-ConfigInfo-v1610-IEs ::= SEQUENCE { drx-InfoMCG2 DRX-Info2 OPTIONAL, alignedDRX-Indication ENUMERATED {true} OPTIONAL, scgFailureInfo-r16 SEQUENCE { failureType-r16 ENUMERATED { scg-lbtFailure-r16, beamFailureRecoveryFailure-r16, t312-Expiry-r16, bh-RLF-r16, beamFailure-r17, spare3, spare2, spare1}, measResultSCG-r16 OCTET STRING (CONTAINING MeasResultSCG-Failure) } OPTIONAL, dummy1 SEQUENCE { failureTypeEUTRA-r16 ENUMERATED { scg-lbtFailure-r16, beamFailureRecoveryFailure-r16, t312-Expiry-r16, spare5, spare4, spare3, spare2, spare1}, measResultSCG-EUTRA-r16 OCTET STRING } OPTIONAL, sidelinkUEInformationNR-r16 OCTET STRING (CONTAINING SidelinkUEInformationNR-r16) OPTIONAL, sidelinkUEInformationEUTRA-r16 OCTET STRING OPTIONAL, nonCriticalExtension CG-ConfigInfo-v1620-IEs OPTIONAL } CG-ConfigInfo-v1620-IEs ::= SEQUENCE { ueAssistanceInformationSourceSCG-r16 OCTET STRING (CONTAINING UEAssistanceInformation) OPTIONAL, nonCriticalExtension CG-ConfigInfo-v1640-IEs OPTIONAL } CG-ConfigInfo-v1640-IEs ::= SEQUENCE { servCellInfoListMCG-NR-r16 ServCellInfoListMCG-NR-r16 OPTIONAL, servCellInfoListMCG-EUTRA-r16 ServCellInfoListMCG-EUTRA-r16 OPTIONAL, nonCriticalExtension CG-ConfigInfo-v1700-IEs OPTIONAL } CG-ConfigInfo-v1700-IEs ::= SEQUENCE { candidateCellListCPC-r17 CandidateCellListCPC-r17 OPTIONAL, twoPHRModeMCG-r17 ENUMERATED {enabled} OPTIONAL, lowMobilityEvaluationConnectedInPCell-r17 ENUMERATED {enabled} OPTIONAL, nonCriticalExtension CG-ConfigInfo-v1730-IEs OPTIONAL } CG-ConfigInfo-v1730-IEs ::= SEQUENCE { fr1-Carriers-MCG-r17 INTEGER (1..32) OPTIONAL, fr2-Carriers-MCG-r17 INTEGER (1.',\n",
       "  'question': 'What are the possible failure types in scgFailureInfo-r16?\\n',\n",
       "  'answer': 'The possible failure types in scgFailureInfo-r16 include scg-lbtFailure-r16, beamFailureRecoveryFailure-r16, t312-Expiry-r16, bh-RLF-r16, beamFailure-r17, spare3, spare2, and spare1.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 2409}},\n",
       " {'context': '. For SRS for positioning configuration in multiple cells, the value of this field applies to all cells in the validity area. nrofSRS-Ports Number of ports. For CLI SRS-RSRP measurement, the network always configures this parameter to \\'port1\\'. nrofSRS-Ports-n8 Number of ports if the number of antenna ports is 8. The value \\'ports8\\' configures UE with 8 antenna ports and the value \\'ports8tdm\\' configures the UE with 8 antenna ports which are partitioned into 2 subsets with each subset having 4 different ports, and the subsets are mapped to different OFDM symbols, see TS 38.211 [16], clause 6.4.1.4.2. If combOffsetHopping-r18 or cyclicShiftHopping-r18 is configured, this field is not set to ports8tdm. If this field is present UE ignores the field nrofSRS-Ports. periodicityAndOffset-p, periodicityAndOffset-p-Ext Periodicity and slot offset for this SRS resource. All values are in \"number of slots\". Value sl1 corresponds to a periodicity of 1 slot, value sl2 corresponds to a periodicity of 2 slots, and so on. For each periodicity the corresponding offset is given in number of slots. For periodicity sl1 the offset is 0 slots (see TS 38.214 [19], clause 6.2.1). For CLI SRS-RSRP measurement, sl1280 and sl2560 cannot be configured. For SRS-PosResource, values sl20480, sl40960 and sl81920 cannot be configured for SCS=15kHz, values sl40960 and sl81920 cannot be configured for SCS=30kHz, and value sl81920 cannot be configured for SCS=60kHz except when periodicity of 20480ms is configured. When periodicityAndOffset-p-Ext is present, periodicityAndOffset-p shall be ignored by the UE. periodicityAndOffset-sp, periodicityAndOffset-sp-Ext Periodicity and slot offset for this SRS resource. All values are in \"number of slots\". Value sl1 corresponds to a periodicity of 1 slot, value sl2 corresponds to a periodicity of 2 slots, and so on. For each periodicity the corresponding offset is given in number of slots. For periodicity sl1 the offset is 0 slots (see TS 38.214 [19], clause 6.2.1)',\n",
       "  'question': 'What is the value of periodicityAndOffset-p when it corresponds to a periodicity of 1 slot?\\n',\n",
       "  'answer': 'The value of periodicityAndOffset-p is sl1, which corresponds to a periodicity of 1 slot with an offset of 0 slots.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1789}},\n",
       " {'context': '..maxSimultaneousBands)) OF SRS-SwitchingTimeNR }, eutra SEQUENCE { srs-SwitchingTimesListEUTRA SEQUENCE (SIZE (1..maxSimultaneousBands)) OF SRS-SwitchingTimeEUTRA } } OPTIONAL, srs-TxSwitch SEQUENCE { supportedSRS-TxPortSwitch ENUMERATED {t1r2, t1r4, t2r4, t1r4-t2r4, t1r1, t2r2, t4r4, notSupported}, txSwitchImpactToRx INTEGER (1..32) OPTIONAL, txSwitchWithAnotherBand INTEGER (1..32) OPTIONAL } OPTIONAL } BandParameters-v1610 ::= SEQUENCE { srs-TxSwitch-v1610 SEQUENCE { supportedSRS-TxPortSwitch-v1610 ENUMERATED {t1r1-t1r2, t1r1-t1r2-t1r4, t1r1-t1r2-t2r2-t2r4, t1r1-t1r2-t2r2-t1r4-t2r4, t1r1-t2r2, t1r1-t2r2-t4r4} } OPTIONAL } BandParameters-v1710 ::= SEQUENCE { -- R1 23-8-3\\tSRS Antenna switching for >4Rx srs-AntennaSwitchingBeyond4RX-r17 SEQUENCE { -- 1. Support of SRS antenna switching xTyR with y>4 supportedSRS-TxPortSwitchBeyond4Rx-r17 BIT STRING (SIZE (11)), -- 2. Report the entry number of the first-listed band with UL in the band combination that affects this DL entryNumberAffectBeyond4Rx-r17 INTEGER (1..32) OPTIONAL, -- 3. Report the entry number of the first-listed band with UL in the band combination that switches together with this UL entryNumberSwitchBeyond4Rx-r17 INTEGER (1..32) OPTIONAL } OPTIONAL } BandParameters-v1730 ::= SEQUENCE { -- R1 39-3-2\\tAffected bands for inter-band CA during SRS carrier switching srs-SwitchingAffectedBandsListNR-r17 SEQUENCE (SIZE (1.',\n",
       "  'question': 'What are the supported SRS Tx port switch options in BandParameters-v1610?\\n',\n",
       "  'answer': 'The supported SRS Tx port switch options in BandParameters-v1610 include t1r1-t1r2, t1r1-t1r2-t1r4, t1r1-t1r2-t2r2-t2r4, t1r1-t1r2-t2r2-t1r4-t2r4, t1r1-t2r2, and t1r1-t2r2-t4r4.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1851}},\n",
       " {'context': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses Quantization and its Statistical Characterization\\nTypically, an analog message signal (e.g., voice) has a continuous range of amplitudes\\nand, therefore, its samples have a continuous amplitude range. In other words, within the\\nfinite amplitude range of the signal, we find an infinite number of amplitude levels. In\\nactual fact, however, it is not necessary to transmit the exact amplitudes of the samples for\\nthe following reason: any human sense (the ear or the eye) as ultimate receiver can detect\\nonly finite intensity differences. This means that the message signal may be approximated\\nby a signal constructed of discrete amplitudes selected on a minimum error basis from an\\navailable set. The existence of a finite number of discrete amplitude levels is a basic\\ncondition of waveform coding exemplified by PCM. Clearly, if we assign the discrete\\namplitude levels with sufficiently close spacing, then we may make the approximated\\nsignal practically indistinguishable from the original message signal. For a formal\\ndefinition of amplitude quantization, or just quantization for short, we say:\\nQuantization is the process of transforming the sample amplitude m(nTs) of a\\nmessage signal m(t) at time t = nTs into a discrete amplitude v(nTs) taken from a\\nfinite set of possible amplitudes.\\nThis definition assumes that the quantizer (i.e., the device performing the quantization\\nprocess) is memoryless and instantaneous, which means that the transformation at time\\nt = nTs is not affected by earlier or later samples of the message signal m(t). This simple\\nform of scalar quantization, though not optimum, is commonly used in practice.\\nWhen dealing with a memoryless quantizer, we may simplify the notation by dropping\\nthe time index. Henceforth, the symbol mk is used in place of m(kTs), as indicated in the\\nblock diagram of a quantizer shown in Figure 6.8a. Then, as shown in Figure 6.8b, the',\n",
       "  'question': 'What is the process of transforming sample amplitude into a discrete amplitude?\\n',\n",
       "  'answer': 'Quantization is the process of transforming the sample amplitude m(nTs) of a message signal m(t) at time t = nTs into a discrete amplitude v(nTs) taken from a finite set of possible amplitudes.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 298,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': 'As a normalization, let us suppose that E[|hij|2] = 1. As in our study earlier, we con-\\nsider coherent communication: the receiver tracks the channel fading process exactly.\\nWe first start with the situation when the transmitter has only a statistical character-\\nization of the fading channel. Finally, we look at the case when the transmitter also\\nexactly tracks the fading channel (full CSI); this situation is very similar to that of the\\ntime-invariant MIMO channel. Capacity with CSI at Receiver\\nConsider using the V-BLAST architecture (Figure 8.1) with a channel-independent\\nmultiplexing coordinate system Q and power allocations P1, . . . , Pnt. The covariance',\n",
       "  'question': 'What does E[|hij|2] represent in the context of channel fading?\\n',\n",
       "  'answer': 'It represents the expected value of the squared magnitude of the fading channel coefficient hij, which is normalized to 1 for normalization purposes.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 397,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': '..8), sl-CPE-StartingPositions-r18 SEQUENCE (SIZE (1..9)) OF INTEGER (1..9) } SL-ZoneConfigMCR-r16 ::= SEQUENCE { sl-ZoneConfigMCR-Index-r16 INTEGER (0..15), sl-TransRange-r16 ENUMERATED {m20, m50, m80, m100, m120, m150, m180, m200, m220, m250, m270, m300, m350, m370, m400, m420, m450, m480, m500, m550, m600, m700, m1000, spare9, spare8, spare7, spare6, spare5, spare4, spare3, spare2, spare1} OPTIONAL, -- Need M sl-ZoneConfig-r16 SL-ZoneConfig-r16 OPTIONAL, -- Need M ... } SL-SyncAllowed-r16 ::= SEQUENCE { gnss-Sync-r16 ENUMERATED {true} OPTIONAL, -- Need R gnbEnb-Sync-r16 ENUMERATED {true} OPTIONAL, -- Need R ue-Sync-r16 ENUMERATED {true} OPTIONAL -- Need R } SL-PSCCH-Config-r16 ::= SEQUENCE { sl-TimeResourcePSCCH-r16 ENUMERATED {n2, n3} OPTIONAL, -- Need M sl-FreqResourcePSCCH-r16 ENUMERATED {n10,n12, n15, n20, n25} OPTIONAL, -- Need M sl-DMRS-ScrambleID-r16 INTEGER (0..65535) OPTIONAL, -- Need M sl-NumReservedBits-r16 INTEGER (2..4) OPTIONAL, -- Need M ... } SL-PSSCH-Config-r16 ::= SEQUENCE { sl-PSSCH-DMRS-TimePatternList-r16 SEQUENCE (SIZE (1..3)) OF INTEGER (2..4) OPTIONAL, -- Need M sl-BetaOffsets2ndSCI-r16 SEQUENCE (SIZE (4)) OF SL-BetaOffsets-r16 OPTIONAL, -- Need M sl-Scaling-r16 ENUMERATED {f0p5, f0p65, f0p8, f1} OPTIONAL, -- Need M ... } SL-PSFCH-Config-r16 ::= SEQUENCE { sl-PSFCH-Period-r16 ENUMERATED {sl0, sl1, sl2, sl4} OPTIONAL, -- Need M sl-PSFCH-RB-Set-r16 BIT STRING (SIZE (10..275)) OPTIONAL, -- Need M sl-NumMuxCS-Pair-r16 ENUMERATED {n1, n2, n3, n6} OPTIONAL, -- Need M sl-MinTimeGapPSFCH-r16 ENUMERATED {sl2, sl3} OPTIONAL, -- Need M sl-PSFCH-HopID-r16 INTEGER (0..1023) OPTIONAL, -- Need M sl-PSFCH-CandidateResourceType-r16 ENUMERATED {startSubCH, allocSubCH} OPTIONAL, -- Need M ... } SL-PTRS-Config-r16 ::= SEQUENCE { sl-PTRS-FreqDensity-r16 SEQUENCE (SIZE (2)) OF INTEGER (1..276) OPTIONAL, -- Need M sl-PTRS-TimeDensity-r16 SEQUENCE (SIZE (3)) OF INTEGER (0.',\n",
       "  'question': 'What is the maximum value for sl-ZoneConfigMCR-Index-r16?\\n',\n",
       "  'answer': 'The maximum value for sl-ZoneConfigMCR-Index-r16 is 15.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 2235}},\n",
       " {'context': '156 Chapter 4 Stochastic Processes Specifically, we have\\n(4.19) and (4.20) where t and u denote two values of the global time at which the processes are observed.\\nAll four correlation parameters of the two stochastic processes X(t) and Y(t) may now be\\ndisplayed conveniently in the form of the two-by-two matrix\\nwhich is called the cross-correlation matrix of the stochastic processes X(t) and Y(t). If the\\nstochastic processes X(t) and Y(t) are each weakly stationary and, in addition, they are\\njointly stationary, then the correlation matrix can be expressed by\\n(4.21)\\nwhere the time shift = u - t.\\nIn general, the cross-correlation function is not an even function of the time-shift  as\\nwas true for the autocorrelation function, nor does it have a maximum at the origin.\\nHowever, it does obey a certain symmetry relationship, described by\\n(4.22)\\nEXAMPLE\\nQuadrature-Modulated Processes\\nConsider a pair of quadrature-modulated processes X1(t) and X2(t) that are respectively\\nrelated to a weakly stationary process X(t) as follows:\\nwhere fc is a carrier frequency and the random variable  is uniformly distributed over the\\ninterval [0, 2]. Moreover,  is independent of X(t). One cross-correlation function of\\nX1(t) and X2(t) is given by\\n(4.23) MXY t u    \\x03 X tY u    = MYX t u    \\x03 Y tX u    = Mtu    MXX t u    MXY t u    MYX t u    MYY t u    = R  RXX   RXY  RYX   RYY  = RXY  RYX  -   = X1 t X t 2fct  +   cos = X2 t X t 2fct  +   sin = R12  \\x03 X1 tX2 t  -     = \\x03 X tX t  -   2fct  +   2fct 2fc  + -   sin cos   = \\x03 X tX t  -  ]\\x03[ 2fct  +   2fct 2fc  + -   sin cos   = 1 2---RXX  \\x03 4fc 2fct - 2 +   2fc   sin - sin   = 1 2---RXX  2fc   sin - =',\n",
       "  'question': 'What is the relationship between the cross-correlation functions MYX(t, u) and MXY(t, u)?\\n',\n",
       "  'answer': 'The cross-correlation functions MYX(t, u) and MXY(t, u) are equal, as described by the symmetry relationship (4.22).',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 176,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '. } BandSidelinkEUTRA-r16 ::= SEQUENCE { freqBandSidelinkEUTRA-r16 FreqBandIndicatorEUTRA, -- R1 15-7: Transmitting LTE sidelink mode 3 scheduled by NR Uu gnb-ScheduledMode3SidelinkEUTRA-r16 SEQUENCE { gnb-ScheduledMode3DelaySidelinkEUTRA-r16 ENUMERATED {ms0, ms0dot25, ms0dot5, ms0dot625, ms0dot75, ms1, ms1dot25, ms1dot5, ms1dot75, ms2, ms2dot5, ms3, ms4, ms5, ms6, ms8, ms10, ms20} } OPTIONAL, -- R1 15-9: Transmitting LTE sidelink mode 4 configured by NR Uu gnb-ScheduledMode4SidelinkEUTRA-r16 ENUMERATED {supported} OPTIONAL } BandSidelink-r16 ::= SEQUENCE { freqBandSidelink-r16 FreqBandIndicatorNR, --15-1 sl-Reception-r16 SEQUENCE { harq-RxProcessSidelink-r16 ENUMERATED {n16, n24, n32, n48, n64}, pscch-RxSidelink-r16 ENUMERATED {value1, value2}, scs-CP-PatternRxSidelink-r16 CHOICE { fr1-r16 SEQUENCE { scs-15kHz-r16 BIT STRING (SIZE (16)) OPTIONAL, scs-30kHz-r16 BIT STRING (SIZE (16)) OPTIONAL, scs-60kHz-r16 BIT STRING (SIZE (16)) OPTIONAL }, fr2-r16 SEQUENCE { scs-60kHz-r16 BIT STRING (SIZE (16)) OPTIONAL, scs-120kHz-r16 BIT STRING (SIZE (16)) OPTIONAL } } OPTIONAL, extendedCP-RxSidelink-r16 ENUMERATED {supported} OPTIONAL } OPTIONAL, --15-2 sl-TransmissionMode1-r16 SEQUENCE { harq-TxProcessModeOneSidelink-r16 ENUMERATED {n8, n16}, scs-CP-PatternTxSidelinkModeOne-r16 CHOICE { fr1-r16 SEQUENCE { scs-15kHz-r16 BIT STRING (SIZE (16)) OPTIONAL, scs-30kHz-r16 BIT STRING (SIZE (16)) OPTIONAL, scs-60kHz-r16 BIT STRING (SIZE (16)) OPTIONAL }, fr2-r16 SEQUENCE { scs-60kHz-r16 BIT STRING (SIZE (16)) OPTIONAL, scs-120kHz-r16 BIT STRING (SIZE (16)) OPTIONAL } }, extendedCP-TxSidelink-r16 ENUMERATED {supported} OPTIONAL, harq-ReportOnPUCCH-r16 ENUMERATED {supported} OPTIONAL } OPTIONAL, --15-4 sync-Sidelink-r16 SEQUENCE { gNB-Sync-r16 ENUMERATED {supported} OPTIONAL, gNB-GNSS-UE-SyncWithPriorityOnGNB-ENB-r16 ENUMERATED {supported} OPTIONAL, gNB-GNSS-UE-SyncWithPriorityOnGNSS-r16 ENUMERATED {supported} OPTIONAL } OPTIONAL, --15-10 sl-Tx-256QAM-r16 ENUMERATED {supported} OPTIONAL,',\n",
       "  'question': 'What are the possible values for the gnb-ScheduledMode3DelaySidelinkEUTRA-r16?\\n',\n",
       "  'answer': 'The possible values for the gnb-ScheduledMode3DelaySidelinkEUTRA-r16 are ms0, ms0.25, ms0.5, ms0.625, ms0.75, ms1, ms1.25, ms1.5, ms1.75, ms2, ms2.5, ms3, ms4, ms5, ms6, ms8, ms10, and ms20.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 2096}},\n",
       " {'context': \"3\\tL1-RSRP Reporting For L1-RSRP computation -\\tthe UE may be configured with CSI-RS resources, SS/PBCH Block resources or both CSI-RS and SS/PBCH block resources, when resource-wise quasi co-located with 'type C' and 'typeD' when applicable. -\\tthe UE may be configured with CSI-RS resource setting up to 16 CSI-RS resource sets having up to 64 resources within each set. The total number of different CSI-RS resources over all resource sets is no more than 128. For L1-RSRP reporting, if the higher layer parameter nrofReportedRS in CSI-ReportConfig is configured to be one, or if the higher layer parameters nrOfReportedCells and nrOfReportedRS-PerCell are both configured to be one, the reported L1-RSRP value is defined by a 7-bit value in the range [-140, -44] dBm with 1dB step size, if the higher layer parameter nrofReportedRS is configured to be larger than one, or if the higher layer parameter groupBasedBeamReporting is configured as 'enabled', or if the higher layer parameter groupBasedBeamReporting-r17 is configured, or if the higher layer parameter groupBasedBeamReporting-r18 is configured, or if any of the higher layer parameters nrOfReportedCells and nrOfReportedRS-PerCell is configured to be larger than one, the UE shall use differential L1-RSRP based reporting, where the largest measured value of L1-RSRP is quantized to a 7-bit value in the range [-140, -44] dBm with 1dB step size, and the differential L1-RSRP is quantized to a 4-bit value. The differential L1-RSRP value is computed with 2 dB step size with a reference to the largest measured L1-RSRP value which is part of the same L1-RSRP reporting instance. The mapping between the reported L1-RSRP value and the measured quantity is described in [11, TS 38.133]\",\n",
       "  'question': 'What is the range of L1-RSRP value in dBm?\\n',\n",
       "  'answer': 'The L1-RSRP value is in the range of [-140, -44] dBm with a 1dB step size.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38214-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 212}},\n",
       " {'context': \". If a UE -\\tis configured with two UL carriers for a serving cell, and -\\tdetermines a Type 1 power headroom report and a Type 3 power headroom report for the serving cell the UE -\\tprovides the Type 1 power headroom report if both the Type 1 and Type 3 power headroom reports are based on respective actual transmissions or on respective reference transmissions -\\tprovides the power headroom report that is based on a respective actual transmission if either the Type 1 report or the Type 3 report is based on a respective reference transmission If a UE is configured with a SCG and if phr-ModeOtherCG for a CG indicates 'virtual' then, for power headroom reports transmitted on the CG, the UE computes PH assuming that the UE does not transmit PUSCH/PUCCH on any serving cell of the other CG. For NR-DC when both the MCG and the SCG operate either in FR1 or in FR2 and for a power headroom report transmitted on the MCG or the SCG, the UE computes PH assuming that the UE does not transmit PUSCH/PUCCH on any serving cell of the SCG or the MCG, respectively. If the UE is configured with a SCG, -\\tFor computing power headroom for cells belonging to MCG, the term 'serving cell' in this clause refers to serving cell belonging to the MCG. -\\tFor computing power headroom for cells belonging to SCG, the term 'serving cell' in this clause refers to serving cell belonging to the SCG. The term 'primary cell' in this clause refers to the PSCell of the SCG. If the UE is configured with a PUCCH-SCell, -\\tFor computing power headroom for cells belonging to primary PUCCH group, the term 'serving cell' in this clause refers to serving cell belonging to the primary PUCCH group. -\\tFor computing power headroom for cells belonging to secondary PUCCH group, the term 'serving cell' in this clause refers to serving cell belonging to the secondary PUCCH group. The term 'primary cell' in this clause refers to the PUCCH-SCell of the secondary PUCCH group\",\n",
       "  'question': 'What does a UE provide if both the Type 1 and Type 3 power headroom reports are based on actual transmissions?\\n',\n",
       "  'answer': 'The UE provides the Type 1 power headroom report in this scenario.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38213-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 115}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications where w is CN(0, N0). There is an average power constraint of P per (complex) symbol.\\nOne way to derive the capacity of this channel is to think of each use of the complex\\nchannel as two uses of a real AWGN channel, with SNR = (P/2)/(N0/2) = P/N0.\\nHence, the capacity of the channel is\\n1 2 log  1 + P N0  bits per real dimension,\\n(B.45) or log  1 + P N0  bits per complex dimension.\\n(B.46)\\nAlternatively we may just as well work directly with the complex channel and the\\nassociated complex random variables. This will be useful when we deal with other\\nmore complicated wireless channel models later on. To this end, one can think of the\\ndierential entropy of a complex random variable x as that of a real random vector\\n((x), (x)). Hence, if w is CN(0, N0), h(w) = h((w)) + h((w)) = log (eN0). The\\nmutual information I(x; y) of the complex AWGN channel y = x + w is then\\nI(x; y) = h(y) log(eN0).\\n(B.47)\\nWith a power constraint E[|x|2] P on the complex input x, y is constrained to\\nsatisfy E[|y|2] P + N0. Here, we use an important fact: among all complex random\\nvariables, the circular symmetric Gaussian random variable maximizes the dierential\\nentropy for a given second moment constraint. (See Exercise B.7.) Hence, the capacity\\nof the complex Gaussian channel is\\nC = log (e (P + N0)) log(eN0) = log\\n 1 + P N0  , (B.48) which is the same as eqn. (5.11).\\nB.5\\nSphere Packing Interpretation\\nIn this section we consider a more precise version of the heuristic sphere-packing argu-\\nment in Section 5.1 for the capacity of the real AWGN channel. Furthermore, we will\\noutline how the capacity as predicted by the sphere packing argument can be achieved.\\nThe material here will be particularly useful when we discuss precoding in Chapter 10.\\nB.5.1\\nUpper Bound\\nConsider transmissions over a block of N symbols, where N is large. Suppose we use\\na code C consisting of |C| equally likely codewords {x1, . . . , x|C|}. By the law of large',\n",
       "  'question': 'What is the capacity of the complex Gaussian channel with a given average power constraint?\\n',\n",
       "  'answer': 'The capacity is log(1 + P/N0), where P is the average power constraint per complex symbol and N0 is the noise power spectral density.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 613,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': 'Chapter\\nSignaling over AWGN Channels\\nYet another useful relation involving the vector representations of the energy signals\\nsi(t) and sk(t) is described by\\n(7.13)\\nwhere is the Euclidean distance dik between the points represented by the signal\\nvectors si and sk.\\nTo complete the geometric representation of energy signals, we need to have a\\nrepresentation for the angle ik subtended between two signal vectors si and sk. By\\ndefinition, the cosine of the angle ik is equal to the inner product of these two vectors\\ndivided by the product of their individual norms, as shown by\\n(7.14)\\nThe two vectors si and sk are thus orthogonal or perpendicular to each other if their inner\\nproduct is zero, in which case ik = 90; this condition is intuitively satisfying.\\nEXAMPLE\\nThe Schwarz Inequality\\nConsider any pair of energy signals s1(t) and s2(t). The Schwarz inequality states\\n(7.15)\\nThe equality holds if, and only if, s2(t) = cs1(t), where c is any constant.\\nTo prove this important inequality, let s1(t) and s2(t) be expressed in terms of the pair of\\northonormal basis functions 1(t) and 2(t) as follows:\\nwhere 1(t) and 2(t) satisfy the orthonormality conditions over the time interval\\n:\\nOn this basis, we may represent the signals s1(t) and s2(t) by the following respective pair\\nof vectors, as illustrated in Figure 7.4:\\nsi sk - 2 sij skj -  2 j 1 = N  = si t sk t -  2 dt 0 T  = si sk - ik   cos si Tsk si sk ------------------ = si Tsk s1 ts2 t dt  -      2 s1 2 t dt  -       s2 2 t dt  -        s1 t s111 t s122 t + = s2 t s211 t s222 t + = , -   i tj t dt  -   ij 1 for j i = 0 otherwise    = = s1 s11 s12 = s2 s21 s22 =',\n",
       "  'question': 'What does the Schwarz Inequality state about the inner product of two energy signals s1(t) and s2(t)?\\n',\n",
       "  'answer': 'The Schwarz Inequality states that the inner product of two energy signals s1(t) and s2(t) is always less than or equal to the product of their individual norms, with equality holding if and only if one signal is a constant multiple of the other.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 348,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications users in general have dierent fading distributions and there is no longer a complete\\nordering of the users. In this case, we say that the downlink channel is non-degraded\\nand little is known about good strategies for communication.\\nAnother interesting\\nsituation when the downlink channel is non-degraded arises when the transmitter has\\nan array of multiple antennas; this is studied in Chapter 10. Full Channel Side Information\\nWe saw in the uplink that the communication scenario becomes more interesting when\\nthe transmitters can track the channel as well.\\nIn this case, the transmitters can\\nvary their powers as a function of the channel. Let us now turn to the analogous\\nsituation in the downlink where the single transmitter tracks all the channels of the\\nusers it is communicating to (the users continue to track their individual channels).\\nAs in the uplink, we can allocate powers to the users as a function of the channel fade\\nlevel. To see the eect, let us continue focusing on sum capacity. We have seen that\\nwithout fading, the sum capacity is achieved by transmitting only to the best user.\\nNow as the channels vary, we can pick the best user at each time and further allocate\\nit an appropriate power subject to a constraint on the average power.\\nUnder this\\nstrategy, the downlink channel reduces to a point-to-point channel with the channel\\ngain distributed as: max k=1...K |hk|2. The optimal power allocation is the, by now familiar, waterfilling solution:\\nP (h) = 1   N0 maxk=1...K |hk|2 + , (6.53) where h = (h1, . . . , hK) is the joint fading state and  > 0 is chosen such that the\\naverage power constraint is met. The optimal strategy is exactly the same as in the\\nsum capacity of the uplink. The sum capacity of the downlink is\\nE  log  1 + P (h) (maxk=1...K |h2\\nk|) N0  . (6.54) 6.5 Frequency-Selective Fading Channels\\nThe extension of the flat fading analysis in the uplink and the downlink to underspread',\n",
       "  'question': 'What is the optimal power allocation strategy for the downlink channel?\\n',\n",
       "  'answer': 'The optimal power allocation strategy for the downlink channel is the waterfilling solution, which involves allocating power based on the channel gain and meeting an average power constraint.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 295,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '.331, Clause 5.2.1] Table 7.3.1.2.1-3: PRACH retransmission indicator Bit field PRACH retransmission indicator Initial transmission of PRACH Retransmission of PRACH Table 7.3.1.2.1-4: Number of repetitions as a function of 2 bits of Downlink assignment index field Bit field First repetition factor configured by numberOfMsg4HARQ-ACK-Repetitions Second repetition factor configured by numberOfMsg4HARQ-ACK-Repetitions Third repetition factor configured by numberOfMsg4HARQ-ACK-Repetitions if provided, otherwise reserved Fourth repetition factor configured by numberOfMsg4HARQ-ACK-Repetitions if provided, otherwise reserved DCI format 1_1 is used for the scheduling of one or multiple PDSCH in one cell. The following information is transmitted by means of the DCI format 1_1 with CRC scrambled by C-RNTI or CS-RNTI or MCS-C-RNTI: -\\tIdentifier for DCI formats - 1 bits -\\tThe value of this bit field is always set to 1, indicating a DL DCI format -\\tCarrier indicator - 0 or 3 bits as defined in Clause 10.1 of [5, TS 38.213]. This field is reserved when this format is carried by PDCCH on the primary cell and the UE is configured for scheduling on the primary cell from an SCell, with the same number of bits as that in this format carried by PDCCH on the SCell for scheduling on the primary cell. -\\tBandwidth part indicator - 0, 1 or 2 bits as determined by the number of DL BWPs configured by higher layers, excluding the initial DL bandwidth part. The bitwidth for this field is determined as bits, where - if , in which case the bandwidth part indicator is equivalent to the ascending order of the higher layer parameter BWP-Id; -\\totherwise , in which case the bandwidth part indicator is defined in Table 7.3.1.1.2-1; If a UE does not support active BWP change via DCI, the UE ignores this bit field',\n",
       "  'question': 'What is the bit field that always indicates a DL DCI format?\\n',\n",
       "  'answer': 'The bit field that always indicates a DL DCI format is the Identifier for DCI formats, which is always set to 1.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38212-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 310}},\n",
       " {'context': '.3.5.16; 1>\\tperform the sidelink dedicated configuration procedure in accordance with the received sl-ConfigDedicatedNR as specified in 5.3.5.14; 1>\\tif the UE has radio link failure or handover failure information available in VarRLF-Report and if the RPLMN is included in plmn-IdentityList stored in VarRLF-Report; or 1>\\tif the UE has radio link failure or handover failure information available in VarRLF-Report and if the current registered SNPN identity is included in snpn-IdentityList stored in VarRLF-Report: 2>\\tif reconnectCellId in VarRLF-Report is not set after failing to perform reestablishment and if this is the first RRCSetup received by the UE after declaring the failure: 3>\\tif the UE supports RLF-Report for conditional handover and if choCellId in VarRLF-Report is set: 4>\\tset timeUntilReconnection in VarRLF-Report to the time that elapsed since the radio link failure or handover failure experienced in the failedPCellId stored in VarRLF-Report; 3>\\telse: 4>\\tset timeUntilReconnection in VarRLF-Report to the time that elapsed since the last radio link failure or handover failure; 3>\\tset nrReconnectCellId in reconnectCellId in VarRLF-Report to the global cell identity and the tracking area code of the PCell; 1>\\tif the UE supports RLF report for inter-RAT MRO NR as defined in TS 36.306 [62], and if the UE has radio link failure or handover failure information available in VarRLF-Report of TS 36.331 [10] and if the RPLMN is included in plmn-IdentityList stored in VarRLF-Report of TS 36.331 [10]: 2>\\tif reconnectCellId in VarRLF-Report of TS 36.331[10] is not set after failing to perform reestablishment and if this is the first RRCSetup received by the UE after declaring the failure: 3>\\tset timeUntilReconnection in VarRLF-Report of TS 36.331[10] to the time that elapsed since the last radio link failure or handover failure in LTE; 3>\\tset nrReconnectCellId in reconnectCellId in VarRLF-Report of TS 36',\n",
       "  'question': 'What is set in VarRLF-Report when the UE supports RLF-Report for inter-RAT MRO NR and the RPLMN is included in plmn-IdentityList?\\n',\n",
       "  'answer': 'The time until reconnection in VarRLF-Report is set to the time that elapsed since the last radio link failure or handover failure in LTE.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 125}},\n",
       " {'context': '1\\tCRC calculation Denote the input bits to the CRC computation by , and the parity bits by , where is the size of the input sequence and is the number of parity bits. The parity bits are generated by one of the following cyclic generator polynomials: - for a CRC length ; - for a CRC length ; - for a CRC length ; - for a CRC length ; - for a CRC length ; - for a CRC length . The encoding is performed in a systematic form, which means that in GF(2), the polynomial: yields a remainder equal to 0 when divided by the corresponding CRC generator polynomial. The bits after CRC attachment are denoted by , where . The relation between and is: for for .\\n2\\tCode block segmentation and code block CRC attachment\\n1\\tPolar coding The input bit sequence to the code block segmentation is denoted by , where . if Number of code blocks: ; else Number of code blocks: end if ; for to ; end for for to ; end for ; for to for to ; ; end for The sequence is used to calculate the CRC parity bits according to Clause 5.1 with a generator polynomial of length . for to ; end for end for The value of is no larger than 1706.',\n",
       "  'question': 'What are the generator polynomials used for CRC lengths of 5 and 6?\\n',\n",
       "  'answer': 'For a CRC length of 5, the generator polynomial is used, and for a CRC length of 6, the generator polynomial is used.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38212-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 9}},\n",
       " {'context': '. LTM-SSB-Config field descriptions ssb-Periodicity The SSB periodicity in ms. If the field is absent, the UE applies the value ms5. (see TS 38.213 [13], clause 4.1). ssb-PositionsInBurst For operation in licensed spectrum, indicates the time domain positions of the transmitted SS-blocks in a half frame with SS/PBCH blocks as defined in TS 38.213 [13], clause 4.1. The first/leftmost bit corresponds to SS/PBCH block index 0, the second bit corresponds to SS/PBCH block index 1, and so on. Value 0 in the bitmap indicates that the corresponding SS/PBCH block is not transmitted while value 1 indicates that the corresponding SS/PBCH block is transmitted. The network always includes this field if ltm-SSB-Config is configured. ss-PBCH-BlockPower Average EPRE of the resources elements that carry secondary synchronization signals in dBm that the NW used for SSB transmission, see TS 38.213 [13], clause 7. The network always includes this field if ltm-SSB-Config is configured. -\\tLTM-Config The IE LTM-Config is used to provide LTM configurations. LTM-Config information element -- ASN1START -- TAG-LTM-CONFIG-START LTM-Config-r18 ::= SEQUENCE { ltm-ReferenceConfiguration-r18 SetupRelease {ReferenceConfiguration-r18} OPTIONAL, -- Need M ltm-CandidateToReleaseList-r18 SEQUENCE (SIZE (1..maxNrofLTM-Configs-r18)) OF LTM-CandidateId-r18 OPTIONAL, -- Need N ltm-CandidateToAddModList-r18 SEQUENCE (SIZE (1..maxNrofLTM-Configs-r18)) OF LTM-Candidate-r18 OPTIONAL, -- Need N ltm-ServingCellNoResetID-r18 INTEGER (1..maxNrofLTM-Configs-plus1-r18) OPTIONAL, -- Need N ltm-CSI-ResourceConfigToAddModList-r18 SEQUENCE (SIZE (1..maxNrofLTM-CSI-ResourceConfigurations-r18)) OF LTM-CSI-ResourceConfig-r18 OPTIONAL, -- Need N ltm-CSI-ResourceConfigToReleaseList-r18 SEQUENCE (SIZE (1..maxNrofLTM-CSI-ResourceConfigurations-r18)) OF LTM-CSI-ResourceConfigId-r18 OPTIONAL, -- Need N attemptLTM-Switch-r18 ENUMERATED {true} OPTIONAL, -- Cond LTM-MCG ltm-ServingCellUE-MeasuredTA-ID-r18 INTEGER (1.',\n",
       "  'question': 'What is the default SSB periodicity value if the ssb-Periodicity field is absent?\\n',\n",
       "  'answer': 'The UE applies the value of ms5 as the default SSB periodicity when the ssb-Periodicity field is absent.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1376}},\n",
       " {'context': \". When is zero, , PMI and RI reporting are not allowed to correspond to any precoder associated with layers. The parameter typeI-SinglePanel-ri-RestrictionSDM applies to a reported joint RI index when conditioned on a CRI corresponding to an entry of the Resource Pairs and indicates one or more of the four rank combinations that are allowed to correspond to the reported PMIs and RIs. The bitmap parameter typeI-SinglePanel-ri-RestrictionSDM forms the bit sequence where is the LSB and is the MSB. When is zero, , PMI and RI reporting are not allowed to correspond to any precoder associated with the -th rank combination in the following order: {1,1}, {1,2}, {2,1},{2,2}. -\\tThe CodebookConfig in CSI-ReportConfig can be configured with two Codebook Subset Restrictions. The first restriction applies to a reported PMI associated to a CSI-RS resource in Group 1. The second restriction applies to a reported PMI associated to a CSI-RS resource in Group 2. If the UE is configured with a CSI-ReportConfig that contains a list of sub-configurations, provided by csi-ReportSubConfigToAddModList: -\\tThe UE expects to be configured with the higher layer parameter codebookType set to 'typeI-SinglePanel' or 'typeI-MultiPanel'. If the UE indicates a capability for supporting mixed codebook combination in a slot with mixCodeBookSpatialAdaptation, each sub-configuration which is configured with portSubsetIndictor can be configured with the higher layer parameter codebookType set to 'typeI-SinglePanel' or 'typeI-MultiPanel'. -\\tEach sub-configuration can be configured with an antenna port subset using the higher layer bitmap parameter portSubsetIndicator which contains the bit sequence , where is the MSB and is the LSB, bit corresponds to antenna port , and is the number of ports nrofPorts configured for the CSI-RS resources(s) within a NZP-CSI-RS-ResourceSet contained in the CSI-ResourceConfig for channel measurement that corresponds to the CSI-ReportConfig\",\n",
       "  'question': 'What are the rank combinations that are allowed to correspond to the reported PMIs and RIs when the parameter typeI-SinglePanel-ri-RestrictionSDM is applied?\\n',\n",
       "  'answer': 'The allowed rank combinations are {1,1}, {1,2}, {2,1}, and {2,2}.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38214-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 205}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications is called proportional fair scheduling) simply transmits to the user kwith the largest\\nRk[m]\\nTk[m]\\namong all active users in the system. The average throughputs Tk[m] are updated\\nusing an exponentially weighted low-pass filter:\\nTk[m + 1] =    (1 1 tc)Tk[m] + 1 tcRk[m] k = k (1 1 tc)Tk[m] k = k. (6.56) One can get an intuitive feel of how this algorithm works by inspecting Figures 6.14\\nand 6.15. We plot the sample paths of the requested data rates of two users as a\\nfunction of time slots (each time slot is 1.67 ms in IS-856). In Figure 6.14, the two\\nusers have identical fading statistics. If the scheduling time-scale tc is much larger than\\nthe correlation time-scale of the fading dynamics, then by symmetry the throughput of\\neach user Tk[m] converges to the same quantity. The scheduling algorithm reduces to\\nalways picking the user with the highest requested rate. Thus, each user is scheduled\\nwhen its channel is good and at the same time the scheduling algorithm is perfectly\\nfair in the long term.\\nIn Figure 6.15, due to perhaps dierent distances from the base station, one users\\nchannel is much stronger than that of the other user on average, even though both\\nchannels fluctuate due to multipath fading. Always picking the user with the highest\\nrequested rate means giving all the system resources to the statistically stronger user,\\nand would be highly unfair. In contrast, under the scheduling algorithm described\\nabove, users compete for resources not directly based on their requested rates but\\nbased on the rates normalized by their respective average throughputs. The user with\\nthe statistically stronger channel will have a higher average throughput.\\nThus, the algorithm schedules a user when its instantaneous channel quality is high\\nrelative to its own average channel condition over the time-scale tc. In short, data is\\ntransmitted to a user when its channel is near its own peaks. Multiuser diversity benefit',\n",
       "  'question': 'What is the purpose of proportional fair scheduling in wireless communications?\\n',\n",
       "  'answer': 'It schedules users based on the relative quality of their instantaneous channel compared to their own average channel condition over a time-scale tc, ensuring fairness in the long term by prioritizing users whose channels are near their own peaks.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 303,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '2\\tDerivation of NR sidelink measurement results The UE may be configured by the peer UE associated to derive NR sidelink RSRP measurement results per PC5-RRC connection associated to the NR sidelink measurement objects based on parameters configured in the sl-MeasObject and in the sl-ReportConfig. The UE shall: 1>\\tfor each NR sidelink measurement quantity to be derived based on NR sidelink DMRS/SL-PRS: 2>\\tderive the corresponding measurement of NR sidelink frequency indicated quantity based on PSSCH DMRS/SL-PRS as described in TS 38.215 [9] in the concerned sl-MeasObject; 2>\\tapply layer 3 filtering as described in 5.5.3.2;\\n4\\tSidelink measurement report triggering',\n",
       "  'question': 'What are the two measurement quantities that the UE derives based on NR sidelink DMRS/SL-PRS?\\n',\n",
       "  'answer': 'The UE derives the corresponding measurement of NR sidelink frequency indicated quantity based on PSSCH DMRS/SL-PRS and applies layer 3 filtering.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 864}},\n",
       " {'context': '. 2>\\tif sl-lbt-FailureRecoveryConfig is configured: 3>\\tset SL_LBT_COUNTER to 0 for all RB sets in the SL BWP; 3>\\tmonitors SL LBT failure indications from lower layers as specified in clause 5.31.2. 1>\\tif the BWP is deactivated: 2>\\tnot transmit SL-BCH on the BWP, if configured; 2>\\tnot transmit S-PSS and S-SSS on the BWP, if configured; 2>\\tnot transmit PSCCH on the BWP; 2>\\tnot transmit SL-PRS on the BWP; 2>\\tnot transmit SL-SCH on the BWP; 2>\\tnot receive PSFCH on the BWP, if configured; 2>\\tnot receive SL-BCH on the BWP, if configured; 2>\\tnot receive S-PSS and S-SSS on the BWP, if configured; 2>\\tnot receive PSCCH on the BWP; 2>\\tnot receive SL-PRS on the BWP; 2>\\tnot receive SL-SCH on the BWP; 2>\\tnot transmit PSFCH on the BWP, if configured; 2>\\tsuspend any configured sidelink grant of configured grant Type 1; 2>\\tclear any configured sidelink grant of configured grant Type 2; 2>\\tcancel, if any, triggered Scheduling Request procedure for sidelink; 2>\\tcancel, if any, triggered Sidelink Buffer Status Reporting procedure; 2>\\tcancel, if any, triggered Sidelink CSI Reporting procedure; 2>\\tcancel, if any, triggered Sidelink DRX Command MAC CE; 2>\\tcancel, if any, triggered Sidelink IUC-Request transmission procedure; 2>\\tcancel, if any, triggered Sidelink IUC-Information Reporting procedure; 2>\\tcancel, if any, triggered Sidelink consistent LBT failure.',\n",
       "  'question': 'What happens to SL_LBT_COUNTER when sl-lbt-FailureRecoveryConfig is configured?\\n',\n",
       "  'answer': 'It is set to 0 for all RB sets in the SL BWP.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38321-i50.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 230}},\n",
       " {'context': \".3 -\\tif the UE is provided two SRS resource sets in srs-ResourceSetToAddModList or srs-ResourceSetToAddModListDCI-0-2 with usage set to 'codebook' or 'nonCodebook', and is provided p0-PUSCH-Alpha2, for a retransmission of a configured grant Type 1 PUSCH, or for activation or retransmission of a configured grant Type 2 PUSCH, scheduled by a DCI format that includes a SRS resource set indicator field, and for active UL BWP of carrier of serving cell -\\tif the SRS resource set indicator value is 00, is equal to the value of powerControlLoopToUse in ConfiguredGrantConfig -\\tif the SRS resource set indicator value is 01, is equal to the value of powerControlLoopToUse2 in ConfiguredGrantConfig -\\tif the SRS resource set indicator value is 10 or 11, a first and a second respectively associated with the first and second SRS resource set are respectively equal to powerControlLoopToUse and powerControlLoopToUse2 in ConfiguredGrantConfig -\\telse if the UE is provided two SRS resource sets in srs-ResourceSetToAddModList or srs-ResourceSetToAddModListDCI-0-2 with usage set to 'codebook' or 'nonCodebook' and is provided p0-PUSCH-Alpha2, for a transmission of a configured grant Type 1 PUSCH and for active UL BWP of carrier of serving cell -\\ta first is equal to the value of powerControlLoopToUse in ConfiguredGrantConfig that is associated with the first srs-ResourceIndicator in rrc-ConfiguredUplinkGrant -\\ta second is equal to the value of powerControlLoopToUse2 in ConfiguredGrantConfig that is associated with the second srs-ResourceIndicator in rrc-ConfiguredUplinkGrant -\\telse if the UE is provided two SRS resource sets in srs-ResourceSetToAddModList or srs-ResourceSetToAddModListDCI-0-2 with usage set to 'codebook' or 'nonCodebook' and is provided p0-PUSCH-Alpha2, for a retransmission of a configured grant Type 1 PUSCH, or for activation or retransmission of a configured grant Type 2 PUSCH, scheduled by a DCI format 0_0 and for active UL BWP of carrier of serving cell - is equal to\",\n",
       "  'question': 'What is the power control loop value associated with SRS resource set indicator 00?\\n',\n",
       "  'answer': 'It is equal to the value of powerControlLoopToUse in ConfiguredGrantConfig.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38213-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 69}},\n",
       " {'context': '. The bitwidth for PMI of codebookType=typeII-Doppler-PortSelection is provided in Tables 6.3.2.1.2-2D, where the values of ,, , , and are given by Clause 5.2.2.2.11 in [6, TS 38.214]. Table 6.3.2.1.2-2D: PMI of codebookType= typeII-Doppler-PortSelection Information fields Rank=1 if N > M=2, N/A otherwise N/A N/A N/A Rank=2 if N > M=2, N/A otherwise N/A N/A Rank=3 if N > M=2, N/A otherwise N/A Rank=4 if N > M=2, N/A otherwise Information fields Rank=1 N/A N/A N/A N/A if ; otherwise Rank=2 N/A N/A N/A if ; otherwise Rank=3 N/A Rank=4 NOTE:\\tthe bitwidth for , and shown in Table 6.3.2.1.2-2D is the total bitwidth of , and up to Rank = , respectively, and the corresponding per layer bitwidths are , , and 4, (i.e., 1, 3, and 4 bits for each respective indicator elements , , and , respectively), where as defined in Clause 5.2.2.2.11 in [6, TS 38.214] is the number of nonzero coefficients for layer such that . For CSI on PUSCH, two UCI bit sequences are generated, and . The CSI fields of all CSI reports, in the order from upper part to lower part in Table 6.3.2.1.2-6, are mapped to the UCI bit sequence starting with . The CSI fields of all CSI reports, in the order from upper part to lower part in Table 6.3.2.1.2-7, are mapped to the UCI bit sequence starting with . The mapping order of CSI fields of one report for CRI/RSRP or SSBRI/RSRP or CRI/RSRP/CapabilityIndex or SSBRI/RSRP/CapabilityIndex reporting is provided in Table 6.3.1.1.2-8. The mapping order of CSI fields of one report for inter-cell SSBRI/RSRP reporting is provided in Table 6.3.1.1.2-8. The mapping order of CSI fields of one report for CRI/SINR or SSBRI/SINR or CRI/SINR/CapabilityIndex or SSBRI/SINR/CapabilityIndex reporting is provided in Table6.3.1.1.2-8A. The mapping order of CSI fields of one report for group-based CRI/RSRP or SSBRI/RSRP reporting is provided in Table 6.3.1.1.2-8B. The mapping order of CSI fields of one report for TDCP reporting is provided in Table 6.3.2.1.2-3C',\n",
       "  'question': 'What is the total bitwidth for the PMI of codebookType=typeII-Doppler-PortSelection up to Rank 4?\\n',\n",
       "  'answer': 'The total bitwidth for the PMI of codebookType=typeII-Doppler-PortSelection up to Rank 4 is the sum of the bitwidths for each rank, with specific values provided in Table 6.3.2.1.2-2D, as per Clause 5.2.2.2.11 in [6, TS 38.214].',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38212-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 77}},\n",
       " {'context': '. -\\tCBG flushing out information (CBGFI) - 1 bit if higher layer parameter codeBlockGroupFlushIndicator is configured as \"TRUE\", 0 bit otherwise. If higher layer parameter priorityIndicatorDCI-1-1 is configured, if the bit width of the CBG flushing out information in DCI format 1_1 for one HARQ-ACK codebook is not equal to that of the CBG flushing out information in DCI format 1_1 for the other HARQ-ACK codebook, a number of most significant bits with value set to \\'0\\' are inserted to smaller CBG flushing out information until the bit width of the CBG flushing out information in DCI format 1_1 for the two HARQ-ACK codebooks are the same. -\\tDMRS sequence initialization - 1 bit. -\\tPriority indicator - 0 bit if higher layer parameter priorityIndicatorDCI-1-1 is not configured; otherwise 1 bit as defined in Clause 9 in [5, TS 38.213]. -\\tChannelAccess-CPext - 0, 1, 2, 3 or 4 bits. The bitwidth for this field is determined as bits, where I is the number of entries in the higher layer parameter ul-AccessConfigListDCI-1-1 or in Table 7.3.1.1.1-4A if channelAccessMode-r16 = \"semiStatic\" is provided, for operation in a cell with shared spectrum channel access in frequency range 1, or for operation in frequency range 2-2 if ChannelAccessMode2-r17 is provided; otherwise 0 bit. One or more entries from Table 7.3.1.2.2-6 or Table 7.3.1.2.2-6A are configured by the higher layer parameter ul-AccessConfigListDCI-1-1. -\\tMinimum applicable scheduling offset indicator - 0 or 1 bit -\\t0 bit if higher layer parameter minimumSchedulingOffsetK0 is not configured; -\\t1 bit if higher layer parameter minimumSchedulingOffsetK0 is configured. The 1 bit indication is used to determine the minimum applicable K0 for the active DL BWP and the minimum applicable K2 value for the active UL BWP, if configured respectively, according to Table 7.3.1.1.2-33',\n",
       "  'question': 'What determines the bit width of the ChannelAccess-CPext field?\\n',\n",
       "  'answer': 'The bitwidth for this field is determined as bits, where I is the number of entries in the higher layer parameter ul-AccessConfigListDCI-1-1 or in Table 7.3.1.1.1-4A if channelAccessMode-r16 = \"semiStatic\" is provided, for operation in a cell with shared spectrum channel access in frequency range 1, or for operation in frequency range 2-2 if ChannelAccessMode2-r17 is provided; otherwise 0 bit.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38212-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 321}},\n",
       " {'context': 'sl-U2U-InfoList and set its fields (if needed) for each entry as follows to report the related end-to-end and the first hop information for the end-to-end PC5 connection with each target L2 U2U Remote UE: 6>\\tset sl-TargetUE-Identity to the destination identity configured by upper layer for NR sidelink L2 U2U relay communication transmission to the target L2 U2U Remote UE; 6>\\tset sl-E2E-QoS-InfoList to include end-to-end QoS profile(s) of the sidelink QoS flow(s) of the associated destination configured by the upper layer for the NR sidelink L2 U2U relay communication transmission to the target L2 U2U Remote UE; 6>\\tset sl-PerHop-QoS-InfoList to include the first-hop split PDB of the sidelink QoS flow(s) received from the sl-SplitQoS-InfoListPC5 in UEInformationResponseSidelink message for the associated destination in accordance with the received sl-TargetUE-Identity; 6>\\tset sl-CapabilityInformationTargetRemoteUE to include the related UE capability information received from the target L2 U2U Remote UE, if any; 3>\\tif sl-DRX-ConfigCommonGC-BC is included in SIB12-IEs: 4>\\tif configured by upper layers to perform NR sidelink reception: 5>\\tinclude sl-RxDRX-ReportList and set its fields (if needed) as follows for each destination for which it reports to network: 6>\\tset sl-DRX-ConfigFromTx to include the accepted sidelink DRX configuration of the associated destination for NR sidelink unicast communication, if received from the associated peer UE; 5>\\tinclude sl-RxInterestedGC-BC-DestList and set its fields (if needed) as follows for each Destination Layer-2 ID for which it reports to network: 6>\\tset sl-RxInterestedQoS-InfoList to include the QoS profile of its interested service(s) that sidelink DRX is applied for the associated destination for NR sidelink groupcast or broadcast reception; NOTE 1:\\tIt is up to UE implementation to set the QoS profile in sl-RxInterestedQoS-InfoList for reception of NR sidelink discovery message or ProSe Direct Link Establishment Request',\n",
       "  'question': 'What does sl-PerHop-QoS-InfoList include for the associated destination?\\n',\n",
       "  'answer': 'It includes the first-hop split PDB of the sidelink QoS flow(s) received from the sl-SplitQoS-InfoListPC5 in UEInformationResponseSidelink message.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 761}},\n",
       " {'context': '.214] to indicate resources from a set of resources selected by higher layers as described in [11, TS 38.321] with smallest slot indices for such that , where: -\\t, where is a number of resources in the set with slot indices , , such that , and is provided by sl-MaxNumPerReserve -\\teach resource, from the set of resources, corresponds to contiguous sub-channels and a slot in a set of slots , where is the number of sub-channels available for PSSCH/PSCCH transmission in a slot - is a set of slots in a sidelink resource pool [6, TS 38.214] - is an index of a slot where the PSCCH with SCI format 1-A is transmitted. A UE that transmits a PSCCH with SCI format 1-A usingsidelink resource allocation mode 1[6, TS 38.214] sets -\\tthe values of the frequency resource assignment field and the time resource assignment field for the SCI format 1-A transmitted in the -th resource for PSCCH/PSSCH transmission provided by a dynamic grant or by a SL configured grant, where and M is the total number of resources for PSCCH/PSSCH transmission provided by a dynamic grant or the number of resources for PSCCH/PSSCH transmission in a period provided by a SL configured grant type 1 or SL configured grant type 2, as follows: -\\tthe frequency resource assignment field and time resource assignment field indicate the -th to -th resources as described in [6, TS 38.214]. For decoding of a SCI format 1-A, a UE may assume that a number of bits provided by sl-NumReservedBits can have any value as described in [4, TS 38.212].',\n",
       "  'question': 'What is the number of sub-channels available for PSSCH/PSCCH transmission in a slot?\\n',\n",
       "  'answer': 'The number of sub-channels available for PSSCH/PSCCH transmission in a slot is represented by the variable .',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38213-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 589}},\n",
       " {'context': 'lengths. Of particular concern is fast fading, which arises out of reflections from objects in\\nthe local vicinity of the transmitter, the receiver, or both. The term fast refers to the speed\\nof fluctuations in the received signal due to these reflections, relative to the speeds of other\\npropagation phenomena. Compared with transmit data rates, even fast fading can be\\nrelatively slow. That is, fast fading can be approximately constant over a number of\\ntransmission symbols, depending upon the data transmission speed and the mobile units\\nvelocity. Consequently, fast fading may be viewed as a time-correlated form of channel\\nimpairment, the presence of which results in statistical dependence among continuous\\n(sets of) symbol transmissions. That is, instead of being isolated events, transmission\\nerrors due to fast fading tend to occur in bursts.\\nNow, most FEC channel codes are designed to deal with a limited number of bit errors,\\nassumed to be randomly distributed and statistically independent from one bit to the next.\\nTo be specific, in Section 10.8 on convolutional decoding, we indicated that the Viterbi\\nalgorithm, as powerful as it is, will fail if there are dfree2 closely spaced bit errors in the\\nreceived signal, where dfree is the free distance of the convolutional code. Accordingly, in\\nthe design of a reliable wireless communication system, we are confronted with two\\nconflicting phenomena:',\n",
       "  'question': 'What is fast fading in wireless communication?\\n',\n",
       "  'answer': 'Fast fading is caused by reflections from objects near the transmitter or receiver, leading to rapid fluctuations in the received signal speed, which can be relatively constant over several transmission symbols depending on data speed and mobile unit velocity.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 749,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications is sucient. Here [w] N(0, N0/2).\\nWith antipodal signalling, x = a, the ML error probability is simply\\nQ  ah p N0/2 ! . (A.57) Via a translation, the binary signal detection problem in the first part of the\\nsummary can be reduced to this antipodal signalling scenario.\\nA.3\\nEstimation in Gaussian Noise\\nA.3.1\\nScalar Estimation\\nConsider a zero mean real transmit signal x embedded in independent additive real\\nGaussian noise (w N (0, N0/2)):\\ny = x + w.\\n(A.58)\\nSuppose we wish to come up with an estimate x of x and we use the mean squared\\nerror (MSE) to evaluate the performance:\\nMSE := E  (x x)2 , (A.59) where the averaging is over the randomness of both the transmit signal x and the noise\\nw. This problem is quite dierent from the detection problem studied in Section A.2.\\nThe estimate that yields the smallest mean squared error is the classical conditional\\nmean operator: x = E [x|y] , (A.60) which has the important orthogonality property: the error is independent of the obser-\\nvation. In particular, this implies that\\nE [(x x)y] = 0.\\n(A.61)\\nThe orthogonality principle is a classical result and all standard text books dealing\\nwith probability theory and random variables treat this material.\\nIn general, the conditional mean operator E [x|y] is some complicated nonlinear\\nfunction of y. To simplify the analysis, one studies the restricted class of linear es-\\ntimates that minimize the MSE. This restriction is without loss of generality in the\\nimportant case when x is a Gaussian random variable because, in this case, the condi-\\ntional mean operator is actually linear.',\n",
       "  'question': 'What is the error probability in antipodal signalling?\\n',\n",
       "  'answer': 'The ML error probability in antipodal signalling is Q(ah * sqrt(N0/2)).',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 593,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications\\nWriting the singular values of D as 1, . . . , nt, show that\\nnt X =1 2 4l. (9.115) Thus, each of the singular values is upper bounded by\\n\\nl, a constant that does\\nnot increase with SNR.\\nExercise 9.25. [151] Consider the following transmission scheme (spanning two sym-\\nbols) for the two transmit MIMO channel. The entries of the transmit codeword matrix\\nX := [xij] are defined as\\n x11 x22  := R(1)  u1 u2  , and  x21 x12  := R(2)  u3 u4  . (9.116) Here u1, u2, u3, u4 are independent QAMs of size 2R/2 each (so the data rate of this\\nscheme is R bits/s/Hz). The rotation matrix R() is (c.f. (3.46))\\nR() :=  cos  sin  sin  cos   . (9.117) With the choice of the angles 1, 2 equal to 1/2 tan1 2 and 1/2 tan1(1/2) radians\\nrespectively, Theorem 2 of [151] shows that the determinant of every normalized code-\\nword dierence matrix D satisfies\\n| det D|2  1 10  2R. (9.118) Conclude that the code described in (9.116), with the appropriate choice of the angles\\n1, 2 above, is approximately universal for every MIMO channel with two transmit\\nantennas.',\n",
       "  'question': 'What is the upper bound on each singular value in equation 9.115?\\n',\n",
       "  'answer': 'Each singular value is upper bounded by the constant l, which does not increase with Signal-to-Noise Ratio (SNR).',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 497,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '. P0-PUSCH-Set field descriptions p0-List Configuration of {p0-PUSCH, p0-PUSCH} sets for PUSCH. If SRI is present in the DCI, then one p0-PUSCH can be configured in P0-PUSCH-Set. If SRI is not present in the DCI, and both olpc-ParameterSetDCI-0-1 and olpc-ParameterSetDCI-0-2 are configured to be 1 bit, then one p0-PUSCH can be configured in P0-PUSCH-Set. If SRI is not present in the DCI, and if any of olpc-ParameterSetDCI-0-1 and olpc-ParameterSetDCI-0-2 is configured to be 2 bits, then two p0-PUSCH values can be configured in P0-PUSCH-Set (see TS 38.213 [13] clause 7 and TS 38.212 [17] clause 7.3.1). p0-PUSCH-SetId Configure the index of a p0-PUSCH-Set (see TS 38.213 [13] clause 7 and TS 38.212 [17] clause 7.3.1). PUSCH-PowerControl field descriptions deltaMCS Indicates whether to apply delta MCS. When the field is absent, the UE applies Ks = 0 in delta_TFC formula for PUSCH (see TS 38.213 [13], clause 7.1). dummy This field is not used in the specification. If received it shall be ignored by the UE. msg3-Alpha Dedicated alpha value for msg3 PUSCH (see TS 38.213 [13], clause 7.1). When the field is absent the UE applies the value 1. olpc-ParameterSetDCI-0-1, olpc-ParameterSetDCI-0-2 Configures the number of bits for Open-loop power control parameter set indication for DCI format 0_1/0_2 in case SRI is not configured in the DCI. 2 bits is applicable only if SRI is not present in the DCI format 0_1. The field olpc-ParameterSetDCI-0-1 applies to DCI format 0_1 and the field olpc-ParameterSetDCI-0-2 applies to DCI format 0_2 (see TS 38.212 [17], clause 7.3.1 and TS 38.213 [13], clause 11). p0-AlphaSets Configuration {p0-pusch, alpha} sets for PUSCH (except msg3 and msgA PUSCH), i.e., { {p0,alpha,index1}, {p0,alpha,index2},...} (see TS 38.213 [13], clause 7.1). When no set is configured, the UE uses the P0-nominal for msg3/msgA PUSCH, P0-UE is set to 0 and alpha is set according to either msg3-Alpha or msgA-Alpha (see TS 38.213 [13], clause 7.1)',\n",
       "  'question': 'What does the deltaMCS field in PUSCH-PowerControl indicate?\\n',\n",
       "  'answer': 'It shows whether to apply delta MCS, and when absent, the UE uses Ks = 0 in the delta_TFC formula for PUSCH.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1597}},\n",
       " {'context': '.304 [20]; 3>\\tperform cell re-selection to other cells on the same frequency as the barred cell as specified in TS 38.304 [20], upon which the procedure ends; 2> else: 3>\\tif the cellBarredRedCap1Rx is present in the acquired SIB1 and is set to barred and the UE supports 1 Rx branch; or 3>\\tif the cellBarredRedCap2Rx is present in the acquired SIB1 and is set to barred and the UE supports 2 Rx branches: 4>\\tevaluate the cell barring criteria in accordance with TS 38.304 [20]; 4>\\tif the cell is considered as barred; 5>\\tperform cell re-selection to other cells on the same frequency as the barred cell as specified in TS 38.304 [20], upon which the procedure ends; 1>\\tif the UE is a 2Rx XR UE and is in RRC_IDLE or in RRC_INACTIVE, or if the 2Rx XR UE is in RRC_CONNECTED while T311 is running: 2>\\tif the cellBarred2RxXR is present in the acquired SIB1: 3>\\tevaluate the cell barring criteria in accordance with TS 38.304 [20]; 3>\\tif the cell is considered as barred; 4>\\tperform cell re-selection to other cells on the same frequency as the barred cell as specified in TS 38.304 [20] upon which the procedure ends; 1>\\tif the UE supports nes-CellDTX-DRX and it is in RRC_IDLE or in RRC_INACTIVE, or if the UE supporting nes-CellDTX-DRX is in RRC_CONNECTED while T311 is running: 2>\\tif cellBarred in the acquired MIB is set to barred: 3>\\tif cellBarredNES is absent in the acquired SIB1: 4>\\tconsider the cell as barred in accordance with TS 38.304 [20]; 4>\\tperform cell re-selection to other cells on the same frequency as the barred cell as specified in TS 38.304 [20], upon which the procedure ends; 1>\\tif the UE is an eRedCap UE and it is in RRC_IDLE or in RRC_INACTIVE, or if the eRedCap UE is in RRC_CONNECTED while T311 is running: 2>\\tif intraFreqReselection-eRedCap is not present in SIB1; or 2>\\tif the halfDuplexRedCapAllowed is not present in the acquired SIB1 and the UE supports only half-duplex FDD operation: 3>\\tconsider the cell as barred in accordance with TS 38',\n",
       "  'question': 'What happens when a cell is considered as barred?\\n',\n",
       "  'answer': 'The procedure ends after performing cell re-selection to other cells on the same frequency as the barred cell as specified in TS 38.304 [20].',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 76}},\n",
       " {'context': '.101-1 [14], TS 38.101-2 [15], and TS 38.101-3 [16]). The MAC entity shall set the P field to 1 if the corresponding PCMAX,f,c field would have had a different value if no power backoff due to power management had been applied; -\\tPCMAX,f,c: If present, this field indicates the PCMAX,f,c (as specified in TS 38.213 [6]) for the NR Serving Cell and the PCMAX,c or PCMAX,c (as specified in TS 36.213 [17]) for the E-UTRA Serving Cell used for calculation of the preceding PH field. The reported PCMAX,f,c and the corresponding nominal UE transmit power levels are shown in Table 6.1.3.8-2 (the corresponding measured values in dBm for the NR Serving Cell are specified in TS 38.133 [11] while the corresponding measured values in dBm for the E-UTRA Serving Cell are specified in TS 36.133 [12]); -\\tPCMAX,f,c for assumed PUSCH: If present, this field indicates the PCMAX,f,c for assumed PUSCH(as specified in TS 38.213 [6]) for the NR Serving Cell. The reported PCMAX,f,c and the corresponding nominal UE transmit power levels are shown in [Table 6.1.3.8-2] (the corresponding measured values in dBm for the NR Serving Cell are specified in TS 38.133 [11]; -\\tMPE: If mpe-Reporting-FR2 is configured, and the Serving Cell operates on FR2, and if the P field is set to 1, this field indicates the applied power backoff to meet MPE requirements, as specified in TS 38.101-2 [15]. This field indicates an index to Table 6.1.3.8-3 and the corresponding measured values of P-MPR levels in dB are specified in TS 38.133 [11]. The length of the field is 2 bits. If mpe-Reporting-FR2 is not configured, or if the Serving Cell operates on FR1, or if the P field is set to 0, R bits are present instead. Figure 6.1.3.79-1: Multiple Entry PHR with assumed PUSCH MAC CE with the highest ServCellIndex of Serving Cell with configured uplink is less than Figure 6.1.3.79-2: Multiple Entry PHR with assumed PUSCH MAC CE with the highest ServCellIndex of Serving Cell with configured uplink is equal to or higher than',\n",
       "  'question': 'What does the P field indicate when set to 1?\\n',\n",
       "  'answer': 'It indicates that the PCMAX,f,c field would have had a different value if no power backoff due to power management had been applied.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38321-i50.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 558}},\n",
       " {'context': 'Appendix A\\nDetection and Estimation in\\nAdditive Gaussian Noise\\nA.1\\nGaussian Random Variables\\nA.1.1\\nScalar Real Gaussian Random Variable\\nA standard Gaussian random variable w takes values over the real line and has the\\nprobability density function\\nf(w) = 1  2exp  w2 2  , w . (A.1) The mean of w is zero and the variance is 1. This random variable is called a standard\\nGaussian random variable. A (general) Gaussian random variable x is of the form\\nx = w + .\\n(A.2)\\nThe mean of x is  and the variance is equal to 2.\\nThe random variable x is a\\none-to-one function of w and thus the probability density function follows from (A.1)\\nas f(x) = 1  22exp  (x )2 22 ! , x . (A.3) Since the random variable is completely characterized by its mean and variance, we\\ndenote x by N (, 2). In particular, the standard Gaussian random variable is denoted\\nby N (0, 1). The tail of the Gaussian random variable w:\\nQ(a) := P {w > a} .\\n(A.4)\\nis plotted in Figure A.1. The plot and the computations Q (1) = 0.159 and Q (3) =\\n00015 give a sense for how rapidly the tails decay. The tails decay exponentially fast',\n",
       "  'question': 'What is the probability density function of a standard Gaussian random variable?\\n',\n",
       "  'answer': 'It is defined as f(w) = 1/(2) * exp(-w/2) for values of w over the real line, with a mean of zero and a variance of one.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 579,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '. Otherwise, it is optional, Need R. InitialBWP-Only This field is optionally present, Need R, if this BWP is the initial BWP of SpCell. Otherwise, the field is absent. L139 The field is mandatory present if prach-RootSequenceIndex L=139, or if L=571 for FR2-2, otherwise the field is absent, Need S. SUL The field is mandatory present in rach-ConfigCommon in initialUplinkBWP if supplementaryUplink is configured in ServingCellConfigCommonSIB or if supplementaryUplinkConfig is configured in ServingCellConfigCommon; otherwise, the field is absent. This field is not configured in additionalRACH-Config. -\\tRACH-ConfigCommonTwoStepRA The IE RACH-ConfigCommonTwoStepRA is used to specify cell specific 2-step random-access type parameters. RACH-ConfigCommonTwoStepRA information element -- ASN1START -- TAG-RACH-CONFIGCOMMONTWOSTEPRA-START RACH-ConfigCommonTwoStepRA-r16 ::= SEQUENCE { rach-ConfigGenericTwoStepRA-r16 RACH-ConfigGenericTwoStepRA-r16, msgA-TotalNumberOfRA-Preambles-r16 INTEGER (1..63) OPTIONAL, -- Need S msgA-SSB-PerRACH-OccasionAndCB-PreamblesPerSSB-r16 CHOICE { oneEighth ENUMERATED {n4,n8,n12,n16,n20,n24,n28,n32,n36,n40,n44,n48,n52,n56,n60,n64}, oneFourth ENUMERATED {n4,n8,n12,n16,n20,n24,n28,n32,n36,n40,n44,n48,n52,n56,n60,n64}, oneHalf ENUMERATED {n4,n8,n12,n16,n20,n24,n28,n32,n36,n40,n44,n48,n52,n56,n60,n64}, one ENUMERATED {n4,n8,n12,n16,n20,n24,n28,n32,n36,n40,n44,n48,n52,n56,n60,n64}, two ENUMERATED {n4,n8,n12,n16,n20,n24,n28,n32}, four INTEGER (1..16), eight INTEGER (1..8), sixteen INTEGER (1..4) } OPTIONAL, -- Cond 2StepOnly msgA-CB-PreamblesPerSSB-PerSharedRO-r16 INTEGER (1..60) OPTIONAL, -- Cond SharedRO msgA-SSB-SharedRO-MaskIndex-r16 INTEGER (1..15) OPTIONAL, -- Need S groupB-ConfiguredTwoStepRA-r16 GroupB-ConfiguredTwoStepRA-r16 OPTIONAL, -- Need S msgA-PRACH-RootSequenceIndex-r16 CHOICE { l839 INTEGER (0..837), l139 INTEGER (0..137), l571 INTEGER (0..569), l1151 INTEGER (0.',\n",
       "  'question': \"What is the specific value of L139 for the field's presence?\\n\",\n",
       "  'answer': 'The field is mandatory present if prach-RootSequenceIndex L=139.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1614}},\n",
       " {'context': '. 1>\\tif sl-DRX-ConfigGC-BC is included in sl-ConfigDedicatedNR within RRCReconfiguration: 2>\\tstore the NR sidelink DRX configuration and configure lower layers to perform sidelink DRX operation for groupcast and broadcast as specified in TS 38.321 [3]; 1>\\tif sl-DRX-ConfigUC-ToReleaseList is included in sl-ConfigDedicatedNR within RRCReconfiguration: 2>\\tfor each SL-DestinationIndex included in the received sl-DRX-ConfigUC-ToReleaseList that is part of the current UE configuration: 3>\\tremove the entry with the matching SL-DestinationIndex from the stored NR sidelink DRX configuration information; 1>\\tif sl-DRX-ConfigUC-ToAddModList is included in sl-ConfigDedicatedNR within RRCReconfiguration: 2>\\tfor each sl-DestinationIndex included in the received sl-DRX-ConfigUC-ToAddModList that is part of the current stored NR sidelink DRX configuration: 3>\\treconfigure the entry according to the value received for this sl-DestinationIndex from the stored NR sidelink DRX configuration information; 2>\\tfor each sl-DestinationIndex included in the received sl-DRX-ConfigUC-ToAddModList that is not part of the current stored NR sidelink DRX configuration: 3>\\tadd a new entry for this sl-DestinationIndex to the stored NR sidelink DRX configuration. NOTE 2:\\tThe UE is expected to update the mapping between the Destination Layer-2 ID and the destination index for the stored NR sidelink DRX configuration after the UE updates the destination list and reports to the gNB. 1>\\tif sl-RLC-ChannelToReleaseList is included in sl-ConfigDedicatedNR within RRCReconfiguration: 2>\\tperform PC5 Relay RLC channel release as specified in 5.8.9.7.1; 1>\\tif sl-RLC-ChannelToAddModList is included in sl-ConfigDedicatedNR within RRCReconfiguration or RRCSetup: 2>\\tperform PC5 Relay RLC channel addition/modification as specified in 5.8.9.7.2;',\n",
       "  'question': 'What does the inclusion of sl-DRX-ConfigGC-BC in sl-ConfigDedicatedNR within RRCReconfiguration result in?\\n',\n",
       "  'answer': 'It stores the NR sidelink DRX configuration and configures lower layers to perform sidelink DRX operation for groupcast and broadcast as specified in TS 38.321.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 271}},\n",
       " {'context': \".3-3: Mapping of four beta_offset indicator values to offset indexes beta_offset indicator ( or or ), ( or or ), ( or or ), ( or ), ( or ) '00' 1st offset index provided by higher layers '01' 2nd offset index provided by higher layers '10' 3rd offset index provided by higher layers '11' 4th offset index provided by higher layers Table 9.3-3A: Mapping of two beta_offset indicator values to offset indexes beta_offset indicator ( or or ), ( or or ), ( or or ), ( or ), ( or ) '0' 1st offset index provided by higher layers '1' 2nd offset index provided by higher layers\",\n",
       "  'question': \"What does the beta_offset indicator '00' represent?\\n\",\n",
       "  'answer': \"The beta_offset indicator '00' signifies the 1st offset index provided by higher layers.\",\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38213-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 340}},\n",
       " {'context': \".3.1.1.2-36. If the number of ports for a configured SRS resource in the set is less than the maximum number of ports in an SRS resource among the configured SRS resources, a number of most significant bits with value set to '0' are inserted to the field. For the higher layer parameter txConfig = codebook, when the Transform precoder indicator field is present, if the bit width of the Second Precoding information field for the case with transform precoder enabled is not equal to that for the case with transform precoder disabled, a number of most significant bits with value set to '0' are inserted to the Second Precoding information field for the case with smaller bit width until the bit width of the Second Precoding information field for the two cases are the same. -\\tAntenna ports - number of bits determined by the following -\\t2 bits as defined by Tables 7.3.1.1.2-6, if transform precoder is enabled, dmrs-Type=1, and maxLength=1, except that dmrs-UplinkTransformPrecoding and tp-pi2BPSK are both configured and /2 BPSK modulation is used; -\\t2 bits as defined by Tables 7.3.1.1.2-6A, if transform precoder is enabled and dmrs-UplinkTransformPrecoding and tp-pi2BPSK are both configured, /2 BPSK modulation is used, dmrs-Type=1, and maxLength=1, where nSCID is the scrambling identity for antenna ports defined in Clause6.4.1.1.1.2, TS 38.211 [4]; -\\t4 bits as defined by Tables 7.3.1.1.2-7, if transform precoder is enabled, dmrs-Type=1, and maxLength=2, except that dmrs-UplinkTransformPrecoding and tp-pi2BPSK are both configured and /2 BPSK modulation is used; -\\t4 bits as defined by Tables 7.3.1.1.2-7A, if transform precoder is enabled and dmrs-UplinkTransformPrecoding and tp-pi2BPSK are both configured, /2 BPSK modulation is used, dmrs-Type=1, and maxLength=2, where nSCID is the scrambling identity for antenna ports defined in Clause6.4.1.1.1.2, TS 38.211 [4]; -\\t3 bits as defined by Tables 7.3.1.1\",\n",
       "  'question': 'How many bits are used for antenna ports if transform precoder is enabled, dmrs-Type=1, and maxLength=1?\\n',\n",
       "  'answer': '2 bits are used for antenna ports under these conditions.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38212-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 184}},\n",
       " {'context': '13b1\\tSCG activation without SN message Upon initiating the procedure, the UE shall: 1>\\tif the SCG was deactivated before the reception of the RRCReconfiguration message or the E-UTRA RRCConnectionReconfiguration message for which the procedure invoking this clause is executed: 2>\\tconsider the SCG to be activated; 2>\\tindicate to lower layers that the SCG is activated; 2>\\tresume performing radio link monitoring on the SCG, if previously stopped; 2>\\tindicate to lower layers to resume beam failure detection on the PSCell, if previously stopped; 2>\\tif bfd-and-RLM was not configured to true before the reception of the RRCReconfiguration message or the E-UTRA RRCConnectionReconfiguration message for which the procedure invoking this clause is executed; or 2>\\tif lower layers indicate that a Random Access procedure is needed for SCG activation: 3>\\tinitiate the Random Access procedure on the PSCell, as specified in TS 38.321 [3].\\n13c\\tFR2 UL gap configuration The UE shall: 1>\\tif ul-GapFR2-Config is set to setup: 2>\\tif an FR2 UL gap configuration is already setup, release the FR2 UL gap configuration; 2>\\tsetup the FR2 UL gap configuration indicated by the ul-GapFR2-Config in accordance with the received gapOffset, i.e., the first subframe of each gap occurs at an SFN and subframe meeting the following condition: SFN mod T = FLOOR (gapOffset/10); if the UGRP is larger than 5ms: subframe = gapOffset mod 10; else: subframe = gapOffset or (gapOffset +5); with T = CEIL(UGRP/10). 1>\\telse if ul-GapFR2-Config is set to release: 2>\\trelease the FR2 UL gap configuration. NOTE 1:\\tFor ul-GapFR2-Config configuration with synchronous CA, the SFN and subframe of a serving cell on FR2 frequency is used in the gap calculation. For ul-GapFR2-Config configuration with asynchronous CA, the SFN and subframe of a serving cell on FR2 frequency indicated by the refFR2-ServCellAsyncCA in ul-GapFR2-Config is used in the gap calculation.',\n",
       "  'question': 'What does the UE do if the SCG was deactivated before receiving certain RRC messages?\\n',\n",
       "  'answer': 'The UE considers the SCG to be activated, indicates this to lower layers, resumes radio link monitoring on the SCG, and indicates to lower layers to resume beam failure detection on the PSCell.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 263}},\n",
       " {'context': '4\\tPhysical random access channel A UE determines a transmission power for a physical random access channel (PRACH), , on active UL BWP of carrier of cell based on DL RS for cell in transmission occasion as [dBm], where - is the UE configured maximum output power defined in [8-1, TS 38.101-1], [8-2, TS 38.101-2], [8-3, TS 38.101-3] and [8-5, TS 38.101-5] for carrier of cell within transmission occasion , - is the PRACH target reception power PREAMBLE_RECEIVED_TARGET_POWER provided by higher layers [11, TS 38.321] for the active UL BWP of carrier of cell , and - is a pathloss for the active UL BWP of carrier based on the DL RS associated with the PRACH transmission on the active DL BWP of cell and calculated by the UE in dB as referenceSignalPower - higher layer filtered RSRP in dBm, where RSRP is defined in [7, TS 38.215] and the higher layer filter configuration is defined in [12, TS 38.331]. If the active DL BWP is the initial DL BWP and for SS/PBCH block and CORESET multiplexing pattern 2 or 3 as described in clause 13, or for a non-serving cell, the UE determines based on the SS/PBCH block associated with the PRACH transmission. If a PRACH transmission from a UE is not in response to a detection of a PDCCH order by the UE, or is in response to a detection of a PDCCH order by the UE that triggers a contention based random access procedure, or is associated with a link recovery procedure where a corresponding index is associated with a SS/PBCH block, as described in clause 6, referenceSignalPower is provided by ss-PBCH-BlockPower. If a PRACH transmission from a UE is in response to a detection of a PDCCH order by the UE that triggers a contention-free random access procedure and depending on the DL RS that the DM-RS of the PDCCH order is quasi-collocated with as described in clause 10',\n",
       "  'question': 'What is the formula for determining transmission power for a physical random access channel (PRACH)?\\n',\n",
       "  'answer': 'The transmission power for PRACH is calculated by subtracting the pathloss from the UE-configured maximum output power and adding the PRACH target reception power.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38213-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 99}},\n",
       " {'context': '.1.2.1. In sidelink resource allocation mode 1 for a dedicated SL PRS resource pool, the time domain behaviour for sidelink dynamic grants and sidelink configured grants for SL PRS follows the behaviour in clause 8.1.2.1, with the following modifications: -\\t\"DCI format 3_0\" is replaced by \"DCI format 3_2\". -\\t\"PSSCH\" is replaced by \"SL PRS\".',\n",
       "  'question': 'What replaces \"DCI format 3_0\" in sidelink resource allocation mode 1?\\n',\n",
       "  'answer': '\"DCI format 3_2\" replaces \"DCI format 3_0\" in sidelink resource allocation mode 1 for a dedicated SL PRS resource pool.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38214-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 599}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications Let h1, . . . , hK be i.i.d. real random variables with a common cdf F() and pdf\\nf() satisfying F(h) is less than 1 and is twice dierentiable for all h, and is such\\nthat lim h d dh 1 F(h) f(h)  = 0. (6.89) Then Kf(lK)  max 1kK hk lK  converges in distribution to a limiting random variable with cdf\\nexp(ex),\\nwhere lK is given by F(lK) = 1 1/K. This result states that the maximum of\\nK such i.i.d. random variables grows like lK.\\nExercise 6.22. (Selective feedback) The downlink of IS-856 has K users each expe-\\nriencing i.i.d. Rayleigh fading with average SNR of 0 dB. Each user selectively feeds\\nback the requested rate only if its channel is greater than a threshold . Suppose\\n is chosen such that the probability that no one sends a requested rate is . Find\\nthe expected number of users that sends in a requested rate. Plot this number for\\nK = 2, 4, 8, 16, 32, 64 and for  = 0.1 and  = 0.01. Is selective feedback eective?\\nExercise 6.23. The discussions in Section 6.7.2 about channel measurement, predic-\\ntion and feedback are based on an FDD system. Discuss the analogous issues for a\\nTDD system, both in the uplink and in the downlink.\\nExercise 6.24. Consider the two user downlink AWGN channel (c.f. (6.16)):\\nyk [m] = hkx [m] + zk [m] ,\\nk = 1, 2.\\n(6.90)\\nHere zk [m] are i.i.d. CN(0, N0) Gaussian processes marginally (k = 1, 2). Let us take\\n|h1| > |h2| for this problem.\\nArgue that the capacity region of this downlink channel does not depend on\\nthe correlation between the additive Gaussian noise processes z1 [m] and z2 [m].\\nHint: Since the two users cannot cooperate, it should be intuitive that the error\\nprobability for user k depends only on the marginal distribution of zk [m] (for\\nboth k = 1, 2).\\nNow consider the following specific correlation between the two additive noises\\nof the users. The pair (z1 [m] , z2 [m]) is i.i.d. with time m with the distribution',\n",
       "  'question': 'What is the limiting cdf of the maximum of K i.i.d. real random variables with a common cdf F()?\\n',\n",
       "  'answer': 'The limiting cdf is exp(exp(x)), derived from the given convergence in distribution result.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 338,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': 'A1\\nAPPENDIX\\nA Advanced Probabilistic Models\\nIn the study of digital communications presented in preceding chapters, the Gaussian,\\nRayleigh, and Rician distributions featured in the formulation of probabilistic models in\\nvarying degrees. In this appendix we describe three relatively advanced distributions:\\n\\nthe chi distribution;\\n\\nthe log-normal distribution;\\n\\nthe Nakagami distribution.\\nThe chi distribution is featured in the study of diversity-on-receive techniques in Chapter\\non signaling across fading channels. Just as importantly, the log-normal distribution was\\nmentioned in passing in the context of shadowing in wireless communications, also in\\nChapter 9. The Nakagami distribution is the most advanced of all the three:\\n\\nit includes the Rayleigh distribution as a special case;\\n\\nits shape is similar to the Rician distribution;\\n\\nit is flexible in its applicability.\\nA.1\\nThe Chi-Square Distribution\\nA chi-square distributed random variable is produced, for example, when a Gaussian\\nrandom variable is passed through a squaring device. Viewed in this manner, there are two\\nkinds of distributions: 1. Central distribution, which is produced when the Gaussian random variable has\\nzero mean.\\nNoncentral distribution, which is produced when the Gaussian random variable\\nhas a nonzero mean.\\nIn this appendix, we will discuss only the central form of the distribution.\\nConsider, then, a standard Gaussian random variable X, which has zero mean and unit\\nvariance, as shown by\\n(A.1)\\nLet the variable X be applied to a square-law device, producing a new random variable Y,\\nwhose sample value is defined by\\n(A.2) or, equivalently, (A.3) The cumulative distribution function of the random variable Y produced at the output of\\nthe square-law device is therefore defined by\\n(A.4) 2 2 2 2 fX x  1 2 ---------- x2 2----- -      x    -  exp = y x2 = x y  = FY y  fX x  dx y - y  =',\n",
       "  'question': 'What is produced when a Gaussian random variable is passed through a squaring device?\\n',\n",
       "  'answer': 'A chi-square distributed random variable is produced when a Gaussian random variable is passed through a squaring device, characterized by its central form with zero mean and unit variance.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 721,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '. NOTE 3A1:\\tIf sl-InterUE-CoordinationScheme1 enabling reception/transmission of preferred resource set and non-preferred resource set is configured by RRC and if multiple preferred resource sets are received from the same UE, it is up to UE implementation to use one or multiple of them in its resource (re)selection. NOTE 3B1:\\tIf retransmission resource(s) cannot be selected by ensuring that the resource(s) can be indicated by the time resource assignment of a prior SCI, how to select the time and frequency resources for one or more transmission opportunities from the available resources is left for UE implementation by ensuring the minimum time gap between any two selected resources in case that PSFCH is configured for this pool of resources. NOTE 3B2:\\tWhen the UE receives both a single preferred resource set and a single non-preferred resource set from the same peer UE or different peer UEs, when the UE has own sensing results, it is up to the UE implementation to use the preferred resource set in its resource (re)selection for transmissions to the peer UE providing the preferred resource set. NOTE 3B3:\\tThe UE is not required to use any resource from the preferred resource set in its resource (re-)selection if that resource is earlier than (++) after the resource of Inter-UE Coordination Information transmission, where is equal to (+) when only MAC CE is used for inter-UE Coordination Information transmission, or is equal to when MAC CE and SCI format 2-C are both used for Inter-UE Coordination Information transmission. The case when is equal to is assuming that SCI format 2-C is received. and are specified in clause 8.1.4 of TS 38.214 [7]',\n",
       "  'question': 'What determines how a UE uses multiple preferred resource sets?\\n',\n",
       "  'answer': 'It is up to the UE implementation to decide whether to use one or multiple preferred resource sets in its resource selection process.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38321-i50.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 317}},\n",
       " {'context': \"7a\\tDiscontinuous Reception (DRX) for MBS Broadcast For MBS broadcast, the MAC entity may be configured by RRC with a DRX functionality per G-RNTI that controls the UE's PDCCH monitoring activity for the MAC entity's G-RNTI(s) as specified in TS 38.331 [5]. When in RRC_IDLE or RRC_INACTIVE or RRC_CONNECTED, if broadcast DRX is configured for a G-RNTI, the MAC entity is allowed to monitor the PDCCH for this G-RNTI discontinuously using the broadcast DRX operation specified in this clause; otherwise the MAC entity monitors each PDCCH for this G-RNTI as specified in TS 38.213 [6]. The broadcast DRX operation specified in this clause is performed independently for each G-RNTI and independently from the DRX operation specified in clauses 5.7 and 5.7b. RRC controls broadcast DRX operation by configuring the following parameters: -\\tdrx-onDurationTimerPTM: the duration at the beginning of a DRX cycle; -\\tdrx-SlotOffsetPTM: the delay before starting the drx-onDurationTimerPTM; -\\tdrx-InactivityTimerPTM: the duration after the PDCCH occasion in which a PDCCH indicates a new DL broadcast transmission for the MAC entity; -\\tdrx-LongCycleStartOffsetPTM: the long DRX cycle drx-LongCycle-PTM and drx-StartOffset-PTM which defines the subframe where the DRX cycle starts. When broadcast DRX is configured for a G-RNTI, the Active Time includes the time while: -\\tdrx-onDurationTimerPTM or drx-InactivityTimerPTM for this G-RNTI is running. When broadcast DRX is configured for a G-RNTI, the MAC entity shall for this G-RNTI: 1>\\tif [(SFN  10) + subframe number] modulo (drx-LongCycle-PTM) = drx-StartOffset-PTM: 2>\\tstart drx-onDurationTimerPTM after drx-SlotOffsetPTM from the beginning of the subframe. 1>\\tif the MAC entity is in Active Time for this G-RNTI: 2>\\tmonitor the PDCCH for this G-RNTI as specified in TS 38.213 [6]; 2>\\tif the PDCCH indicates a DL transmission for MBS broadcast: 3>\\tstart or restart drx-InactivityTimerPTM in the first symbol after the end of the PDCCH reception\",\n",
       "  'question': 'What is the purpose of the drx-onDurationTimerPTM in broadcast DRX operation?\\n',\n",
       "  'answer': 'The drx-onDurationTimerPTM defines the duration at the beginning of a DRX cycle, controlling when the UE starts monitoring the PDCCH for a specific G-RNTI.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38321-i50.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 183}},\n",
       " {'context': '7 Square-Root Raised-Cosine Spectrum To avoid confusion, we use G(f ) as the symbol for the SRRC spectrum, and so we may write\\n(8.33)\\nwhere, as before, the roll-off factor  is defined in terms of the frequency parameter f1 and\\nthe bandwidth W as in (8.23).\\nIf, now, the transmitter includes a pre-modulation filter with the transfer function\\ndefined in (8.33) and the receiver includes an identical post-modulation filter, then under\\nideal conditions the overall pulse waveform will experience the squared spectrum G2( f),\\nwhich is the regular RC spectrum. In effect, by adopting the SRRC spectrum G( f) of\\n(8.33) for pulse shaping, we would be working with G2(f ) = P( f) in an overall\\ntransmitter-receiver sense. On this basis, we find that in wireless communications, for\\nexample, if the channel is affected by both fading and AWGN and the pulse-shape filtering\\nis partitioned equally between the transmitter and the receiver in the manner described\\nherein, then effectively the receiver would maximize the output SNR at the sampling\\ninstants.\\nThe inverse Fourier transform of (8.33) defines the SRRC shaping pulse:\\n(8.34)\\nThe important point to note here is the fact that the SRRC shaping pulse g(t) of (8.34) is\\nradically different from the conventional RC shaping pulse of (8.25). In particular, the\\nnew shaping pulse has the distinct property of satisfying the orthogonality constraint\\nunder T-shifts, described by\\n(8.35)\\nwhere T is the symbol duration. Yet, the new pulse g(t) has exactly the same excess\\nbandwidth as the conventional RC pulse.\\nIt is also important to note, however, that despite the added property of orthogonality,\\nthe SRRC shaping pulse of (8.34) lacks the zero-crossing property of the conventional RC\\nshaping pulse defined in (8.25).\\nFigure 8.9a plots the SRRC spectrum G( f) for the roll-off factor  = 0, 0.5, 1; the\\ncorresponding time-domain plots are shown in Figure 8.9b. These plots are naturally',\n",
       "  'question': 'What is the roll-off factor symbol defined in terms of the frequency parameter f1 and the bandwidth W?\\n',\n",
       "  'answer': 'The roll-off factor symbol is defined in terms of the frequency parameter f1 and the bandwidth W.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 479,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '. For measurements, integer value for RSRQ measurements is according to Table 10.1.11.1-1 in TS 38.133 [14]. For thresholds, the actual value is (IE value - 87) / 2 dB. RSRQ-Range information element -- ASN1START -- TAG-RSRQ-RANGE-START RSRQ-Range ::= INTEGER(0..127) -- TAG-RSRQ-RANGE-STOP -- ASN1STOP -\\tRSSI-Range The IE RSSI-Range specifies the value range used in RSSI measurements and thresholds for NR operation with shared spectrum channel access. The integer value for RSSI measurements is according to Table 10.1.34.3-1 in TS 38.133 [14]. RSSI-Range information element -- ASN1START -- TAG-RSSI-RANGE-START RSSI-Range-r16 ::= INTEGER(0..76) -- TAG-RSSI-RANGE-STOP -- ASN1STOP -\\tRxTxTimeDiff The IE RxTxTimeDiff contains the Rx-Tx time difference measurement at either the UE or the gNB. RxTxTimeDiff information element -- ASN1START -- TAG-RXTXTIMEDIFF-START RxTxTimeDiff-r17 ::= SEQUENCE { result-k5-r17 INTEGER (0..61565) OPTIONAL, -- Need N ... } -- TAG-RXTXTIMEDIFF-STOP -- ASN1STOP RxTxTimeDiff field descriptions result-k5 This field indicates the Rx-Tx time difference measurement, see TS 38.215 [9], clause 10.1.25.3.1 of TS 38.133 [14] for UE Rx-Tx time difference and clause 13.2.1 of TS 38.133 [14] for gNB Rx-Tx time difference. -\\tSCellActivationRS-Config The IE SCellActivationRS-Config is used to configure a Reference Signal for fast activation of the SCell where the IE is included (see TS 38.214 [19], clause 5.2.1.5.3. Usage of an SCellActivationRS-Config is indicated by including its scellActivationRS-Id in the Enhanced SCell activation MAC CE (see TS 38.321 [3] clause 6.1.3.55). SCellActivationRS-Config information element -- ASN1START -- TAG-SCELLACTIVATIONRS-CONFIG-START SCellActivationRS-Config-r17 ::= SEQUENCE { scellActivationRS-Id-r17 SCellActivationRS-ConfigId-r17, resourceSet-r17 NZP-CSI-RS-ResourceSetId, gapBetweenBursts-r17 INTEGER (2..31) OPTIONAL, -- Need R qcl-Info-r17 TCI-StateId, ..',\n",
       "  'question': 'What is the formula for calculating the threshold value?\\n',\n",
       "  'answer': 'The threshold value is calculated by subtracting 87 from the IE value and then dividing the result by 2 dB.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1686}},\n",
       " {'context': '8 Channel-coding Theorem The channel encoder and channel decoder in Figure 5.11 are both under the designers\\ncontrol and should be designed to optimize the overall reliability of the communication\\nsystem. The approach taken is to introduce redundancy in the channel encoder in a\\ncontrolled manner, so as to reconstruct the original source sequence as accurately as\\npossible. In a rather loose sense, we may thus view channel coding as the dual of source\\ncoding, in that the former introduces controlled redundancy to improve reliability whereas\\nthe latter reduces redundancy to improve efficiency.\\nTreatment of the channel-coding techniques is deferred to Chapter 10. For the purpose\\nof our present discussion, it suffices to confine our attention to block codes. In this class of\\ncodes, the message sequence is subdivided into sequential blocks each k bits long, and\\neach k-bit block is mapped into an n-bit block, where n > k. The number of redundant bits\\nadded by the encoder to each transmitted block is n - k bits. The ratio kn is called the code\\nrate. Using r to denote the code rate, we write\\n(5.61)\\nwhere, of course, r is less than unity. For a prescribed k, the code rate r (and, therefore, the\\nsystems coding efficiency) approaches zero as the block length n approaches infinity.\\nThe accurate reconstruction of the original source sequence at the destination requires\\nthat the average probability of symbol error be arbitrarily low. This raises the following\\nimportant question:\\nDoes a channel-coding scheme exist such that the probability that a message bit\\nwill be in error is less than any positive number  (i.e., as small as we want it),\\nand yet the channel-coding scheme is efficient in that the code rate need not be\\ntoo small?\\nThe answer to this fundamental question is an emphatic yes. Indeed, the answer to the\\nquestion is provided by Shannons second theorem in terms of the channel capacity C, as\\ndescribed in what follows.',\n",
       "  'question': 'What is the ratio of the number of bits in the message block to the number of bits in the encoded block called?\\n',\n",
       "  'answer': 'The ratio kn is called the code rate, where n is the number of bits in the encoded block and k is the number of bits in the message block.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 253,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '- F Correction on enhancements to measurement report [meas_report_enh] RP-105 RP-242240 - F Correction on extension of ToAddModList RP-105 RP-242237 F Rel-18 SONMDT Corrections RP-105 RP-242239 F Correction on MIMOevo RP-105 RP-242237 F Correction on NR MUSIM enhancements RP-105 RP-242230 - B Updates and Introduction of UE capabilities for Rel-18 WIs 12/2024 RP-106 RP-243233 F Correction for cell barring for 2Rx XR UE [2Rx_XR_Device] RP-106 RP-243221 A Correction on IE SRS-CarrierSwitching RP-106 RP-243227 A Correction for CFRA configuration due to PRACH partitioning RP-106 RP-243233 F Correction for Paging monitoring during SDT [CG-SDT-Enh] RP-106 RP-243231 F RRC correction on NR sidelink positioning RP-106 RP-243232 F Co-configuration of random/partial-sensing resource selection and Co-Ex RP-106 RP-243227 A Corrections on the L2 U2N Remote UE measurement RP-106 RP-243229 F Miscellaneous corrections for Rel-18 SON/MDT RP-106 RP-243232 F Miscellaneous CR for Rel-18 SL relay enhancement RP-106 RP-243224 A Correction to unified TCI signalling RP-106 RP-243226 A Misc RRC corrections for SL enhancements RP-106 RP-243229 F Correction to the musim-AffectedBandsList and musim-AvoidedBandsList RP-106 RP-243224 A Correction on UE behavior of setting failedPSCellId RP-106 RP-243228 F Network energy savings for NR rapporteur RRC CR RP-106 RP-243219 B Introduction of new capability for intra-band EN-DC channel spacing [Intra-Band_EN-DC_Channelspacing] RP-106 RP-243218 A Correction on UE capabilities for TCI state indication RP-106 RP-243228 F Clarification of MeasurementTimingConfiguration use RP-106 RP-243225 A Corrections on measurement gap configuration RP-106 RP-243231 F Correction of Enhancement on NR QoE management and optimizations for diverse services RP-106 RP-243224 A Correction on IE perRAInfoList for SCGFailureInformation RP-106 RP-243232 F RRC correction on NR SL U2U relay operation RP-106 RP-243222 A RRC correction on sl-X-Overhead field description of',\n",
       "  'question': 'What are some of the enhancements and corrections being made for Rel-18 in the context provided?\\n',\n",
       "  'answer': 'The context lists multiple enhancements and corrections for Rel-18, including updates to UE capabilities, enhancements to measurement reports, corrections related to MIMOevo and NR MUSIM, and various RRC and SON/MDT improvements.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 2559}},\n",
       " {'context': 'Chapter\\nSignaling over Fading Channels Code-Division Multiple Access\\nModern wireless networks are commonly of a multiuser type, in that the multiple\\ncommunication links within the network are shared among multiple users. Specifically,\\neach individual user is permitted to share the available radio resources (i.e., time and\\nfrequency) with other users in the network and do so in an independent manner.\\nStated in another way, a multiple access technique permits the radio resources to be\\nshared among multiple users seeking to communicate with each other. In the context of\\ntime and frequency domains, we recall from Chapter 1 that frequency-division multiple\\naccess (FDMA) and time-division multiple access (TDMA) techniques allocate the radio\\nresources of a wireless channel through the use of disjointedness (i.e., orthogonality) in\\nfrequency and time, respectively. On the other hand, the code-division multiple access\\n(CDMA) technique, building on spread spectrum signals and benefiting from their\\nattributes, provides an alternative to the traditional techniques of FDMA and TDMA; it\\ndoes so by not requiring the bandwidth allocation of FDMA nor the time synchronization\\nneeded in TDMA. Rather, CDMA operates on the following principle:\\nThe users of a common wireless channel are permitted access to the channel\\nthrough the assignment of a spreading code to each individual user under the\\numbrella of spread spectrum modulation.\\nThis statement is testimony to what we said in the first paragraph of Section 9.13, namely\\nthat spread spectrum signals provide a novel way of thinking about wireless\\ncommunications.\\nTo elaborate on the way in which CDMA distinguishes itself from FDMA and TDMA\\nin graphical terms, consider Figure 9.32. Parts a and b of the figure depict the ways in\\nwhich the radio resources are distributed in FDMA and TDMA, respectively. To be\\nspecific:',\n",
       "  'question': 'What principle allows users of a common wireless channel to access it through CDMA?\\n',\n",
       "  'answer': 'CDMA operates by assigning a unique spreading code to each user under spread spectrum modulation, enabling them to share the channel without requiring bandwidth allocation like FDMA or time synchronization like TDMA.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 580,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': 'Nonlinear solid-state power\\namplifiers, A39-A43\\nNonnegativeness property, weakly\\nstationary stochastic processing, Nonnegativity function,\\nNormalization function, 99-100\\nNormalization property\\nautocorrelation function,\\nweakly stationary stochastic\\nprocessing,\\nO\\nOFDM (orthogonal frequency\\ndivision multiplexing), PAPR\\nproblem\\nclipping-filtering, PAPR\\nreduction, A37-A38\\nfading channels, 556-557\\nintroduction, A35\\nmaximum PAPR using M-ary\\nPSK, A36-A37\\nproperties of OFDM signals,\\nA35-A36\\nOutage probability\\nfor maximal-ratio combiner,\\nof selection combiner,\\nP\\nPAM (pulse-amplitude modulation),\\n274-277\\nPAPR (peak-to-average power ratio)\\nproblem\\nclipping-filtering, PAPR\\nreduction, A37-A38\\nfading channels, 556-557\\nintroduction, A35\\nmaximum PAPR using M-ary\\nPSK, A36-A37\\nproperties of OFDM signals,\\nA35-A36\\nParameter estimation\\nin additive noise, 124-125\\nintroduction, 122-124\\nPartitioning continuous-time\\nchannels\\ngeometric SNR, 481-482\\nintroduction, 478-481\\nloading the DMT system,\\n482-484\\nPCM (pulse-code modulation)\\nencoding the transmitter,\\nintroduction, 285-286\\ninverse operations in the receiver,\\n288-289\\nquantization of the transmitter,\\n286-288\\nregeneration along the transmitter\\npath, 288-290\\nPCM (pulse-code modulation), noise\\nconsiderations\\nerror threshold, 291-292\\ninformation capacity law,\\n292-294\\nintroduction, 290-291\\nPeriodic signals, Fourier transform,\\n34-36\\nPhase components, narrowband\\nnoise, 191-193\\nPhase delays, 66-69\\nPhase-shift keying (PSK). See PSK\\n(phase-shift keying).\\nPoisson process, weakly stationary\\nstochastic processing, 174-176\\nPolar NRZ signaling,\\nPrediction-error filtering, redundancy\\nreduction\\ndiscrete time structure for\\npredictions, 296-299\\nintroduction, 294-295',\n",
       "  'question': 'What is the maximum peak-to-average power ratio using M-ary PSK?\\n',\n",
       "  'answer': 'The maximum PAPR using M-ary PSK is detailed in the range A36-A37 of the context.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 797,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': '. 1>\\tif transmission of the UEAssistanceInformation message is initiated to provide preference on FR2 UL gap according to 5.7.4.2 or 5.3.5.3: 2>\\tif the UE has a preference for FR2 UL gap configuration: 3>\\tset ul-GapFR2-PatternPreference to the preferred FR2 UL gap pattern; 2>\\telse (if the UE has no preference for the FR2 UL gap configuration): 3>\\tdo not include ul-GapFR2-PatternPreference in the UL-GapFR2-Preference IE. 1>\\tif transmission of the UEAssistanceInformation message is initiated to provide musim-GapPreferenceList and/or musim-GapPriorityPreferenceList and/or musimGap-KeepPreference, or provide MUSIM assistance information for leaving RRC_CONNECTED according to 5.7.4.2 or 5.3.5',\n",
       "  'question': 'What happens if the UE has a preference for FR2 UL gap configuration?\\n',\n",
       "  'answer': 'The ul-GapFR2-PatternPreference is set to the preferred FR2 UL gap pattern.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 627}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications\\n28 t Er(t) Figure 2.5: The received waveform oscillating at frequency f with a slowly varying\\nenvelope at frequency Ds/2.\\ncan approximate the denominator of the second term by r = r0 + vt. Then, combining\\nthe two sinusoids, we get\\nEr(f, t)  2 sin 2f h v ct + (r0d) c i sin 2f[t d c] r0 + vt . (2.13) This is the product of two sinusoids, one at the input frequency f, which is typically\\non the order of GHz, and the other one at fv/c = Ds/2, which might be on the\\norder of 50Hz. Thus, the response to a sinusoid at f is another sinusoid at f with a\\ntime-varying envelope, with peaks going to zeros around every 5 ms (Figure 2.5). The\\nenvelope is at its widest when the mobile is at a peak of the interference pattern and\\nat its narrowest when the mobile is at a valley. Thus, the Doppler spread determines\\nthe rate of traversal across the interference pattern and is inversely proportional to the\\ncoherence time of the channel.\\nWe now see why we have partially ignored the denominator terms in (2.11) and\\n(2.13). When the dierence in the length between two paths changes by a quarter\\nwavelength, the phase dierence between the responses on the two paths changes by\\n/2, which causes a very significant change in the overall received amplitude. Since\\nthe carrier wavelength is very small relative to the path lengths, the time over which\\nthis phase eect causes a significant change is far smaller than the time over which the',\n",
       "  'question': 'What is the order of the input frequency f typically on?\\n',\n",
       "  'answer': 'The input frequency f is typically on the order of GHz.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 29,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '..256) OPTIONAL, dummy2 ENUMERATED {supported} OPTIONAL, twoPortsPTRS-UL ENUMERATED {supported} OPTIONAL, dummy5 SRS-Resources OPTIONAL, dummy3 INTEGER (1..4) OPTIONAL, beamReportTiming SEQUENCE { scs-15kHz ENUMERATED {sym2, sym4, sym8} OPTIONAL, scs-30kHz ENUMERATED {sym4, sym8, sym14, sym28} OPTIONAL, scs-60kHz ENUMERATED {sym8, sym14, sym28} OPTIONAL, scs-120kHz ENUMERATED {sym14, sym28, sym56} OPTIONAL } OPTIONAL, ptrs-DensityRecommendationSetDL SEQUENCE { scs-15kHz PTRS-DensityRecommendationDL OPTIONAL, scs-30kHz PTRS-DensityRecommendationDL OPTIONAL, scs-60kHz PTRS-DensityRecommendationDL OPTIONAL, scs-120kHz PTRS-DensityRecommendationDL OPTIONAL } OPTIONAL, ptrs-DensityRecommendationSetUL SEQUENCE { scs-15kHz PTRS-DensityRecommendationUL OPTIONAL, scs-30kHz PTRS-DensityRecommendationUL OPTIONAL, scs-60kHz PTRS-DensityRecommendationUL OPTIONAL, scs-120kHz PTRS-DensityRecommendationUL OPTIONAL } OPTIONAL, dummy4 DummyH OPTIONAL, aperiodicTRS ENUMERATED {supported} OPTIONAL, ..., [[ dummy6 ENUMERATED {true} OPTIONAL, beamManagementSSB-CSI-RS BeamManagementSSB-CSI-RS OPTIONAL, beamSwitchTiming SEQUENCE { scs-60kHz ENUMERATED {sym14, sym28, sym48, sym224, sym336} OPTIONAL, scs-120kHz ENUMERATED {sym14, sym28, sym48, sym224, sym336} OPTIONAL } OPTIONAL, codebookParameters CodebookParameters OPTIONAL, csi-RS-IM-ReceptionForFeedback CSI-RS-IM-ReceptionForFeedback OPTIONAL, csi-RS-ProcFrameworkForSRS CSI-RS-ProcFrameworkForSRS OPTIONAL, csi-ReportFramework CSI-ReportFramework OPTIONAL, csi-RS-ForTracking CSI-RS-ForTracking OPTIONAL, srs-AssocCSI-RS SEQUENCE (SIZE (1.',\n",
       "  'question': 'What are the supported symbole values for scs-60kHz beamReportTiming?\\n',\n",
       "  'answer': 'The supported symbol values for scs-60kHz beamReportTiming are sym8, sym14, and sym28.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1985}},\n",
       " {'context': \". The value connected can only be indicated if the UE is configured with connectedReporting. propagationDelayDifference Indicates the one-way service link propagation delay difference between serving cell and each neighbour cell included in neighCellInfoList, defined as neighbour cell's service link propagation delay minus serving cell's service link propagation delay, in number of ms. First entry in propagationDelayDifference corresponds to first entry in neighCellInfoList, second entry in propagationDelayDifference corresponds to second entry in neighCellInfoList, and so on. reducedCCsDL Indicates the UE's preference on reduced configuration corresponding to the maximum number of downlink SCells indicated by the field, to address overheating or power saving. When indicated to address overheating, this maximum number includes SCells of the NR MCG, PSCell and SCells of the SCG. This maximum number only includes PSCell and SCells of the SCG in (NG)EN-DC. When indicated to address power saving, this maximum number includes PSCell and SCells of the cell group that this UE assistance information is associated with. The maximum number of downlink SCells can only range up to the current active configuration when indicated to address power savings. reducedCCsUL Indicates the UE's preference on reduced configuration corresponding to the maximum number of uplink SCells indicated by the field, to address overheating or power saving. When indicated to address overheating, this maximum number includes SCells of the NR MCG, PSCell and SCells of the SCG. This maximum number only includes PSCell and SCells of the SCG in (NG)EN-DC. When indicated to address power saving, this maximum number includes PSCell and SCells of the cell group that this UE assistance information is associated with. The maximum number of uplink SCells can only range up to the current active configuration when indicated to address power savings\",\n",
       "  'question': 'What does propagationDelayDifference indicate?\\n    ',\n",
       "  'answer': \"It shows the one-way service link propagation delay difference between the serving cell and each neighbor cell, calculated as the neighbor cell's service link propagation delay minus the serving cell's service link propagation delay, measured in milliseconds.\",\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1069}},\n",
       " {'context': '3\\tUE procedure for reporting HARQ-ACK In this clause, for the purpose of determining a PUCCH resource for a PUCCH transmission in a slot using a PUCCH resource indicator field in a DCI format that schedules a PDSCH reception, and for the purpose of determining the slot for the PUCCH transmission -\\ta UE is assumed to generate HARQ-ACK information regardless of whether or not the PDSCH reception provides a transport block for a HARQ process with disabled HARQ-ACK information as indicated by downlinkHARQ-FeedbackDisabled, if provided -\\ta UE is assumed to not generate HARQ-ACK information associated with a G-RNTI for multicast or a G-CS-RNTI with disabled HARQ-ACK information as described in clause 18. The UE determines a number of HARQ-ACK information bits as described in clauses 9.1 through 9.1.5 and a corresponding set of PUCCH resources as described in clause 9.2.1. If , the UE does not transmit a PUCCH that only includes HARQ-ACK information bits. A UE does not expect to transmit more than one PUCCH with HARQ-ACK information in a slot per priority index, if the UE is not provided ackNackFeedbackMode = separate. For DCI format 1_0, the PDSCH-to-HARQ_feedback timing indicator field values map to {1, 2, 3, 4, 5, 6, 7, 8} for SCS configuration of PUCCH transmission , to {7, 8, 12, 16, 20, 24, 28, 32} for , and to {13, 16, 24, 32, 40, 48, 56, 64} for . For a unicast DCI format, other than DCI format 1_0, the PDSCH-to-HARQ_feedback timing indicator field values, if present, map to values for a set of number of slots provided by dl-DataToUL-ACK, dl-DataToUL-ACK-r16, or dl-DataToUL-ACK-DCI-1-2, or dl-DataToUL-ACK-r17, or dl-DataToUL-ACK-DCI-1-2-r17, or dl-DataToUL-ACK-v1700 as defined in Table 9.2.3-1. If the DCI format indicates a cell for the PUCCH transmission, as described in clause 9',\n",
       "  'question': 'What values do PDSCH-to-HARQ_feedback timing indicator field map to for SCS configuration of PUCCH transmission?\\n',\n",
       "  'answer': 'The PDSCH-to-HARQ_feedback timing indicator field maps to the values {1, 2, 3, 4, 5, 6, 7, 8} for SCS configuration of PUCCH transmission.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38213-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 289}},\n",
       " {'context': '.e., as specified in 5.3.5.10) from the UE Inactive AS context, if stored; 2>\\trestore the masterCellGroup, mrdc-SecondaryCellGroup, if stored, and pdcp-Config from the UE Inactive AS context; 2>\\tconfigure lower layers to consider the restored MCG and SCG SCell(s) (if any) to be in deactivated state; 1>\\tdiscard the UE Inactive AS context; 1>\\tstore the used nextHopChainingCount value associated to the current KgNB; 1>\\tif the UE is configured to receive MBS multicast in RRC_INACTIVE: 2>\\treset MAC; 1>\\tif sdt-MAC-PHY-CG-Config is configured: 2>\\tinstruct the MAC entity to stop the cg-SDT-TimeAlignmentTimer, if it is running; 2>\\tinstruct the MAC entity to start the timeAlignmentTimer associated with the PTAG indicated by tag-Id, if it is not running; 1>\\tif srs-PosRRC-Inactive is configured: 2>\\tinstruct the MAC entity to stop inactivePosSRS-TimeAlignmentTimer, if it is running; 1>\\tif srs-PosRRC-InactiveValidityAreaNonPreConfig is configured; or 1>\\tif srs-PosRRC-InactiveValidityAreaPreConfigList is configured and if the cell is not listed in srs-PosConfigValidityArea: 2>\\tinstruct the MAC entity to stop inactivePosSRS-ValidityAreaTAT, if it is running; 1>\\trelease the suspendConfig except the ran-NotificationAreaInfo; 1>\\tif the RRCResume includes the masterCellGroup: 2>\\tperform the cell group configuration for the received masterCellGroup according to 5.3.5.5; 1>\\tif the RRCResume includes the mrdc-SecondaryCellGroup: 2>\\tif the received mrdc-SecondaryCellGroup is set to nr-SCG: 3>\\tperform the RRC reconfiguration according to 5.3.5.3 for the RRCReconfiguration message included in nr-SCG; 2>\\tif the received mrdc-SecondaryCellGroup is set to eutra-SCG: 3>\\tperform the RRC connection reconfiguration as specified in TS 36.331 [10], clause 5.3.5.3 for the RRCConnectionReconfiguration message included in eutra-SCG; 1>\\tif the RRCResume includes the radioBearerConfig: 2>\\tperform the radio bearer configuration according to 5.3.5',\n",
       "  'question': 'What happens to the MAC entity when sdt-MAC-PHY-CG-Config is configured?\\n',\n",
       "  'answer': 'The MAC entity is instructed to stop the cg-SDT-TimeAlignmentTimer if it is running and start the timeAlignmentTimer associated with the PTAG indicated by tag-Id if it is not running.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 368}},\n",
       " {'context': 'Chapter\\nConversion of Analog Waveforms into Coded Pulses\\nprovision for synchronization by adding an extra pulse of sufficient amplitude and also 1 s duration.\\nThe highest frequency component of each voice signal is 3.4 kHz.\\na. Assuming a sampling rate of 8 kHz, calculate the spacing between successive pulses of the\\nmultiplexed signal.\\nb. Repeat your calculation assuming the use of Nyquist rate sampling. Twelve different message signals, each with a bandwidth of 10 kHz, are to be multiplexed and\\ntransmitted. Determine the minimum bandwidth required if the multiplexing/modulation method\\nused is time-division multiplexing (TDM), which was discussed in Chapter 1.\\nPulse-Code Modulation A speech signal has a total duration of 10 s. It is sampled at the rate of 8 kHz and then encoded. The\\nsignal-to-(quantization) noise ratio is required to be 40 dB. Calculate the minimum storage capacity\\nneeded to accommodate this digitized speech signal. Consider a uniform quantizer characterized by the input-output relation illustrated in Figure 6.9a.\\nAssume that a Gaussian-distributed random variable with zero mean and unit variance is applied to\\nthis quantizer input.\\na. What is the probability that the amplitude of the input lies outside the range -4 to +4?\\nb. Using the result of part a, show that the output SNR of the quantizer is given by\\nwhere R is the number of bits per sample. Specifically, you may assume that the quantizer input\\nextends from -4 to 4. Compare the result of part b with that obtained in Example 2. A PCM system uses a uniform quantizer followed by a 7-bit binary encoder. The bit rate of the\\nsystem is equal to 50  106 bits/s.\\na. What is the maximum message bandwidth for which the system operates satisfactorily?\\nb. Determine the output signal-to-(quantization) noise when a full-load sinusoidal modulating wave\\nof frequency 1 MHz is applied to the input. Show that with a nonuniform quantizer the mean-square value of the quantization error is\\napproximately equal to',\n",
       "  'question': 'What is the highest frequency component of each voice signal?\\n',\n",
       "  'answer': 'The highest frequency component of each voice signal is 3.4 kHz.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 334,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': \"2\\tModulation The block of bits shall be QPSK modulated as described in clause5.1.3, resulting in a block of complex-valued modulation symbols where .\\n3\\tMapping to physical resources Mapping to physical resources is described in clause 8.4.3.\\n4\\tPhysical sidelink feedback channel\\n1\\tGeneral\\n1\\tSequence generation The sequence shall be generated according to where is given by clause 6.3.2.2 with the following exceptions: - is given by clause 16.3 of [5, TS 38.213]; - is given by clause 16.3 of [5, TS 38.213]; - is given by - if the higher-layer parameter sl-TransmissionStructureForPSFCH is configured and set to 'dedicatedInterlace' and where is the resource block number within the interlace; - otherwise -\\t; - is the index of the OFDM symbol in the slot that corresponds to the second OFDM symbol of the PSFCH transmission in the slot given by [5, TS 38.213]; - and with given by the higher-layer parameter sl-PSFCH-HopID if configured; otherwise, . - with given by the higher-layer parameter sl-PSFCH-HopID if configured; otherwise, .\",\n",
       "  'question': 'What is the clause number for the sequence generation process?\\n',\n",
       "  'answer': 'The sequence generation process is defined according to clause 6.3.2.2, with exceptions specified in clauses 16.3 of [5, TS 38.213] and other higher-layer parameters like sl-TransmissionStructureForPSFCH and sl-PSFCH-HopID.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38211-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 137}},\n",
       " {'context': \"add to the set of time indices for PT-RS\\nincrement by one\\nrepeat from step 2 above as long as is inside the PDSCH allocation where . For the purpose of PT-RS mapping, the resource blocks allocated for PDSCH transmission are numbered from 0 to from the lowest scheduled resource block to the highest. The corresponding subcarriers in this set of resource blocks are numbered in increasing order starting from the lowest frequency from 0 to . The subcarriers to which the UE shall assume the PT-RS is mapped are given by where - - is given by Table 7.4.1.2.2-1 for the DM-RS port associated with the PT-RS port according to clause 5.1.6.3 in [6, TS 38.214]. If the higher-layer parameter resourceElementOffset in the PTRS-DownlinkConfig IE is not configured, the column corresponding to 'offset00' shall be used. - is the RNTI associated with the DCI scheduling the transmission - is the number of resource blocks scheduled - is given by [6, TS 38.214]. Table 7.4.1.2.2-1: The parameter . DM-RS antenna port DM-RS Configuration type DM-RS Configuration type resourceElementOffset resourceElementOffset offset00 offset01 offset10 offset11 offset00 offset01 offset10 offset11 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\\n3\\tDemodulation reference signals for PDCCH\\n1\\tSequence generation The UE shall assume the reference-signal sequence for OFDM symbol is defined by . where the pseudo-random sequence is defined in clause 5.2.1. The pseudo-random sequence generator shall be initialized with where is the OFDM symbol number within the slot, is the slot number within a frame, and - is given by the higher-layer parameter pdcch-DMRS-ScramblingID if provided; - is given by the higher-layer parameter pdcch-DMRS-ScramblingID if configured for a common search space in a common MBS frequency resource; - otherwise.\",\n",
       "  'question': 'What are the resource blocks allocated for PDSCH transmission numbered from?\\n',\n",
       "  'answer': 'From 0 to N, from the lowest scheduled resource block to the highest.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38211-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 111}},\n",
       " {'context': '3\\tActions related to transmission of RRCResumeRequest or RRCResumeRequest1 message The UE shall set the contents of RRCResumeRequest or RRCResumeRequest1 message as follows: 1>\\tif useFullResumeID is signalled in SIB1: 2>\\tselect RRCResumeRequest1 as the message to use; 2>\\tset the resumeIdentity to the stored fullI-RNTI value; 1>\\telse: 2>\\tselect RRCResumeRequest as the message to use; 2>\\tset the resumeIdentity to the stored shortI-RNTI value; 1>\\trestore the RRC configuration, RoHC state, the EHC context(s), the UDC state, the stored QoS flow to DRB mapping rules and the KgNB and KRRCint keys from the stored UE Inactive AS context except for the following: -\\tmasterCellGroup; -\\tmrdc-SecondaryCellGroup, if stored; and -\\tpdcp-Config; 1>\\tset the resumeMAC-I to the 16 least significant bits of the MAC-I calculated: 2>\\tover the ASN.1 encoded as per clause 8 (i.e., a multiple of 8 bits) VarResumeMAC-Input; 2>\\twith the KRRCint key in the UE Inactive AS Context and the previously configured integrity protection algorithm; and 2>\\twith all input bits for COUNT, BEARER and DIRECTION set to binary ones; 1>\\tderive the KgNB key based on the current KgNB key or the NH, using the nextHopChainingCount value received in the previous RRCRelease message and stored in the UE Inactive AS Context, as specified in TS 33.501 [11]; 1>\\tderive the KRRCenc key, the KRRCint key, the KUPint key and the KUPenc key; 1>\\tconfigure lower layers to apply integrity protection for all radio bearers except SRB0 and MRBs using the configured algorithm and the KRRCint key and KUPint key derived in this clause immediately, i.e., integrity protection shall be applied to all subsequent messages received and sent by the UE; NOTE 1:\\tOnly DRBs with previously configured UP integrity protection shall resume integrity protection',\n",
       "  'question': 'What message is selected if useFullResumeID is signalled in SIB1?\\n',\n",
       "  'answer': 'RRCResumeRequest1 is selected if useFullResumeID is signalled in SIB1.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 364}},\n",
       " {'context': '.213 2021-03 RAN#91-e RP-210055 - F Correction for cancellation due to PDSCH/CSI-RS/SFI 2021-03 RAN#91-e RP-210049 - F Correction of Type-3 HARQ-ACK codebook generation for a PDSCH with one transport block for a configuration with a maximum number of two TBs 2021-03 RAN#91-e RP-210049 - F Correction of UE reception of DL control when a DCI indicates a request for a Type-3 HARQ-ACK codebook report without scheduling PDSCH 2021-03 RAN#91-e RP-210049 - D Correction on PUCCH power control for enhanced Type-2 HARQ-ACK codebook and for Type-3 HARQ-ACK codebook 2021-03 RAN#91-e RP-210052 - F CR on HARQ-ACK 2021-03 RAN#91-e RP-210049 - F Correction on search space set group switching without channel occupancy duration field 2021-03 RAN#91-e RP-210052 - F Corrections on Scell BFR in Rel-16 2021-03 RAN#91-e RP-210058 - F Correction on uplink Tx switching 2021-03 RAN#91-e RP-210050 - F Corrections related to prioritization between uplink and sidelink 2021-03 RAN#91-e RP-210050 - F Determination of indexes for slots for S-SS/PSBCH block transmission(s) 2021-03 RAN#91-e RP-210050 - F Restrictions of the slots for S-SSB transmission/reception 2021-03 RAN#91-e RP-210051 - F CR on Timing for secondary cell activation / deactivation with sub-slot PUCCH 2021-03 RAN#91-e RP-210051 - F CR on number of PUCCHs with HARQ-ACK in a slot 2021-03 RAN#91-e RP-210050 F Correction of SL HARQ-ACK information reporting to the gNB in Mode 2021-03 RAN#91-e RP-210049 - F Correction on LBT Type and CP Extension Indication for Semi-Static Channel Occupancy in RAR 2021-03 RAN#91-e RP-210054 - F CR to 38.213 on PRACH handling for NR-DC power control 2021-03 RAN#91-e RP-210054 - F CR to 38.213 on HARQ-ACK priority determination for SCell dormancy indication 2021-03 RAN#91-e RP-210055 - F',\n",
       "  'question': 'What specific technical aspect was corrected in the document numbered 2021-03 RAN#91-e RP-210052?\\n',\n",
       "  'answer': 'The document 2021-03 RAN#91-e RP-210052 focused on fixing errors related to HARQ-ACK (Hybrid ARQ Acknowledgment) processes, ensuring more reliable communication by addressing flaws in the HARQ-ACK mechanism.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38213-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 665}},\n",
       " {'context': '. If the field is set to 1, the octet containing the field SSB index is present; otherwise, the octet is omitted; -\\tResource Serving Cell IDi: This field indicates the identity of the Serving Cell on which the resource used for spatial relationship derivation for the ith Positioning SRS resource is located. The length of the field is 5 bits; -\\tResource BWP IDi: This field indicates a UL BWP as the codepoint of the DCI bandwidth part indicator field as specified in TS 38.212 [9], on which the resource used for spatial relationship derivation for the ith Positioning SRS resource is located. The length of the field is 2 bits.',\n",
       "  'question': 'What does setting the field to 1 indicate?\\n',\n",
       "  'answer': 'The presence of the octet containing the SSB index.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38321-i50.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 477}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications then it suces to consider covariance matrices of the form in (10.26); i.e., user k has\\nthe transmit covariance matrix:\\nKxk = UtkkU tk, (10.27) where the diagonal entries of k represent the powers allocated to the data streams,\\none in each of the angular windows (so their sum is equal to Pk, the power constraint\\nfor user k). (See Exercise 10.13.) With this choice of transmit strategy, the pair of\\nrates (R1, R2) at which users can jointly reliably communicate is constrained, as in\\n(10.12) and (10.13), by:\\nRk  E  log det  Inr + 1 N0 HkKxkH k  , k = 1, 2, (10.28) R1 + R2  E \" log det  Inr + 1 N0 2 X k=1 HkKxkH k !# . (10.29) This constraint forms a pentagon and the corner points are achieved by the architec-\\nture of linear MMSE filter combined with successive cancellation of data streams (c.f.\\nFigure 10.12).\\nThe capacity region is the convex hull of the union of these pentagons, one for\\neach power allocation to the data streams of the users (i.e., the diagonal entries of\\n1, 2). In the point-to-point MIMO channel, with some additional symmetry (such\\nas in the i.i.d. Rayleigh fading model), we have seen that the capacity achieving power\\nallocation is equal powers to the data streams (c.f. (8.12)). An analogous result holds\\nin the MIMO uplink as well. With i.i.d. Rayleigh fading for all the users, the equal\\npower allocation to the data streams, i.e.,\\nKxk = Pk ntk Intk, (10.30) achieves the entire capacity region; thus in this case the capacity region is simply a\\npentagon. (See Exercise 10.14.)\\nThe analysis of the capacity region with full CSI is very similar to our previous\\nanalysis (c.f. Section 10.1.5). Due to the increase in number of parameters to feedback\\n(so that the users can change their transmit strategies as a function of the time varying\\nchannels), this scenario is also somewhat less relevant in engineering practice. Downlink with Multiple Transmit Antennas',\n",
       "  'question': 'What does the diagonal entry of Kxk represent in the context of wireless communications?\\n',\n",
       "  'answer': 'The diagonal entry of Kxk represents the powers allocated to the data streams in each angular window for user k.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 523,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '-\\tNotificationMessageSidelink -\\tRemoteUEInformationSidelink -\\tRRCReconfigurationSidelink -\\tRRCReconfigurationCompleteSidelink -\\tRRCReconfigurationFailureSidelink -\\tUEAssistanceInformationSidelink -\\tUECapabilityEnquirySidelink -\\tUECapabilityInformationSidelink -\\tUEInformationRequestSidelink -\\tUEInformationResponseSidelink -\\tUuMessageTransferSidelink -\\tEnd of PC5-RRC-Definitions 7\\tVariables and constants -\\tNR-UE-Variables -\\tVarAppLayerIdleConfig -\\tVarAppLayerPLMN-ListConfig -\\tVarConditionalReconfig -\\tVarConnEstFailReport -\\tVarConnEstFailReportList -\\tVarLogMeasConfig -\\tVarLogMeasReport -\\tVarLTM-ServingCellNoResetID -\\tVarLTM-ServingCellUE-MeasuredTA-ID -\\tVarMeasConfig -\\tVarMeasConfigSL -\\tVarMeasIdleConfig -\\tVarMeasIdleReport -\\tVarMeasReportList -\\tVarMeasReportListSL -\\tVarMeasReselectionConfig -\\tVarMobilityHistoryReport -\\tVarPendingRNA-Update -\\tVarRA-Report -\\tVarResumeMAC-Input -\\tVarRLF-Report -\\tVarServingSecurityCellSetID -\\tVarShortMAC-Input -\\tVarSuccessHO-Report -\\tVarSuccessPSCell-Report -\\tVarTSS-Info -\\tEnd of NR-UE-Variables 8\\tProtocol data unit abstract syntax 9\\tSpecified and default radio configurations -\\tNR-Sidelink-Preconf -\\tSL-PreconfigurationNR -\\tEnd of NR-Sidelink-Preconf -\\tSL-AccessInfo-L2U2N 10\\tGeneric error handling 11\\tRadio information related interactions between network nodes -\\tCG-CandidateList -\\tHandoverCommand -\\tHandoverPreparationInformation -\\tCG-Config -\\tCG-ConfigInfo -\\tMeasurementTimingConfiguration -\\tUERadioPagingInformation -\\tUERadioAccessCapabilityInformation -\\tL1-MeasConfigNRDC -\\tResourceConfigNRDC -\\tMultiplicity and type constraints definitions -\\tEnd of NR-InterNodeDefinitions 12\\tProcessing delay requirements for RRC procedures Annex A (informative):\\tGuidelines mainly on use of ASN',\n",
       "  'question': 'What is the purpose of UEAssistanceInformationSidelink?\\n',\n",
       "  'answer': 'UEAssistanceInformationSidelink is used to provide the network with additional assistance information from the user equipment for enhanced connectivity and performance in Sidelink communication scenarios.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 10}},\n",
       " {'context': '.1 review Q2 RP-104 RP-241551 A Correction on posSIB(s) acquisition [SI-SCHEDULING] RP-104 RP-241564 F Miscellaneous corrections for eRedCap RP-104 RP-241555 A Correction on SidelinkUEInformationNR RP-104 RP-241551 A Introduction of Inter-node Coordination on the Aggregated Bandwidth for the NR-DC (r18) RP-104 RP-241553 A Clarification on usage of LEO or NGSO RP-104 RP-241551 A Introduction of new intra-band EN-DC capabilities for inter-band EN-DC RP-104 RP-241544 F Correction on TRS in idle and inactive RP-104 RP-241554 A Correction on tx profile for SL DRX RP-104 RP-241566 F Miscellaneous and RRC Positioning RILs based Corrections RP-104 RP-241558 F Miscellaneous corrections to Rel-18 NR NTN RP-104 RP-241543 F [B021] Missing posSibType2-17a in list of posSIB types [PosL2RemoteUE] RP-104 RP-241544 F Clarification RLM/BFD relaxation and short DRX RP-104 RP-241561 F Correction to MIMO Evolution RP-104 RP-241568 F Correction on NR MUSIM enhancements RP-104 RP-241551 A Clarification of configured grant in shared spectrum RP-104 RP-241576 F Correction of Enhancement on NR QoE management and optimizations for diverse services RP-104 RP-241567 F [S081] Clarification for order of list in PagingRecordList RP-104 RP-241550 A Correction to Positioning SRS Configuration RP-104 RP-241555 A Correction on the UL TEG report RP-104 RP-241544 F Miscellaneous non-controversial corrections Set XXI RP-104 RP-241543 B Enhancements to measurement report [meas_report_enh] RP-104 RP-241564 F Clarification on RACH-ConfigCommon for PDCCH order based CFRA and SI request RP-104 RP-241563 - F Miscellaneous RRC corrections for NCR RP-104 RP-241552 A Adding PCI and ARFCN of target cell for intra-RAT SHR RP-104 RP-241552 A Misclassification of RLF reports as Too Early HO failure RP-104 RP-241575 F Corrections to 38331 for Rel-18 SONMDT RP-104 RP-241543 F Correction on the configuration of Redcap CFR [RedCapMBS_Bcast] RP-104 RP-241549 - A Clarification on SIB1 reception for ETWS/CMAS RP-104',\n",
       "  'question': 'What is the specific item number for the correction on posSIB types?\\n',\n",
       "  'answer': 'RP-104 RP-241543 F [B021] Missing posSibType2-17a in list of posSIB types [PosL2RemoteUE]',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 2556}},\n",
       " {'context': 'The use of requires having to handle complex notations. Canonical Representation of Band-Pass Signals\\nBy definition, the real part of the pre-envelope s+(t) is equal to the original band-pass\\nsignal s(t). We may therefore express the band-pass signal s(t) in terms of its\\ncorresponding complex envelope as\\n(2.66)\\nwhere the operator Re[.] denotes the real part of the quantity enclosed inside the square\\nbrackets. Since, in general, is a complex-valued quantity, we emphasize this property\\nby expressing it in the Cartesian form\\n(2.67)\\nwhere sI(t) and sQ(t) are both real-valued low-pass functions; their low-pass property is\\ninherited from the complex envelope\\n. We may therefore use (2.67) in (2.66) to\\nexpress the original band-pass signal s(t) in the canonical or standard form\\n(2.68)\\nWe refer to sI(t) as the in-phase component of the band-pass signal s(t) and refer to sQ(t) as\\nthe quadrature-phase component or simply the quadrature component of the signal s(t).\\ns+ t( ) s t( ) j2fct ( ) exp = s t( ) s t( ) s t( ) s t( ) s t( ) s t( ) s t( ) s t( ) s t( ) s t( ) Re s t( ) j2fct ( ) exp [ ] = s t( ) s t( ) sI t( ) jsQ t( ) + = s t( ) s t( ) sI t( ) 2fct ( ) sQ t( ) 2fct ( ) sin - cos =',\n",
       "  'question': 'What are the two real-valued low-pass functions that represent the complex envelope of a band-pass signal?\\n',\n",
       "  'answer': 'The in-phase component sI(t) and the quadrature-phase component sQ(t) are the two real-valued low-pass functions that represent the complex envelope of a band-pass signal.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 69,\n",
       "   'chunk_idx': 1}},\n",
       " {'context': \". preferredDRX-InactivityTimer Indicates the UE's preferred DRX inactivity timer length for power saving. Value in ms (milliSecond). ms0 corresponds to 0, ms1 corresponds to 1 ms, ms2 corresponds to 2 ms, and so on. If the field is absent from the DRX-Preference IE, it is interpreted as the UE having no preference for the DRX inactivity timer. If secondary DRX group is configured, the preferredDRX-InactivityTimer only applies to the default DRX group. preferredDRX-LongCycle Indicates the UE's preferred long DRX cycle length for power saving. Value in ms. ms10 corresponds to 10ms, ms20 corresponds to 20 ms, ms32 corresponds to 32 ms, and so on. If preferredDRX-ShortCycle is provided, the value of preferredDRX-LongCycle shall be a multiple of the preferredDRX-ShortCycle value. If the field is absent from the DRX-Preference IE, it is interpreted as the UE having no preference for the long DRX cycle. preferredDRX-ShortCycle Indicates the UE's preferred short DRX cycle length for power saving. Value in ms. ms2 corresponds to 2ms, ms3 corresponds to 3 ms, ms4 corresponds to 4 ms, and so on. If the field is absent from the DRX-Preference IE, it is interpreted as the UE having no preference for the short DRX cycle. preferredDRX-ShortCycleTimer Indicates the UE's preferred short DRX cycle timer for power saving. Value in multiples of preferredDRX-ShortCycle. A value of 1 corresponds to preferredDRX-ShortCycle, a value of 2 corresponds to 2 * preferredDRX-ShortCycle and so on. If the field is absent from the DRX-Preference IE, it is interpreted as the UE having no preference for the short DRX cycle timer. A preference for the short DRX cycle is indicated when a preference for the short DRX cycle timer is indicated. preferredK0 Indicates the UE's preferred value of k0 (slot offset between DCI and its scheduled PDSCH - see TS 38.214 [19], clause 5.1.2.1) for cross-slot scheduling for power saving. Value is defined for each subcarrier spacing (numerology) in units of slots\",\n",
       "  'question': \"What does ms0 correspond to in the UE's preferred DRX inactivity timer?\\n\",\n",
       "  'answer': \"ms0 corresponds to 0 milliseconds in the UE's preferred DRX inactivity timer, indicating no preference for a specific timer length.\",\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1067}},\n",
       " {'context': '. The UE shall select the first listed band which it supports in the frequencyBandList field to represent the NR neighbour carrier frequency. includeBeamMeasurements Indicates whether or not the UE shall include beam measurements in the NR idle/inactive measurement results. maxNrofRS-IndexesToReport Max number of beam indices to include in the idle/inactive measurement result. measCellListEUTRA Indicates the list of E-UTRA cells which the UE is requested to measure and report for idle/inactive measurements. measCellListNR Indicates the list of NR cells which the UE is requested to measure and report for idle/inactive measurements. measIdleCarrierListEUTRA Indicates the E-UTRA carriers to be measured during RRC_IDLE or RRC_INACTIVE. measIdleCarrierListNR Indicates the NR carriers to be measured during RRC_IDLE or RRC_INACTIVE. measIdleCarrierListNR-LessThan5MHz Indicates the NR carriers to be measured during RRC_IDLE or RRC_INACTIVE for the cell(s) supporting 12 PRB, 15 PRB or 20 PRB transmission bandwidth configuration as defined in TS 38.101-1 [15], TS 38.211 [16] and TS 38.213 [13]. Total number of MeasIdleCarrierNR included in measIdleCarrierListNR and measIdleCarrierListNR-LessThan5MHz does not exceed maxFreqIdle-r16. measIdleDuration Indicates the duration for performing idle/inactive measurements while in RRC_IDLE or RRC_INACTIVE. Value sec10 correspond to 10 seconds, value sec30 to 30 seconds and so on. measIdleValidityDuration, measReselectionValidityDuration Indicates time values for UE to determine validity of reported idle/inactive and reselection measurements as defined in TS 38.133[14]. Value s5 correspond to 5 seconds, value s10 correspond to 10 seconds and so on. measReselectionCarrierListNR Indicates the NR carriers for reselection measurement reporting',\n",
       "  'question': 'What does the UE select to represent the NR neighbour carrier frequency?\\n',\n",
       "  'answer': 'The UE selects the first listed band it supports in the frequencyBandList field.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1399}},\n",
       " {'context': '582 Chapter 10 Error-Control Coding Notation Many of the codes described in this chapter are binary codes, for which the alphabet\\nconsists only of binary symbols 0 and 1. In such a code, the encoding and decoding\\nfunctions involve the binary arithmetic operations of modulo-2 addition and multiplication\\nperformed on codewords in the code.\\nThroughout this chapter, we use the ordinary plus sign (+) to denote modulo-2 addition.\\nThe use of this terminology will not lead to confusion because the whole chapter relies on\\nbinary arithmetic. In so doing, we avoid use of the special symbol as we did in previous\\nparts of the book. Thus, according to the notation used in this chapter, the rules for\\nmodulo-2 addition are as follows:\\nBecause 1 + 1 = 0, it follows that 1 = -1. Hence, in binary arithmetic, subtraction is the\\nsame as addition. The rules for modulo-2 multiplication are as follows:\\nDivision is trivial, in that we have\\nand division by 0 is not permitted. Modulo-2 addition is the EXCLUSIVE-OR operation\\nin logic and modulo-2 multiplication is the AND operation.\\n4 Linear Block Codes By definition: A code is said to be linear if any two codewords in the code can be added in\\nmodulo-2 arithmetic to produce a third codeword in the code.\\nConsider, then, an (n,k) linear block code, in which k bits of the n code bits are always\\nidentical to the message sequence to be transmitted. The (n - k) bits in the remaining\\nportion are computed from the message bits in accordance with a prescribed encoding rule\\nthat determines the mathematical structure of the code. Accordingly, these (n - k) bits are\\nreferred to as parity-check bits. Block codes in which the message bits are transmitted in\\nunaltered form are called systematic codes. For applications requiring both error detection\\nand error correction, the use of systematic block codes simplifies implementation of the\\ndecoder.\\nLet m0, m1, , mk - 1 constitute a block of k arbitrary message bits. Thus, we have 2k',\n",
       "  'question': 'What are the two binary symbols that make up the alphabet in binary codes?\\n',\n",
       "  'answer': 'The alphabet in binary codes consists of the binary symbols 0 and 1, which are used in encoding and decoding functions involving modulo-2 arithmetic operations.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 602,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': '. Signalling radio bearer: SRB1 RLC-SAP: AM Logical channel: DCCH Direction: Network to UE UEInformationRequest message -- ASN1START -- TAG-UEINFORMATIONREQUEST-START UEInformationRequest-r16 ::= SEQUENCE { rrc-TransactionIdentifier RRC-TransactionIdentifier, criticalExtensions CHOICE { ueInformationRequest-r16 UEInformationRequest-r16-IEs, criticalExtensionsFuture SEQUENCE {} } } UEInformationRequest-r16-IEs ::= SEQUENCE { idleModeMeasurementReq-r16 ENUMERATED{true} OPTIONAL, -- Need N logMeasReportReq-r16 ENUMERATED {true} OPTIONAL, -- Need N connEstFailReportReq-r16 ENUMERATED {true} OPTIONAL, -- Need N ra-ReportReq-r16 ENUMERATED {true} OPTIONAL, -- Need N rlf-ReportReq-r16 ENUMERATED {true} OPTIONAL, -- Need N mobilityHistoryReportReq-r16 ENUMERATED {true} OPTIONAL, -- Need N lateNonCriticalExtension OCTET STRING OPTIONAL, nonCriticalExtension UEInformationRequest-v1700-IEs OPTIONAL } UEInformationRequest-v1700-IEs ::= SEQUENCE { successHO-ReportReq-r17 ENUMERATED {true} OPTIONAL, -- Need N coarseLocationRequest-r17 ENUMERATED {true} OPTIONAL, -- Need N nonCriticalExtension UEInformationRequest-v1800-IEs OPTIONAL } UEInformationRequest-v1800-IEs ::= SEQUENCE { flightPathInfoReq-r18 FlightPathInfoReportConfig-r18 OPTIONAL, -- Need N successPSCell-ReportReq-r18 ENUMERATED {true} OPTIONAL, -- Need N reselectionMeasurementReq-r18 ENUMERATED {true} OPTIONAL, -- Need N validatedMeasurementsReq-r18 ENUMERATED {true} OPTIONAL, -- Need N nonCriticalExtension SEQUENCE {} OPTIONAL } FlightPathInfoReportConfig-r18 ::= SEQUENCE { maxWayPointNumber-r18 INTEGER (1..maxWayPoint-r18), includeTimeStamp-r18 ENUMERATED {true} OPTIONAL -- Need N } -- TAG-UEINFORMATIONREQUEST-STOP -- ASN1STOP UEInformationRequest-IEs field descriptions coarseLocationRequest This field is used to request UE to report coarse location information. connEstFailReportReq This field is used to indicate whether the UE shall report information about the connection failure',\n",
       "  'question': 'What is the purpose of the coarseLocationRequest field in UEInformationRequest?\\n',\n",
       "  'answer': 'The coarseLocationRequest field is used to request the UE to report coarse location information.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1079}},\n",
       " {'context': '.3.1.1.2-8B. The mapping order of CSI fields of one report for TDCP reporting is provided in Table 6.3.2.1.2-3C. The mapping order of CSI fields of one report for SSBRI/RSRP reporting for L1/L2triggered mobility is provided in Table 6.3.1.1.2-8C. The procedure in clause 6.3.2 described for CSI part 1 is also applicable for one report for CRI/RSRP, SSBRI/RSRP, CRI/SINR, SSBRI/SINR reporting, or TDCP reporting. Table 6.3.2.1.2-3: Mapping order of CSI fields of one CSI report, CSI part CSI report number CSI fields CSI report #n CSI part CRI as in Tables 6.3.1.1.2-3/4/6, if reported Rank Indicator as in Tables 6.3.1.1.2-3/4/5 or 6.3.2.1.2-8/8A/8B/9/9A, if reported Wideband CQI for the first TB as in Tables 6.3.1.1.2-3/4/5 or 6.3.2.1.2-8/8A/8B/9/9A, if reported Subband differential CQI for the first TB with increasing order of subband number as in Tables 6.3.1.1.2-3/4/5 or 6.3.2.1.2-8/8A/8B/9/9A, if reported Indicator of the number of non-zero wideband amplitude coefficients for layer 0 as in Table 6.3.1.1.2-5, if reported Indicator of the number of non-zero wideband amplitude coefficients for layer 1 as in Table 6.3.1.1.2-5 (if the rank according to the reported RI is equal to one, this field is set to all zeros), if 2-layer PMI reporting is allowed according to the rank restriction in Clauses 5.2.2.2.3 and 5.2.2.2.4 [6, TS 38.214] and if reported Indicator of the selected CSI-RS resources by a bitmap with bits, this field is present only if and restrictedCMR-Selection is configured to OFF Indicator of selected value combination or value combination with bitwidth of , this field is present only if Indicator of the total number of non-zero coefficients summed across all layers as in Tables6.3.2.1.2-8/8A/8B/9/9A, if reported NOTE:\\tSubbands for given CSI report n indicated by the higher layer parameter csi-ReportingBand are numbered continuously in the increasing order with the lowest subband of csi-ReportingBand as subband 0. Table 6.3.2.1',\n",
       "  'question': 'What is the mapping order of CSI fields for one report in TDCP reporting?\\n',\n",
       "  'answer': 'It is provided in Table 6.3.2.1.2-3C.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38212-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 78}},\n",
       " {'context': '71\\tEnhanced Unified TCI States Activation/Deactivation MAC CE for Separate TCI States The Enhanced Unified TCI States Activation/Deactivation MAC CE CE for Separate TCI States is identified by a MAC subheader with eLCID as specified in Table 6.2.1-1b. It has a variable size consisting of following fields: -\\tServing Cell ID: This field indicates the identity of the Serving Cell for which the MAC CE applies. The length of the field is 5 bits. If the indicated Serving Cell is configured as part of a simultaneousU-TCI-UpdateList1, simultaneousU-TCI-UpdateList2, simultaneousU-TCI-UpdateList3 or simultaneousU-TCI-UpdateList4 as specified in TS 38.331 [5], this MAC CE applies to all theServing Cells in the set simultaneousU-TCI-UpdateList1, simultaneousU-TCI-UpdateList2, simultaneousU-TCI-UpdateList3 or simultaneousU-TCI-UpdateList4, respectively; -\\tDL BWP ID: This field indicates a DL BWP for which the MAC CE applies as the codepoint of the DCI bandwidth part indicator field as specified in TS 38.212 [9]. The length of the BWP ID field is 2 bits; -\\tUL BWP ID: This field indicates a UL BWP for which the MAC CE applies as the codepoint of the DCI bandwidth part indicator field as specified in TS 38.212 [9]. The length of the BWP ID field is 2 bits; -\\tFi,j: This field indicates for the TCI state ID fields associated with the codepoint i of the DCI Transmission Configuration Indication field whether the j-th DL TCI state is present or not, where j=1, 2. If Fi,j field is set to 1, it indicates the j-th DL TCI state for codepoint i is present. If Fi,j field is set to 0, it indicates the j-th DL TCI state for codepoint i is absent; -\\tSi,j: This field indicates for the TCI state ID fields associated with the codepoint i of the DCI Transmission Configuration Indication field whether the j-th UL TCI state is present or not, where j=1, 2. If Si,j field is set to 1, it indicates the j-th UL TCI state for codepoint i is present',\n",
       "  'question': 'What is the length of the Serving Cell ID field in the Enhanced Unified TCI States Activation/Deactivation MAC CE?\\n',\n",
       "  'answer': 'The length of the Serving Cell ID field is 5 bits, and it indicates the identity of the Serving Cell for which the MAC CE applies.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38321-i50.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 541}},\n",
       " {'context': '. If used as BWP-level rate matching pattern, the bitmap identifies \"physical resource blocks\" inside the BWP or MBS multicast CFR. The first/ leftmost bit corresponds to resource block 0, and so on (see TS 38.214 [19], clause 5.1.4.1). subcarrierSpacing The SubcarrierSpacing for this resource pattern. If the field is absent, the UE applies the SCS of the associated BWP. The value kHz15 corresponds to =0, the value kHz30 corresponds to =1, and so on. Only the following values are applicable depending on the used frequency (see TS 38.214 [19], clause 5.1.4.1): FR1: 15, 30 or 60 kHz FR2-1/FR2-NTN: 60 or 120 kHz FR2-2: 120, 480, or 960 kHz symbolsInResourceBlock A symbol level bitmap in time domain. It indicates with a bit set to true that the UE shall rate match around the corresponding symbol. This pattern recurs (in time domain) with the configured periodicityAndPattern (see TS 38.214 [19], clause 5.1.4.1). For oneSlot, if ECP is configured, the first 12 bits represent the symbols within the slot and the last two bits within the bitstring are ignored by the UE; Otherwise, the 14 bits represent the symbols within the slot. For twoSlots, if ECP is configured, the first 12 bits represent the symbols within the first slot and the next 12 bits represent the symbols in the second slot and the last four bits within the bit string are ignored by the UE; Otherwise, the first 14 bits represent the symbols within the first slot and the next 14 bits represent the symbols in the second slot. For the bits representing symbols in a slot, the most significant bit of the bit string represents the first symbol in the slot and the second most significant bit represents the second symbol in the slot and so on. Conditional Presence Explanation CellLevel The field is mandatory present if the RateMatchPattern is defined on cell level. The field is absent when the RateMatchPattern is defined on BWP level or defined for MBS broadcast CFR',\n",
       "  'question': 'What does the first bit in the bitmap correspond to in the context of physical resource blocks?\\n',\n",
       "  'answer': 'The first bit in the bitmap corresponds to resource block 0, with each subsequent bit representing the next sequential resource block in the BWP or MBS multicast CFR.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1646}},\n",
       " {'context': 'Tse and Viswanath: Fundamentals of Wireless Communications\\n607 0.3 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.5 0.4 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.2 0.1 0 0 0.1 0.2 0.6 C() (a)  C() (a)  Figure B.4: The capacity of (a) the binary symmetric channel and (b) the binary\\nerasure channel.\\nThe maximization in (B.28) is over all distributions of the input random variable\\nx. Note that the input distribution together with the channel transition probabilities\\nspecifies a joint distribution on x and y. This determines the value of I(x; y). The\\nmaximization is over all possible input distributions. It can be shown that the mutual\\ninformation I(x; y) is a concave function of the input probabilities and hence the input\\nmaximization is a convex optimization problem which can be solved very eciently.\\nSometimes one can even appeal to symmetry to obtain the optimal distribution in\\nclosed form.',\n",
       "  'question': 'What is the nature of the mutual information function I(x; y) concerning the input probabilities?\\n',\n",
       "  'answer': 'The mutual information function I(x; y) is a concave function of the input probabilities.',\n",
       "  'source_doc': {'document': 'documents/Fundamentals of Wireless Communication.pdf',\n",
       "   'page': 608,\n",
       "   'chunk_idx': 0}},\n",
       " {'context': \". When repetitionFactor-v1730 is signalled, the UE shall ignore repetitionFactor-r17. For CLI SRS-RSRP measurement, the network always configures nrofSymbols and repetitionFactor to 'n1'. If srs-PosRRC-InactiveValidityAreaPreConfigList or srs-PosRRC-InactiveValidityAreaNonPreConfig is configured, the value of this field applies to all cells in the validity area. nrofSymbols is same for all the hops when TxHoppingConfig is configured. resourceType Periodicity and offset for semi-persistent and periodic SRS resource, or slot offset for aperiodic SRS resource for positioning (see TS 38.214 [19], clause 6.2.1). For CLI SRS-RSRP measurement, only 'periodic' is applicable for resourceType. If srs-PosRRC-InactiveValidityAreaPreConfigList or srs-PosRRC-InactiveValidityAreaNonPreConfig is configured, the value of this field applies to all cells in the validity area. sequenceId Sequence ID used to initialize pseudo random group and sequence hopping (see TS 38.214 [19], clause 6.2.1). If srs-PosRRC-InactiveValidityAreaPreConfigList or srs-PosRRC-InactiveValidityAreaNonPreConfig is configured, the value of this field applies to all cells in the validity area. slotOffset An offset in number of slots between the triggering DCI and the actual transmission of this SRS-PosResource. If the field is absent the UE applies no offset (value 0). spatialRelationInfo Configuration of the spatial relation between a reference RS and the target SRS. Reference RS can be SSB/CSI-RS/SRS (see TS 38.214 [19], clause 6.2.1). This parameter is not applicable to CLI SRS-RSRP measurement. This field is not configured if unifiedTCI-StateType is configured for the serving cell. spatialRelationInfo-PDC Configuration of the spatial relation between a reference RS and the target SRS. Reference RS can be SSB/CSI-RS/SRS/DL-PRS-PDC (see TS 38.214 [19], clause 6.2.1). The field is present in case of resourceType=periodic and usagePDC-r17=true in the SRS-ResourceSet, otherwise the field is absent\",\n",
       "  'question': 'What should the UE do when repetitionFactor-v1730 is signalled?\\n    ',\n",
       "  'answer': 'The UE should ignore repetitionFactor-r17.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 1791}},\n",
       " {'context': 'message did not include sl-TxResourceReqL2-U2U; or if the information carried by the sl-TxResourceReqL2-U2U has changed since the last transmission of the SidelinkUEInformationNR message; or 3>\\tif configured by upper layers not to transmit either NR sidelink L2 U2N relay communication or NR sidelink L3 U2N relay communication, and if the last transmission of the SidelinkUEInformationNR message includes both sl-TxResourceReqL2U2N-Relay and sl-TxResourceReqL3U2N-Relay: 4>\\tif the UE is capable of U2N Relay UE; or 4>\\tif the UE is selecting a U2N Relay UE / has a selected U2N Relay UE; or 4>\\tif the UE is capable of L2 U2U Relay UE; or 4>\\tif the UE is selecting a L2 U2U Relay UE / has a selected L2 U2U Relay UE: 5>\\tinitiate transmission of the SidelinkUEInformationNR message to indicate the NR sidelink relay communication transmission resources required by the UE in accordance with 5',\n",
       "  'question': 'What triggers the transmission of the SidelinkUEInformationNR message?\\n',\n",
       "  'answer': 'The transmission of the SidelinkUEInformationNR message is initiated when the UE requires NR sidelink relay communication transmission resources, indicating its needs to upper layers.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38331-i51.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 744}},\n",
       " {'context': '.213]), the set of values is determined by cg-StartingFullBW-InsideCOT; -\\totherwise, the set of values is determined by cg-StartingFullBW-OutsideCOT. For operation with shared spectrum channel access, and when the higher layer parameter semiStaticChannelAccessConfigUE is not configured, where a UE is performing uplink transmission with configured grants in contiguous OFDM symbols on fewer than allresource blocks of an RB set, for the first such UL transmission the UE determines a duration of a cyclic prefix extension Text to be applied for transmission according to [4, TS 38.211] according to the following rule: -\\tIf the first such UL transmission is within a channel occupancy initiated by the gNB (defined in Clause 4 of [16, TS 37.213]), the index for [4, TS 38.211] is equal to cg-StartingPartialBW-InsideCOT; -\\totherwise, the index for [4, TS 38.211] is equal to cg-StartingPartialBW-OutsideCOT.',\n",
       "  'question': 'What determines the set of values for operation with shared spectrum channel access?\\n',\n",
       "  'answer': 'The set of values is determined by cg-StartingFullBW-InsideCOT if the first UL transmission is within a channel occupancy initiated by the gNB, otherwise it is determined by cg-StartingFullBW-OutsideCOT.',\n",
       "  'source_doc': {'document': 'documents/3GPP_38/38214-i60.docx',\n",
       "   'page': 1,\n",
       "   'chunk_idx': 417}},\n",
       " {'context': 'b. Binary FSK using coherent detection\\nc. Binary DPSK\\nd. Binary FSK using noncoherent detection Using the formulas derived in Problem 9.1, plot the BER charts for the schemes described therein.\\nSelective Channels Consider a time-selective channel, for which the modulated received signal is defined by\\nwhere m(t) is the message signal, is the result of angle modulation; the amplitude and\\nphase are contributed by the nth path, where n = 1, 2, , N.\\na. Using complex notation, show that the received signal is described as follows:\\nwhere\\nWhat is the formula for\\n? x t n tm t 2fct t n t + +   cos n 1 = N  = t n t n t x t  ts t =  t  n t n 1 = N  = s t',\n",
       "  'question': 'What is the formula for the received signal in a time-selective channel?\\n',\n",
       "  'answer': 'The received signal is described by the formula x(t) = sum from n=1 to N of [tm(t)cos(2*pi*fct*t + theta_n)], where m(t) is the message signal, N is the number of paths, and theta_n represents the phase contribution of the nth path.',\n",
       "  'source_doc': {'document': 'documents/Digital-Communication-Systems.pdf',\n",
       "   'page': 588,\n",
       "   'chunk_idx': 1}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeb07529",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"qa_outputs_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(outputs,f,ensure_ascii=False, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb599f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_llm(context, question, answer, client, model):\n",
    "    critique_prompt = \"\"\"\n",
    "    You will be given a question, answer, and a context.\n",
    "    Your task is to provide a total rating using the additive point scoring system described below.\n",
    "    Points start at 0 and are accumulated based on the satisfaction of each evaluation criterion:\n",
    "\n",
    "    Evaluation Criteria:\n",
    "    - Groundedness: Can the question be answered from the given context? Add 1 point if the question can be answered from the context\n",
    "    - Stand-alone: Is the question understandable free of any context, for someone with domain knowledge/Internet access? Add 1 point if the question is independent and can stand alone.\n",
    "    - Faithfulness: The answer should be grounded in the given context. Add 1 point if the answer can be derived from the context\n",
    "    - Answer Relevance: The generated answer should address the actual question that was provided. Add 1 point if the answer actually answers the question\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Answer:::\n",
    "    Evaluation: (your rationale for the rating, as a text)\n",
    "    Total rating: (your rating, as a number between 0 and 4)\n",
    "\n",
    "    You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "    Now here are the question, answer, and context.\n",
    "\n",
    "    Question: {question}\\n\n",
    "    Answer: {answer}\\n\n",
    "    Context: {context}\\n\n",
    "    Answer::: \"\"\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a neutral judge.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": critique_prompt.format(\n",
    "                    question=question, answer=answer, context=context\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "        model=model,\n",
    "        temperature=0.1,\n",
    "        top_p=0.99,\n",
    "        max_tokens=800\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023e270",
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in tqdm(outputs):\n",
    "    try:\n",
    "        evaluation = judge_llm(\n",
    "            context=output[\"context\"],\n",
    "            question=output[\"question\"],\n",
    "            answer=output[\"answer\"],\n",
    "            client=client,\n",
    "        )\n",
    "        score, eval = (\n",
    "            int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "            evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "        )\n",
    "        output.update(\n",
    "            {\n",
    "                \"score\": score,\n",
    "                \"eval\": eval\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [doc for doc in outputs if doc[\"score\"] >= 4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
